{"2024-10-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.12791v1","updated":"2024-10-16T17:59:52Z","published":"2024-10-16T17:59:52Z","title":"Context is Key(NMF): Modelling Topical Information Dynamics in Chinese\n  Diaspora Media","summary":"  Does the People's Republic of China (PRC) interfere with European elections\nthrough ethnic Chinese diaspora media? This question forms the basis of an\nongoing research project exploring how PRC narratives about European elections\nare represented in Chinese diaspora media, and thus the objectives of PRC news\nmedia manipulation. In order to study diaspora media efficiently and at scale,\nit is necessary to use techniques derived from quantitative text analysis, such\nas topic modelling. In this paper, we present a pipeline for studying\ninformation dynamics in Chinese media. Firstly, we present KeyNMF, a new\napproach to static and dynamic topic modelling using transformer-based\ncontextual embedding models. We provide benchmark evaluations to demonstrate\nthat our approach is competitive on a number of Chinese datasets and metrics.\nSecondly, we integrate KeyNMF with existing methods for describing information\ndynamics in complex systems. We apply this pipeline to data from five news\nsites, focusing on the period of time leading up to the 2024 European\nparliamentary elections. Our methods and results demonstrate the effectiveness\nof KeyNMF for studying information dynamics in Chinese media and lay groundwork\nfor further work addressing the broader research questions.\n","authors":["Ross Deans Kristensen-McLachlan","Rebecca M. M. Hicke","Márton Kardos","Mette Thunø"],"pdf_url":"https://arxiv.org/pdf/2410.12791v1.pdf","comment":"Accepted to the 2024 Computational Humanities Research Conference\n  (CHR)"},{"id":"http://arxiv.org/abs/2410.12788v1","updated":"2024-10-16T17:59:32Z","published":"2024-10-16T17:59:32Z","title":"Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception","summary":"  Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed two strategies based on LLMs: Margin\nSampling Chunking and Perplexity Chunking. The former employs LLMs to perform\nbinary classification on whether consecutive sentences need to be segmented,\nmaking decisions based on the probability difference obtained from margin\nsampling. The latter precisely identifies text chunk boundaries by analyzing\nthe characteristics of perplexity distribution. Additionally, considering the\ninherent complexity of different texts, we propose a strategy that combines\nMeta-Chunking with dynamic merging to achieve a balance between fine-grained\nand coarse-grained text chunking. Experiments conducted on eleven datasets\ndemonstrate that Meta-Chunking can more efficiently improve the performance of\nsingle-hop and multi-hop question answering based on RAG. For instance, on the\n2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only\nconsuming 45.8% of the time. Our code is available at\nhttps://github.com/IAAR-Shanghai/Meta-Chunking.\n","authors":["Jihao Zhao","Zhiyuan Ji","Pengnian Qi","Simin Niu","Bo Tang","Feiyu Xiong","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.12788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12784v1","updated":"2024-10-16T17:58:19Z","published":"2024-10-16T17:58:19Z","title":"JudgeBench: A Benchmark for Evaluating LLM-based Judges","summary":"  LLM-based judges have emerged as a scalable alternative to human evaluation\nand are increasingly used to assess, compare, and improve models. However, the\nreliability of LLM-based judges themselves is rarely scrutinized. As LLMs\nbecome more advanced, their responses grow more sophisticated, requiring\nstronger judges to evaluate them. Existing benchmarks primarily focus on a\njudge's alignment with human preferences, but often fail to account for more\nchallenging tasks where crowdsourced human preference is a poor indicator of\nfactual and logical correctness. To address this, we propose a novel evaluation\nframework to objectively evaluate LLM-based judges. Based on this framework, we\npropose JudgeBench, a benchmark for evaluating LLM-based judges on challenging\nresponse pairs spanning knowledge, reasoning, math, and coding. JudgeBench\nleverages a novel pipeline for converting existing difficult datasets into\nchallenging response pairs with preference labels reflecting objective\ncorrectness. Our comprehensive evaluation on a collection of prompted judges,\nfine-tuned judges, multi-agent judges, and reward models shows that JudgeBench\nposes a significantly greater challenge than previous benchmarks, with many\nstrong models (e.g., GPT-4o) performing just slightly better than random\nguessing. Overall, JudgeBench offers a reliable platform for assessing\nincreasingly advanced LLM-based judges. Data and code are available at\nhttps://github.com/ScalerLab/JudgeBench .\n","authors":["Sijun Tan","Siyuan Zhuang","Kyle Montgomery","William Y. Tang","Alejandro Cuadron","Chenguang Wang","Raluca Ada Popa","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.12784v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.12782v1","updated":"2024-10-16T17:56:49Z","published":"2024-10-16T17:56:49Z","title":"In-Context Learning Enables Robot Action Prediction in LLMs","summary":"  Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings.\n","authors":["Yida Yin","Zekai Wang","Yuvan Sharma","Dantong Niu","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2410.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12777v1","updated":"2024-10-16T17:51:25Z","published":"2024-10-16T17:51:25Z","title":"Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned\n  Concepts","summary":"  With the rapid progress of diffusion-based content generation, significant\nefforts are being made to unlearn harmful or copyrighted concepts from\npretrained diffusion models (DMs) to prevent potential model misuse. However,\nit is observed that even when DMs are properly unlearned before release,\nmalicious finetuning can compromise this process, causing DMs to relearn the\nunlearned concepts. This occurs partly because certain benign concepts (e.g.,\n\"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"),\nfacilitating their relearning via finetuning. To address this, we propose\nmeta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an\nunlearned DM when used as is; moreover, if the meta-unlearned DM undergoes\nmalicious finetuning on unlearned concepts, the related benign concepts\nretained within it will be triggered to self-destruct, hindering the relearning\nof unlearned concepts. Our meta-unlearning framework is compatible with most\nexisting unlearning methods, requiring only the addition of an\neasy-to-implement meta objective. We validate our approach through empirical\nexperiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4\nand SDXL), supported by extensive ablation studies. Our code is available at\nhttps://github.com/sail-sg/Meta-Unlearning.\n","authors":["Hongcheng Gao","Tianyu Pang","Chao Du","Taihang Hu","Zhijie Deng","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.12777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12774v1","updated":"2024-10-16T17:49:45Z","published":"2024-10-16T17:49:45Z","title":"Identifying Task Groupings for Multi-Task Learning Using Pointwise\n  V-Usable Information","summary":"  The success of multi-task learning can depend heavily on which tasks are\ngrouped together. Naively grouping all tasks or a random set of tasks can\nresult in negative transfer, with the multi-task models performing worse than\nsingle-task models. Though many efforts have been made to identify task\ngroupings and to measure the relatedness among different tasks, it remains a\nchallenging research topic to define a metric to identify the best task\ngrouping out of a pool of many potential task combinations. We propose a metric\nof task relatedness based on task difficulty measured by pointwise V-usable\ninformation (PVI). PVI is a recently proposed metric to estimate how much\nusable information a dataset contains given a model. We hypothesize that tasks\nwith not statistically different PVI estimates are similar enough to benefit\nfrom the joint learning process. We conduct comprehensive experiments to\nevaluate the feasibility of this metric for task grouping on 15 NLP datasets in\nthe general, biomedical, and clinical domains. We compare the results of the\njoint learners against single learners, existing baseline methods, and recent\nlarge language models, including Llama 2 and GPT-4. The results show that by\ngrouping tasks with similar PVI estimates, the joint learners yielded\ncompetitive results with fewer total parameters, with consistent performance\nacross domains.\n","authors":["Yingya Li","Timothy Miller","Steven Bethard","Guergana Savova"],"pdf_url":"https://arxiv.org/pdf/2410.12774v1.pdf","comment":"main paper 12 pages, Appendix 7 pages, 1 figure, 18 tables"},{"id":"http://arxiv.org/abs/2404.12494v2","updated":"2024-10-16T17:45:10Z","published":"2024-04-18T20:17:23Z","title":"BIRD: A Trustworthy Bayesian Inference Framework for Large Language\n  Models","summary":"  Predictive models often need to work with incomplete information in\nreal-world tasks. Consequently, they must provide reliable probability or\nconfidence estimation, especially in large-scale decision making and planning\ntasks. Current large language models (LLM) are insufficient for such accurate\nestimations, but they can generate relevant factors that may affect the\nprobabilities, produce coarse-grained probabilities when the information is\nmore complete, and help determine which factors are relevant to specific\ndownstream contexts. In this paper, we make use of these capabilities of LLMs\nto provide a significantly more accurate probabilistic estimation. We propose\nBIRD, a novel probabilistic inference framework that aligns a Bayesian network\nwith LLM abductions and then estimates more accurate probabilities in a\ndeduction step. We show BIRD provides reliable probability estimations that are\n30\\% better than those provided directly by LLM baselines. These estimates can\nfurther contribute to better and more trustworthy decision-making.\n","authors":["Yu Feng","Ben Zhou","Weidong Lin","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2404.12494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12759v1","updated":"2024-10-16T17:30:58Z","published":"2024-10-16T17:30:58Z","title":"Unitary Multi-Margin BERT for Robust Natural Language Processing","summary":"  Recent developments in adversarial attacks on deep learning leave many\nmission-critical natural language processing (NLP) systems at risk of\nexploitation. To address the lack of computationally efficient adversarial\ndefense methods, this paper reports a novel, universal technique that\ndrastically improves the robustness of Bidirectional Encoder Representations\nfrom Transformers (BERT) by combining the unitary weights with the multi-margin\nloss. We discover that the marriage of these two simple ideas amplifies the\nprotection against malicious interference. Our model, the unitary multi-margin\nBERT (UniBERT), boosts post-attack classification accuracies significantly by\n5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,\nthe pre-attack and post-attack accuracy tradeoff can be adjusted via a single\nscalar parameter to best fit the design requirements for the target\napplications.\n","authors":["Hao-Yuan Chang","Kang L. Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12757v1","updated":"2024-10-16T17:25:25Z","published":"2024-10-16T17:25:25Z","title":"StyleDistance: Stronger Content-Independent Style Embeddings with\n  Synthetic Parallel Examples","summary":"  Style representations aim to embed texts with similar writing styles closely\nand texts with different styles far apart, regardless of content. However, the\ncontrastive triplets often used for training these representations may vary in\nboth style and content, leading to potential content leakage in the\nrepresentations. We introduce StyleDistance, a novel approach to training\nstronger content-independent style embeddings. We use a large language model to\ncreate a synthetic dataset of near-exact paraphrases with controlled style\nvariations, and produce positive and negative examples across 40 distinct style\nfeatures for precise contrastive learning. We assess the quality of our\nsynthetic data and embeddings through human and automatic evaluations.\nStyleDistance enhances the content-independence of style embeddings, which\ngeneralize to real-world benchmarks and outperform leading style\nrepresentations in downstream applications. Our model can be found at\nhttps://huggingface.co/StyleDistance/styledistance .\n","authors":["Ajay Patel","Jiacheng Zhu","Justin Qiu","Zachary Horvitz","Marianna Apidianaki","Kathleen McKeown","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2410.12757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10267v2","updated":"2024-10-16T17:22:54Z","published":"2023-11-17T01:27:01Z","title":"Energy and Carbon Considerations of Fine-Tuning BERT","summary":"  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP\ncommunity, existing work quantifying energy costs and associated carbon\nemissions has largely focused on language model pre-training. Although a single\npre-training run draws substantially more energy than fine-tuning, fine-tuning\nis performed more frequently by many more individual actors, and thus must be\naccounted for when considering the energy and carbon footprint of NLP. In order\nto better characterize the role of fine-tuning in the landscape of energy and\ncarbon emissions in NLP, we perform a careful empirical study of the\ncomputational costs of fine-tuning across tasks, datasets, hardware\ninfrastructure and measurement modalities. Our experimental results allow us to\nplace fine-tuning energy and carbon costs into perspective with respect to\npre-training and inference, and outline recommendations to NLP researchers and\npractitioners who wish to improve their fine-tuning energy efficiency.\n","authors":["Xiaorong Wang","Clara Na","Emma Strubell","Sorelle Friedler","Sasha Luccioni"],"pdf_url":"https://arxiv.org/pdf/2311.10267v2.pdf","comment":"EMNLP 2023 Findings; First two authors contributed equally; 12 pages"},{"id":"http://arxiv.org/abs/2410.12750v1","updated":"2024-10-16T17:12:06Z","published":"2024-10-16T17:12:06Z","title":"Comparative Analysis of Extrinsic Factors for NER in French","summary":"  Named entity recognition (NER) is a crucial task that aims to identify\nstructured information, which is often replete with complex, technical terms\nand a high degree of variability. Accurate and reliable NER can facilitate the\nextraction and analysis of important information. However, NER for other than\nEnglish is challenging due to limited data availability, as the high expertise,\ntime, and expenses are required to annotate its data. In this paper, by using\nthe limited data, we explore various factors including model structure, corpus\nannotation scheme and data augmentation techniques to improve the performance\nof a NER model for French. Our experiments demonstrate that these approaches\ncan significantly improve the model's F1 score from original CRF score of 62.41\nto 79.39. Our findings suggest that considering different extrinsic factors and\ncombining these techniques is a promising approach for improving NER\nperformance where the size of data is limited.\n","authors":["Grace Yang","Zhiyi Li","Yandong Liu","Jungyeul Park"],"pdf_url":"https://arxiv.org/pdf/2410.12750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14391v3","updated":"2024-10-16T17:01:16Z","published":"2023-11-24T10:15:34Z","title":"ÚFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual\n  Coreference Resolution","summary":"  We present CorPipe, the winning entry to the CRAC 2023 Shared Task on\nMultilingual Coreference Resolution. Our system is an improved version of our\nearlier multilingual coreference pipeline, and it surpasses other participants\nby a large margin of 4.5 percent points. CorPipe first performs mention\ndetection, followed by coreference linking via an antecedent-maximization\napproach on the retrieved spans. Both tasks are trained jointly on all\navailable corpora using a shared pretrained language model. Our main\nimprovements comprise inputs larger than 512 subwords and changing the mention\ndecoding to support ensembling. The source code is available at\nhttps://github.com/ufal/crac2023-corpipe.\n","authors":["Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2311.14391v3.pdf","comment":"Accepted to CRAC 2023 (the Sixth Workshop on Computational Models of\n  Reference, Anaphora and Coreference)"},{"id":"http://arxiv.org/abs/2209.07278v3","updated":"2024-10-16T16:56:17Z","published":"2022-09-15T13:11:39Z","title":"ÚFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for\n  Coreference Resolution","summary":"  We describe the winning submission to the CRAC 2022 Shared Task on\nMultilingual Coreference Resolution. Our system first solves mention detection\nand then coreference linking on the retrieved spans with an\nantecedent-maximization approach, and both tasks are fine-tuned jointly with\nshared Transformer weights. We report results of fine-tuning a wide range of\npretrained models. The center of this contribution are fine-tuned multilingual\nmodels. We found one large multilingual model with sufficiently large encoder\nto increase performance on all datasets across the board, with the benefit not\nlimited only to the underrepresented languages or groups of typologically\nrelative languages. The source code is available at\nhttps://github.com/ufal/crac2022-corpipe.\n","authors":["Milan Straka","Jana Straková"],"pdf_url":"https://arxiv.org/pdf/2209.07278v3.pdf","comment":"Accepted to CRAC 2022 (Fifth Workshop on Computational Models of\n  Reference, Anaphora and Coreference)"},{"id":"http://arxiv.org/abs/2410.12735v1","updated":"2024-10-16T16:51:01Z","published":"2024-10-16T16:51:01Z","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","summary":"  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe rewarding consistency across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.12735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12722v1","updated":"2024-10-16T16:31:24Z","published":"2024-10-16T16:31:24Z","title":"WorldMedQA-V: a multilingual, multimodal medical examination dataset for\n  multimodal language models evaluation","summary":"  Multimodal/vision language models (VLMs) are increasingly being deployed in\nhealthcare settings worldwide, necessitating robust benchmarks to ensure their\nsafety, efficacy, and fairness. Multiple-choice question and answer (QA)\ndatasets derived from national medical examinations have long served as\nvaluable evaluation tools, but existing datasets are largely text-only and\navailable in a limited subset of languages and countries. To address these\nchallenges, we present WorldMedQA-V, an updated multilingual, multimodal\nbenchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V\nincludes 568 labeled multiple-choice QAs paired with 568 medical images from\nfour countries (Brazil, Israel, Japan, and Spain), covering original languages\nand validated English translations by native clinicians, respectively. Baseline\nperformance for common open- and closed-source models are provided in the local\nlanguage and English translations, and with and without images provided to the\nmodel. The WorldMedQA-V benchmark aims to better match AI systems to the\ndiverse healthcare environments in which they are deployed, fostering more\nequitable, effective, and representative applications.\n","authors":["João Matos","Shan Chen","Siena Placino","Yingya Li","Juan Carlos Climent Pardo","Daphna Idan","Takeshi Tohyama","David Restrepo","Luis F. Nakayama","Jose M. M. Pascual-Leone","Guergana Savova","Hugo Aerts","Leo A. Celi","A. Ian Wong","Danielle S. Bitterman","Jack Gallifant"],"pdf_url":"https://arxiv.org/pdf/2410.12722v1.pdf","comment":"submitted for review, total of 14 pages"},{"id":"http://arxiv.org/abs/2407.00463v5","updated":"2024-10-16T16:13:32Z","published":"2024-06-29T15:20:11Z","title":"Open-Source Conversational AI with SpeechBrain 1.0","summary":"  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.\n","authors":["Mirco Ravanelli","Titouan Parcollet","Adel Moumen","Sylvain de Langen","Cem Subakan","Peter Plantinga","Yingzhi Wang","Pooneh Mousavi","Luca Della Libera","Artem Ploujnikov","Francesco Paissan","Davide Borra","Salah Zaiem","Zeyu Zhao","Shucong Zhang","Georgios Karakasidis","Sung-Lin Yeh","Pierre Champion","Aku Rouhe","Rudolf Braun","Florian Mai","Juan Zuluaga-Gomez","Seyed Mahed Mousavi","Andreas Nautsch","Xuechen Liu","Sangeet Sagar","Jarod Duret","Salima Mdhaffar","Gaelle Laperriere","Mickael Rouvier","Renato De Mori","Yannick Esteve"],"pdf_url":"https://arxiv.org/pdf/2407.00463v5.pdf","comment":"Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software"},{"id":"http://arxiv.org/abs/2410.12705v1","updated":"2024-10-16T16:11:49Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12704v1","updated":"2024-10-16T16:10:59Z","published":"2024-10-16T16:10:59Z","title":"Sarcasm Detection in a Less-Resourced Language","summary":"  The sarcasm detection task in natural language processing tries to classify\nwhether an utterance is sarcastic or not. It is related to sentiment analysis\nsince it often inverts surface sentiment. Because sarcastic sentences are\nhighly dependent on context, and they are often accompanied by various\nnon-verbal cues, the task is challenging. Most of related work focuses on\nhigh-resourced languages like English. To build a sarcasm detection dataset for\na less-resourced language, such as Slovenian, we leverage two modern\ntechniques: a machine translation specific medium-size transformer model, and a\nvery large generative language model. We explore the viability of translated\ndatasets and how the size of a pretrained transformer affects its ability to\ndetect sarcasm. We train ensembles of detection models and evaluate models'\nperformance. The results show that larger models generally outperform smaller\nones and that ensembling can slightly improve sarcasm detection performance.\nOur best ensemble approach achieves an $\\text{F}_1$-score of 0.765 which is\nclose to annotators' agreement in the source language.\n","authors":["Lazar Đoković","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2410.12704v1.pdf","comment":"4 pages, published in the Slovenian Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2404.15219v2","updated":"2024-10-16T16:01:59Z","published":"2024-04-23T16:51:26Z","title":"Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel","summary":"  Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline.\n","authors":["Brendan King","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2404.15219v2.pdf","comment":"To be presented at Empirical Methods in Natural Language Processing\n  (EMNLP 2024). 18 Pages, 8 Figures"},{"id":"http://arxiv.org/abs/2410.12694v1","updated":"2024-10-16T15:54:11Z","published":"2024-10-16T15:54:11Z","title":"VividMed: Vision Language Model with Versatile Visual Grounding for\n  Medicine","summary":"  Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.\n","authors":["Lingxiao Luo","Bingda Tang","Xuanzhong Chen","Rong Han","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12691v1","updated":"2024-10-16T15:51:18Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01474v2","updated":"2024-10-16T15:45:35Z","published":"2024-05-02T17:07:25Z","title":"Understanding Figurative Meaning through Explainable Visual Entailment","summary":"  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of these models' capabilities when presented with\nimages and captions containing figurative meaning, such as metaphors or humor.\nTo close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present either in the image, the caption, or both. Utilizing a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning via human evaluation.\n","authors":["Arkadiy Saakyan","Shreyas Kulkarni","Tuhin Chakrabarty","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2405.01474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11167v2","updated":"2024-10-16T15:40:51Z","published":"2024-02-17T02:25:57Z","title":"ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack\n  AI-Generated Text Detection","summary":"  The robustness of AI-content detection models against sophisticated\nadversarial strategies, such as paraphrasing or word switching, is a rising\nconcern in natural language generation (NLG) applications. This study proposes\nToBlend, a novel token-level ensemble text generation method to challenge the\nrobustness of current AI-content detection approaches by utilizing multiple\nsets of candidate generative large language models (LLMs). By randomly sampling\ntoken(s) from candidate LLMs sets, we find ToBlend significantly drops the\nperformance of most mainstream AI-content detection methods. We evaluate the\ntext quality produced under different ToBlend settings based on annotations\nfrom experienced human experts. We proposed a fine-tuned Llama3.1 model to\ndistinguish the ToBlend generated text more accurately. Our findings underscore\nour proposed text generation approach's great potential in deceiving and\nimproving detection models. Our datasets, codes, and annotations are\nopen-sourced.\n","authors":["Fan Huang","Haewoon Kwak","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2402.11167v2.pdf","comment":"Submitted to ARR Oct-2024 Cycle"},{"id":"http://arxiv.org/abs/2402.07204v4","updated":"2024-10-16T15:28:18Z","published":"2024-02-11T13:30:53Z","title":"ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning","summary":"  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions. Source codes of\nITINERA are available at https://github.com/YihongT/ITINERA.\n","authors":["Yihong Tang","Zhaokai Wang","Ao Qu","Yihao Yan","Zhaofeng Wu","Dingyi Zhuang","Jushi Kai","Kebing Hou","Xiaotong Guo","Jinhua Zhao","Zhan Zhao","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07204v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12662v1","updated":"2024-10-16T15:20:08Z","published":"2024-10-16T15:20:08Z","title":"Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models","summary":"  Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).\n","authors":["Shicheng Xu","Liang Pang","Yunchang Zhu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12656v1","updated":"2024-10-16T15:17:20Z","published":"2024-10-16T15:17:20Z","title":"Evaluating Morphological Compositional Generalization in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.\n","authors":["Mete Ismayilzada","Defne Circi","Jonne Sälevä","Hale Sirin","Abdullatif Köksal","Bhuwan Dhingra","Antoine Bosselut","Lonneke van der Plas","Duygu Ataman"],"pdf_url":"https://arxiv.org/pdf/2410.12656v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2406.11785v2","updated":"2024-10-16T15:15:44Z","published":"2024-06-17T17:39:10Z","title":"CELL your Model: Contrastive Explanations for Large Language Models","summary":"  The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing, to the best of our\nknowledge, the first contrastive explanation methods requiring simply\nblack-box/query access. Our explanations suggest that an LLM outputs a reply to\na given prompt because if the prompt was slightly modified, the LLM would have\ngiven a different response that is either less preferable or contradicts the\noriginal response. The key insight is that contrastive explanations simply\nrequire a scoring function that has meaning to the user and not necessarily a\nspecific real valued quantity (viz. class label). We offer two algorithms for\nfinding contrastive explanations: i) A myopic algorithm, which although\neffective in creating contrasts, requires many model calls and ii) A budgeted\nalgorithm, our main algorithmic contribution, which intelligently creates\ncontrasts adhering to a query budget, necessary for longer contexts. We show\nthe efficacy of these methods on diverse natural language tasks such as\nopen-text generation, automated red teaming, and explaining conversational\ndegradation.\n","authors":["Ronny Luss","Erik Miehling","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.11785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20366v4","updated":"2024-10-16T15:09:14Z","published":"2024-09-30T15:04:17Z","title":"Disentangling Singlish Discourse Particles with Task-Driven\n  Representation","summary":"  Singlish, or formally Colloquial Singapore English, is an English-based\ncreole language originating from the SouthEast Asian country Singapore. The\nlanguage contains influences from Sinitic languages such as Chinese dialects,\nMalay, Tamil and so forth. A fundamental task to understanding Singlish is to\nfirst understand the pragmatic functions of its discourse particles, upon which\nSinglish relies heavily to convey meaning. This work offers a preliminary\neffort to disentangle the Singlish discourse particles (lah, meh and hor) with\ntask-driven representation learning. After disentanglement, we cluster these\ndiscourse particles to differentiate their pragmatic functions, and perform\nSinglish-to-English machine translation. Our work provides a computational\nmethod to understanding Singlish discourse particles, and opens avenues towards\na deeper comprehension of the language and its usage.\n","authors":["Linus Tze En Foo","Lynnette Hui Xian Ng"],"pdf_url":"https://arxiv.org/pdf/2409.20366v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13745v4","updated":"2024-10-16T15:07:41Z","published":"2024-08-25T07:10:36Z","title":"DOCE: Finding the Sweet Spot for Execution-Based Code Generation","summary":"  Recently, a diverse set of decoding and reranking procedures have been shown\neffective for LLM-based code generation. However, a comprehensive framework\nthat links and experimentally compares these methods is missing. We address\nthis by proposing Decoding Objectives for Code Execution, a comprehensive\nframework that includes candidate generation, $n$-best reranking, minimum Bayes\nrisk (MBR) decoding, and self-debugging as the core components. We then study\nthe contributions of these components through execution-based evaluation\nmetrics. Our findings highlight the importance of execution-based methods and\nthe difference gap between execution-based and execution-free methods.\nFurthermore, we assess the impact of filtering based on trial unit tests, a\nsimple and effective strategy that has been often overlooked in prior works. We\nalso propose self-debugging on multiple candidates, obtaining state-of-the-art\nperformance on reranking for code generation. We expect our framework to\nprovide a solid guideline for future research on code generation.\n","authors":["Haau-Sing Li","Patrick Fernandes","Iryna Gurevych","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2408.13745v4.pdf","comment":"10 pages (32 including appendix), 5 figures, 25 tables. Prompts are\n  provided in the GitHub repository to avoid potential text overlap with other\n  papers"},{"id":"http://arxiv.org/abs/2409.13832v3","updated":"2024-10-16T14:56:59Z","published":"2024-09-20T18:18:14Z","title":"GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music\n  Scores for All Singing Tasks","summary":"  The scarcity of high-quality and multi-task singing datasets significantly\nhinders the development of diverse controllable and personalized singing tasks,\nas existing singing datasets suffer from low quality, limited diversity of\nlanguages and singers, absence of multi-technique information and realistic\nmusic scores, and poor task suitability. To tackle these problems, we present\nGTSinger, a large global, multi-technique, free-to-use, high-quality singing\ncorpus with realistic music scores, designed for all singing tasks, along with\nits benchmarks. Particularly, (1) we collect 80.59 hours of high-quality\nsinging voices, forming the largest recorded singing dataset; (2) 20\nprofessional singers across nine widely spoken languages offer diverse timbres\nand styles; (3) we provide controlled comparison and phoneme-level annotations\nof six commonly used singing techniques, helping technique modeling and\ncontrol; (4) GTSinger offers realistic music scores, assisting real-world\nmusical composition; (5) singing voices are accompanied by manual\nphoneme-to-audio alignments, global style labels, and 16.16 hours of paired\nspeech for various singing tasks. Moreover, to facilitate the use of GTSinger,\nwe conduct four benchmark experiments: technique-controllable singing voice\nsynthesis, technique recognition, style transfer, and speech-to-singing\nconversion. The corpus and demos can be found at http://gtsinger.github.io. We\nprovide the dataset and the code for processing data and conducting benchmarks\nat https://huggingface.co/datasets/GTSinger/GTSinger and\nhttps://github.com/GTSinger/GTSinger.\n","authors":["Yu Zhang","Changhao Pan","Wenxiang Guo","Ruiqi Li","Zhiyuan Zhu","Jialei Wang","Wenhao Xu","Jingyu Lu","Zhiqing Hong","Chuxin Wang","LiChao Zhang","Jinzheng He","Ziyue Jiang","Yuxin Chen","Chen Yang","Jiecheng Zhou","Xinyu Cheng","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.13832v3.pdf","comment":"Accepted by NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2409.15360v3","updated":"2024-10-16T14:56:15Z","published":"2024-09-18T02:35:41Z","title":"Reward-Robust RLHF in LLMs","summary":"  As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.\n","authors":["Yuzi Yan","Xingzhou Lou","Jialian Li","Yiping Zhang","Jian Xie","Chao Yu","Yu Wang","Dong Yan","Yuan Shen"],"pdf_url":"https://arxiv.org/pdf/2409.15360v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13300v2","updated":"2024-10-16T14:52:16Z","published":"2024-07-18T09:05:49Z","title":"Robust ASR Error Correction with Conservative Data Filtering","summary":"  Error correction (EC) based on large language models is an emerging\ntechnology to enhance the performance of automatic speech recognition (ASR)\nsystems. Generally, training data for EC are collected by automatically pairing\na large set of ASR hypotheses (as sources) and their gold references (as\ntargets). However, the quality of such pairs is not guaranteed, and we observed\nvarious types of noise which can make the EC models brittle, e.g. inducing\novercorrection in out-of-domain (OOD) settings. In this work, we propose two\nfundamental criteria that EC training data should satisfy: namely, EC targets\nshould (1) improve linguistic acceptability over sources and (2) be inferable\nfrom the available context (e.g. source phonemes). Through these criteria, we\nidentify low-quality EC pairs and train the models not to make any correction\nin such cases, the process we refer to as conservative data filtering. In our\nexperiments, we focus on Japanese ASR using a strong Conformer-CTC as the\nbaseline and finetune Japanese LLMs for EC. Through our evaluation on a suite\nof 21 internal benchmarks, we demonstrate that our approach can significantly\nreduce overcorrection and improve both the accuracy and quality of ASR results\nin the challenging OOD settings.\n","authors":["Takuma Udagawa","Masayuki Suzuki","Masayasu Muraoka","Gakuto Kurata"],"pdf_url":"https://arxiv.org/pdf/2407.13300v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2406.09988v2","updated":"2024-10-16T14:48:38Z","published":"2024-06-14T12:52:42Z","title":"Details Make a Difference: Object State-Sensitive Neurorobotic Task\n  Planning","summary":"  The state of an object reflects its current status or condition and is\nimportant for a robot's task planning and manipulation. However, detecting an\nobject's state and generating a state-sensitive plan for robots is challenging.\nRecently, pre-trained Large Language Models (LLMs) and Vision-Language Models\n(VLMs) have shown impressive capabilities in generating plans. However, to the\nbest of our knowledge, there is hardly any investigation on whether LLMs or\nVLMs can also generate object state-sensitive plans. To study this, we\nintroduce an Object State-Sensitive Agent (OSSA), a task-planning agent\nempowered by pre-trained neural networks. We propose two methods for OSSA: (i)\na modular model consisting of a pre-trained vision processing module (dense\ncaptioning model, DCM) and a natural language processing model (LLM), and (ii)\na monolithic model consisting only of a VLM. To quantitatively evaluate the\nperformances of the two methods, we use tabletop scenarios where the task is to\nclear the table. We contribute a multimodal benchmark dataset that takes object\nstates into consideration. Our results show that both methods can be used for\nobject state-sensitive tasks, but the monolithic approach outperforms the\nmodular approach. The code for OSSA is available at\nhttps://github.com/Xiao-wen-Sun/OSSA\n","authors":["Xiaowen Sun","Xufeng Zhao","Jae Hee Lee","Wenhao Lu","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2406.09988v2.pdf","comment":"ICANN24, Switzerland"},{"id":"http://arxiv.org/abs/2410.12622v1","updated":"2024-10-16T14:42:23Z","published":"2024-10-16T14:42:23Z","title":"From Measurement Instruments to Training Data: Leveraging Theory-Driven\n  Synthetic Training Data for Measuring Social Constructs","summary":"  Computational text classification is a challenging task, especially for\nmulti-dimensional social constructs. Recently, there has been increasing\ndiscussion that synthetic training data could enhance classification by\noffering examples of how these constructs are represented in texts. In this\npaper, we systematically examine the potential of theory-driven synthetic\ntraining data for improving the measurement of social constructs. In\nparticular, we explore how researchers can transfer established knowledge from\nmeasurement instruments in the social sciences, such as survey scales or\nannotation codebooks, into theory-driven generation of synthetic data. Using\ntwo studies on measuring sexism and political topics, we assess the added value\nof synthetic training data for fine-tuning text classification models. Although\nthe results of the sexism study were less promising, our findings demonstrate\nthat synthetic data can be highly effective in reducing the need for labeled\ndata in political topic classification. With only a minimal drop in\nperformance, synthetic data allows for substituting large amounts of labeled\ndata. Furthermore, theory-driven synthetic data performed markedly better than\ndata generated without conceptual information in mind.\n","authors":["Lukas Birkenmaier","Matthias Roth","Indira Sen"],"pdf_url":"https://arxiv.org/pdf/2410.12622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12621v1","updated":"2024-10-16T14:40:32Z","published":"2024-10-16T14:40:32Z","title":"Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,\n  Toxicity, and Legal Reasoning","summary":"  As large language models (LLMs) continue to advance, ensuring their alignment\nwith human values becomes increasingly critical. Traditional alignment methods\nheavily rely on human feedback to fine-tune models. With the emergence of\nsuperhuman models whose outputs may surpass human understanding, evaluating and\naligning these models using human judgments poses significant challenges. To\naddress the challenges, recent works use weak supervisors to elicit knowledge\nfrom much stronger models. However, there are important disanalogies between\nthe empirical setup in the existing works and the genuine goal of alignment. We\nremark that existing works investigate the phenomenon of weak-to-strong\ngeneration in analogous setup (i.e., binary classification), rather than\npractical alignment-relevant tasks (e.g., safety). In this paper, we bridge\nthis gap by extending weak-to-strong generation to the context of practical\nalignment. We empirically demonstrate the widespread phenomenon of\nweak-to-strong generation in three complicated alignment tasks: safety,\ntoxicity, and legal reasoning}. Furthermore, we explore efficient strategies\nfor improving alignment performance to enhance the quality of model outcomes.\nLastly, we summarize and analyze the challenges and potential solutions in\nregard to specific alignment tasks, which we hope to catalyze the research\nprogress on the topic of weak-to-strong generalization. Our code is released at\nhttps://github.com/yeruimeng/WTS.git.\n","authors":["Ruimeng Ye","Yang Xiao","Bo Hui"],"pdf_url":"https://arxiv.org/pdf/2410.12621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12617v1","updated":"2024-10-16T14:34:30Z","published":"2024-10-16T14:34:30Z","title":"Parsing Akkadian Verbs with Prolog","summary":"  This paper describes a parsing/generation system for finite verbal forms in\nAkkadian, with the possible addition of suffixes, implemented in Prolog. The\nwork described provides the framework and engine to interpret the D, N, and G\nstems along with accusative, dative and ventive endings.\n","authors":["Aaron Macks"],"pdf_url":"https://arxiv.org/pdf/2410.12617v1.pdf","comment":"6 pages, 9 figures, presented at ACL-02 the Association of\n  Computational Linguistics, 2002"},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2305.14463v4","updated":"2024-10-16T14:27:49Z","published":"2023-05-23T18:37:30Z","title":"ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain\n  Readability Assessment","summary":"  We present a comprehensive evaluation of large language models for\nmultilingual readability assessment. Existing evaluation resources lack domain\nand language diversity, limiting the ability for cross-domain and cross-lingual\nanalyses. This paper introduces ReadMe++, a multilingual multi-domain dataset\nwith human annotations of 9757 sentences in Arabic, English, French, Hindi, and\nRussian, collected from 112 different data sources. This benchmark will\nencourage research on developing robust multilingual readability assessment\nmethods. Using ReadMe++, we benchmark multilingual and monolingual language\nmodels in the supervised, unsupervised, and few-shot prompting settings. The\ndomain and language diversity in ReadMe++ enable us to test more effective\nfew-shot prompting, and identify shortcomings in state-of-the-art unsupervised\nmethods. Our experiments also reveal exciting results of superior domain\ngeneralization and enhanced cross-lingual transfer capabilities by models\ntrained on ReadMe++. We will make our data publicly available and release a\npython package tool for multilingual sentence readability prediction using our\ntrained models at: https://github.com/tareknaous/readme\n","authors":["Tarek Naous","Michael J. Ryan","Anton Lavrouk","Mohit Chandra","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14463v4.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12608v1","updated":"2024-10-16T14:24:55Z","published":"2024-10-16T14:24:55Z","title":"Not All Votes Count! Programs as Verifiers Improve Self-Consistency of\n  Language Models for Math Reasoning","summary":"  Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove.\n","authors":["Vernon Y. H. Toh","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2410.12608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12601v1","updated":"2024-10-16T14:21:52Z","published":"2024-10-16T14:21:52Z","title":"CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization","summary":"  To broaden the dissemination of scientific knowledge to diverse audiences,\nscientific document summarization must simultaneously control multiple\nattributes such as length and empirical focus. However, existing research\ntypically focuses on controlling single attributes, leaving the compositional\ncontrol of multiple attributes underexplored. To address this gap, we introduce\nCCSBench, a benchmark for compositional controllable summarization in the\nscientific domain. Our benchmark enables fine-grained control over both\nexplicit attributes (e.g., length), which are objective and straightforward,\nand implicit attributes (e.g., empirical focus), which are more subjective and\nconceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our findings reveal significant\nlimitations in large language models' ability to balance trade-offs between\ncontrol attributes, especially implicit ones that require deeper understanding\nand abstract reasoning.\n","authors":["Yixi Ding","Jiaying Wu","Tongyao Zhu","Yanxia Qin","Qian Liu","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2410.12601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12600v1","updated":"2024-10-16T14:17:53Z","published":"2024-10-16T14:17:53Z","title":"On the Risk of Evidence Pollution for Malicious Social Text Detection in\n  the Era of LLMs","summary":"  Evidence-enhanced detectors present remarkable abilities in identifying\nmalicious social text with related evidence. However, the rise of large\nlanguage models (LLMs) brings potential risks of evidence pollution to confuse\ndetectors. This paper explores how to manipulate evidence, simulating potential\nmisuse scenarios including basic pollution, and rephrasing or generating\nevidence by LLMs. To mitigate its negative impact, we propose three defense\nstrategies from both the data and model sides, including machine-generated text\ndetection, a mixture of experts, and parameter updating. Extensive experiments\non four malicious social text detection tasks with ten datasets present that\nevidence pollution, especially the generate strategy, significantly compromises\nexisting detectors. On the other hand, the defense strategies could mitigate\nevidence pollution, but they faced limitations for practical employment, such\nas the need for annotated data and huge inference costs. Further analysis\nillustrates that polluted evidence is of high quality, would compromise the\nmodel calibration, and could ensemble to amplify the negative impact.\n","authors":["Herun Wan","Minnan Luo","Zhixiong Su","Guang Dai","Xiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11894v4","updated":"2024-10-16T14:14:27Z","published":"2024-03-18T15:53:33Z","title":"From Explainable to Interpretable Deep Learning for Natural Language\n  Processing in Healthcare: How Far from Reality?","summary":"  Deep learning (DL) has substantially enhanced natural language processing\n(NLP) in healthcare research. However, the increasing complexity of DL-based\nNLP necessitates transparent model interpretability, or at least\nexplainability, for reliable decision-making. This work presents a thorough\nscoping review of explainable and interpretable DL in healthcare NLP. The term\n\"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to\ndistinguish XAI from IAI. Different models are further categorized based on\ntheir functionality (model-, input-, output-based) and scope (local, global).\nOur analysis shows that attention mechanisms are the most prevalent emerging\nIAI technique. The use of IAI is growing, distinguishing it from XAI. The major\nchallenges identified are that most XIAI does not explore \"global\" modelling\nprocesses, the lack of best practices, and the lack of systematic evaluation\nand benchmarks. One important opportunity is to use attention mechanisms to\nenhance multi-modal XIAI for personalized medicine. Additionally, combining DL\nwith causal logic holds promise. Our discussion encourages the integration of\nXIAI in Large Language Models (LLMs) and domain-specific smaller models. In\nconclusion, XIAI adoption in healthcare requires dedicated in-house expertise.\nCollaboration with domain experts, end-users, and policymakers can lead to\nready-to-use XIAI methods across NLP and medical tasks. While challenges exist,\nXIAI techniques offer a valuable foundation for interpretable NLP algorithms in\nhealthcare.\n","authors":["Guangming Huang","Yingya Li","Shoaib Jameel","Yunfei Long","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2403.11894v4.pdf","comment":"This paper has been accepted by Computational and Structural\n  Biotechnology Journal"},{"id":"http://arxiv.org/abs/2402.19350v6","updated":"2024-10-16T14:12:47Z","published":"2024-02-29T16:56:36Z","title":"Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process","summary":"  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Jiaxing Shen","Xia Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19350v6.pdf","comment":"This paper has been accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2410.03293v3","updated":"2024-10-16T14:11:21Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08469v2","updated":"2024-10-16T14:09:14Z","published":"2024-10-11T02:42:13Z","title":"Semantic Token Reweighting for Interpretable and Controllable Text\n  Embeddings in CLIP","summary":"  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial\nrole in translating textual input into an embedding space shared with images,\nthereby facilitating the interpretative analysis of vision tasks through\nnatural language. Despite the varying significance of different textual\nelements within a sentence depending on the context, efforts to account for\nvariation of importance in constructing text embeddings have been lacking. We\npropose a framework of Semantic Token Reweighting to build Interpretable text\nembeddings (SToRI), which incorporates controllability as well. SToRI refines\nthe text encoding process in CLIP by differentially weighting semantic elements\nbased on contextual importance, enabling finer control over emphasis responsive\nto data-driven insights and user preferences. The efficacy of SToRI is\ndemonstrated through comprehensive experiments on few-shot image classification\nand image retrieval tailored to user preferences.\n","authors":["Eunji Kim","Kyuhong Shim","Simyung Chang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.08469v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.12586v1","updated":"2024-10-16T14:04:26Z","published":"2024-10-16T14:04:26Z","title":"Can We Reverse In-Context Knowledge Edits?","summary":"  In-context knowledge editing (IKE) enables efficient modification of large\nlanguage model (LLM) outputs without parameter changes and at zero-cost.\nHowever, it can be misused to manipulate responses opaquely, e.g., insert\nmisinformation or offensive content. Such malicious interventions could be\nincorporated into high-level wrapped APIs where the final input prompt is not\nshown to end-users. To address this issue, we investigate the detection and\nreversal of IKE-edits. First, we demonstrate that IKE-edits can be detected\nwith high accuracy (F1 > 80\\%) using only the top-10 output probabilities of\nthe next token, even in a black-box setting, e.g. proprietary LLMs with limited\noutput information. Further, we introduce the novel task of reversing IKE-edits\nusing specially tuned reversal tokens. We explore using both continuous and\ndiscrete reversal tokens, achieving over 80\\% accuracy in recovering original,\nunedited outputs across multiple LLMs. Our continuous reversal tokens prove\nparticularly effective, with minimal impact on unedited prompts. Through\nanalysis of output distributions, attention patterns, and token rankings, we\nprovide insights into IKE's effects on LLMs and how reversal tokens mitigate\nthem. This work represents a significant step towards enhancing LLM resilience\nagainst potential misuse of in-context editing, improving their transparency\nand trustworthiness.\n","authors":["Paul Youssef","Zhixue Zhao","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2410.12586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12583v1","updated":"2024-10-16T14:01:22Z","published":"2024-10-16T14:01:22Z","title":"STRUX: An LLM for Decision-Making with Structured Explanations","summary":"  Countless decisions shape our daily lives, and it is paramount to understand\nthe how and why behind these choices. In this paper, we introduce a new LLM\ndecision-making framework called STRUX, which enhances LLM decision-making by\nproviding structured explanations. These include favorable and adverse facts\nrelated to the decision, along with their respective strengths. STRUX begins by\ndistilling lengthy information into a concise table of key facts. It then\nemploys a series of self-reflection steps to determine which of these facts are\npivotal, categorizing them as either favorable or adverse in relation to a\nspecific decision. Lastly, we fine-tune an LLM to identify and prioritize these\nkey facts to optimize decision-making. STRUX has been evaluated on the\nchallenging task of forecasting stock investment decisions based on earnings\ncall transcripts and demonstrated superior performance against strong\nbaselines. It enhances decision transparency by allowing users to understand\nthe impact of different factors, representing a meaningful step towards\npractical decision-making with LLMs.\n","authors":["Yiming Lu","Yebowen Hu","Hassan Foroosh","Wei Jin","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.12583v1.pdf","comment":"10 pages, 7 figures, submitted to NAACL 2025"},{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.12319v3","updated":"2024-10-16T13:39:49Z","published":"2024-06-18T06:43:04Z","title":"The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences\n  of LLM Evaluators","summary":"  As large language models (LLMs) are increasingly used as evaluators for\nnatural language generation tasks, ensuring unbiased assessments is essential.\nHowever, LLM evaluators often display biased preferences, such as favoring\nverbosity and authoritative tones. Our empirical analysis reveals that these\nbiases are exacerbated in pairwise evaluation, where LLMs directly compare two\noutputs and easily prioritize superficial attributes. In contrast, pointwise\nevaluation, which assesses outputs independently, is less susceptible to such\nbias because each output is judged in isolation. To address the limitations of\nthe pairwise evaluation, we introduce a novel evaluation method, PRePair, which\nintegrates pointwise reasoning within a pairwise framework. PRePair effectively\nalleviates biased preference, improving performance on the adversarial\nbenchmark (LLMBar) while outperforming pointwise evaluation on the standard\nbenchmark (MT-Bench).\n","authors":["Hawon Jeong","ChaeHun Park","Jimin Hong","Hojoon Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.12319v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12558v1","updated":"2024-10-16T13:34:51Z","published":"2024-10-16T13:34:51Z","title":"A Claim Decomposition Benchmark for Long-form Answer Verification","summary":"  The advancement of LLMs has significantly boosted the performance of complex\nlong-form question answering tasks. However, one prominent issue of LLMs is the\ngenerated \"hallucination\" responses that are not factual. Consequently,\nattribution for each claim in responses becomes a common solution to improve\nthe factuality and verifiability. Existing researches mainly focus on how to\nprovide accurate citations for the response, which largely overlook the\nimportance of identifying the claims or statements for each response. To bridge\nthis gap, we introduce a new claim decomposition benchmark, which requires\nbuilding system that can identify atomic and checkworthy claims for LLM\nresponses. Specifically, we present the Chinese Atomic Claim Decomposition\nDataset (CACDD), which builds on the WebCPM dataset with additional expert\nannotations to ensure high data quality. The CACDD encompasses a collection of\n500 human-annotated question-answer pairs, including a total of 4956 atomic\nclaims. We further propose a new pipeline for human annotation and describe the\nchallenges of this task. In addition, we provide experiment results on\nzero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the\nclaim decomposition is highly challenging and requires further explorations.\nAll code and data are publicly available at\n\\url{https://github.com/FBzzh/CACDD}.\n","authors":["Zhihao Zhang","Yixing Fan","Ruqing Zhang","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2410.12558v1.pdf","comment":"Accepted by CCIR 2024"},{"id":"http://arxiv.org/abs/2406.18221v3","updated":"2024-10-16T13:31:05Z","published":"2024-06-26T10:08:47Z","title":"Enhancing Data Privacy in Large Language Models through Private\n  Association Editing","summary":"  Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications.\n","authors":["Davide Venditti","Elena Sofia Ruzzetti","Giancarlo A. Xompero","Cristina Giannone","Andrea Favalli","Raniero Romagnoli","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2406.18221v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12543v1","updated":"2024-10-16T13:21:46Z","published":"2024-10-16T13:21:46Z","title":"LLM-based Translation Inference with Iterative Bilingual Understanding","summary":"  The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).\n","authors":["Andong Chen","Kehai Chen","Yang Xiang","Xuefeng Bai","Muyun Yang","Tiejun Zhao","Min zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12543v1.pdf","comment":"work in process"},{"id":"http://arxiv.org/abs/2406.14162v3","updated":"2024-10-16T13:16:25Z","published":"2024-06-20T10:04:09Z","title":"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.\n","authors":["Jingwei Ni","Tobias Schimanski","Meihong Lin","Mrinmaya Sachan","Elliott Ash","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.14162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12532v1","updated":"2024-10-16T13:10:27Z","published":"2024-10-16T13:10:27Z","title":"MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration","summary":"  Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.\n","authors":["Jinjie Wei","Dingkang Yang","Yanshu Li","Qingyao Xu","Zhaoyu Chen","Mingcheng Li","Yue Jiang","Xiaolu Hou","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12532v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.09879v3","updated":"2024-10-16T12:57:56Z","published":"2024-07-13T13:03:45Z","title":"sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through\n  N-shot Guided Prompting","summary":"  Despite the remarkable success of LLMs in English, there is a significant gap\nin performance in non-English languages. In order to address this, we introduce\na novel recipe for creating a multilingual synthetic instruction tuning\ndataset, sPhinX, which is created by selectively translating instruction\nresponse pairs from English into 50 languages. We test the effectiveness of\nsPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and\nPhi-Small and then evaluating them across a comprehensive suite of multilingual\nbenchmarks that test reasoning, question answering, reading comprehension and\nmachine translation. Our results show that Mistral-7B and Phi-Small fine-tuned\nwith sPhinX perform better on an average by 5%pt for both the models when\ncompared to the base variants of these models. We also devise a strategy to\nincorporate N-shot examples in each fine-tuning sample which further boosts the\nperformance of these models by 9%pt and 4%pt respectively respectively compared\nto vanilla fine-tuning. To show efficacy of our data curation approach, we also\ndirectly translate our original dataset to the target languages, and observe an\nincrease of 7%pt and 4%pt on both the models respectively. sPhinX outperforms\nother multilingual instruction tuning datasets in both efficiency and\ndiversity, reducing dataset creation costs. It also maintains strong\nperformance on standard English LLM benchmarks, with minimal regression.\n","authors":["Sanchit Ahuja","Kumar Tanmay","Hardik Hansrajbhai Chauhan","Barun Patra","Kriti Aggarwal","Luciano Del Corro","Arindam Mitra","Tejas Indulal Dhamecha","Ahmed Awadallah","Monojit Choudhary","Vishrav Chaudhary","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2407.09879v3.pdf","comment":"20 pages, 12 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.12513v1","updated":"2024-10-16T12:45:35Z","published":"2024-10-16T12:45:35Z","title":"FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction","summary":"  Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.\n","authors":["Akriti Jain","Saransh Sharma","Koyel Mukherjee","Soumyabrata Pal"],"pdf_url":"https://arxiv.org/pdf/2410.12513v1.pdf","comment":"17 pages, 6 figures, Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.12511v1","updated":"2024-10-16T12:38:58Z","published":"2024-10-16T12:38:58Z","title":"Advancing Fairness in Natural Language Processing: From Traditional\n  Methods to Explainability","summary":"  The burgeoning field of Natural Language Processing (NLP) stands at a\ncritical juncture where the integration of fairness within its frameworks has\nbecome an imperative. This PhD thesis addresses the need for equity and\ntransparency in NLP systems, recognizing that fairness in NLP is not merely a\ntechnical challenge but a moral and ethical necessity, requiring a rigorous\nexamination of how these technologies interact with and impact diverse human\npopulations. Through this lens, this thesis undertakes a thorough investigation\ninto the development of equitable NLP methodologies and the evaluation of\nbiases that prevail in current systems.\n  First, it introduces an innovative algorithm to mitigate biases in\nmulti-class classifiers, tailored for high-risk NLP applications, surpassing\ntraditional methods in both bias mitigation and prediction accuracy. Then, an\nanalysis of the Bios dataset reveals the impact of dataset size on\ndiscriminatory biases and the limitations of standard fairness metrics. This\nawareness has led to explorations in the field of explainable AI, aiming for a\nmore complete understanding of biases where traditional metrics are limited.\nConsequently, the thesis presents COCKATIEL, a model-agnostic explainability\nmethod that identifies and ranks concepts in Transformer models, outperforming\nprevious approaches in sentiment analysis tasks. Finally, the thesis\ncontributes to bridging the gap between fairness and explainability by\nintroducing TaCo, a novel method to neutralize bias in Transformer model\nembeddings.\n  In conclusion, this thesis constitutes a significant interdisciplinary\nendeavor that intertwines explicability and fairness to challenge and reshape\ncurrent NLP paradigms. The methodologies and critiques presented contribute to\nthe ongoing discourse on fairness in machine learning, offering actionable\nsolutions for more equitable and responsible AI systems.\n","authors":["Fanny Jourdan"],"pdf_url":"https://arxiv.org/pdf/2410.12511v1.pdf","comment":"PhD Thesis, Toulouse University"},{"id":"http://arxiv.org/abs/2410.10114v2","updated":"2024-10-16T12:30:53Z","published":"2024-10-14T03:05:12Z","title":"Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models","summary":"  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has\ndemonstrated potent applicability across diverse downstream tasks. This\nlightweight approach has quickly gained traction from federated learning (FL)\nresearchers who seek to efficiently adapt VLMs to heterogeneous scenarios.\nHowever, current federated prompt learning methods are habitually restricted to\nthe traditional FL paradigm, where the participating clients are generally only\nallowed to download a single globally aggregated model from the server. While\njustifiable for training full-sized models under federated settings, in this\nwork, we argue that this paradigm is ill-suited for lightweight prompts. By\nfacilitating the clients to download multiple pre-aggregated prompts as fixed\nnon-local experts, we propose Personalized Federated Mixture of Adaptive\nPrompts (pFedMoAP), a novel FL framework that personalizes the prompt learning\nprocess through the lens of Mixture of Experts (MoE). pFedMoAP implements a\nlocal attention-based gating network that learns to generate enhanced text\nfeatures for better alignment with local image data on the client, benefiting\nfrom both local and downloaded non-local adaptive prompt experts. The non-local\nexperts are sparsely selected from a server-maintained pool, fostering\ncollaborative learning across clients. To evaluate the proposed algorithm, we\nconduct extensive experiments across 9 datasets under various heterogeneous\nfederated settings. The results show that pFedMoAP consistently outperforms the\nstate-of-the-art alternatives, underscoring its efficacy in personalizing\nprompt learning for CLIP within the federated learning paradigm.\n","authors":["Jun Luo","Chen Chen","Shandong Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10114v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.12499v1","updated":"2024-10-16T12:22:47Z","published":"2024-10-16T12:22:47Z","title":"With a Grain of SALT: Are LLMs Fair Across Social Dimensions?","summary":"  This paper presents an analysis of biases in open-source Large Language\nModels (LLMs) across various genders, religions, and races. We introduce a\nmethodology for generating a bias detection dataset using seven bias triggers:\nGeneral Debate, Positioned Debate, Career Advice, Story Generation,\nProblem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to\ngenerate a diverse set of prompts for each trigger across various genders,\nreligious and racial groups. We evaluate models from Llama and Gemma family on\nthe generated dataset. We anonymise the LLM-generated text associated with each\ngroup using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.\nTo quantify bias in the LLM-generated text we use the number of wins and losses\nin the pairwise comparison. Our analysis spans three languages, English,\nGerman, and Arabic to explore how language influences bias manifestation. Our\nfindings reveal that LLMs exhibit strong polarization toward certain groups\nacross each category, with a notable consistency observed across models.\nHowever, when switching languages, variations and anomalies emerge, often\nattributable to cultural cues and contextual differences.\n","authors":["Samee Arif","Zohaib Khan","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2410.12499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05770v2","updated":"2024-10-16T12:18:20Z","published":"2024-10-08T07:52:35Z","title":"Efficient Few-shot Learning for Multi-label Classification of Scientific\n  Documents with Many Classes","summary":"  Scientific document classification is a critical task and often involves many\nclasses. However, collecting human-labeled data for many classes is expensive\nand usually leads to label-scarce scenarios. Moreover, recent work has shown\nthat sentence embedding model fine-tuning for few-shot classification is\nefficient, robust, and effective. In this work, we propose FusionSent\n(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free\napproach for few-shot classification of scientific documents with many classes.\nFusionSent uses available training examples and their respective label texts to\ncontrastively fine-tune two different sentence embedding models. Afterward, the\nparameters of both fine-tuned models are fused to combine the complementary\nknowledge from the separate fine-tuning steps into a single model. Finally, the\nresulting sentence embedding model is frozen to embed the training instances,\nwhich are then used as input features to train a classification head. Our\nexperiments show that FusionSent significantly outperforms strong baselines by\nan average of $6.0$ $F_{1}$ points across multiple scientific document\nclassification datasets. In addition, we introduce a new dataset for\nmulti-label classification of scientific documents, which contains 203,961\nscientific articles and 130 classes from the arXiv category taxonomy. Code and\ndata are available at https://github.com/sebischair/FusionSent.\n","authors":["Tim Schopf","Alexander Blatzheim","Nektarios Machner","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2410.05770v2.pdf","comment":"Accepted to the 7th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2024)"},{"id":"http://arxiv.org/abs/2408.09945v2","updated":"2024-10-16T12:15:39Z","published":"2024-08-19T12:34:31Z","title":"Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating\n  Adequacy, Fluency, and Elegance","summary":"  Large language models (LLMs) have shown remarkable performance in general\ntranslation tasks. However, the increasing demand for high-quality translations\nthat are not only adequate but also fluent and elegant. To assess the extent to\nwhich current LLMs can meet these demands, we introduce a suitable benchmark\nfor translating classical Chinese poetry into English. This task requires not\nonly adequacy in translating culturally and historically significant content\nbut also a strict adherence to linguistic fluency and poetic elegance. Our\nstudy reveals that existing LLMs fall short of this task. To address these\nissues, we propose RAT, a \\textbf{R}etrieval-\\textbf{A}ugmented machine\n\\textbf{T}ranslation method that enhances the translation process by\nincorporating knowledge related to classical poetry. Additionally, we propose\nan automatic evaluation metric based on GPT-4, which better assesses\ntranslation quality in terms of adequacy, fluency, and elegance, overcoming the\nlimitations of traditional metrics. Our dataset and code will be made\navailable.\n","authors":["Andong Chen","Lianzhang Lou","Kehai Chen","Xuefeng Bai","Yang Xiang","Muyun Yang","Tiejun Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09945v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.08688v4","updated":"2024-10-16T12:15:19Z","published":"2024-08-16T12:01:55Z","title":"The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation","summary":"  This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline.\n","authors":["Samee Arif","Sualeha Farid","Abdul Hameed Azeemi","Awais Athar","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2408.08688v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12492v1","updated":"2024-10-16T12:14:29Z","published":"2024-10-16T12:14:29Z","title":"End-to-end Planner Training for Language Modeling","summary":"  Through end-to-end training to predict the next token, LLMs have become\nvaluable tools for various tasks. Enhancing their core training in language\nmodeling can improve numerous downstream applications. A successful approach to\nenhance language modeling uses a separate planning module to predict abstract\nlabels of future sentences and conditions the LM on these predictions. However,\nthis method is non-differentiable, preventing joint end-to-end tuning of the\nplanner with the LM. We propose an effective method to improve this approach by\nenabling joint fine-tuning of the planner and the LM. We show that a naive way\nof approximating the gradient of selecting a label via the straight-through\nestimator is not effective. Instead, we propose to use the predicted label\nprobabilities as mixing weights to condition the LM on a weighted average of\nlabel embeddings in a differentiable manner. This not only enables joint\nfine-tuning of the planner and the LM, but also allows the LM to draw on the\nfull label distribution predicted by the planner, retaining more information.\nOur experimental results show consistent improvements in perplexity.\n","authors":["Nathan Cornille","Florian Mai","Jingyuan Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2410.12492v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.12491v1","updated":"2024-10-16T12:14:25Z","published":"2024-10-16T12:14:25Z","title":"Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse RL","summary":"  Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.\n","authors":["Jared Joselowitz","Arjun Jagota","Satyapriya Krishna","Sonali Parbhoo"],"pdf_url":"https://arxiv.org/pdf/2410.12491v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.16535v2","updated":"2024-10-16T12:00:46Z","published":"2024-06-24T11:16:26Z","title":"Token-based Decision Criteria Are Suboptimal in In-context Learning","summary":"  In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL.\n","authors":["Hakaze Cho","Yoshihiro Sakai","Mariko Kato","Kenshiro Tanaka","Akira Ishii","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2406.16535v2.pdf","comment":"24 pages, 15 figures, 13 tables"},{"id":"http://arxiv.org/abs/2402.10052v2","updated":"2024-10-16T11:50:27Z","published":"2024-02-15T16:21:14Z","title":"UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in\n  Large Language Models","summary":"  Mitigating the retention of sensitive or private information in large\nlanguage models is essential for enhancing privacy and safety. Existing\nunlearning methods, like Gradient Ascent and Negative Preference Optimization,\ndirectly tune models to remove unwanted information. However, these methods\noften become unstable because they fine-tune by maximizing cross-entropy loss,\nwhich is the opposite of traditional loss minimization in learning. This\nreversal creates instability, especially on larger datasets, as the model\nstruggles to balance unlearning with maintaining language capacity, leading to\nover-unlearning. In this paper, we introduce UnDIAL (Unlearning via\nSelf-Distillation on Adjusted Logits), a novel and robust unlearning method.\nOur approach leverages self-distillation to adjust logits and selectively\nreduce the influence of targeted tokens. This technique ensures smooth\nconvergence and avoids catastrophic forgetting, even in challenging unlearning\ntasks with large datasets and sequential unlearning requests. Extensive\nexperiments show that UnDIAL can achieve both robustness in unlearning and\nscalability while maintaining stable training dynamics and resilience to\nhyperparameter tuning.\n","authors":["Yijiang River Dong","Hongzhou Lin","Mikhail Belkin","Ramon Huerta","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2402.10052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12480v1","updated":"2024-10-16T11:50:02Z","published":"2024-10-16T11:50:02Z","title":"KcMF: A Knowledge-compliant Framework for Schema and Entity Matching\n  with Fine-tuning-free LLMs","summary":"  Schema and entity matching tasks are crucial for data integration and\nmanagement. While large language models (LLMs) have shown promising results in\nthese tasks, they suffer from hallucinations and confusion about task\ninstructions. In this paper, we present the Knowledge-Compliant Matching\nFramework (KcMF), an LLM-based approach that addresses these issues without the\nneed for domain-specific fine-tuning. KcMF employs a pseudo-code-based task\ndecomposition strategy to adopt task-specific natural language statements that\nguide LLM reasoning and reduce confusion. We also propose two mechanisms,\nDataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain\nknowledge sets when unstructured domain knowledge is lacking. Additionally, we\nintroduce a result-ensembling strategy to leverage multiple knowledge sources\nand suppress poorly formatted outputs. Comprehensive evaluations on schema and\nentity matching tasks demonstrate that KcMF outperforms previous non-LLM\nstate-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes\neffectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across\ndifferent LLMs.\n","authors":["Yongqin Xu","Huan Li","Ke Chen","Lidan Shou"],"pdf_url":"https://arxiv.org/pdf/2410.12480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08440v3","updated":"2024-10-16T11:49:48Z","published":"2024-07-11T12:26:55Z","title":"Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models","summary":"  Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/\n","authors":["Wangtao Sun","Chenxiang Zhang","XueYou Zhang","Xuanqing Yu","Ziyang Huang","Pei Chen","Haotian Xu","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12478v1","updated":"2024-10-16T11:46:55Z","published":"2024-10-16T11:46:55Z","title":"MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.12478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12476v1","updated":"2024-10-16T11:46:32Z","published":"2024-10-16T11:46:32Z","title":"Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial\n  Generation","summary":"  Machine learning (ML) exhibits promise in the clinical domain. However, it is\nconstrained by data scarcity and ethical considerations, as the generation of\nclinical trials presents significant challenges due to stringent privacy\nregulations, high costs, and the extended duration required for conducting\nstudies with human participants. Despite the advancements of large language\nmodels (LLMs) in general generation tasks, their potential in facilitating the\ngeneration of synthetic clinical trials is under-explored. To address this gap,\nwe introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs\nto generate artificial yet realistic and diverse clinical trials with binary\nsuccess/failure labels. Experiments conducted on real clinical trials from the\n\\url{ClinicalTrials.gov} database demonstrate that our synthetic data can\neffectively augment real datasets. Furthermore, by fine-tuning a pre-trained\nmodel as a binary classifier on synthetic clinical trial datasets, we\ndemonstrate that this augmentation enhances model training for downstream tasks\nsuch as trial outcome prediction. Our findings suggest that LLMs for synthetic\nclinical trial generation hold promise for accelerating clinical research and\nupholding ethical standards for patient privacy. The code is publicly available\nat\nhttps://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.\n","authors":["Zerui Xu","Fang Wu","Tianfan Fu","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12470v1","updated":"2024-10-16T11:34:33Z","published":"2024-10-16T11:34:33Z","title":"Learning to Predict Usage Options of Product Reviews with LLM-Generated\n  Labels","summary":"  Annotating large datasets can be challenging. However, crowd-sourcing is\noften expensive and can lack quality, especially for non-trivial tasks. We\npropose a method of using LLMs as few-shot learners for annotating data in a\ncomplex natural language task where we learn a standalone model to predict\nusage options for products from customer reviews. We also propose a new\nevaluation metric for this scenario, HAMS4, that can be used to compare a set\nof strings with multiple reference sets. Learning a custom model offers\nindividual control over energy efficiency and privacy measures compared to\nusing the LLM directly for the sequence-to-sequence task. We compare this data\nannotation approach with other traditional methods and demonstrate how LLMs can\nenable considerable cost savings. We find that the quality of the resulting\ndata exceeds the level attained by third-party vendor services and that\nGPT-4-generated labels even reach the level of domain experts. We make the code\nand generated labels publicly available.\n","authors":["Leo Kohlenberg","Leonard Horns","Frederic Sadrieh","Nils Kiele","Matthis Clausen","Konstantin Ketterer","Avetis Navasardyan","Tamara Czinczoll","Gerard de Melo","Ralf Herbrich"],"pdf_url":"https://arxiv.org/pdf/2410.12470v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.12462v1","updated":"2024-10-16T11:23:03Z","published":"2024-10-16T11:23:03Z","title":"Bridging the Language Gaps in Large Language Models with Inference-Time\n  Cross-Lingual Intervention","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing but exhibit significant performance gaps among different\nlanguages. Most existing approaches to address these disparities rely on\npretraining or fine-tuning, which are resource-intensive. To overcome these\nlimitations without incurring significant costs, we propose Inference-Time\nCross-Lingual Intervention (INCLINE), a novel framework that enhances LLM\nperformance on low-performing (source) languages by aligning their internal\nrepresentations with those of high-performing (target) languages during\ninference. INCLINE initially learns alignment matrices using parallel sentences\nfrom source and target languages through a Least-Squares optimization, and then\napplies these matrices during inference to transform the low-performing\nlanguage representations toward the high-performing language space. Extensive\nexperiments on nine benchmarks with five LLMs demonstrate that INCLINE\nsignificantly improves performance across diverse tasks and languages, compared\nto recent strong baselines. Our analysis demonstrates that INCLINE is highly\ncost-effective and applicable to a wide range of applications. In addition, we\nrelease the code to foster research along this line:\nhttps://github.com/weixuan-wang123/INCLINE.\n","authors":["Weixuan Wang","Minghao Wu","Barry Haddow","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2410.12462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12458v1","updated":"2024-10-16T11:16:34Z","published":"2024-10-16T11:16:34Z","title":"The Best of Both Worlds: Bridging Quality and Diversity in Data\n  Selection with Bipartite Graph","summary":"  The performance of large language models (LLMs) in natural language\nprocessing (NLP) tasks is significantly influenced by the quality and diversity\nof data used for supervised fine-tuning (SFT). Current data selection methods\noften focus solely on quality or diversity, leading to underperforming models\ndue to suboptimal training data. In this paper, we introduce GraphFilter, a\nnovel method that represents the dataset as a bipartite graph, linking\nsentences to their constituent n-grams. This representation effectively\ncaptures the relationships between sentences and linguistic patterns,\nfacilitating the selection of sentences that enhance n-gram diversity. To\nbalance quality and diversity during selection, we propose a priority function\nthat combines the quality metric with the diversity metric in a multiplicative\nmanner. GraphFilter iteratively selects high-priority sentences, updates the\nbipartite graph by removing covered n-grams, and re-calculates priorities to\nreflect the evolving data landscape. We conduct extensive experiments using\nthree model backbones across six widely used benchmarks. The results\ndemonstrate that GraphFilter outperforms all nine baseline approaches,\nachieving superior model performance and computational efficiency. Our analyses\nvalidate the effectiveness of our design choices, examine the subsets selected\nby GraphFilter and other methods, highlight the importance of instruction\ndiversity, and explore the role of quality and diversity in relation to subset\nsizes. GraphFilter establishes a new foundation for effective data selection\nstrategies, encouraging further research in data selection for LLMs.\n","authors":["Minghao Wu","Thuy-Trang Vu","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2410.12458v1.pdf","comment":"19 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.07411v2","updated":"2024-10-16T10:56:24Z","published":"2024-06-11T16:15:06Z","title":"VersiCode: Towards Version-controllable Code Generation","summary":"  Large Language Models (LLMs) have made tremendous strides in code generation,\nbut existing research fails to account for the dynamic nature of software\ndevelopment, marked by frequent library updates. This gap significantly limits\nLLMs' deployment in realistic settings. In this paper, we propose two novel\ntasks aimed at bridging this gap: version-specific code completion (VSCC) and\nversion-aware code migration (VACM). In conjunction, we introduce VersiCode, a\ncomprehensive Python dataset specifically designed to evaluate LLMs on these\ntwo tasks, together with a novel evaluation metric, Critical Diff Check\n(CDC@1), which assesses code generation against evolving API requirements. We\nconduct an extensive evaluation on VersiCode, which reveals that\nversion-controllable code generation is indeed a significant challenge, even\nfor GPT-4o and other strong frontier models. We believe the novel tasks,\ndataset, and metric open up a new, important research direction that will\nfurther enhance LLMs' real-world applicability. The code and resources can be\nfound at https://github.com/wutong8023/VersiCode.\n","authors":["Tongtong Wu","Weigang Wu","Xingyu Wang","Kang Xu","Suyu Ma","Bo Jiang","Ping Yang","Zhenchang Xing","Yuan-Fang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.07411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12445v1","updated":"2024-10-16T10:49:22Z","published":"2024-10-16T10:49:22Z","title":"Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation\n  for Korean LLMs","summary":"  The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean\nLarge Language Models (LLMs), yet it has certain limitations. Notably, the\ndisconnect between quantitative improvements on the overly academic leaderboard\nbenchmarks and the qualitative impact of the models should be addressed.\nFurthermore, the benchmark suite is largely composed of translated versions of\ntheir English counterparts, which may not fully capture the intricacies of the\nKorean language. To address these issues, we propose Open Ko-LLM Leaderboard2,\nan improved version of the earlier Open Ko-LLM Leaderboard. The original\nbenchmarks are entirely replaced with new tasks that are more closely aligned\nwith real-world capabilities. Additionally, four new native Korean benchmarks\nare introduced to better reflect the distinct characteristics of the Korean\nlanguage. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide\na more meaningful evaluation for advancing Korean LLMs.\n","authors":["Hyeonwoo Kim","Dahyun Kim","Jihoo Kim","Sukyung Lee","Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.12445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12444v1","updated":"2024-10-16T10:48:14Z","published":"2024-10-16T10:48:14Z","title":"Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models","summary":"  Reliable responses of service chatbots are often achieved by employing\nretrieval-based methods that restrict answers to a knowledge base comprising\npredefined question-answer pairs (QA pairs). To accommodate potential\nvariations in how a customer's query may be expressed, it emerges as the\nfavored solution to augment these QA pairs with similar questions that are\npossibly diverse while remaining semantic consistency. This augmentation task\nis known as Similar Question Generation (SQG). Traditional methods that heavily\nrely on human efforts or rule-based techniques suffer from limited diversity or\nsignificant semantic deviation from the source question, only capable of\nproducing a finite number of useful questions.\n  To address these limitations, we propose an SQG approach based on Large\nLanguage Models (LLMs), capable of producing a substantial number of diverse\nquestions while maintaining semantic consistency to the source QA pair. This is\nachieved by leveraging LLMs' natural language understanding capability through\nfine-tuning with specially designed prompts. The experiments conducted on a\nreal customer-service dataset demonstrate that our method surpasses baseline\nmethods by a significant margin in terms of semantic diversity. Human\nevaluation further confirms that integrating the answer that reflects the\ncustomer's intention is crucial for increasing the number of generated\nquestions that meet business requirements.\n","authors":["Mengze Hong","Yuanfeng Song","Di Jiang","Lu Wang","Zichang Guo","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11507v2","updated":"2024-10-16T10:36:18Z","published":"2024-10-15T11:20:42Z","title":"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs","summary":"  While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant. Current benchmark-based evaluation\nmethods exhibit rigid, aimless interactions and rely on pre-collected static\ndatasets that are costly to build, inflexible across domains, and misaligned\nwith practical user needs. To address this issue, we revisit the evaluation\ncomponents and introduce two concepts: Benchmark+, which extends traditional\nquestion-answer benchmark into a more flexible \"strategy-criterion\" format; and\nAssessment+, which enhances the interaction process, enabling deeper\nexploration and supporting both quantitative metrics and qualitative insights.\nThese concepts capture the nuanced behaviors of LLMs through richer, multi-turn\ninteractions. We propose an agent-based evaluation framework called TestAgent,\nwhich implements these concepts through retrieval augmented generation and\nreinforcement learning. Experiments on tasks ranging from constructing vertical\ndomain evaluation to activating existing benchmarks demonstrate the\neffectiveness of TestAgent across various scenarios. We believe this work\noffers an interesting perspective on automatic evaluation for LLMs.\n","authors":["Wanying Wang","Zeyu Ma","Pengfei Liu","Mingang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16264v2","updated":"2024-10-16T10:19:45Z","published":"2024-08-29T05:02:52Z","title":"LoraMap: Harnessing the Power of LoRA Connections","summary":"  Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.\n","authors":["Hyeryun Park","Jeongwon Kwak","Dongsuk Jang","Sumin Park","Jinwook Choi"],"pdf_url":"https://arxiv.org/pdf/2408.16264v2.pdf","comment":"17 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.12428v1","updated":"2024-10-16T10:16:34Z","published":"2024-10-16T10:16:34Z","title":"Conformity in Large Language Models","summary":"  The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin state-of-the-art LLMs. Our findings reveal that all models tested exhibit\nvarying levels of conformity toward the majority, regardless of their initial\nchoice or correctness, across different knowledge domains. Notably, we are the\nfirst to show that LLMs are more likely to conform when they are more uncertain\nin their own prediction. We further explore factors that influence conformity,\nsuch as training paradigms and input characteristics, finding that\ninstruction-tuned models are less susceptible to conformity, while increasing\nthe naturalness of majority tones amplifies conformity. Finally, we propose two\ninterventions--Devil's Advocate and Question Distillation--to mitigate\nconformity, providing insights into building more robust language models.\n","authors":["Xiaochen Zhu","Caiqi Zhang","Tom Stafford","Nigel Collier","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2410.12428v1.pdf","comment":"16 pages (8 pages main body), 14 figures"},{"id":"http://arxiv.org/abs/2410.07129v2","updated":"2024-10-16T10:14:54Z","published":"2024-10-09T17:51:55Z","title":"Mental Disorders Detection in the Era of Large Language Models","summary":"  This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.\n","authors":["Gleb Kuzmin","Petr Strepetov","Maksim Stankevich","Artem Shelmanov","Ivan Smirnov"],"pdf_url":"https://arxiv.org/pdf/2410.07129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11288v2","updated":"2024-10-16T09:59:17Z","published":"2024-06-17T07:51:44Z","title":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have significantly improved multimodal\nreasoning tasks, such as visual question answering and image captioning. These\nmodels embed multimodal facts within their parameters, rather than relying on\nexternal knowledge bases to store factual information explicitly. However, the\ncontent discerned by LVLMs may deviate from actual facts due to inherent bias\nor incorrect inference. To address this issue, we introduce MFC-Bench, a\nrigorous and comprehensive benchmark designed to evaluate the factual accuracy\nof LVLMs across three stages of verdict prediction for MFC: Manipulation,\nOut-of-Context, and Veracity Classification. Through our evaluation on\nMFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering\nthat current models still fall short in multimodal fact-checking and\ndemonstrate insensitivity to various forms of manipulated content. We hope that\nMFC-Bench could raise attention to the trustworthy AI potentially assisted by\nLVLMs in the future. The MFC-Bench and accompanying resources are publicly\naccessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing\nresearch in the multimodal fact-checking field.\n","authors":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2406.11288v2.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12413v1","updated":"2024-10-16T09:56:01Z","published":"2024-10-16T09:56:01Z","title":"Theoretical Analysis of Hierarchical Language Recognition and Generation\n  by Transformers without Positional Encoding","summary":"  In this study, we provide constructive proof that Transformers can recognize\nand generate hierarchical language efficiently with respect to model size, even\nwithout the need for a specific positional encoding. Specifically, we show that\ncausal masking and a starting token enable Transformers to compute positional\ninformation and depth within hierarchical structures. We demonstrate that\nTransformers without positional encoding can generate hierarchical languages.\nFurthermore, we suggest that explicit positional encoding might have a\ndetrimental effect on generalization with respect to sequence length.\n","authors":["Daichi Hayakawa","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2410.12413v1.pdf","comment":"55 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.07891v2","updated":"2024-10-16T09:53:40Z","published":"2024-09-12T09:51:56Z","title":"A corpus-based investigation of pitch contours of monosyllabic words in\n  conversational Taiwan Mandarin","summary":"  In Mandarin, the tonal contours of monosyllabic words produced in isolation\nor in careful speech are characterized by four lexical tones: a high-level tone\n(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,\nin spontaneous speech, the actual tonal realization of monosyllabic words can\ndeviate significantly from these canonical tones due to intra-syllabic\nco-articulation and inter-syllabic co-articulation with adjacent tones. In\naddition, Chuang et al. (2024) recently reported that the tonal contours of\ndisyllabic Mandarin words with T2-T4 tone pattern are co-determined by their\nmeanings. Following up on their research, we present a corpus-based\ninvestigation of how the pitch contours of monosyllabic words are realized in\nspontaneous conversational Mandarin, focusing on the effects of contextual\npredictors on the one hand, and the way in words' meanings co-determine pitch\ncontours on the other hand. We analyze the F0 contours of 3824 tokens of 63\ndifferent word types in a spontaneous Taiwan Mandarin corpus, using the\ngeneralized additive (mixed) model to decompose a given observed pitch contour\ninto a set of component pitch contours. We show that the tonal context\nsubstantially modify a word's canonical tone. Once the effect of tonal context\nis controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a\nhigh tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),\nwhich in standard descriptions, is realized based on the preceding tone,\nemerges as a low tone in its own right, modified by the other predictors in the\nsame way as the standard tones T1, T2, T3, and T4. We also show that word, and\neven more so, word sense, co-determine words' F0 contours. Analyses of variable\nimportance using random forests further supported the substantial effect of\ntonal context and an effect of word sense.\n","authors":["Xiaoyun Jin","Mirjam Ernestus","R. Harald Baayen"],"pdf_url":"https://arxiv.org/pdf/2409.07891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12409v1","updated":"2024-10-16T09:44:38Z","published":"2024-10-16T09:44:38Z","title":"Revealing the Barriers of Language Agents in Planning","summary":"  Autonomous planning has been an ongoing pursuit since the inception of\nartificial intelligence. Based on curated problem solvers, early planning\nagents could deliver precise solutions for specific tasks but lacked\ngeneralization. The emergence of large language models (LLMs) and their\npowerful reasoning capabilities has reignited interest in autonomous planning\nby automatically generating reasonable solutions for given tasks. However,\nprior research and our experiments show that current language agents still lack\nhuman-level planning abilities. Even the state-of-the-art reasoning model,\nOpenAI o1, achieves only 15.6% on one of the complex real-world planning\nbenchmarks. This highlights a critical question: What hinders language agents\nfrom achieving human-level planning? Although existing studies have highlighted\nweak performance in agent planning, the deeper underlying issues and the\nmechanisms and limitations of the strategies proposed to address them remain\ninsufficiently understood. In this work, we apply the feature attribution study\nand identify two key factors that hinder agent planning: the limited role of\nconstraints and the diminishing influence of questions. We also find that\nalthough current strategies help mitigate these challenges, they do not fully\nresolve them, indicating that agents still have a long way to go before\nreaching human-level intelligence.\n","authors":["Jian Xie","Kexun Zhang","Jiangjie Chen","Siyu Yuan","Kai Zhang","Yikai Zhang","Lei Li","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.12409v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.12407v1","updated":"2024-10-16T09:42:29Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.12406v1","updated":"2024-10-16T09:41:48Z","published":"2024-10-16T09:41:48Z","title":"Nominal Class Assignment in Swahili: A Computational Account","summary":"  We discuss the open question of the relation between semantics and nominal\nclass assignment in Swahili. We approach the problem from a computational\nperspective, aiming first to quantify the extent of this relation, and then to\nexplicate its nature, taking extra care to suppress morphosyntactic confounds.\nOur results are the first of their kind, providing a quantitative evaluation of\nthe semantic cohesion of each nominal class, as well as a nuanced taxonomic\ndescription of its semantic content.\n","authors":["Giada Palmieri","Konstantinos Kogkalidis"],"pdf_url":"https://arxiv.org/pdf/2410.12406v1.pdf","comment":"Tenth Italian Conference on Computational Linguistics (CliC-it-2024)"},{"id":"http://arxiv.org/abs/2410.12405v1","updated":"2024-10-16T09:38:13Z","published":"2024-10-16T09:38:13Z","title":"ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs","summary":"  Large language models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but their performance is highly sensitive to the prompts\nutilized. This variability poses challenges for accurate assessment and user\nsatisfaction. Current research frequently overlooks instance-level prompt\nvariations and their implications on subjective evaluations. To address these\nshortcomings, we introduce ProSA, a framework designed to evaluate and\ncomprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity\nmetric, PromptSensiScore, and leverages decoding confidence to elucidate\nunderlying mechanisms. Our extensive study, spanning multiple tasks, uncovers\nthat prompt sensitivity fluctuates across datasets and models, with larger\nmodels exhibiting enhanced robustness. We observe that few-shot examples can\nalleviate this sensitivity issue, and subjective evaluations are also\nsusceptible to prompt sensitivities, particularly in complex,\nreasoning-oriented tasks. Furthermore, our findings indicate that higher model\nconfidence correlates with increased prompt robustness. We believe this work\nwill serve as a helpful tool in studying prompt sensitivity of LLMs. The\nproject is released at: https://github.com/open-compass/ProSA .\n","authors":["Jingming Zhuo","Songyang Zhang","Xinyu Fang","Haodong Duan","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12405v1.pdf","comment":"EMNLP 2024, Findings"},{"id":"http://arxiv.org/abs/2308.01472v2","updated":"2024-10-16T09:37:11Z","published":"2023-08-02T23:39:29Z","title":"Reverse Stable Diffusion: What prompt was used to generate this image?","summary":"  Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2308.01472v2.pdf","comment":"Accepted for publication in Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2405.00716v4","updated":"2024-10-16T09:18:58Z","published":"2024-04-25T15:51:06Z","title":"Large Language Models in the Clinic: A Comprehensive Benchmark","summary":"  The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and clinical tasks that are complex but common\nin real-world practice, e.g., open-ended decision-making, long document\nprocessing, and emerging drug analysis. We conduct an extensive evaluation of\ntwenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite\nmedical experts to evaluate the clinical usefulness of LLMs. The benchmark data\nis available at https://github.com/AI-in-Health/ClinicBench.\n","authors":["Fenglin Liu","Zheng Li","Hongjian Zhou","Qingyu Yin","Jingfeng Yang","Xianfeng Tang","Chen Luo","Ming Zeng","Haoming Jiang","Yifan Gao","Priyanka Nigam","Sreyashi Nag","Bing Yin","Yining Hua","Xuan Zhou","Omid Rohanian","Anshul Thakur","Lei Clifton","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2405.00716v4.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12391v1","updated":"2024-10-16T09:18:39Z","published":"2024-10-16T09:18:39Z","title":"Tracking Universal Features Through Fine-Tuning and Model Merging","summary":"  We study how features emerge, disappear, and persist across models fine-tuned\non different domains of text. More specifically, we start from a base one-layer\nTransformer language model that is trained on a combination of the BabyLM\ncorpus, and a collection of Python code from The Stack. This base model is\nadapted to two new domains of text: TinyStories, and the Lua programming\nlanguage, respectively; and then these two models are merged using these two\nmodels using spherical linear interpolation. Our exploration aims to provide\ndeeper insights into the stability and transformation of features across\ntypical transfer-learning scenarios using small-scale models and sparse\nauto-encoders.\n","authors":["Niels Horn","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2410.12391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12388v1","updated":"2024-10-16T09:13:23Z","published":"2024-10-16T09:13:23Z","title":"Prompt Compression for Large Language Models: A Survey","summary":"  Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality fusion, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality.\n","authors":["Zongqian Li","Yinhong Liu","Yixuan Su","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.12388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03514v2","updated":"2024-10-16T08:59:22Z","published":"2024-03-06T07:43:43Z","title":"CLongEval: A Chinese Benchmark for Evaluating Long-Context Large\n  Language Models","summary":"  Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs are released.\n","authors":["Zexuan Qiu","Jingjing Li","Shijue Huang","Xiaoqi Jiao","Wanjun Zhong","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2403.03514v2.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12380v1","updated":"2024-10-16T08:55:49Z","published":"2024-10-16T08:55:49Z","title":"Evaluation of Attribution Bias in Retrieval-Augmented Large Language\n  Models","summary":"  Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs.\n","authors":["Amin Abolghasemi","Leif Azzopardi","Seyyed Hadi Hashemi","Maarten de Rijke","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2410.12380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06499v4","updated":"2024-10-16T08:53:23Z","published":"2023-12-11T16:22:37Z","title":"TaCo: Targeted Concept Erasure Prevents Non-Linear Classifiers From\n  Detecting Protected Attributes","summary":"  Ensuring fairness in NLP models is crucial, as they often encode sensitive\nattributes like gender and ethnicity, leading to biased outcomes. Current\nconcept erasure methods attempt to mitigate this by modifying final latent\nrepresentations to remove sensitive information without retraining the entire\nmodel. However, these methods typically rely on linear classifiers, which leave\nmodels vulnerable to non-linear adversaries capable of recovering sensitive\ninformation.\n  We introduce Targeted Concept Erasure (TaCo), a novel approach that removes\nsensitive information from final latent representations, ensuring fairness even\nagainst non-linear classifiers. Our experiments show that TaCo outperforms\nstate-of-the-art methods, achieving greater reductions in the prediction\naccuracy of sensitive attributes by non-linear classifier while preserving\noverall task performance. Code is available on\nhttps://github.com/fanny-jourdan/TaCo.\n","authors":["Fanny Jourdan","Louis Béthune","Agustin Picard","Laurent Risser","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2312.06499v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12377v1","updated":"2024-10-16T08:49:17Z","published":"2024-10-16T08:49:17Z","title":"HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying\n  Real-World Claims","summary":"  To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a\nsystem that only employs publicly available large language models (LLMs) for\neach step of automated fact-checking, dubbed the Herd of Open LLMs for\nverifying real-world claims (HerO). HerO employs multiple LLMs for each step of\nautomated fact-checking. For evidence retrieval, a language model is used to\nenhance a query by generating hypothetical fact-checking documents. We prompt\npretrained and fine-tuned LLMs for question generation and veracity prediction\nby crafting prompts with retrieved in-context samples. HerO achieved 2nd place\non the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of\nopen LLMs for verifying real-world claims. For future research, we make our\ncode publicly available at https://github.com/ssu-humane/HerO.\n","authors":["Yejun Yoon","Jaeyoon Jung","Seunghyun Yoon","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2410.12377v1.pdf","comment":"A system description paper for the AVeriTeC shared task, hosted by\n  the seventh FEVER workshop (co-located with EMNLP 2024)"},{"id":"http://arxiv.org/abs/2410.12375v1","updated":"2024-10-16T08:46:26Z","published":"2024-10-16T08:46:26Z","title":"PRefLexOR: Preference-based Recursive Language Modeling for Exploratory\n  Optimization of Reasoning and Agentic Thinking","summary":"  PRefLexOR (Preference-based Recursive Language Modeling for Exploratory\nOptimization of Reasoning) combines preference optimization with concepts from\nReinforcement Learning to enable models to self-teach through iterative\nreasoning improvements. We propose a recursive learning approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps\nbefore producing a final output in training and inference phases. Through\nmultiple training stages, the model first learns to align its reasoning with\naccurate decision paths by optimizing the log odds between preferred and\nnon-preferred responses. During this process, PRefLexOR builds a dynamic\nknowledge graph by generating questions from random text chunks and\nretrieval-augmentation to contextualize relevant details from the entire\ntraining corpus. In the second stage, preference optimization enhances model\nperformance by using rejection sampling to fine-tune reasoning quality by\ncontinually producing in-situ training data while masking the reasoning steps.\nRecursive optimization within a thinking token framework introduces iterative\nfeedback loops, where the model refines reasoning, achieving deeper coherence,\nconsistency, and adaptability. Implemented in small language models with only 3\nbillion parameters, we should that even tiny models can iteratively teach\nthemselves to reason with greater depth and reflectivity. Our implementation is\nstraightforward and can be incorporated into any existing pretrained LLM. We\nfocus our examples on applications in biological materials science and\ndemonstrate the method in a variety of case studies that range from in-domain\nto cross-domain applications. Using reasoning strategies that include thinking\nand reflection modalities we build a multi-agent recursive self-improving\ninference approach to successively improve responses via repeated sampling in\ninference time.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2410.12375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12361v1","updated":"2024-10-16T08:24:09Z","published":"2024-10-16T08:24:09Z","title":"Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance","summary":"  Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.\n","authors":["Yaxi Lu","Shenzhi Yang","Cheng Qian","Guirong Chen","Qinyu Luo","Yesai Wu","Huadong Wang","Xin Cong","Zhong Zhang","Yankai Lin","Weiwen Liu","Yasheng Wang","Zhiyuan Liu","Fangming Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.12361v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.09822v2","updated":"2024-10-16T08:20:43Z","published":"2024-09-15T18:43:11Z","title":"Causal Inference with Large Language Model: A Survey","summary":"  Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.\n","authors":["Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09822v2.pdf","comment":"12 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2312.03003v3","updated":"2024-10-16T08:15:53Z","published":"2023-12-04T06:13:35Z","title":"Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation","summary":"  The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 185 tasks across 18 mobile apps. The results\nindicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,\nand is able to adapt them to different contexts with near perfect (98.75%)\naccuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,\ncompared to the GPT-4 powered baseline.\n","authors":["Sunjae Lee","Junyoung Choi","Jungjae Lee","Munim Hasan Wasi","Hojun Choi","Steven Y. Ko","Sangeun Oh","Insik Shin"],"pdf_url":"https://arxiv.org/pdf/2312.03003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12350v1","updated":"2024-10-16T08:13:54Z","published":"2024-10-16T08:13:54Z","title":"GECTurk WEB: An Explainable Online Platform for Turkish Grammatical\n  Error Detection and Correction","summary":"  Sophisticated grammatical error detection/correction tools are available for\na small set of languages such as English and Chinese. However, it is not\nstraightforward -- if not impossible -- to adapt them to morphologically rich\nlanguages with complex writing rules like Turkish which has more than 80\nmillion speakers. Even though several tools exist for Turkish, they primarily\nfocus on spelling errors rather than grammatical errors and lack features such\nas web interfaces, error explanations and feedback mechanisms. To fill this\ngap, we introduce GECTurk WEB, a light, open-source, and flexible web-based\nsystem that can detect and correct the most common forms of Turkish writing\nerrors, such as the misuse of diacritics, compound and foreign words, pronouns,\nlight verbs along with spelling mistakes. Our system provides native speakers\nand second language learners an easily accessible tool to detect/correct such\nmistakes and also to learn from their mistakes by showing the explanation for\nthe violated rule(s). The proposed system achieves 88,3 system usability score,\nand is shown to help learn/remember a grammatical rule (confirmed by 80% of the\nparticipants). The GECTurk WEB is available both as an offline tool at\nhttps://github.com/GGLAB-KU/gecturkweb or online at www.gecturk.net.\n","authors":["Ali Gebeşçe","Gözde Gül Şahin"],"pdf_url":"https://arxiv.org/pdf/2410.12350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01308v2","updated":"2024-10-16T08:08:51Z","published":"2024-08-02T15:00:05Z","title":"Reconsidering Degeneration of Token Embeddings with Definitions for\n  Encoder-based Pre-trained Language Models","summary":"  Learning token embeddings based on token co-occurrence statistics has proven\neffective for both pre-training and fine-tuning in natural language processing.\nHowever, recent studies have pointed out that the distribution of learned\nembeddings degenerates into anisotropy (i.e., non-uniform distribution), and\neven pre-trained language models (PLMs) suffer from a loss of semantics-related\ninformation in embeddings for low-frequency tokens. This study first analyzes\nthe fine-tuning dynamics of encoder-based PLMs and demonstrates their\nrobustness against degeneration. On the basis of this analysis, we propose\nDefinitionEMB, a method that utilizes definitions to re-construct isotropically\ndistributed and semantics-related token embeddings for encoder-based PLMs while\nmaintaining original robustness during fine-tuning. Our experiments demonstrate\nthe effectiveness of leveraging definitions from Wiktionary to re-construct\nsuch embeddings for two encoder-based PLMs: RoBERTa-base and BART-large.\nFurthermore, the re-constructed embeddings for low-frequency tokens improve the\nperformance of these models across various GLUE and four text summarization\ndatasets.\n","authors":["Ying Zhang","Dongyuan Li","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2408.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07109v2","updated":"2024-10-16T08:06:22Z","published":"2024-10-09T17:45:47Z","title":"I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy","summary":"  As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.\n","authors":["Gian Maria Campedelli","Nicolò Penzo","Massimo Stefan","Roberto Dessì","Marco Guerini","Bruno Lepri","Jacopo Staiano"],"pdf_url":"https://arxiv.org/pdf/2410.07109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15729v3","updated":"2024-10-16T08:04:46Z","published":"2024-02-24T05:40:01Z","title":"How Do Humans Write Code? Large Models Do It the Same Way Too","summary":"  Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought\n(CoT) as the most popular method in Large Language Models (LLMs) mathematical\nreasoning tasks by utilizing external tool calls to circumvent computational\nerrors. However, our evaluation of the GPT-4 and Llama series reveals that\nusing PoT introduces more reasoning errors, such as incorrect formulas or\nflawed logic, compared to CoT. To address this issue, we propose Human-Think\nLanguage (HTL), which leverages a suite of strategies that help integrate PoT\nand CoT, encompassing: (1) a new generation paradigm that uses full CoT\nreasoning to control code generation. (2) Focus Attention, that directs model\nattention to the CoT reasoning during PoT to generate more logical code. (3)\nreinforcement learning that utilizes the accuracy of both CoT and PoT responses\nas rewards to prevent repetitive reasoning steps in LLMs when solving difficult\nmath problems. Our method achieves an average improvement of 6.5% on the\nLlama-Base model and 4.3% on the Mistral-Base model across 8 mathematical\ncalculation datasets. It also shows significant effectiveness on five\nout-of-domain datasets by controlling the model's information flow, exhibiting\nstrong transferability. Additionally, HTL shows the most significant\nimprovement in non-mathematical natural language inference task, contributing\nto a unified reasoning task framework\n","authors":["Long Li","Xuzheng He","Haozhe Wang","Linlin Wang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2402.15729v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12341v1","updated":"2024-10-16T08:02:48Z","published":"2024-10-16T08:02:48Z","title":"A linguistic analysis of undesirable outcomes in the era of generative\n  AI","summary":"  Recent research has focused on the medium and long-term impacts of generative\nAI, posing scientific and societal challenges mainly due to the detection and\nreliability of machine-generated information, which is projected to form the\nmajor content on the Web soon. Prior studies show that LLMs exhibit a lower\nperformance in generation tasks (model collapse) as they undergo a fine-tuning\nprocess across multiple generations on their own generated content\n(self-consuming loop). In this paper, we present a comprehensive simulation\nframework built upon the chat version of LLama2, focusing particularly on the\nlinguistic aspects of the generated content, which has not been fully examined\nin existing studies. Our results show that the model produces less lexical rich\ncontent across generations, reducing diversity. The lexical richness has been\nmeasured using the linguistic measures of entropy and TTR as well as\ncalculating the POSTags frequency. The generated content has also been examined\nwith an $n$-gram analysis, which takes into account the word order, and\nsemantic networks, which consider the relation between different words. These\nfindings suggest that the model collapse occurs not only by decreasing the\ncontent diversity but also by distorting the underlying linguistic patterns of\nthe generated text, which both highlight the critical importance of carefully\nchoosing and curating the initial input text, which can alleviate the model\ncollapse problem. Furthermore, we conduct a qualitative analysis of the\nfine-tuned models of the pipeline to compare their performances on generic NLP\ntasks to the original model. We find that autophagy transforms the initial\nmodel into a more creative, doubtful and confused one, which might provide\ninaccurate answers and include conspiracy theories in the model responses,\nspreading false and biased information on the Web.\n","authors":["Daniele Gambetta","Gizem Gezici","Fosca Giannotti","Dino Pedreschi","Alistair Knott","Luca Pappalardo"],"pdf_url":"https://arxiv.org/pdf/2410.12341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17753v2","updated":"2024-10-16T07:59:39Z","published":"2024-06-25T17:40:47Z","title":"Measuring and Benchmarking Large Language Models' Capabilities to\n  Generate Persuasive Language","summary":"  We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive language - both when explicitly instructed to rewrite\ntext to be more or less persuasive and when only instructed to paraphrase. We\nconstruct the new dataset Persuasive-Pairs of pairs of a short text and its\nrewrite by an LLM to amplify or diminish persuasive language. We multi-annotate\nthe pairs on a relative scale for persuasive language: a valuable resource in\nitself, and for training a regression model to score and benchmark persuasive\nlanguage, including for new LLMs across domains. In our analysis, we find that\ndifferent 'personas' in LLaMA3's system prompt change persuasive language\nsubstantially, even when only instructed to paraphrase.\n","authors":["Amalie Brogaard Pauli","Isabelle Augenstein","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2406.17753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12329v1","updated":"2024-10-16T07:49:13Z","published":"2024-10-16T07:49:13Z","title":"Understanding the Role of LLMs in Multimodal Evaluation Benchmarks","summary":"  The rapid advancement of Multimodal Large Language Models (MLLMs) has been\naccompanied by the development of various benchmarks to evaluate their\ncapabilities. However, the true nature of these evaluations and the extent to\nwhich they assess multimodal reasoning versus merely leveraging the underlying\nLarge Language Model (LLM) backbone remain unclear. This paper presents a\ncomprehensive investigation into the role of LLM backbones in MLLM evaluation,\nfocusing on two critical aspects: the degree to which current benchmarks truly\nassess multimodal reasoning and the influence of LLM prior knowledge on\nperformance. Specifically, we introduce a modified evaluation protocol to\ndisentangle the contributions of the LLM backbone from multimodal integration,\nand an automatic knowledge identification technique for diagnosing whether LLMs\nequip the necessary knowledge for corresponding multimodal questions. Our study\nencompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key\nfindings reveal that some benchmarks allow high performance even without visual\ninputs and up to 50\\% of error rates can be attributed to insufficient world\nknowledge in the LLM backbone, indicating a heavy reliance on language\ncapabilities. To address knowledge deficiencies, we propose a knowledge\naugmentation pipeline that achieves significant performance gains, with\nimprovements of up to 60\\% on certain datasets, resulting in a approximately 4x\nincrease in performance. Our work provides crucial insights into the role of\nthe LLM backbone in MLLMs, and highlights the need for more nuanced\nbenchmarking approaches.\n","authors":["Botian Jiang","Lei Li","Xiaonan Li","Zhaowei Li","Xiachong Feng","Lingpeng Kong","Qi Liu","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.12329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12327v1","updated":"2024-10-16T07:47:45Z","published":"2024-10-16T07:47:45Z","title":"Neuron-based Personality Trait Induction in Large Language Models","summary":"  Large language models (LLMs) have become increasingly proficient at\nsimulating various personality traits, an important capability for supporting\nrelated applications (e.g., role-playing). To further improve this capacity, in\nthis paper, we present a neuron-based approach for personality trait induction\nin LLMs, with three major technical contributions. First, we construct\nPersonalityBench, a large-scale dataset for identifying and evaluating\npersonality traits in LLMs. This dataset is grounded in the Big Five\npersonality traits from psychology and is designed to assess the generative\ncapabilities of LLMs towards specific personality traits. Second, by leveraging\nPersonalityBench, we propose an efficient method for identifying\npersonality-related neurons within LLMs by examining the opposite aspects of a\ngiven trait. Third, we develop a simple yet effective induction method that\nmanipulates the values of these identified personality-related neurons. This\nmethod enables fine-grained control over the traits exhibited by LLMs without\ntraining and modifying model parameters. Extensive experiments validate the\nefficacy of our neuron identification and trait induction methods. Notably, our\napproach achieves comparable performance as fine-tuned models, offering a more\nefficient and flexible solution for personality trait induction in LLMs. We\nprovide access to all the mentioned resources at\nhttps://github.com/RUCAIBox/NPTI.\n","authors":["Jia Deng","Tianyi Tang","Yanbin Yin","Wenhao Yang","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.12327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12325v1","updated":"2024-10-16T07:45:56Z","published":"2024-10-16T07:45:56Z","title":"Optimizing Low-Resource Language Model Training: Comprehensive Analysis\n  of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches","summary":"  In this paper, we address the challenge of optimizing training setups for\nLarge Language Models (LLMs) of low-resource language with a limited amount of\ncorpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training\nto utilize the limited target language corpus efficiently. However, there is\nstill a lack of understanding about the optimal hyperparameter setups for\ncombining these three approaches to train LLMs. We exhaustively explore\ntraining setups for low-resource language LLM, combining these three\napproaches, and found the following insights for efficiently reducing the cost\nof hyperparameter search: (1) As the amount of target language corpus\ndecreases, the optimal training approach shifts from monolingual single-stage\ntraining to multi-lingual two-stage training at a compute budget dependent\nthreshold. (2) The optimal model scale remains stable regardless of the amount\nof target language corpus, allowing the use of the compute-optimal scale of\nmonolingual training. (3) The optimal number of epochs can be extrapolated from\nsmaller-scale experiments to larger scale using our proposed model. Also, we\nprovide evidence that, in single-stage training, the target language validation\nloss follows a power law with respect to the target language ratio, with an\nexponent independent of the amount of data, model scale, and language pair.\n","authors":["Kosuke Akimoto","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2410.12325v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.12323v1","updated":"2024-10-16T07:44:28Z","published":"2024-10-16T07:44:28Z","title":"Reversal of Thought: Enhancing Large Language Models with\n  Preference-Guided Reverse Reasoning Warm-up","summary":"  Large language models (LLMs) have shown remarkable performance in reasoning\ntasks but face limitations in mathematical and complex logical reasoning.\nExisting methods to improve LLMs' logical capabilities either involve traceable\nor verifiable logical sequences that generate more reliable responses by\nconstructing logical structures yet increase computational costs, or introduces\nrigid logic template rules, reducing flexibility. In this paper, we propose\nReversal of Thought (RoT), a novel framework aimed at enhancing the logical\nreasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning\nwarm-up strategy, which integrates logical symbols for pseudocode planning\nthrough meta-cognitive mechanisms and pairwise preference self-evaluation to\ngenerate task-specific prompts solely through demonstrations, aligning with\nLLMs' cognitive preferences shaped by Reinforcement Learning with Human\nFeedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference\nManager to assess knowledge boundaries and further expand LLMs' reasoning\ncapabilities by aggregating solution logic for known tasks and stylistic\ntemplates for unknown tasks. Experiments across various tasks demonstrate that\nRoT surpasses existing baselines in both reasoning accuracy and efficiency.\n","authors":["Jiahao Yuan","Dehui Du","Hao Zhang","Zixiang Di","Usman Naseem"],"pdf_url":"https://arxiv.org/pdf/2410.12323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12311v1","updated":"2024-10-16T07:24:28Z","published":"2024-10-16T07:24:28Z","title":"Open Domain Question Answering with Conflicting Contexts","summary":"  Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts.\n","authors":["Siyi Liu","Qiang Ning","Kishaloy Halder","Wei Xiao","Zheng Qi","Phu Mon Htut","Yi Zhang","Neha Anna John","Bonan Min","Yassine Benajiba","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2410.12311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07167v2","updated":"2024-10-16T07:23:03Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v2.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2406.13993v2","updated":"2024-10-16T07:14:20Z","published":"2024-06-20T04:44:20Z","title":"Exploring Changes in Nation Perception with Nationality-Assigned\n  Personas in LLMs","summary":"  Persona assignment has become a common strategy for customizing LLM use to\nparticular tasks and contexts. In this study, we explore how evaluation of\ndifferent nations change when LLMs are assigned specific nationality personas.\nWe assign 193 different nationality personas (e.g., an American person) to four\nLLMs and examine how the LLM evaluations (or ''perceptions'')of countries\nchange. We find that all LLM-persona combinations tend to favor Western\nEuropean nations, though nation-personas push LLM behaviors to focus more on\nand treat the nation-persona's own region more favorably. Eastern European,\nLatin American, and African nations are treated more negatively by different\nnationality personas. We additionally find that evaluations by nation-persona\nLLMs of other nations correlate with human survey responses but fail to match\nthe values closely. Our study provides insight into how biases and stereotypes\nare realized within LLMs when adopting different national personas. In line\nwith the ''Blueprint for an AI Bill of Rights'', our findings underscore the\ncritical need for developing mechanisms to ensure that LLM outputs promote\nfairness and avoid over-generalization.\n","authors":["Mahammed Kamruzzaman","Gene Louis Kim"],"pdf_url":"https://arxiv.org/pdf/2406.13993v2.pdf","comment":"Pre-print, Under review"},{"id":"http://arxiv.org/abs/2410.05581v2","updated":"2024-10-16T07:07:20Z","published":"2024-10-08T00:37:16Z","title":"Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes\n  Fail to Improve?","summary":"  In the last decade, the generalization and adaptation abilities of deep\nlearning models were typically evaluated on fixed training and test\ndistributions. Contrary to traditional deep learning, large language models\n(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text\ncorpora curated from the Internet with minimal human intervention, and (iii)\ntrained in an online fashion. These stark contrasts prevent researchers from\ntransferring lessons learned on model generalization and adaptation in deep\nlearning contexts to LLMs. To this end, our short paper introduces empirical\nobservations that aim to shed light on further training of already pretrained\nlanguage models. Specifically, we demonstrate that training a model on a text\ndomain could degrade its perplexity on the test portion of the same domain. We\nobserve with our subsequent analysis that the performance degradation is\npositively correlated with the similarity between the additional and the\noriginal pretraining dataset of the LLM. Our further token-level perplexity\nobservations reveals that the perplexity degradation is due to a handful of\ntokens that are not informative about the domain. We hope these findings will\nguide us in determining when to adapt a model vs when to rely on its\nfoundational capabilities.\n","authors":["Fırat Öncel","Matthias Bethge","Beyza Ermis","Mirco Ravanelli","Cem Subakan","Çağatay Yıldız"],"pdf_url":"https://arxiv.org/pdf/2410.05581v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.12299v1","updated":"2024-10-16T06:58:49Z","published":"2024-10-16T06:58:49Z","title":"Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering\n  Vectors","summary":"  Large language models (LLMs) have achieved remarkable performance across many\ntasks, yet aligning them with desired behaviors remains challenging. Activation\nintervention has emerged as an effective and economical method to modify the\nbehavior of LLMs. Despite considerable interest in this area, current\nintervention methods exclusively employ a fixed steering vector to modify model\nactivations, lacking adaptability to diverse input semantics. To address this\nlimitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel\nmethod that constructs a dynamic steering vector to intervene model activations\nat inference time. More specifically, SADI utilizes activation differences in\ncontrastive pairs to precisely identify critical elements of an LLM (i.e.,\nattention heads, hidden states, and neurons) for targeted intervention. During\ninference, SADI dynamically steers model behavior by scaling element-wise\nactivations based on the directions of input semantics. Experimental results\nshow that SADI outperforms established baselines by substantial margins,\nimproving task performance without training. SADI's cost-effectiveness and\ngeneralizability across various LLM backbones and tasks highlight its potential\nas a versatile alignment technique. In addition, we release the code to foster\nresearch along this line:https://github.com/weixuan-wang123/SADI.\n","authors":["Weixuan Wang","Jingyuan Yang","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2410.12299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12298v1","updated":"2024-10-16T06:57:18Z","published":"2024-10-16T06:57:18Z","title":"Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large\n  Language Models and Knowledge Graphs","summary":"  Large Language Models (LLMs) possess impressive reasoning abilities but are\nprone to generating incorrect information, often referred to as hallucinations.\nWhile incorporating external Knowledge Graphs (KGs) can partially mitigate this\nissue, existing methods primarily treat KGs as static knowledge repositories,\noverlooking the critical disparity between KG and LLM knowledge, and failing to\nfully exploit the reasoning capabilities inherent in KGs. To address these\nlimitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for\nseamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis\nto construct a hierarchical pyramid structure. This structure is designed to\nreflect the input question and generate more validated deductive knowledge,\nthereby enhancing the alignment of LLMs and KGs and ensuring more cohesive\nintegration. Furthermore, PDA employs a recursive mechanism to harness the\nunderlying reasoning abilities of KGs, resulting in more accurate knowledge\nretrieval for question-answering tasks. Our experimental results reveal a\nsubstantial performance advantage of PDA over state-of-the-art baselines, with\nimprovements reaching 26.70% and 26.78%.\n","authors":["Lei Sun","Xinchen Wang","Youdi Li"],"pdf_url":"https://arxiv.org/pdf/2410.12298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12294v1","updated":"2024-10-16T06:51:09Z","published":"2024-10-16T06:51:09Z","title":"Towards LLM-based Cognitive Models of Students with Misconceptions","summary":"  Accurately modeling student cognition is crucial for developing effective\nAI-driven educational technologies. A key challenge is creating realistic\nstudent models that satisfy two essential properties: (1) accurately\nreplicating specific misconceptions, and (2) correctly solving problems where\nthese misconceptions are not applicable. This dual requirement reflects the\ncomplex nature of student understanding, where misconceptions coexist with\ncorrect knowledge. This paper investigates whether Large Language Models (LLMs)\ncan be instruction-tuned to meet this dual requirement and effectively simulate\nstudent thinking in algebra. We introduce MalAlgoPy, a novel Python library\nthat generates datasets reflecting authentic student solution patterns through\na graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,\nwe define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned\nto faithfully emulate realistic student behavior. Our findings reveal that LLMs\ntrained on misconception examples can efficiently learn to replicate errors.\nHowever, the training diminishes the model's ability to solve problems\ncorrectly, particularly for problem types where the misconceptions are not\napplicable, thus failing to satisfy second property of CSMs. We demonstrate\nthat by carefully calibrating the ratio of correct to misconception examples in\nthe training data - sometimes as low as 0.25 - it is possible to develop CSMs\nthat satisfy both properties. Our insights enhance our understanding of\nAI-based student models and pave the way for effective adaptive learning\nsystems.\n","authors":["Shashank Sonkar","Xinghe Chen","Naiming Liu","Richard G. Baraniuk","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.12294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12292v1","updated":"2024-10-16T06:49:54Z","published":"2024-10-16T06:49:54Z","title":"How much do contextualized representations encode long-range context?","summary":"  We analyze contextual representations in neural autoregressive language\nmodels, emphasizing long-range contexts that span several thousand tokens. Our\nmethodology employs a perturbation setup and the metric\n\\emph{Anisotropy-Calibrated Cosine Similarity}, to capture the degree of\ncontextualization of long-range patterns from the perspective of representation\ngeometry. We begin the analysis with a case study on standard decoder-only\nTransformers, demonstrating that similar perplexity can exhibit markedly\ndifferent downstream task performance, which can be explained by the difference\nin contextualization of long-range content. Next, we extend the analysis to\nother models, covering recent novel architectural designs and various training\nconfigurations. The representation-level results illustrate a reduced capacity\nfor high-complexity (i.e., less compressible) sequences across architectures,\nand that fully recurrent models rely heavily on local context, whereas hybrid\nmodels more effectively encode the entire sequence structure. Finally,\npreliminary analysis of model size and training configurations on the encoding\nof long-range context suggest potential directions for improving existing\nlanguage models.\n","authors":["Simeng Sun","Cheng-Ping Hsieh"],"pdf_url":"https://arxiv.org/pdf/2410.12292v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12288v1","updated":"2024-10-16T06:47:18Z","published":"2024-10-16T06:47:18Z","title":"A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context\n  Reasoning","summary":"  Extensive knowledge graphs (KGs) have been constructed to facilitate\nknowledge-driven tasks across various scenarios. However, existing work usually\ndevelops separate reasoning models for different KGs, lacking the ability to\ngeneralize and transfer knowledge across diverse KGs and reasoning settings. In\nthis paper, we propose a prompt-based KG foundation model via in-context\nlearning, namely KG-ICL, to achieve a universal reasoning ability.\nSpecifically, we introduce a prompt graph centered with a query-related example\nfact as context to understand the query relation. To encode prompt graphs with\nthe generalization ability to unseen entities and relations in queries, we\nfirst propose a unified tokenizer that maps entities and relations in prompt\ngraphs to predefined tokens. Then, we propose two message passing neural\nnetworks to perform prompt encoding and KG reasoning, respectively. We conduct\nevaluation on 43 different KGs in both transductive and inductive settings.\nResults indicate that the proposed KG-ICL outperforms baselines on most\ndatasets, showcasing its outstanding generalization and universal reasoning\ncapabilities. The source code is accessible on GitHub:\nhttps://github.com/nju-websoft/KG-ICL.\n","authors":["Yuanning Cui","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2410.12288v1.pdf","comment":"Accepted in the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.12284v1","updated":"2024-10-16T06:43:02Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12279v1","updated":"2024-10-16T06:35:56Z","published":"2024-10-16T06:35:56Z","title":"Beyond Oversmoothing: Evaluating DDPM and MSE for Scalable Speech\n  Synthesis in ASR","summary":"  Synthetically generated speech has rapidly approached human levels of\nnaturalness. However, the paradox remains that ASR systems, when trained on TTS\noutput that is judged as natural by humans, continue to perform badly on real\nspeech. In this work, we explore whether this phenomenon is due to the\noversmoothing behaviour of models commonly used in TTS, with a particular focus\non the behaviour of TTS-for-ASR as the amount of TTS training data is scaled\nup. We systematically compare Denoising Diffusion Probabilistic Models (DDPM)\nto Mean Squared Error (MSE) based models for TTS, when used for ASR model\ntraining. We test the scalability of the two approaches, varying both the\nnumber hours, and the number of different speakers. We find that for a given\nmodel size, DDPM can make better use of more data, and a more diverse set of\nspeakers, than MSE models. We achieve the best reported ratio between real and\nsynthetic speech WER to date (1.46), but also find that a large gap remains.\n","authors":["Christoph Minixhofer","Ondrej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2410.12279v1.pdf","comment":"Under review at ICASSP 2025"},{"id":"http://arxiv.org/abs/2406.14036v2","updated":"2024-10-16T06:33:44Z","published":"2024-06-20T06:56:35Z","title":"Towards Infinite-Long Prefix in Transformer","summary":"  Prompting and context-based fine-tuning methods, which we call Prefix\nLearning, have been proposed to enhance the performance of language models on\nvarious downstream tasks. They are empirically efficient and effective,\nmatching the performance of full parameter fine-tuning, but the theoretical\nunderstandings are limited. In this paper, we aim to address this limitation by\nstudying their ability from the perspective of prefix length. In particular, we\nprovide a convergence guarantee for training an ultra-long prefix in a stylized\nsetting using the Neural Tangent Kernel (NTK) framework. Based on this strong\ntheoretical guarantee, we design and implement an algorithm that only needs to\nintroduce and fine-tune a few extra trainable parameters instead of an\ninfinite-long prefix in each layer of a transformer, and can approximate the\nprefix attention to a guaranteed polynomial-small error. Preliminary\nexperimental results on vision, natural language, and math data show that our\nmethod achieves superior or competitive performance compared to existing\nmethods like full parameters fine-tuning, P-Tuning V2, and LoRA. This\ndemonstrates our method is promising for parameter-efficient fine-tuning. Our\ncode can be found at\n\\url{https://github.com/ChristianYang37/chiwun/tree/main/src/NTK-Attention}.\n","authors":["Yingyu Liang","Zhenmei Shi","Zhao Song","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2406.14036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11258v2","updated":"2024-10-16T06:32:50Z","published":"2024-06-17T06:48:31Z","title":"SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented\n  Generation","summary":"  Large Language Models (LLMs) have shown great potential in the biomedical\ndomain with the advancement of retrieval-augmented generation (RAG). However,\nexisting retrieval-augmented approaches face challenges in addressing diverse\nqueries and documents, particularly for medical knowledge queries, resulting in\nsub-optimal performance. To address these limitations, we propose a novel\nplug-and-play LLM-based retrieval method called Self-Rewarding Tree Search\n(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.\nBy combining the reasoning capabilities of LLMs with the effectiveness of tree\nsearch, SeRTS boosts the zero-shot performance of retrieving high-quality and\ninformative results for RAG. We further enhance retrieval performance by\nfine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the\ntrajectories collected by SeRTS as feedback. Controlled experiments using the\nBioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method\nsignificantly improves the performance of the BM25 retriever and surpasses the\nstrong baseline of self-reflection in both efficiency and scalability.\nMoreover, SeRTS generates higher-quality feedback for PPO training than\nself-reflection. Our proposed method effectively adapts LLMs to document\nretrieval tasks, enhancing their ability to retrieve highly relevant documents\nfor RAG in the context of medical knowledge queries. This work presents a\nsignificant step forward in leveraging LLMs for accurate and comprehensive\nbiomedical question answering.\n","authors":["Minda Hu","Licheng Zong","Hongru Wang","Jingyan Zhou","Jingjing Li","Yichen Gao","Kam-Fai Wong","Yu Li","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2406.11258v2.pdf","comment":"This work has been accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12278v1","updated":"2024-10-16T06:31:59Z","published":"2024-10-16T06:31:59Z","title":"Controlled Automatic Task-Specific Synthetic Data Generation for\n  Hallucination Detection","summary":"  We present a novel approach to automatically generate non-trivial\ntask-specific synthetic datasets for hallucination detection. Our approach\nfeatures a two-step generation-selection pipeline, using hallucination pattern\nguidance and a language style alignment during generation. Hallucination\npattern guidance leverages the most important task-specific hallucination\npatterns while language style alignment aligns the style of the synthetic\ndataset with benchmark text. To obtain robust supervised detectors from\nsynthetic datasets, we also adopt a data mixture strategy to improve\nperformance robustness and generalization. Our results on three datasets show\nthat our generated hallucination text is more closely aligned with\nnon-hallucinated text versus baselines, to train hallucination detectors with\nbetter generalization. Our hallucination detectors trained on synthetic\ndatasets outperform in-context-learning (ICL)-based detectors by a large margin\nof 32%. Our extensive experiments confirm the benefits of our approach with\ncross-task and cross-generator generalization. Our data-mixture-based training\nfurther improves the generalization and robustness of hallucination detection.\n","authors":["Yong Xie","Karan Aggarwal","Aitzaz Ahmad","Stephen Lau"],"pdf_url":"https://arxiv.org/pdf/2410.12278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11291v2","updated":"2024-10-16T06:25:57Z","published":"2024-10-15T05:26:57Z","title":"Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset\n  Repository","summary":"  This paper introduces a centralized, open-source dataset repository designed\nto advance NLP and NMT for Assamese, a low-resource language. The repository,\navailable at GitHub, supports various tasks like sentiment analysis, named\nentity recognition, and machine translation by providing both pre-training and\nfine-tuning corpora. We review existing datasets, highlighting the need for\nstandardized resources in Assamese NLP, and discuss potential applications in\nAI-driven research, such as LLMs, OCR, and chatbots. While promising,\nchallenges like data scarcity and linguistic diversity remain. The repository\naims to foster collaboration and innovation, promoting Assamese language\nresearch in the digital age.\n","authors":["S. Tamang","D. J. Bora"],"pdf_url":"https://arxiv.org/pdf/2410.11291v2.pdf","comment":"6 pages, 1 table, 1 figure"},{"id":"http://arxiv.org/abs/2407.12508v2","updated":"2024-10-16T06:25:50Z","published":"2024-07-17T11:45:02Z","title":"MERLIN: Multimodal Embedding Refinement via LLM-based Iterative\n  Navigation for Text-Video Retrieval-Rerank Pipeline","summary":"  The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.\n","authors":["Donghoon Han","Eunhwan Park","Gisang Lee","Adam Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.12508v2.pdf","comment":"EMNLP 2024 Industry Track Accepted (Camera-Ready Version)"},{"id":"http://arxiv.org/abs/2410.12271v1","updated":"2024-10-16T06:16:30Z","published":"2024-10-16T06:16:30Z","title":"Kallini et al. (2024) do not compare impossible languages with\n  constituency-based ones","summary":"  A central goal of linguistic theory is to find a precise characterization of\nthe notion \"possible human language\", in the form of a computational device\nthat is capable of describing all and only the languages that can be acquired\nby a typically developing human child. The success of recent large language\nmodels (LLMs) in NLP applications arguably raises the possibility that LLMs\nmight be computational devices that meet this goal. This would only be the case\nif, in addition to succeeding in learning human languages, LLMs struggle to\nlearn \"impossible\" human languages. Kallini et al. (2024; \"Mission: Impossible\nLanguage Models\", Proc. ACL) conducted experiments aiming to test this by\ntraining GPT-2 on a variety of synthetic languages, and found that it learns\nsome more successfully than others. They present these asymmetries as support\nfor the idea that LLMs' inductive biases align with what is regarded as\n\"possible\" for human languages, but the most significant comparison has a\nconfound that makes this conclusion unwarranted. In this paper I explain the\nconfound and suggest some ways forward towards constructing a comparison that\nappropriately tests the underlying issue.\n","authors":["Tim Hunter"],"pdf_url":"https://arxiv.org/pdf/2410.12271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04070v3","updated":"2024-10-16T06:15:35Z","published":"2024-10-05T08:00:55Z","title":"PAD: Personalized Alignment at Decoding-Time","summary":"  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n","authors":["Ruizhe Chen","Xiaotian Zhang","Meng Luo","Wenhao Chai","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.04070v3.pdf","comment":"This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase"},{"id":"http://arxiv.org/abs/2410.12265v1","updated":"2024-10-16T06:06:06Z","published":"2024-10-16T06:06:06Z","title":"An Automatic and Cost-Efficient Peer-Review Framework for Language\n  Generation Evaluation","summary":"  With the rapid development of large language models (LLMs), how to\nefficiently evaluate them has become an important research question. Existing\nevaluation methods often suffer from high costs, limited test formats, the need\nof human references, and systematic evaluation biases. To address these\nlimitations, our study introduces the Auto-PRE, an automatic LLM evaluation\nframework based on peer review. In contrast to previous studies that rely on\nhuman annotations, Auto-PRE selects evaluator LLMs automatically based on their\ninherent traits including consistency, self-confidence, and pertinence. We\nconduct extensive experiments on three tasks: summary generation, non-factoid\nquestion-answering, and dialogue generation. Experimental results indicate our\nAuto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our\nstudy highlights the impact of prompt strategies and evaluation formats on\nevaluation performance, offering guidance for method optimization in the\nfuture.\n","authors":["Junjie Chen","Weihang Su","Zhumin Chu","Haitao Li","Qinyao Ai","Yiqun Liu","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10148v2","updated":"2024-10-16T05:59:33Z","published":"2024-10-14T04:29:57Z","title":"$α$-DPO: Adaptive Reward Margin is What Direct Preference\n  Optimization Needs","summary":"  Aligning large language models (LLMs) with human values and intentions is\ncrucial for their utility, honesty, and safety. Reinforcement learning from\nhuman feedback (RLHF) is a popular approach to achieve this alignment, but it\nfaces challenges in computational efficiency and training stability. Recent\nmethods like Direct Preference Optimization (DPO) and Simple Preference\nOptimization (SimPO) have proposed offline alternatives to RLHF, simplifying\nthe process by reparameterizing the reward function. However, DPO depends on a\npotentially suboptimal reference model, and SimPO's assumption of a fixed\ntarget reward margin may lead to suboptimal decisions in diverse data settings.\nIn this work, we propose $\\alpha$-DPO, an adaptive preference optimization\nalgorithm designed to address these limitations by introducing a dynamic reward\nmargin. Specifically, $\\alpha$-DPO employs an adaptive preference distribution,\nbalancing the policy model and the reference model to achieve personalized\nreward margins. We provide theoretical guarantees for $\\alpha$-DPO,\ndemonstrating its effectiveness as a surrogate optimization objective and its\nability to balance alignment and diversity through KL divergence control.\nEmpirical evaluations on AlpacaEval 2 and Arena-Hard show that $\\alpha$-DPO\nconsistently outperforms DPO and SimPO across various model settings,\nestablishing it as a robust approach for fine-tuning LLMs. Our method achieves\nsignificant improvements in win rates, highlighting its potential as a powerful\ntool for LLM alignment. The code is available at\nhttps://github.com/junkangwu/alpha-DPO\n","authors":["Junkang Wu","Xue Wang","Zhengyi Yang","Jiancan Wu","Jinyang Gao","Bolin Ding","Xiang Wang","Rong Jin","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.10148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10792v6","updated":"2024-10-16T05:44:07Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v6.pdf","comment":"V3; Last update: Oct 16, 2024"},{"id":"http://arxiv.org/abs/2406.11632v2","updated":"2024-10-16T05:22:53Z","published":"2024-06-17T15:13:52Z","title":"Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding\n  for Neural Machine Translation","summary":"  Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding (\\citealp{kumar2004minimum}) offers\nan alternative by seeking hypotheses with the highest expected utility. In this\npaper, we show that Quality Estimation (QE) reranking\n(\\citealp{fernandes-etal-2022-quality}), which uses a QE model as a reranker,\ncan be viewed as a variant of MBR. Inspired by this, we propose source-based\nMBR (sMBR) decoding, a novel approach that utilizes synthetic sources\n(generated via back-translation or paraphrasing) as ``support hypotheses'' and\na reference-free quality estimation metric as the utility function, marking the\nfirst work to solely use sources in MBR decoding. Experiments show that sMBR\noutperforms QE reranking and the standard MBR decoding. Our findings suggest\nthat sMBR is a promising approach for NMT decoding.\n","authors":["Boxuan Lyu","Hidetaka Kamigaito","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2406.11632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12248v1","updated":"2024-10-16T05:20:32Z","published":"2024-10-16T05:20:32Z","title":"CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for\n  Retrieval-Augmented Generation with Enhanced Data Diversity","summary":"  Retrieval-Augmented Generation (RAG) aims to enhance large language models\n(LLMs) to generate more accurate and reliable answers with the help of the\nretrieved context from external knowledge sources, thereby reducing the\nincidence of hallucinations. Despite the advancements, evaluating these systems\nremains a crucial research area due to the following issues: (1) Limited data\ndiversity: The insufficient diversity of knowledge sources and query types\nconstrains the applicability of RAG systems; (2) Obscure problems location:\nExisting evaluation methods have difficulty in locating the stage of the RAG\npipeline where problems occur; (3) Unstable retrieval evaluation: These methods\noften fail to effectively assess retrieval performance, particularly when the\nchunking strategy changes. To tackle these challenges, we propose a\nComprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough\nevaluation across the entire RAG pipeline, including chunking, retrieval,\nreranking, and generation. To effectively evaluate the first three phases, we\nintroduce multi-granularity keywords, including coarse-grained and fine-grained\nkeywords, to assess the retrieved context instead of relying on the annotation\nof golden chunks. Moreover, we release a holistic benchmark dataset tailored\nfor diverse data scenarios covering a wide range of document formats and query\ntypes. We demonstrate the utility of the CoFE-RAG framework by conducting\nexperiments to evaluate each stage of RAG systems. Our evaluation method\nprovides unique insights into the effectiveness of RAG systems in handling\ndiverse data scenarios, offering a more nuanced understanding of their\ncapabilities and limitations.\n","authors":["Jintao Liu","Ruixue Ding","Linhao Zhang","Pengjun Xie","Fie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.12248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12247v1","updated":"2024-10-16T05:17:49Z","published":"2024-10-16T05:17:49Z","title":"EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference","summary":"  Large Language Model (LLM) has revolutionized the field of artificial\nintelligence, with their capabilities expanding rapidly due to advances in deep\nlearning and increased computational resources. The mixture-of-experts (MoE)\nmodel has emerged as a prominent architecture in the field of LLM, better\nbalancing the model performance and computational efficiency. MoE architecture\nallows for effective scaling and efficient parallel processing, but the GEMM\n(General Matrix Multiply) of MoE and the large parameters introduce challenges\nin terms of computation efficiency and communication overhead, which becomes\nthe throughput bottleneck during inference. Applying a single parallelism\nstrategy like EP, DP, PP, etc. to MoE architecture usually achieves sub-optimal\ninference throughput, the straightforward combinations of existing different\nparallelisms on MoE can not obtain optimal inference throughput yet. This paper\nintroduces EPS-MoE, a novel expert pipeline scheduler for MoE that goes beyond\nthe existing inference parallelism schemes. Our approach focuses on optimizing\nthe computation of MoE FFN (FeedForward Network) modules by dynamically\nselecting the best kernel implementation of GroupGemm and DenseGemm for\ndifferent loads and adaptively overlapping these computations with\n\\textit{all2all} communication, leading to a substantial increase in\nthroughput. Our experimental results demonstrate an average 21% improvement in\nprefill throughput over existing parallel inference methods. Specifically, we\nvalidated our method on DeepSeekV2, a highly optimized model claimed to achieve\na prefill throughput of 100K tokens per second. By applying EPS-MoE, we further\naccelerated it to at least 120K tokens per second.\n","authors":["Yulei Qian","Fengcun Li","Xiangyang Ji","Xiaoyu Zhao","Jianchao Tan","Kefeng Zhang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2410.12247v1.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.06949v2","updated":"2024-10-16T05:04:45Z","published":"2024-10-09T14:45:45Z","title":"Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach","summary":"  In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.\n","authors":["Xuanming Zhang","Yuxuan Chen","Yuan Yuan","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06949v2.pdf","comment":"26 pages, 7 figures. Submitted ICLR 2025"},{"id":"http://arxiv.org/abs/2406.14867v2","updated":"2024-10-16T05:03:04Z","published":"2024-06-21T05:05:39Z","title":"Investigating the Transferability of Code Repair for Low-Resource\n  Programming Languages","summary":"  Large language models (LLMs) have shown remarkable performance on code\ngeneration tasks. A recent use case is iterative code repair, where an LLM\nfixes an incorrect program by rationalizing about errors and generating new\ncode. Recent works augment the code repair process by integrating modern\ntechniques such as chain-of-thought reasoning or distillation, but only study\ntheir benefits on high-resource languages like Python, and ignore low-resource\nlanguages like Perl. To address this gap of knowledge, we investigate the\nbenefits of distilling code repair for both high and low resource languages to\ndetermine if the techniques that are effective in a high resource setting are\nalso applicable in a low resource setting. Our evaluation shows that distilling\nthe ability to repair code has language dependent benefits. To explain this\nbehavior, we perform a further analysis and find that contrary to preexisting\nbeliefs, the correlation between reasoning ability and code correction ability\nis weak. We hypothesize this weak correlation is magnified in low-resource\nsettings where base models lack deep knowledge of a programming language,\nleading to wavering benefits of code repair.\n","authors":["Kyle Wong","Alfonso Amayuelas","Liangming Pan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06554v2","updated":"2024-10-16T04:48:08Z","published":"2024-10-09T05:17:08Z","title":"The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield\n  Better Language Models","summary":"  Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at https://github.com/EIT-NLP/AccuracyParadox-RLHF.\n","authors":["Yanjun Chen","Dawei Zhu","Yirong Sun","Xinghao Chen","Wei Zhang","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06554v2.pdf","comment":"10 pages, 27 figures (including 18 in the appendix), submitted to\n  EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.12228v1","updated":"2024-10-16T04:44:15Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12222v1","updated":"2024-10-16T04:36:17Z","published":"2024-10-16T04:36:17Z","title":"On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness\n  Evaluation","summary":"  Hallucination has been a popular topic in natural language generation (NLG).\nIn real-world applications, unfaithful content can result in bad data quality\nor loss of trust from end users. Thus, it is crucial to fact-check before\nadopting NLG for production usage, which can be expensive if done manually. In\nthis paper, we investigate automated faithfulness evaluation in guided NLG. We\ndeveloped a rubrics template and use large language models (LLMs) to score the\ngeneration into quantifiable scales. We compared popular LLMs as well as the\nwidely adopted natural language inference (NLI) models in scoring quality and\nsensitivity. In addition, we developed methods to generation synthetic\nunfaithful data, as well as a heuristics to quantify the percentage of\nhallucination. Our results on 4 travel-domain industry dataset show that GPT-4\ncan provide accurate judgement and explanation on whether a source and a\ngeneration are factually consistent. Furthermore, we found that tuning NLI\nmodels on synthetic data can improve performance. Lastly, we present insights\non latency and cost for deploying such system.\n","authors":["Xiaonan Jing","Srinivas Billa","Danny Godbout"],"pdf_url":"https://arxiv.org/pdf/2410.12222v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2406.17232v2","updated":"2024-10-16T04:36:09Z","published":"2024-06-25T02:37:29Z","title":"Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human\n  Belief Networks","summary":"  Creating human-like large language model (LLM) agents is crucial for faithful\nsocial simulation. Having LLMs role-play based on demographic information\nsometimes improves human likeness but often does not. This study assessed\nwhether LLM alignment with human behavior can be improved by integrating\ninformation from empirically-derived human belief networks. Using data from a\nhuman survey, we estimated a belief network encompassing 64 topics loading on\nnine non-overlapping latent factors. We then seeded LLM-based agents with an\nopinion on one topic, and assessed the alignment of its expressed opinions on\nremaining test topics with corresponding human data. Role-playing based on\ndemographic information alone did not align LLM and human opinions, but seeding\nthe agent with a single belief greatly improved alignment for topics related in\nthe belief network, and not for topics outside the network. These results\nsuggest a novel path for human-LLM belief alignment in work seeking to simulate\nand understand patterns of belief distributions in society.\n","authors":["Yun-Shiuan Chuang","Krirk Nirunwiroj","Zach Studdiford","Agam Goyal","Vincent V. Frigo","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2406.17232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12219v1","updated":"2024-10-16T04:29:46Z","published":"2024-10-16T04:29:46Z","title":"OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities","summary":"  We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.\n","authors":["Lichang Chen","Hexiang Hu","Mingda Zhang","Yiwen Chen","Zifeng Wang","Yandong Li","Pranav Shyam","Tianyi Zhou","Heng Huang","Ming-Hsuan Yang","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2410.12219v1.pdf","comment":"19 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.12217v1","updated":"2024-10-16T04:26:40Z","published":"2024-10-16T04:26:40Z","title":"Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree","summary":"  When annotators disagree, predicting the labels given by individual\nannotators can capture nuances overlooked by traditional label aggregation. We\nintroduce three approaches to predicting individual annotator ratings on the\ntoxicity of text by incorporating individual annotator-specific information: a\nneural collaborative filtering (NCF) approach, an in-context learning (ICL)\napproach, and an intermediate embedding-based architecture. We also study the\nutility of demographic information for rating prediction. NCF showed limited\nutility; however, integrating annotator history, demographics, and survey\ninformation permits both the embedding-based architecture and ICL to\nsubstantially improve prediction accuracy, with the embedding-based\narchitecture outperforming the other methods. We also find that, if\ndemographics are predicted from survey information, using these imputed\ndemographics as features performs comparably to using true demographic data.\nThis suggests that demographics may not provide substantial information for\nmodeling ratings beyond what is captured in survey responses. Our findings\nraise considerations about the relative utility of different types of annotator\ninformation and provide new approaches for modeling annotators in subjective\nNLP tasks.\n","authors":["Harbani Jaggi","Kashyap Murali","Eve Fleisig","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2410.12217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20246v5","updated":"2024-10-16T04:26:04Z","published":"2023-10-31T08:09:20Z","title":"Breaking Language Barriers in Multilingual Mathematical Reasoning:\n  Insights and Observations","summary":"  Existing research predominantly focuses on developing powerful language\nlearning models (LLMs) for mathematical reasoning within monolingual languages,\nwith few explorations in preserving efficacy in a multilingual context. To\nbridge this gap, this paper pioneers exploring and training powerful\nMultilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we\nconstruct the first multilingual math reasoning instruction dataset,\nMGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue\nof training data scarcity in xMR tasks. Based on the collected dataset, we\npropose different training strategies to build powerful xMR LLMs, named\nMathOctopus, notably outperform conventional open-source LLMs and exhibit\nsuperiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B\nreaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond\nremarkable results, we unearth several pivotal observations and insights from\nextensive experiments: (1) When extending the rejection sampling strategy to\nthe multilingual context, it proves effective for model performances, albeit\nlimited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)\nacross multiple languages not only significantly enhances model performance\nmultilingually but also elevates their monolingual performance. This indicates\nthat crafting multilingual corpora can be regarded as a vital strategy for\nenhancing model performance in a specific language, especially in mathematical\nreasoning tasks. For instance, MathOctopus-7B improves its counterparts that\ntrained on English from 42.2% to 50.8% on GSM8K testset. Codes are available at\nhttps://github.com/microsoft/MathOctopus.\n","authors":["Nuo Chen","Zinan Zheng","Ning Wu","Ming Gong","Dongmei Zhang","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2310.20246v5.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2407.17487v3","updated":"2024-10-16T04:24:59Z","published":"2024-07-03T08:27:51Z","title":"Explainable Natural Language Processing for Corporate Sustainability\n  Analysis","summary":"  Sustainability commonly refers to entities, such as individuals, companies,\nand institutions, having a non-detrimental (or even positive) impact on the\nenvironment, society, and the economy. With sustainability becoming a synonym\nof acceptable and legitimate behaviour, it is being increasingly demanded and\nregulated. Several frameworks and standards have been proposed to measure the\nsustainability impact of corporations, including United Nations' sustainable\ndevelopment goals and the recently introduced global sustainability reporting\nframework, amongst others. However, the concept of corporate sustainability is\ncomplex due to the diverse and intricate nature of firm operations (i.e.\ngeography, size, business activities, interlinks with other stakeholders). As a\nresult, corporate sustainability assessments are plagued by subjectivity both\nwithin data that reflect corporate sustainability efforts (i.e. corporate\nsustainability disclosures) and the analysts evaluating them. This subjectivity\ncan be distilled into distinct challenges, such as incompleteness, ambiguity,\nunreliability and sophistication on the data dimension, as well as limited\nresources and potential bias on the analyst dimension. Put together,\nsubjectivity hinders effective cost attribution to entities non-compliant with\nprevailing sustainability expectations, potentially rendering sustainability\nefforts and its associated regulations futile. To this end, we argue that\nExplainable Natural Language Processing (XNLP) can significantly enhance\ncorporate sustainability analysis. Specifically, linguistic understanding\nalgorithms (lexical, semantic, syntactic), integrated with XAI capabilities\n(interpretability, explainability, faithfulness), can bridge gaps in analyst\nresources and mitigate subjectivity problems within data.\n","authors":["Keane Ong","Rui Mao","Ranjan Satapathy","Ricardo Shirota Filho","Erik Cambria","Johan Sulaeman","Gianmarco Mengaldo"],"pdf_url":"https://arxiv.org/pdf/2407.17487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11086v2","updated":"2024-10-16T04:23:12Z","published":"2024-10-14T20:59:59Z","title":"JOOCI: a Framework for Learning Comprehensive Speech Representations","summary":"  Information in speech can be divided into two categories: what is being said\n(content) and how it is expressed (other). Current state-of-the-art (SOTA)\ntechniques model speech at fixed segments, usually 10-25 ms, using a single\nembedding. Given the orthogonal nature of other and content information,\nattempting to optimize both within a single embedding results in suboptimal\nsolutions. This approach divides the models capacity, limiting its ability to\nbuild complex hierarchical features effectively. In this work, we present an\nend-to-end speech representation learning framework designed to jointly\noptimize the other and content information (JOOCI) in speech. By using separate\nlearnable parameters, JOOCI addresses this optimization challenge by modeling\nother and content information independently. Our results show that JOOCI\nconsistently outperforms other SOTA models of similar size (100 million\nparameters) and pre-training data used (960 hours) by a significant margin when\nevaluated on a range of speech downstream tasks in the SUPERB benchmark, as\nshown in Table 1.\n","authors":["Hemant Yadav","Rajiv Ratn Shah","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.11086v2.pdf","comment":"Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.10861v2","updated":"2024-10-16T03:53:40Z","published":"2024-10-07T16:54:18Z","title":"Translation Canvas: An Explainable Interface to Pinpoint and Analyze\n  Translation Systems","summary":"  With the rapid advancement of machine translation research, evaluation\ntoolkits have become essential for benchmarking system progress. Tools like\nCOMET and SacreBLEU offer single quality score assessments that are effective\nfor pairwise system comparisons. However, these tools provide limited insights\nfor fine-grained system-level comparisons and the analysis of instance-level\ndefects. To address these limitations, we introduce Translation Canvas, an\nexplainable interface designed to pinpoint and analyze translation systems'\nperformance: 1) Translation Canvas assists machine translation researchers in\ncomprehending system-level model performance by identifying common errors\n(their frequency and severity) and analyzing relationships between different\nsystems based on various evaluation metrics. 2) It supports fine-grained\nanalysis by highlighting error spans with explanations and selectively\ndisplaying systems' predictions. According to human evaluation, Translation\nCanvas demonstrates superior performance over COMET and SacreBLEU packages\nunder enjoyability and understandability criteria.\n","authors":["Chinmay Dandekar","Wenda Xu","Xi Xu","Siqi Ouyang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.10861v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.12997v2","updated":"2024-10-16T03:52:13Z","published":"2024-06-18T18:37:24Z","title":"Discovering Elementary Discourse Units in Textual Data Using Canonical\n  Correlation Analysis","summary":"  Canonical Correlation Analysis (CCA) has been exploited immensely for\nlearning latent representations in various fields. This study takes a step\nfurther by demonstrating the potential of CCA in identifying Elementary\nDiscourse Units(EDUs) that captures the latent information within the textual\ndata. The probabilistic interpretation of CCA discussed in this study utilizes\nthe two-view nature of textual data, i.e. the consecutive sentences in a\ndocument or turns in a dyadic conversation, and has a strong theoretical\nfoundation. Furthermore, this study proposes a model for Elementary Discourse\nUnit(EDU) segmentation that discovers EDUs in textual data without any\nsupervision. To validate the model, the EDUs are utilized as textual unit for\ncontent selection in textual similarity task. Empirical results on Semantic\nTextual Similarity(STSB) and Mohler datasets confirm that, despite represented\nas a unigram, the EDUs deliver competitive results and can even beat various\nsophisticated supervised techniques. The model is simple, linear, adaptable and\nlanguage independent making it an ideal baseline particularly when labeled\ntraining data is scarce or nonexistent.\n","authors":["Akanksha Mehndiratta","Krishna Asawa"],"pdf_url":"https://arxiv.org/pdf/2406.12997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2407.07321v2","updated":"2024-10-16T03:33:58Z","published":"2024-07-10T02:33:09Z","title":"Examining Long-Context Large Language Models for Environmental Review\n  Document Comprehension","summary":"  As LLMs become increasingly ubiquitous, researchers have tried various\ntechniques to augment the knowledge provided to these models. Long context and\nretrieval-augmented generation (RAG) are two such methods that have recently\ngained popularity. In this work, we examine the benefits of both of these\ntechniques by utilizing question answering (QA) task in a niche domain. While\nthe effectiveness of LLM-based QA systems has already been established at an\nacceptable level in popular domains such as trivia and literature, it has not\noften been established in niche domains that traditionally require specialized\nexpertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance\nof five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and\nMistral -- when answering questions originating from Environmental Impact\nStatements prepared by U.S. federal government agencies in accordance with the\nNational Environmental Environmental Act (NEPA). We specifically measure the\nability of LLMs to understand the nuances of legal, technical, and\ncompliance-related information present in NEPA documents in different\ncontextual scenarios. We test the LLMs' internal prior NEPA knowledge by\nproviding questions without any context, as well as assess how LLMs synthesize\nthe contextual information present in long NEPA documents to facilitate the\nquestion/answering task. We compare the performance of the models in handling\ndifferent types of questions (e.g., problem-solving, divergent, etc.). Our\nresults suggest that RAG powered models significantly outperform those provided\nwith only the PDF context in terms of answer accuracy, regardless of the choice\nof the LLM. Our further analysis reveals that many models perform better\nanswering closed type questions (Yes/No) than divergent and problem-solving\nquestions.\n","authors":["Hung Phan","Anurag Acharya","Rounak Meyur","Sarthak Chaturvedi","Shivam Sharma","Mike Parker","Dan Nally","Ali Jannesari","Karl Pazdernik","Mahantesh Halappanavar","Sai Munikoti","Sameera Horawalavithana"],"pdf_url":"https://arxiv.org/pdf/2407.07321v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2410.12194v1","updated":"2024-10-16T03:30:09Z","published":"2024-10-16T03:30:09Z","title":"Negative-Prompt-driven Alignment for Generative Language Model","summary":"  Large language models have achieved remarkable capabilities, but aligning\ntheir outputs with human values and preferences remains a significant\nchallenge. Existing alignment methods primarily focus on positive examples\nwhile overlooking the importance of negative responses in guiding models away\nfrom undesirable behaviors. For instance, the widely-used alignment datasets\nreveals a scarcity of explicit negative examples that contradict human values,\nhindering its ability to discourage harmful or biased outputs during training.\nTo address this limitation, we propose NEAT, i.e., NEgative-prompt-driven\nAlignmenT, to introduce negative prompts to generate undesirable responses\nalongside positive examples during the optimization process. NEAT explicitly\npenalizes the model for producing harmful outputs, guiding it not only toward\ndesirable behaviors but also steering it away from generating undesirable,\nbiased responses. This dual feedback mechanism enables better alignment with\nhuman preferences, crucial in contexts where avoiding harm is paramount.\nStarting from a pre-trained language model, NEAT performs online alignment by\nincorporating a ranking loss derived from an expanded preference dataset\ncontaining both positive and negative examples. Extensive experiments validate\nNEAT's effectiveness in significantly enhancing language models' alignment with\nhuman values and preferences.\n","authors":["Shiqi Qiao","Ning Xv","Biao Liu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.12194v1.pdf","comment":null}]},"2024-10-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.13857v1","updated":"2024-10-17T17:59:35Z","published":"2024-10-17T17:59:35Z","title":"How Numerical Precision Affects Mathematical Reasoning Capabilities of\n  LLMs","summary":"  Despite the remarkable success of Transformer-based Large Language Models\n(LLMs) across various domains, understanding and enhancing their mathematical\ncapabilities remains a significant challenge. In this paper, we conduct a\nrigorous theoretical analysis of LLMs' mathematical abilities, with a specific\nfocus on their arithmetic performances. We identify numerical precision as a\nkey factor that influences their effectiveness in mathematical tasks. Our\nresults show that Transformers operating with low numerical precision fail to\naddress arithmetic tasks, such as iterated addition and integer multiplication,\nunless the model size grows super-polynomially with respect to the input\nlength. In contrast, Transformers with standard numerical precision can\nefficiently handle these tasks with significantly smaller model sizes. We\nfurther support our theoretical findings through empirical experiments that\nexplore the impact of varying numerical precision on arithmetic tasks,\nproviding valuable insights for improving the mathematical reasoning\ncapabilities of LLMs.\n","authors":["Guhao Feng","Kai Yang","Yuntian Gu","Xinyue Ai","Shengjie Luo","Jiacheng Sun","Di He","Zhenguo Li","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13854v1","updated":"2024-10-17T17:59:24Z","published":"2024-10-17T17:59:24Z","title":"Can MLLMs Understand the Deep Implication Behind Chinese Images?","summary":"  As the capabilities of Multimodal Large Language Models (MLLMs) continue to\nimprove, the need for higher-order capability evaluation of MLLMs is\nincreasing. However, there is a lack of work evaluating MLLM for higher-order\nperception and understanding of Chinese visual content. To fill the gap, we\nintroduce the **C**hinese **I**mage **I**mplication understanding\n**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception\nand understanding capabilities of MLLMs for Chinese images. CII-Bench stands\nout in several ways compared to existing benchmarks. Firstly, to ensure the\nauthenticity of the Chinese context, images in CII-Bench are sourced from the\nChinese Internet and manually reviewed, with corresponding answers also\nmanually crafted. Additionally, CII-Bench incorporates images that represent\nChinese traditional culture, such as famous Chinese traditional paintings,\nwhich can deeply reflect the model's understanding of Chinese traditional\nculture. Through extensive experiments on CII-Bench across multiple MLLMs, we\nhave made significant findings. Initially, a substantial gap is observed\nbetween the performance of MLLMs and humans on CII-Bench. The highest accuracy\nof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an\nimpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional\nculture images, suggesting limitations in their ability to understand\nhigh-level semantics and lack a deep knowledge base of Chinese traditional\nculture. Finally, it is observed that most models exhibit enhanced accuracy\nwhen image emotion hints are incorporated into the prompts. We believe that\nCII-Bench will enable MLLMs to gain a better understanding of Chinese semantics\nand Chinese-specific images, advancing the journey towards expert artificial\ngeneral intelligence (AGI). Our project is publicly available at\nhttps://cii-bench.github.io/.\n","authors":["Chenhao Zhang","Xi Feng","Yuelin Bai","Xinrun Du","Jinchang Hou","Kaixin Deng","Guangzeng Han","Qinrui Li","Bingli Wang","Jiaheng Liu","Xingwei Qu","Yifei Zhang","Qixuan Zhao","Yiming Liang","Ziqiang Liu","Feiteng Fang","Min Yang","Wenhao Huang","Chenghua Lin","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2410.13854v1.pdf","comment":"32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code:\n  https://github.com/MING_X/CII-Bench Dataset:\n  https://huggingface.co/datasets/m-a-p/CII-Bench"},{"id":"http://arxiv.org/abs/2410.13852v1","updated":"2024-10-17T17:59:03Z","published":"2024-10-17T17:59:03Z","title":"Retrospective Learning from Interactions","summary":"  Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.\n","authors":["Zizhao Chen","Mustafa Omer Gul","Yiwei Chen","Gloria Geng","Anne Wu","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2410.13852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08928v2","updated":"2024-10-17T17:58:53Z","published":"2024-10-11T15:53:24Z","title":"Towards Multilingual LLM Evaluation for European Languages","summary":"  The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of\nlanguage-parallel multilingual benchmarks. We introduce a multilingual\nevaluation approach tailored for European languages. We employ translated\nversions of five widely-used benchmarks to assess the capabilities of 40 LLMs\nacross 21 European languages. Our contributions include examining the\neffectiveness of translated benchmarks, assessing the impact of different\ntranslation services, and offering a multilingual evaluation framework for LLMs\nthat includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,\nEU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly\navailable to encourage further research in multilingual LLM evaluation.\n","authors":["Klaudia Thellmann","Bernhard Stadler","Michael Fromm","Jasper Schulze Buschhoff","Alex Jude","Fabio Barth","Johannes Leveling","Nicolas Flores-Herr","Joachim Köhler","René Jäkel","Mehdi Ali"],"pdf_url":"https://arxiv.org/pdf/2410.08928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13848v1","updated":"2024-10-17T17:58:37Z","published":"2024-10-17T17:58:37Z","title":"Janus: Decoupling Visual Encoding for Unified Multimodal Understanding\n  and Generation","summary":"  In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.\n","authors":["Chengyue Wu","Xiaokang Chen","Zhiyu Wu","Yiyang Ma","Xingchao Liu","Zizheng Pan","Wen Liu","Zhenda Xie","Xingkai Yu","Chong Ruan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.13848v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.13846v1","updated":"2024-10-17T17:58:14Z","published":"2024-10-17T17:58:14Z","title":"SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction","summary":"  Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.\n","authors":["Xuan Zhang","Cunxiao Du","Chao Du","Tianyu Pang","Wei Gao","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13841v1","updated":"2024-10-17T17:56:53Z","published":"2024-10-17T17:56:53Z","title":"A Unified View of Delta Parameter Editing in Post-Trained Large-Scale\n  Models","summary":"  Post-training has emerged as a crucial paradigm for adapting large-scale\npre-trained models to various tasks, whose effects are fully reflected by delta\nparameters (i.e., the disparity between post-trained and pre-trained\nparameters). While numerous studies have explored delta parameter properties\nvia operations like pruning, quantization, low-rank approximation, and\nextrapolation, a unified framework for systematically examining these\ncharacteristics has been lacking. In this paper, we propose a novel perspective\nbased on Riemann sum approximation of the loss function to elucidate delta\nparameter editing operations. Our analysis categorizes existing methods into\nthree classes based on their post-editing performance: competitive, decreased,\nand improved, explaining how they are expressed by the Riemann sum\napproximation term and how they alter the model performance. Extensive\nexperiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,\nand Mistral, corroborate our theoretical findings. Furthermore, we introduce\nextensions to existing techniques like DARE and BitDelta, highlighting their\nlimitations in leveraging the properties of delta parameters and reorganizing\nthem into general expressions to enhance the applicability and effectiveness of\ndelta parameter editing in post-trained models.\n","authors":["Qiaoyu Tang","Le Yu","Bowen Yu","Hongyu Lin","Keming Lu","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2410.13841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13828v1","updated":"2024-10-17T17:52:01Z","published":"2024-10-17T17:52:01Z","title":"A Common Pitfall of Margin-based Language Model Alignment: Gradient\n  Entanglement","summary":"  Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment.\n","authors":["Hui Yuan","Yifan Zeng","Yue Wu","Huazheng Wang","Mengdi Wang","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2410.13828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16833v2","updated":"2024-10-17T17:51:19Z","published":"2024-07-23T20:51:52Z","title":"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive\n  Study and Hybrid Approach","summary":"  Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.\n","authors":["Zhuowan Li","Cheng Li","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2407.16833v2.pdf","comment":"Accepted to EMNLP 2024 industry track"},{"id":"http://arxiv.org/abs/2410.13825v1","updated":"2024-10-17T17:50:38Z","published":"2024-10-17T17:50:38Z","title":"AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents","summary":"  Autonomy via agents using large language models (LLMs) for personalized,\nstandardized tasks boosts human efficiency. Automating web tasks (like booking\nhotels within a budget) is increasingly sought after. Fulfilling practical\nneeds, the web agent also serves as an important proof-of-concept example for\nvarious agent grounding scenarios, with its success promising advancements in\nmany future applications. Prior research often handcrafts web agent strategies\n(e.g., prompting templates, multi-agent systems, search methods, etc.) and the\ncorresponding in-context examples, which may not generalize well across all\nreal-world scenarios. On the other hand, there has been limited study on the\nmisalignment between a web agent's observation/action representation and the\npre-training data of the LLM it's based on. This discrepancy is especially\nnotable when LLMs are primarily trained for language completion rather than\ntasks involving embodied navigation actions and symbolic web elements. Our\nstudy enhances an LLM-based web agent by simply refining its observation and\naction space to better align with the LLM's capabilities. This approach enables\nour base agent to significantly outperform previous methods on a wide variety\nof web tasks. Specifically, on WebArena, a benchmark featuring general-purpose\nweb interaction tasks, our agent AgentOccam surpasses the previous\nstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute\npoints respectively, and boosts the success rate by 26.6 points (+161%) over\nsimilar plain web agents with its observation and action space alignment. We\nachieve this without using in-context examples, new agent roles, online\nfeedback or search strategies. AgentOccam's simple design highlights LLMs'\nimpressive zero-shot performance on web tasks, and underlines the critical role\nof carefully tuning observation and action spaces for LLM-based agents.\n","authors":["Ke Yang","Yao Liu","Sapana Chaudhary","Rasool Fakoor","Pratik Chaudhari","George Karypis","Huzefa Rangwala"],"pdf_url":"https://arxiv.org/pdf/2410.13825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13824v1","updated":"2024-10-17T17:48:54Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48\\% improvement on\nVisualWebBench and a 19.1\\% boost in action accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11018v3","updated":"2024-10-17T17:45:09Z","published":"2024-04-17T02:49:26Z","title":"Many-Shot In-Context Learning","summary":"  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. We also find that inference cost increases\nlinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL\nto varying degrees. Our analysis also reveals the limitations of next-token\nprediction loss as an indicator of downstream ICL performance.\n","authors":["Rishabh Agarwal","Avi Singh","Lei M. Zhang","Bernd Bohnet","Luis Rosias","Stephanie Chan","Biao Zhang","Ankesh Anand","Zaheer Abbas","Azade Nova","John D. Co-Reyes","Eric Chu","Feryal Behbahani","Aleksandra Faust","Hugo Larochelle"],"pdf_url":"https://arxiv.org/pdf/2404.11018v3.pdf","comment":"NeurIPS (Spotlight)"},{"id":"http://arxiv.org/abs/2410.13808v1","updated":"2024-10-17T17:42:10Z","published":"2024-10-17T17:42:10Z","title":"De-mark: Watermark Removal in Large Language Models","summary":"  Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models (LMs). However, the robustness of the watermarking schemes has\nnot been well explored. In this paper, we present De-mark, an advanced\nframework designed to remove n-gram-based watermarks effectively. Our method\nutilizes a novel querying strategy, termed random selection probing, which aids\nin assessing the strength of the watermark and identifying the red-green list\nwithin the n-gram watermark. Experiments on popular LMs, such as Llama3 and\nChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark\nremoval and exploitation tasks.\n","authors":["Ruibo Chen","Yihan Wu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13805v1","updated":"2024-10-17T17:41:28Z","published":"2024-10-17T17:41:28Z","title":"A Watermark for Order-Agnostic Language Models","summary":"  Statistical watermarking techniques are well-established for sequentially\ndecoded language models (LMs). However, these techniques cannot be directly\napplied to order-agnostic LMs, as the tokens in order-agnostic LMs are not\ngenerated sequentially. In this work, we introduce Pattern-mark, a\npattern-based watermarking framework specifically designed for order-agnostic\nLMs. We develop a Markov-chain-based watermark generator that produces\nwatermark key sequences with high-frequency key patterns. Correspondingly, we\npropose a statistical pattern-based detection algorithm that recovers the key\nsequence during detection and conducts statistical tests based on the count of\nhigh-frequency patterns. Our extensive evaluations on order-agnostic LMs, such\nas ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detection\nefficiency, generation quality, and robustness, positioning it as a superior\nwatermarking technique for order-agnostic LMs.\n","authors":["Ruibo Chen","Yihan Wu","Yanshuo Chen","Chenxi Liu","Junfeng Guo","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13804v1","updated":"2024-10-17T17:41:15Z","published":"2024-10-17T17:41:15Z","title":"BenTo: Benchmark Task Reduction with In-Context Transferability","summary":"  Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.\n","authors":["Hongyu Zhao","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14180v2","updated":"2024-10-17T17:38:00Z","published":"2023-12-19T00:36:53Z","title":"Dynamic Topic Language Model on Heterogeneous Children's Mental Health\n  Clinical Notes","summary":"  Mental health diseases affect children's lives and well-beings which have\nreceived increased attention since the COVID-19 pandemic. Analyzing psychiatric\nclinical notes with topic models is critical to evaluating children's mental\nstatus over time. However, few topic models are built for longitudinal\nsettings, and most existing approaches fail to capture temporal trajectories\nfor each document. To address these challenges, we develop a dynamic topic\nmodel with consistent topics and individualized temporal dependencies on the\nevolving document metadata. Our model preserves the semantic meaning of\ndiscovered topics over time and incorporates heterogeneity among documents. In\nparticular, when documents can be categorized, we propose a classifier-free\napproach to maximize topic heterogeneity across different document groups. We\nalso present an efficient variational optimization procedure adapted for the\nmultistage longitudinal setting. In this case study, we apply our method to the\npsychiatric clinical notes from a large tertiary pediatric hospital in Southern\nCalifornia and achieve a 38% increase in the overall coherence of extracted\ntopics. Our real data analysis reveals that children tend to express more\nnegative emotions during state shutdowns and more positive when schools reopen.\nFurthermore, it suggests that sexual and gender minority (SGM) children display\nmore pronounced reactions to major COVID-19 events and a greater sensitivity to\nvaccine-related news than non-SGM children. This study examines children's\nmental health progression during the pandemic and offers clinicians valuable\ninsights to recognize disparities in children's mental health related to their\nsexual and gender identities.\n","authors":["Hanwen Ye","Tatiana Moreno","Adrianne Alpern","Louis Ehwerhemuepha","Annie Qu"],"pdf_url":"https://arxiv.org/pdf/2312.14180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09013v2","updated":"2024-10-17T17:30:52Z","published":"2024-10-11T17:30:02Z","title":"The Impact of Visual Information in Chinese Characters: Evaluating Large\n  Models' Ability to Recognize and Utilize Radicals","summary":"  The glyphic writing system of Chinese incorporates information-rich visual\nfeatures in each character, such as radicals that provide hints about meaning\nor pronunciation. However, there has been no investigation into whether\ncontemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nharness these sub-character features in Chinese through prompting. In this\nstudy, we establish a benchmark to evaluate LLMs' and VLMs' understanding of\nvisual elements in Chinese characters, including radicals, composition\nstructures, strokes, and stroke counts. Our results reveal that models\nsurprisingly exhibit some, but still limited, knowledge of the visual\ninformation, regardless of whether images of characters are provided. To incite\nmodels' ability to use radicals, we further experiment with incorporating\nradicals into the prompts for Chinese language processing (CLP) tasks. We\nobserve consistent improvement in Part-Of-Speech tagging when providing\nadditional information about radicals, suggesting the potential to enhance CLP\nby integrating sub-character information.\n","authors":["Xiaofeng Wu","Karl Stratos","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09013v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13788v1","updated":"2024-10-17T17:29:04Z","published":"2024-10-17T17:29:04Z","title":"Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions","summary":"  Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. We observe existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we propose to assign preference\nlabels by simulating their expected outcomes in the future turns. This allows\nLLMs to learn to ask clarifying questions when it can generate responses that\nare tailored to each user interpretation in future turns. In experiments on\nopen-domain QA, we compare systems that trained using our proposed preference\nlabeling methods against standard methods, which assign preferences based on\nonly prior context. We evaluate systems based on their ability to ask\nclarifying questions that can recover each user's interpretation and expected\nanswer, and find that our training with our proposed method trains LLMs to ask\nclarifying questions with a 5% improvement in F1 measured against the answer\nset from different interpretations of each query\n","authors":["Michael J. Q. Zhang","W. Bradley Knox","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2410.13788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13787v1","updated":"2024-10-17T17:24:10Z","published":"2024-10-17T17:24:10Z","title":"Looking Inward: Language Models Can Learn About Themselves by\n  Introspection","summary":"  Humans acquire knowledge by observing the external world, but also by\nintrospection. Introspection gives a person privileged access to their current\nstate of mind (e.g., thoughts and feelings) that is not accessible to external\nobservers. Can LLMs introspect? We define introspection as acquiring knowledge\nthat is not contained in or derived from training data but instead originates\nfrom internal states. Such a capability could enhance model interpretability.\nInstead of painstakingly analyzing a model's internal workings, we could simply\nask the model about its beliefs, world models, and goals. More speculatively,\nan introspective model might self-report on whether it possesses certain\ninternal states such as subjective feelings or desires and this could inform us\nabout the moral status of these states. Such self-reports would not be entirely\ndictated by the model's training data.\n  We study introspection by finetuning LLMs to predict properties of their own\nbehavior in hypothetical scenarios. For example, \"Given the input P, would your\noutput favor the short- or long-term option?\" If a model M1 can introspect, it\nshould outperform a different model M2 in predicting M1's behavior even if M2\nis trained on M1's ground-truth behavior. The idea is that M1 has privileged\naccess to its own behavioral tendencies, and this enables it to predict itself\nbetter than M2 (even if M2 is generally stronger).\n  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to\npredict itself), we find that the model M1 outperforms M2 in predicting itself,\nproviding evidence for introspection. Notably, M1 continues to predict its\nbehavior accurately even after we intentionally modify its ground-truth\nbehavior. However, while we successfully elicit introspection on simple tasks,\nwe are unsuccessful on more complex tasks or those requiring\nout-of-distribution generalization.\n","authors":["Felix J Binder","James Chua","Tomek Korbak","Henry Sleight","John Hughes","Robert Long","Ethan Perez","Miles Turpin","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2410.13787v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.13785v1","updated":"2024-10-17T17:22:05Z","published":"2024-10-17T17:22:05Z","title":"PopAlign: Diversifying Contrasting Patterns for a More Comprehensive\n  Alignment","summary":"  Alignment of large language models (LLMs) involves training models on\npreference-contrastive output pairs to adjust their responses according to\nhuman preferences. To obtain such contrastive pairs, traditional methods like\nRLHF and RLAIF rely on limited contrasting patterns, such as varying model\nvariants or decoding temperatures. This singularity leads to two issues: (1)\nalignment is not comprehensive; and thereby (2) models are susceptible to\njailbreaking attacks. To address these issues, we investigate how to construct\nmore comprehensive and diversified contrasting patterns to enhance preference\ndata (RQ1) and verify the impact of the diversification of contrasting patterns\non model alignment (RQ2). For RQ1, we propose PopAlign, a framework that\nintegrates diversified contrasting patterns across the prompt, model, and\npipeline levels, introducing six contrasting strategies that do not require\nadditional feedback labeling procedures. Regarding RQ2, we conduct thorough\nexperiments demonstrating that PopAlign significantly outperforms existing\nmethods, leading to more comprehensive alignment.\n","authors":["Zekun Moore Wang","Shawn Wang","Kang Zhu","Jiaheng Liu","Ke Xu","Jie Fu","Wangchunshu Zhou","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2410.13785v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2410.13783v1","updated":"2024-10-17T17:20:40Z","published":"2024-10-17T17:20:40Z","title":"Quantity vs. Quality of Monolingual Source Data in Automatic Text\n  Translation: Can It Be Too Little If It Is Too Good?","summary":"  Monolingual data, being readily available in large quantities, has been used\nto upscale the scarcely available parallel data to train better models for\nautomatic translation. Self-learning, where a model is made to learn from its\noutput, is one approach to exploit such data. However, it has been shown that\ntoo much of this data can be detrimental to the performance of the model if the\navailable parallel data is comparatively extremely low. In this study, we\ninvestigate whether the monolingual data can also be too little and if this\nreduction, based on quality, has any effect on the performance of the\ntranslation model. Experiments have shown that on English-German low-resource\nNMT, it is often better to select only the most useful additional data, based\non quality or closeness to the domain of the test data, than utilizing all of\nthe available data.\n","authors":["Idris Abdulmumin","Bashir Shehu Galadanci","Garba Aliyu","Shamsuddeen Hassan Muhammad"],"pdf_url":"https://arxiv.org/pdf/2410.13783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13780v1","updated":"2024-10-17T17:19:48Z","published":"2024-10-17T17:19:48Z","title":"Optimal Quantization for Matrix Multiplication","summary":"  Recent work in machine learning community proposed multiple methods for\nperforming lossy compression (quantization) of large matrices. This\nquantization is important for accelerating matrix multiplication (main\ncomponent of large language models), which is often bottlenecked by the speed\nof loading these matrices from memory. Unlike classical vector quantization and\nrate-distortion theory, the goal of these new compression algorithms is to be\nable to approximate not the matrices themselves, but their matrix product.\nSpecifically, given a pair of real matrices $A,B$ an encoder (compressor) is\napplied to each of them independently producing descriptions with $R$ bits per\nentry. These representations subsequently are used by the decoder to estimate\nmatrix product $A^\\top B$. In this work, we provide a non-asymptotic lower\nbound on the mean squared error of this approximation (as a function of rate\n$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,\nwe construct a universal quantizer based on nested lattices with an explicit\nguarantee of approximation error for any (non-random) pair of matrices $A$, $B$\nin terms of only Frobenius norms $\\|A\\|_F, \\|B\\|_F$ and $\\|A^\\top B\\|_F$. For\niid Gaussian matrices our quantizer achieves the lower bound and is, thus,\nasymptotically optimal. A practical low-complexity version of our quantizer\nachieves performance quite close to optimal. In information-theoretic terms we\nderive rate-distortion function for matrix multiplication of iid Gaussian\nmatrices.\n","authors":["Or Ordentlich","Yury Polyanskiy"],"pdf_url":"https://arxiv.org/pdf/2410.13780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20967v2","updated":"2024-10-17T17:19:32Z","published":"2024-05-31T16:14:06Z","title":"Superlatives in Context: Modeling the Implicit Semantics of Superlatives","summary":"  Superlatives are used to single out elements with a maximal/minimal property.\nSemantically, superlatives perform a set comparison: something (or some things)\nhas the min/max property out of a set. As such, superlatives provide an ideal\nphenomenon for studying implicit phenomena and discourse restrictions. While\nthis comparison set is often not explicitly defined, its (implicit)\nrestrictions can be inferred from the discourse context the expression appears\nin. In this work we provide an extensive computational study on the semantics\nof superlatives. We propose a unified account of superlative semantics which\nallows us to derive a broad-coverage annotation schema. Using this unified\nschema we annotated a multi-domain dataset of superlatives and their semantic\ninterpretations. We specifically focus on interpreting implicit or ambiguous\nsuperlative expressions, by analyzing how the discourse context restricts the\nset of interpretations. In a set of experiments we then analyze how well models\nperform at variations of predicting superlative semantics, with and without\ncontext. We show that the fine-grained semantics of superlatives in context can\nbe challenging for contemporary models, including GPT-4.\n","authors":["Valentina Pyatkin","Bonnie Webber","Ido Dagan","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2405.20967v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2410.13779v1","updated":"2024-10-17T17:18:30Z","published":"2024-10-17T17:18:30Z","title":"The Mystery of the Pathological Path-star Task for Language Models","summary":"  The recently introduced path-star task is a minimal task designed to\nexemplify limitations to the abilities of language models (Bachmann and\nNagarajan, 2024). It involves a path-star graph where multiple arms radiate\nfrom a single starting node and each node is unique. Given the start node and a\nspecified target node that ends an arm, the task is to generate the arm\ncontaining that target node. This is straightforward for a human but\nsurprisingly difficult for language models, which did not outperform the random\nbaseline. The authors hypothesized this is due to a deficiency in\nteacher-forcing and the next-token prediction paradigm.\n  We demonstrate the task is learnable using teacher-forcing in alternative\nsettings and that the issue is partially due to representation. We introduce a\nregularization method using structured samples of the same graph but with\ndiffering target nodes, improving results across a variety of model types. We\nprovide RASP proofs showing the task is theoretically solvable. Finally, we\nfind settings where an encoder-only model can consistently solve the task.\n","authors":["Arvid Frydenlund"],"pdf_url":"https://arxiv.org/pdf/2410.13779v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.13776v1","updated":"2024-10-17T17:16:00Z","published":"2024-10-17T17:16:00Z","title":"Aggregation Artifacts in Subjective Tasks Collapse Large Language\n  Models' Posteriors","summary":"  In-context Learning (ICL) has become the primary method for performing\nnatural language tasks with Large Language Models (LLMs). The knowledge\nacquired during pre-training is crucial for this few-shot capability, providing\nthe model with task priors. However, recent studies have shown that ICL\npredominantly relies on retrieving task priors rather than \"learning\" to\nperform tasks. This limitation is particularly evident in complex subjective\ndomains such as emotion and morality, where priors significantly influence\nposterior predictions. In this work, we examine whether this is the result of\nthe aggregation used in corresponding datasets, where trying to combine\nlow-agreement, disparate annotations might lead to annotation artifacts that\ncreate detrimental noise in the prompt. Moreover, we evaluate the posterior\nbias towards certain annotators by grounding our study in appropriate,\nquantitative measures of LLM priors. Our results indicate that aggregation is a\nconfounding factor in the modeling of subjective tasks, and advocate focusing\non modeling individuals instead. However, aggregation does not explain the\nentire gap between ICL and the state of the art, meaning other factors in such\ntasks also account for the observed phenomena. Finally, by rigorously studying\nannotator-level labels, we find that it is possible for minority annotators to\nboth better align with LLMs and have their perspectives further amplified.\n","authors":["Georgios Chochlakis","Alexandros Potamianos","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2410.13776v1.pdf","comment":"12 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.13765v1","updated":"2024-10-17T17:03:23Z","published":"2024-10-17T17:03:23Z","title":"Knowledge-Aware Query Expansion with Large Language Models for Textual\n  and Relational Retrieval","summary":"  Large language models (LLMs) have been used to generate query expansions\naugmenting original queries for improving information search. Recent studies\nalso explore providing LLMs with initial retrieval results to generate query\nexpansions more grounded to document corpus. However, these methods mostly\nfocus on enhancing textual similarities between search queries and target\ndocuments, overlooking document relations. For queries like \"Find me a highly\nrated camera for wildlife photography compatible with my Nikon F-Mount lenses\",\nexisting methods may generate expansions that are semantically similar but\nstructurally unrelated to user intents. To handle such semi-structured queries\nwith both textual and relational requirements, in this paper we propose a\nknowledge-aware query expansion framework, augmenting LLMs with structured\ndocument relations from knowledge graph (KG). To further address the limitation\nof entity-based scoring in existing KG-based methods, we leverage document\ntexts as rich KG node representations and use document-based relation filtering\nfor our Knowledge-Aware Retrieval (KAR). Extensive experiments on three\ndatasets of diverse domains show the advantages of our method compared against\nstate-of-the-art baselines on textual and relational semi-structured retrieval.\n","authors":["Yu Xia","Junda Wu","Sungchul Kim","Tong Yu","Ryan A. Rossi","Haoliang Wang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06173v3","updated":"2024-10-17T17:02:02Z","published":"2024-09-10T03:06:17Z","title":"Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks","summary":"  In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.\n","authors":["Georgios Chochlakis","Niyantha Maruthu Pandiyan","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2409.06173v3.pdf","comment":"5 pages, 2 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2403.17125"},{"id":"http://arxiv.org/abs/2409.13057v2","updated":"2024-10-17T16:56:34Z","published":"2024-09-19T19:14:50Z","title":"Natural Language Processing Methods for the Study of Protein-Ligand\n  Interactions","summary":"  Recent advances in Natural Language Processing (NLP) have ignited interest in\ndeveloping effective methods for predicting protein-ligand interactions (PLIs)\ngiven their relevance to drug discovery and protein engineering efforts and the\never-growing volume of biochemical sequence and structural data available. The\nparallels between human languages and the \"languages\" used to represent\nproteins and ligands have enabled the use of NLP machine learning approaches to\nadvance PLI studies. In this review, we explain where and how such approaches\nhave been applied in the recent literature and discuss useful mechanisms such\nas long short-term memory, transformers, and attention. We conclude with a\ndiscussion of the current limitations of NLP methods for the study of PLIs as\nwell as key challenges that need to be addressed in future work.\n","authors":["James Michels","Ramya Bandarupalli","Amin Ahangar Akbari","Thai Le","Hong Xiao","Jing Li","Erik F. Y. Hom"],"pdf_url":"https://arxiv.org/pdf/2409.13057v2.pdf","comment":"52 Pages and 3 Figures"},{"id":"http://arxiv.org/abs/2410.13757v1","updated":"2024-10-17T16:53:50Z","published":"2024-10-17T16:53:50Z","title":"MobA: A Two-Level Agent System for Efficient Mobile Task Automation","summary":"  Current mobile assistants are limited by dependence on system APIs or\nstruggle with complex user instructions and diverse interfaces due to\nrestricted comprehension and decision-making abilities. To address these\nchallenges, we propose MobA, a novel Mobile phone Agent powered by multimodal\nlarge language models that enhances comprehension and planning capabilities\nthrough a sophisticated two-level agent architecture. The high-level Global\nAgent (GA) is responsible for understanding user commands, tracking history\nmemories, and planning tasks. The low-level Local Agent (LA) predicts detailed\nactions in the form of function calls, guided by sub-tasks and memory from the\nGA. Integrating a Reflection Module allows for efficient task completion and\nenables the system to handle previously unseen complex tasks. MobA demonstrates\nsignificant improvements in task execution efficiency and completion rate in\nreal-life evaluations, underscoring the potential of MLLM-empowered mobile\nassistants.\n","authors":["Zichen Zhu","Hao Tang","Yansi Li","Kunyao Lan","Yixuan Jiang","Hao Zhou","Yixiao Wang","Situo Zhang","Liangtai Sun","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13757v1.pdf","comment":"27 pages, 6 figures, and 5 tables. We will release our source code in\n  a few days"},{"id":"http://arxiv.org/abs/2406.19465v2","updated":"2024-10-17T16:53:12Z","published":"2024-06-27T18:07:40Z","title":"Can Large Language Models Generate High-quality Patent Claims?","summary":"  Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness.\n","authors":["Lekang Jiang","Caiqi Zhang","Pascal A Scherz","Stephan Goetz"],"pdf_url":"https://arxiv.org/pdf/2406.19465v2.pdf","comment":"16 pages, 2 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.13727v1","updated":"2024-10-17T16:33:01Z","published":"2024-10-17T16:33:01Z","title":"LLM-Human Pipeline for Cultural Context Grounding of Conversations","summary":"  Conversations often adhere to well-understood social norms that vary across\ncultures. For example, while \"addressing parents by name\" is commonplace in the\nWest, it is rare in most Asian cultures. Adherence or violation of such norms\noften dictates the tenor of conversations. Humans are able to navigate social\nsituations requiring cultural awareness quite adeptly. However, it is a hard\ntask for NLP models.\n  In this paper, we tackle this problem by introducing a \"Cultural Context\nSchema\" for conversations. It comprises (1) conversational information such as\nemotions, dialogue acts, etc., and (2) cultural information such as social\nnorms, violations, etc. We generate ~110k social norm and violation\ndescriptions for ~23k conversations from Chinese culture using LLMs. We refine\nthem using automated verification strategies which are evaluated against\nculturally aware human judgements. We organize these descriptions into\nmeaningful structures we call \"Norm Concepts\", using an interactive\nhuman-in-loop framework. We ground the norm concepts and the descriptions in\nconversations using symbolic annotation. Finally, we use the obtained dataset\nfor downstream tasks such as emotion, sentiment, and dialogue act detection. We\nshow that it significantly improves the empirical performance.\n","authors":["Rajkumar Pujari","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2410.13727v1.pdf","comment":"19 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.14462v2","updated":"2024-10-17T16:32:46Z","published":"2024-06-20T16:24:07Z","title":"Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human\n  Factors in Personas","summary":"  Large language models (LLMs) are increasingly being used in human-centered\nsocial scientific tasks, such as data annotation, synthetic data creation, and\nengaging in dialog. However, these tasks are highly subjective and dependent on\nhuman factors, such as one's environment, attitudes, beliefs, and lived\nexperiences. Thus, it may be the case that employing LLMs (which do not have\nsuch human factors) in these tasks results in a lack of variation in data,\nfailing to reflect the diversity of human experiences. In this paper, we\nexamine the role of prompting LLMs with human-like personas and asking the\nmodels to answer as if they were a specific human. This is done explicitly,\nwith exact demographics, political beliefs, and lived experiences, or\nimplicitly via names prevalent in specific populations. The LLM personas are\nthen evaluated via (1) subjective annotation task (e.g., detecting toxicity)\nand (2) a belief generation task, where both tasks are known to vary across\nhuman factors. We examine the impact of explicit vs. implicit personas and\ninvestigate which human factors LLMs recognize and respond to. Results show\nthat explicit LLM personas show mixed results when reproducing known human\nbiases, but generally fail to demonstrate implicit biases. We conclude that\nLLMs may capture the statistical patterns of how people speak, but are\ngenerally unable to model the complex interactions and subtleties of human\nperceptions, potentially limiting their effectiveness in social science\napplications.\n","authors":["Salvatore Giorgi","Tingting Liu","Ankit Aich","Kelsey Isman","Garrick Sherman","Zachary Fried","João Sedoc","Lyle H. Ungar","Brenda Curtis"],"pdf_url":"https://arxiv.org/pdf/2406.14462v2.pdf","comment":"Accepted at Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.13716v1","updated":"2024-10-17T16:18:49Z","published":"2024-10-17T16:18:49Z","title":"MIRAGE-Bench: Automatic Multilingual Benchmark Arena for\n  Retrieval-Augmented Generation Systems","summary":"  Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different\nheuristic-based metrics for evaluation, but these require human preferences as\nground truth for reference. In contrast, arena-based benchmarks, where two\nmodels compete each other, require an expensive Large Language Model (LLM) as a\njudge for a reliable evaluation. We present an easy and efficient technique to\nget the best of both worlds. The idea is to train a learning to rank model as a\n\"surrogate\" judge using RAG-based evaluation heuristics as input, to produce a\nsynthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a\nstandardized arena-based multilingual RAG benchmark for 18 diverse languages on\nWikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and\nextended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG\nextensively coupling both heuristic features and LLM as a judge evaluator. In\nour work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high\ncorrelation (Kendall Tau ($\\tau$) = 0.909) using our surrogate judge learned\nusing heuristic features with pairwise evaluations and between GPT-4o as a\nteacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We\nobserve proprietary and large open-source LLMs currently dominate in\nmultilingual RAG. MIRAGE-Bench is available at:\nhttps://github.com/vectara/mirage-bench.\n","authors":["Nandan Thakur","Suleman Kazi","Ge Luo","Jimmy Lin","Amin Ahmad"],"pdf_url":"https://arxiv.org/pdf/2410.13716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01257v3","updated":"2024-10-17T16:15:16Z","published":"2024-07-01T13:07:01Z","title":"uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in\n  Low-Data Regimes","summary":"  Recent work on distilling Whisper's knowledge into small models using\npseudo-labels shows promising performance while reducing the size by up to\n50\\%. This results in small, efficient, and dedicated models. However, a\ncritical step of distillation from pseudo-labels involves filtering\nhigh-quality predictions and using only those during training. This step\nrequires ground truth labels to compare and filter low-quality examples making\nthe whole process supervised. In addition to that, the distillation process\nrequires a large amount of data thereby limiting the ability to distill models\nin low-resource settings. To address this challenge, we propose a distillation\nframework that does not require any labeled data. Through experimentation, we\nshow that our best distilled models outperform the teacher model by 5-7 points\nin terms of WER compared to those without filtering and are on par with or\nperform better than similar supervised data filtering setups. When we scale the\ndata, our models significantly outperform all zero-shot and supervised models.\nWe demonstrate that it is possible to distill large Whisper models into\nrelatively small ones without using any labeled data. Our distilled models are\nalso 25-50\\% more compute- and memory-efficient while maintaining performance\nequal to or better than that of the teacher model.\n","authors":["Abdul Waheed","Karima Kadaoui","Bhiksha Raj","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.01257v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.01521v2","updated":"2024-10-17T16:08:15Z","published":"2024-02-02T16:07:05Z","title":"K-Level Reasoning: Establishing Higher Order Beliefs in Large Language\n  Models for Strategic Reasoning","summary":"  Strategic reasoning is a complex yet essential capability for intelligent\nagents. It requires Large Language Model (LLM) agents to adapt their strategies\ndynamically in multi-agent environments. Unlike static reasoning tasks, success\nin these contexts depends on anticipating other agents' beliefs and actions\nwhile continuously adjusting strategies to achieve individual goals. LLMs and\nLLM agents often struggle with strategic reasoning due to the absence of a\nreasoning framework that enables them to dynamically infer others' perspectives\nand adapt to changing environments. Inspired by the Level-K framework from game\ntheory and behavioral economics, which extends reasoning from simple reactions\nto structured strategic depth, we propose a novel framework: \"K-Level Reasoning\nwith Large Language Models (K-R).\" This framework employs recursive mechanisms\nto enable LLMs to achieve varying levels of strategic depth, allowing agents to\nform higher order beliefs - beliefs about others' beliefs. We validate this\nframework through rigorous testing on four testbeds: two classical game theory\nproblems and two social intelligence tasks. The results demonstrate the\nadvantages of K-R in strategic reasoning. Our work presents the first recursive\nimplementation of strategic depth in large language models (LLMs). It\nestablishes a foundation for future research into theory of mind and strategic\nreasoning in LLMs.\n","authors":["Yadong Zhang","Shaoguang Mao","Tao Ge","Xun Wang","Yan Xia","Man Lan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2402.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13708v1","updated":"2024-10-17T16:08:06Z","published":"2024-10-17T16:08:06Z","title":"On the Role of Attention Heads in Large Language Model Safety","summary":"  Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.\n","authors":["Zhenhong Zhou","Haiyang Yu","Xinghua Zhang","Rongwu Xu","Fei Huang","Kun Wang","Yang Liu","Junfeng Fang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.13708v1.pdf","comment":"28 pages, 18 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.13699v1","updated":"2024-10-17T16:04:07Z","published":"2024-10-17T16:04:07Z","title":"Unconstrained Model Merging for Enhanced LLM Reasoning","summary":"  Recent advancements in building domain-specific large language models (LLMs)\nhave shown remarkable success, especially in tasks requiring reasoning\nabilities like logical inference over complex relationships and multi-step\nproblem solving. However, creating a powerful all-in-one LLM remains\nchallenging due to the need for proprietary data and vast computational\nresources. As a resource-friendly alternative, we explore the potential of\nmerging multiple expert models into a single LLM. Existing studies on model\nmerging mainly focus on generalist LLMs instead of domain experts, or the LLMs\nunder the same architecture and size. In this work, we propose an unconstrained\nmodel merging framework that accommodates both homogeneous and heterogeneous\nmodel architectures with a focus on reasoning tasks. A fine-grained layer-wise\nweight merging strategy is designed for homogeneous models merging, while\nheterogeneous model merging is built upon the probabilistic distribution\nknowledge derived from instruction-response fine-tuning data. Across 7\nbenchmarks and 9 reasoning-optimized LLMs, we reveal key findings that\ncombinatorial reasoning emerges from merging which surpasses simple additive\neffects. We propose that unconstrained model merging could serve as a\nfoundation for decentralized LLMs, marking a notable progression from the\nexisting centralized LLM framework. This evolution could enhance wider\nparticipation and stimulate additional advancement in the field of artificial\nintelligence, effectively addressing the constraints posed by centralized\nmodels.\n","authors":["Yiming Zhang","Baoyi He","Shengyu Zhang","Yuhao Fu","Qi Zhou","Zhijie Sang","Zijin Hong","Kejing Yang","Wenjun Wang","Jianbo Yuan","Guangning Han","Linyi Li","Chunlin Ji","Fei Wu","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13699v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.13694v1","updated":"2024-10-17T15:59:52Z","published":"2024-10-17T15:59:52Z","title":"Exploring the Design Space of Visual Context Representation in Video\n  MLLMs","summary":"  Video Multimodal Large Language Models (MLLMs) have shown remarkable\ncapability of understanding the video semantics on various downstream tasks.\nDespite the advancements, there is still a lack of systematic research on\nvisual context representation, which refers to the scheme to select frames from\na video and further select the tokens from a frame. In this paper, we explore\nthe design space for visual context representation, and aim to improve the\nperformance of video MLLMs by finding more effective representation schemes.\nFirstly, we formulate the task of visual context representation as a\nconstrained optimization problem, and model the language modeling loss as a\nfunction of the number of frames and the number of embeddings (or tokens) per\nframe, given the maximum visual context window size. Then, we explore the\nscaling effects in frame selection and token selection respectively, and fit\nthe corresponding function curve by conducting extensive empirical experiments.\nWe examine the effectiveness of typical selection strategies and present\nempirical findings to determine the two factors. Furthermore, we study the\njoint effect of frame selection and token selection, and derive the optimal\nformula for determining the two factors. We demonstrate that the derived\noptimal settings show alignment with the best-performed results of empirical\nexperiments. Our code and model are available at:\nhttps://github.com/RUCAIBox/Opt-Visor.\n","authors":["Yifan Du","Yuqi Huo","Kun Zhou","Zijia Zhao","Haoyu Lu","Han Huang","Wayne Xin Zhao","Bingning Wang","Weipeng Chen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.13694v1.pdf","comment":"Long Video MLLM; work in progress"},{"id":"http://arxiv.org/abs/2410.12407v2","updated":"2024-10-17T15:59:34Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v2.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2406.20052v2","updated":"2024-10-17T15:57:10Z","published":"2024-06-28T17:03:51Z","title":"Understanding and Mitigating Language Confusion in LLMs","summary":"  We investigate a surprising limitation of LLMs: their inability to\nconsistently generate text in a user's desired language. We create the Language\nConfusion Benchmark (LCB) to evaluate such failures, covering 15 typologically\ndiverse languages with existing and newly-created English and multilingual\nprompts. We evaluate a range of LLMs on monolingual and cross-lingual\ngeneration reflecting practical use cases, finding that Llama Instruct and\nMistral models exhibit high degrees of language confusion and even the\nstrongest models fail to consistently respond in the correct language. We\nobserve that base and English-centric instruct models are more prone to\nlanguage confusion, which is aggravated by complex prompts and high sampling\ntemperatures. We find that language confusion can be partially mitigated via\nfew-shot prompting, multilingual SFT and preference tuning. We release our\nlanguage confusion benchmark, which serves as a first layer of efficient,\nscalable multilingual evaluation at\nhttps://github.com/for-ai/language-confusion.\n","authors":["Kelly Marchisio","Wei-Yin Ko","Alexandre Bérard","Théo Dehaze","Sebastian Ruder"],"pdf_url":"https://arxiv.org/pdf/2406.20052v2.pdf","comment":"EMNLP 2024 Main Conference Camera-ready"},{"id":"http://arxiv.org/abs/2406.16635v2","updated":"2024-10-17T15:45:10Z","published":"2024-06-24T13:41:08Z","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models","summary":"  The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated efficiency techniques like quantization\nand sparsity. Contextual sparsity, where the sparsity pattern is\ninput-dependent, is crucial in LLMs because the permanent removal of attention\nheads or neurons from LLMs can significantly degrade accuracy. Prior work has\nattempted to model contextual sparsity using neural networks trained to predict\nactivation magnitudes, which can be used to dynamically prune structures with\nlow predicted activation magnitude. In this paper, we look beyond\nmagnitude-based pruning criteria to assess attention head and neuron importance\nin LLMs. We develop a novel predictor called ShadowLLM, which can shadow the\nLLM behavior and enforce better sparsity patterns, resulting in over 15%\nimprovement in end-to-end accuracy compared to prior methods. In addition,\nShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on Llama-2 and OPT models with up\nto 30 billion parameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.\n","authors":["Yash Akhauri","Ahmed F AbouElhamayed","Jordan Dotzel","Zhiru Zhang","Alexander M Rush","Safeen Huda","Mohamed S Abdelfattah"],"pdf_url":"https://arxiv.org/pdf/2406.16635v2.pdf","comment":"Accepted to EMNLP 2024 (Main, Long Paper)"},{"id":"http://arxiv.org/abs/2410.13675v1","updated":"2024-10-17T15:33:54Z","published":"2024-10-17T15:33:54Z","title":"Pose-Based Sign Language Appearance Transfer","summary":"  We introduce a method for transferring the signer's appearance in sign\nlanguage skeletal poses while preserving the sign content. Using estimated\nposes, we transfer the appearance of one signer to another, maintaining natural\nmovements and transitions. This approach improves pose-based rendering and sign\nstitching while obfuscating identity. Our experiments show that while the\nmethod reduces signer identification accuracy, it slightly harms sign\nrecognition performance, highlighting a tradeoff between privacy and utility.\nOur code is available at\n\\url{https://github.com/sign-language-processing/pose-anonymization}.\n","authors":["Amit Moryossef","Gerard Sant","Zifan Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.13675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13671v1","updated":"2024-10-17T15:29:57Z","published":"2024-10-17T15:29:57Z","title":"HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World\n  Multilingual Settings","summary":"  Assessing the capabilities and limitations of large language models (LLMs)\nhas garnered significant interest, yet the evaluation of multiple models in\nreal-world scenarios remains rare. Multilingual evaluation often relies on\ntranslated benchmarks, which typically do not capture linguistic and cultural\nnuances present in the source language. This study provides an extensive\nassessment of 24 LLMs on real world data collected from Indian patients\ninteracting with a medical chatbot in Indian English and 4 other Indic\nlanguages. We employ a uniform Retrieval Augmented Generation framework to\ngenerate responses, which are evaluated using both automated techniques and\nhuman evaluators on four specific metrics relevant to our application. We find\nthat models vary significantly in their performance and that instruction tuned\nIndic models do not always perform well on Indic language queries. Further, we\nempirically show that factual correctness is generally lower for responses to\nIndic queries compared to English queries. Finally, our qualitative work shows\nthat code-mixed and culturally relevant queries in our dataset pose challenges\nto evaluated models.\n","authors":["Varun Gumma","Anandhita Raghunath","Mohit Jain","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.13671v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.13668v1","updated":"2024-10-17T15:28:45Z","published":"2024-10-17T15:28:45Z","title":"signwriting-evaluation: Effective Sign Language Evaluation via\n  SignWriting","summary":"  The lack of automatic evaluation metrics tailored for SignWriting presents a\nsignificant obstacle in developing effective transcription and translation\nmodels for signed languages. This paper introduces a comprehensive suite of\nevaluation metrics specifically designed for SignWriting, including adaptations\nof standard metrics such as \\texttt{BLEU} and \\texttt{chrF}, the application of\n\\texttt{CLIPScore} to SignWriting images, and a novel symbol distance metric\nunique to our approach. We address the distinct challenges of evaluating single\nsigns versus continuous signing and provide qualitative demonstrations of\nmetric efficacy through score distribution analyses and nearest-neighbor\nsearches within the SignBank corpus. Our findings reveal the strengths and\nlimitations of each metric, offering valuable insights for future advancements\nusing SignWriting. This work contributes essential tools for evaluating\nSignWriting models, facilitating progress in the field of sign language\nprocessing. Our code is available at\n\\url{https://github.com/sign-language-processing/signwriting-evaluation}.\n","authors":["Amit Moryossef","Rotem Zilberman","Ohad Langer"],"pdf_url":"https://arxiv.org/pdf/2410.13668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13667v1","updated":"2024-10-17T15:28:27Z","published":"2024-10-17T15:28:27Z","title":"ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection\n  and Argumentative Dialogue Summarization","summary":"  Dialogue agents have been receiving increasing attention for years, and this\ntrend has been further boosted by the recent progress of large language models\n(LLMs). Stance detection and dialogue summarization are two core tasks of\ndialogue agents in application scenarios that involve argumentative dialogues.\nHowever, research on these tasks is limited by the insufficiency of public\ndatasets, especially for non-English languages. To address this language\nresource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first\nChinese dataset for benchmarking target-independent stance detection and debate\nsummarization. Our dataset consists of 1,218 real-world debates that were\nconducted in Chinese on 476 unique topics, containing 2,436 stance-specific\nsummaries and 14,133 fully annotated utterances. Besides providing a versatile\ntestbed for future research, we also conduct an empirical study on the dataset\nand propose an integrated task. The results show the challenging nature of the\ndataset and suggest a potential of incorporating stance detection in\nsummarization for argumentative dialogue.\n","authors":["Xiutian Zhao","Ke Wang","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2410.13667v1.pdf","comment":"In EMNLP 2023"},{"id":"http://arxiv.org/abs/2409.15355v4","updated":"2024-10-17T15:27:30Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13666v1","updated":"2024-10-17T15:27:17Z","published":"2024-10-17T15:27:17Z","title":"VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic\n  Reasoning Tasks","summary":"  Deriving inference from heterogeneous inputs (such as images, text, and\naudio) is an important skill for humans to perform day-to-day tasks. A similar\nability is desirable for the development of advanced Artificial Intelligence\n(AI) systems. While state-of-the-art models are rapidly closing the gap with\nhuman-level performance on diverse computer vision and NLP tasks separately,\nthey struggle to solve tasks that require joint reasoning over visual and\ntextual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask\nbenchmark for natural language understanding, we propose VL-GLUE in this paper.\nVL-GLUE consists of over 100k samples spanned across seven different tasks,\nwhich at their core require visuo-linguistic reasoning. Moreover, our benchmark\ncomprises of diverse image types (from synthetically rendered figures, and\nday-to-day scenes to charts and complex diagrams) and includes a broad variety\nof domain-specific text (from cooking, politics, and sports to high-school\ncurricula), demonstrating the need for multi-modal understanding in the\nreal-world. We show that this benchmark is quite challenging for existing\nlarge-scale vision-language models and encourage development of systems that\npossess robust visuo-linguistic reasoning capabilities.\n","authors":["Shailaja Keyur Sampat","Mutsumi Nakamura","Shankar Kailas","Kartik Aggarwal","Mandy Zhou","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2410.13666v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.00489v2","updated":"2024-10-17T15:20:23Z","published":"2024-03-30T23:07:58Z","title":"Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt\n  Compression","summary":"  Large Language Models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto substandard results in terms of readability/interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. Prompt-SAW uses the prompt's textual information to build a graph and\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-aug, i.e., an extended version of the\nexisting GSM8K benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by Prompt-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware\nsettings while compressing the original prompt text by 34.9 and 56.7.\n","authors":["Muhammad Asif Ali","Zhengping Li","Shu Yang","Keyuan Cheng","Yang Cao","Tianhao Huang","Guimin Hu","Weimin Lyu","Lijie Hu","Lu Yu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2404.00489v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.13654v1","updated":"2024-10-17T15:19:03Z","published":"2024-10-17T15:19:03Z","title":"Red and blue language: Word choices in the Trump & Harris 2024\n  presidential debate","summary":"  Political debates are a peculiar type of political discourse, in which\ncandidates directly confront one another, addressing not only the the\nmoderator's questions, but also their opponent's statements, as well as the\nconcerns of voters from both parties and undecided voters. Therefore, language\nis adjusted to meet specific expectations and achieve persuasion. We analyse\nhow the language of Trump and Harris during the debate (September 10th 2024)\ndiffers in relation to the following semantic and pragmatic features, for which\nwe formulated targeted hypotheses: framing values and ideology, appealing to\nemotion, using words with different degrees of concreteness and specificity,\naddressing others through singular or plural pronouns. Our findings include:\ndifferences in the use of figurative frames (Harris often framing issues around\nrecovery and empowerment, Trump often focused on crisis and decline); similar\nuse of emotional language, with Trump showing a slight higher tendency toward\nnegativity and toward less subjective language compared to Harris; no\nsignificant difference in the specificity of candidates' responses; similar use\nof abstract language, with Trump showing more variability than Harris,\ndepending on the subject discussed; differences in addressing the opponent,\nwith Trump not mentioning Harris by name, while Harris referring to Trump\nfrequently; different uses of pronouns, with Harris using both singular and\nplural pronouns equally, while Trump using more singular pronouns. The results\nare discussed in relation to previous literature on Red and Blue language,\nwhich refers to distinct linguistic patterns associated with conservative (Red)\nand liberal (Blue) political ideologies.\n","authors":["Philipp Wicke","Marianna M. Bolognesi"],"pdf_url":"https://arxiv.org/pdf/2410.13654v1.pdf","comment":"Submitted to PLOS ONE, under review"},{"id":"http://arxiv.org/abs/2410.13649v1","updated":"2024-10-17T15:15:12Z","published":"2024-10-17T15:15:12Z","title":"A new approach for fine-tuning sentence transformers for intent\n  classification and out-of-scope detection tasks","summary":"  In virtual assistant (VA) systems it is important to reject or redirect user\nqueries that fall outside the scope of the system. One of the most accurate\napproaches for out-of-scope (OOS) rejection is to combine it with the task of\nintent classification on in-scope queries, and to use methods based on the\nsimilarity of embeddings produced by transformer-based sentence encoders.\nTypically, such encoders are fine-tuned for the intent-classification task,\nusing cross-entropy loss. Recent work has shown that while this produces\nsuitable embeddings for the intent-classification task, it also tends to\ndisperse in-scope embeddings over the full sentence embedding space. This\ncauses the in-scope embeddings to potentially overlap with OOS embeddings,\nthereby making OOS rejection difficult. This is compounded when OOS data is\nunknown. To mitigate this issue our work proposes to regularize the\ncross-entropy loss with an in-scope embedding reconstruction loss learned using\nan auto-encoder. Our method achieves a 1-4% improvement in the area under the\nprecision-recall curve for rejecting out-of-sample (OOS) instances, without\ncompromising intent classification performance.\n","authors":["Tianyi Zhang","Atta Norouzian","Aanchan Mohan","Frederick Ducatelle"],"pdf_url":"https://arxiv.org/pdf/2410.13649v1.pdf","comment":"Appearing at Empirical Methods in Natural Language Processing 2025 -\n  Industry Track"},{"id":"http://arxiv.org/abs/2410.13648v1","updated":"2024-10-17T15:15:00Z","published":"2024-10-17T15:15:00Z","title":"SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit\n  ToM Application in LLMs","summary":"  While prior work has explored whether large language models (LLMs) possess a\n\"theory of mind\" (ToM) - the ability to attribute mental states to oneself and\nothers - there has been little work testing whether LLMs can implicitly apply\nsuch knowledge to predict behavior, or to judge whether an observed behavior is\nrational. Such skills are critical for appropriate interaction in social\nenvironments. We create a new dataset, SimpleTom, containing concise, diverse\nstories (e.g., \"The can of Pringles has moldy chips in it. Mary picks up the\ncan in the supermarket and walks to the cashier.\"), each with three questions\nthat test different degrees of ToM reasoning, asking models to predict (a)\nmental state (\"Is Mary aware of the mold?\"), (b) behavior (\"Will Mary pay for\nthe chips or report the mold?\"), and (c) judgment (\"Mary paid for the chips.\nWas that reasonable?\"). To our knowledge, SimpleToM is the first dataset to\nsystematically explore downstream reasoning requiring knowledge of mental\nstates in realistic scenarios. Our experimental results are intriguing: While\nmost models can reliably predict mental state on our dataset (a), they often\nfail to correctly predict the behavior (b), and fare even worse at judging\nwhether given behaviors are reasonable (c), despite being correctly aware of\nthe protagonist's mental state should make such secondary predictions obvious.\nWe further show that we can help models do better at (b) and (c) via\ninterventions such as reminding the model of its earlier mental state answer\nand mental-state-specific chain-of-thought prompting, raising the action\nprediction accuracies (e.g., from 49.5% to 93.5% for GPT-4o) and judgment\naccuracies (e.g., from 15.3% to 94.7% in GPT-4o). While this shows that models\ncan be coaxed to perform well, it requires task-specific interventions, and the\nnatural model performances remain low, a cautionary tale for LLM deployment.\n","authors":["Yuling Gu","Oyvind Tafjord","Hyunwoo Kim","Jared Moore","Ronan Le Bras","Peter Clark","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.13648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11341v3","updated":"2024-10-17T15:12:21Z","published":"2024-06-17T08:59:04Z","title":"A Systematic Analysis of Large Language Models as Soft Reasoners: The\n  Case of Syllogistic Inferences","summary":"  The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.\n","authors":["Leonardo Bertolazzi","Albert Gatt","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.11341v3.pdf","comment":"Accepted to EMNLP 2024 (main conference)"},{"id":"http://arxiv.org/abs/2410.13641v1","updated":"2024-10-17T15:09:35Z","published":"2024-10-17T15:09:35Z","title":"An Active Learning Framework for Inclusive Generation by Large Language\n  Models","summary":"  Ensuring that Large Language Models (LLMs) generate text representative of\ndiverse sub-populations is essential, particularly when key concepts related to\nunder-represented groups are scarce in the training data. We address this\nchallenge with a novel clustering-based active learning framework, enhanced\nwith knowledge distillation. The proposed framework transforms the intermediate\noutputs of the learner model, enabling effective active learning for generative\ntasks for the first time. Integration of clustering and knowledge distillation\nyields more representative models without prior knowledge of underlying data\ndistribution and overbearing human efforts. We validate our approach in\npractice through case studies in counter-narration and style transfer. We\nconstruct two new datasets in tandem with model training, showing a performance\nimprovement of 2%-10% over baseline models. Our results also show more\nconsistent performance across various data subgroups and increased lexical\ndiversity, underscoring our model's resilience to skewness in available data.\nFurther, our results show that the data acquired via our approach improves the\nperformance of secondary models not involved in the learning loop, showcasing\npractical utility of the framework.\n","authors":["Sabit Hassan","Anthony Sicilia","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.13641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13640v1","updated":"2024-10-17T15:09:24Z","published":"2024-10-17T15:09:24Z","title":"Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation","summary":"  LLM self-evaluation relies on the LLM's own ability to estimate response\ncorrectness, which can greatly improve its deployment reliability. In this\nresearch track, we propose the Chain-of-Embedding (CoE) in the latent space to\nenable LLMs to perform output-free self-evaluation. CoE consists of all\nprogressive hidden states produced during the inference time, which can be\ntreated as the latent thinking path of LLMs. We find that when LLMs respond\ncorrectly and incorrectly, their CoE features differ, these discrepancies\nassist us in estimating LLM response correctness. Experiments in four diverse\ndomains and seven LLMs fully demonstrate the effectiveness of our method.\nMeanwhile, its label-free design intent without any training and\nmillisecond-level computational cost ensure real-time feedback in large-scale\nscenarios. More importantly, we provide interesting insights into LLM response\ncorrectness from the perspective of hidden state changes inside LLMs.\n","authors":["Yiming Wang","Pei Zhang","Baosong Yang","Derek F. Wong","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13640v1.pdf","comment":"33 pages, 18 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.13639v1","updated":"2024-10-17T15:09:03Z","published":"2024-10-17T15:09:03Z","title":"A Comparative Study on Reasoning Patterns of OpenAI's o1 Model","summary":"  Enabling Large Language Models (LLMs) to handle a wider range of complex\ntasks (e.g., coding, math) has drawn great attention from many researchers. As\nLLMs continue to evolve, merely increasing the number of model parameters\nyields diminishing performance improvements and heavy computational costs.\nRecently, OpenAI's o1 model has shown that inference strategies (i.e.,\nTest-time Compute methods) can also significantly enhance the reasoning\ncapabilities of LLMs. However, the mechanisms behind these methods are still\nunexplored. In our work, to investigate the reasoning patterns of o1, we\ncompare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent\nWorkflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general\nreasoning benchmarks in three domains (i.e., math, coding, commonsense\nreasoning). Specifically, first, our experiments show that the o1 model has\nachieved the best performance on most datasets. Second, as for the methods of\nsearching diverse responses (e.g., BoN), we find the reward models' capability\nand the search space both limit the upper boundary of these methods. Third, as\nfor the methods that break the problem into many sub-problems, the Agent\nWorkflow has achieved better performance than Step-wise BoN due to the\ndomain-specific system prompt for planning better reasoning processes. Fourth,\nit is worth mentioning that we have summarized six reasoning patterns of o1,\nand provided a detailed analysis on several reasoning benchmarks.\n","authors":["Siwei Wu","Zhongyuan Peng","Xinrun Du","Tuney Zheng","Minghao Liu","Jialong Wu","Jiachen Ma","Yizhi Li","Jian Yang","Wangchunshu Zhou","Qunshu Lin","Junbo Zhao","Zhaoxiang Zhang","Wenhao Huang","Ge Zhang","Chenghua Lin","J. H. Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14545v2","updated":"2024-10-17T15:06:23Z","published":"2024-06-20T17:54:33Z","title":"Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference\n  Attacks in Text-to-SQL Systems","summary":"  Text-to-SQL systems empower users to interact with databases using natural\nlanguage, automatically translating queries into executable SQL code. However,\ntheir reliance on database schema information for SQL generation exposes them\nto significant security vulnerabilities, particularly schema inference attacks\nthat can lead to unauthorized data access or manipulation. In this paper, we\nintroduce a novel zero-knowledge framework for reconstructing the underlying\ndatabase schema of text-to-SQL models without any prior knowledge of the\ndatabase. Our approach systematically probes text-to-SQL models with specially\ncrafted questions and leverages a surrogate GPT-4 model to interpret the\noutputs, effectively uncovering hidden schema elements -- including tables,\ncolumns, and data types. We demonstrate that our method achieves high accuracy\nin reconstructing table names, with F1 scores of up to .99 for generative\nmodels and .78 for fine-tuned models, underscoring the severity of schema\nleakage risks. Furthermore, we propose a simple protection mechanism for\ngenerative models and empirically show its limitations in mitigating these\nattacks.\n","authors":["Đorđe Klisura","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2406.14545v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09693v3","updated":"2024-10-17T15:03:11Z","published":"2023-11-16T09:09:22Z","title":"BLT: Can Large Language Models Handle Basic Legal Text?","summary":"  We find that the best publicly available LLMs like GPT-4 and Claude currently\nperform poorly on basic legal text handling. This motivates the creation of a\nbenchmark consisting of examples that lawyers and paralegals would expect LLMs\nto handle zero-shot, such as looking up the text at a line of a witness\ndeposition or at a subsection of a contract. LLMs' poor performance on this\nbenchmark casts into doubt their reliability as-is for legal practice. However,\nfine-tuning on our training set brings even a small model to near-perfect\nperformance. This benchmark will be useful for fine-tuning LLMs for downstream\nlegal tasks, as well as for tracking LLMs' reliability as-is for basic legal\ntasks.\n","authors":["Andrew Blair-Stanek","Nils Holzenberger","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2311.09693v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11382v2","updated":"2024-10-17T14:59:37Z","published":"2024-08-21T07:23:34Z","title":"Towards Inducing Document-Level Abilities in Standard Multilingual\n  Neural Machine Translation Models","summary":"  Neural Machine Translation (NMT) models have traditionally used Sinusoidal\nPositional Embeddings (PEs), which often struggle to capture long-range\ndependencies and are less efficient for handling extended context or\ndocument-level translation tasks. This work addresses the challenge of\ntransitioning pre-trained NMT models from absolute sinusoidal PEs to relative\nPEs, such as Rotary Positional Embeddings (ROPE) and Attention with Linear\nBiases (ALIBI), without compromising performance. We demonstrate that\nparameter-efficient fine-tuning, using only a small amount of high-quality\ndata, can successfully facilitate this transition. Experimental results\nindicate that switching from sinusoidal to relative PEs results in competitive\ntranslation quality on sentence-level evaluation benchmarks. Additionally,\nmodels trained with ROPE consistently outperform those using ALIBI and\nSinusoidal PEs on document-level benchmarks across both string-based metrics\nand qualitative evaluations. Moreover, we find that a small amount of\nlong-context data in a few languages is sufficient for cross-lingual length\ngeneralization, thereby inducing long-context capabilities.\n","authors":["Varun Gumma","Pranjal A. Chitale","Kalika Bali"],"pdf_url":"https://arxiv.org/pdf/2408.11382v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2407.04952v2","updated":"2024-10-17T14:58:53Z","published":"2024-07-06T04:06:55Z","title":"Granular Privacy Control for Geolocation with Vision Language Models","summary":"  Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.\n","authors":["Ethan Mendes","Yang Chen","James Hays","Sauvik Das","Wei Xu","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2407.04952v2.pdf","comment":"Accepted to EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2410.13611v1","updated":"2024-10-17T14:46:34Z","published":"2024-10-17T14:46:34Z","title":"H2OVL-Mississippi Vision Language Models Technical Report","summary":"  Smaller vision-language models (VLMs) are becoming increasingly important for\nprivacy-focused, on-device applications due to their ability to run efficiently\non consumer hardware for processing enterprise commercial documents and images.\nThese models require strong language understanding and visual capabilities to\nenhance human-machine interaction. To address this need, we present\nH2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs\nusing 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny\nmodel with 0.8 billion parameters that specializes in text recognition,\nachieving state of the art performance on the Text Recognition portion of\nOCRBench and surpassing much larger models in this area. Additionally, we are\nreleasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use\ncases, exhibiting highly competitive metrics across various academic\nbenchmarks. Both models build upon our prior work with H2O-Danube language\nmodels, extending their capabilities into the visual domain. We release them\nunder the Apache 2.0 license, making VLMs accessible to everyone, democratizing\ndocument AI and visual LLMs.\n","authors":["Shaikat Galib","Shanshan Wang","Guanshuo Xu","Pascal Pfeiffer","Ryan Chesler","Mark Landry","Sri Satish Ambati"],"pdf_url":"https://arxiv.org/pdf/2410.13611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13610v1","updated":"2024-10-17T14:46:22Z","published":"2024-10-17T14:46:22Z","title":"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling","summary":"  Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.\n","authors":["Yakun Zhu","Shaohang Wei","Xu Wang","Kui Xue","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07991v2","updated":"2024-10-17T14:44:45Z","published":"2024-10-10T14:48:57Z","title":"Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets","summary":"  The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.\n","authors":["Tommaso Giorgi","Lorenzo Cima","Tiziano Fagni","Marco Avvenuti","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2410.07991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17648v3","updated":"2024-10-17T14:41:39Z","published":"2024-09-26T08:55:21Z","title":"Efficient In-Domain Question Answering for Resource-Constrained\n  Environments","summary":"  Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited.\n","authors":["Isaac Chung","Phat Vo","Arman C. Kizilkale","Aaron Reite"],"pdf_url":"https://arxiv.org/pdf/2409.17648v3.pdf","comment":"6 pages, 2 tables"},{"id":"http://arxiv.org/abs/2410.13604v1","updated":"2024-10-17T14:39:24Z","published":"2024-10-17T14:39:24Z","title":"Large Language Models as Narrative-Driven Recommenders","summary":"  Narrative-driven recommenders aim to provide personalized suggestions for\nuser requests expressed in free-form text such as \"I want to watch a thriller\nwith a mind-bending story, like Shutter Island.\" Although large language models\n(LLMs) have been shown to excel in processing general natural language queries,\ntheir effectiveness for handling such recommendation requests remains\nrelatively unexplored. To close this gap, we compare the performance of 38\nopen- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in\na movie recommendation setting. For this, we utilize a gold-standard,\ncrowdworker-annotated dataset of posts from reddit's movie suggestion community\nand employ various prompting strategies, including zero-shot, identity, and\nfew-shot prompting. Our findings demonstrate the ability of LLMs to generate\ncontextually relevant movie recommendations, significantly outperforming other\nstate-of-the-art approaches, such as doc2vec. While we find that closed-source\nand large-parameterized models generally perform best, medium-sized open-source\nmodels remain competitive, being only slightly outperformed by their more\ncomputationally expensive counterparts. Furthermore, we observe no significant\ndifferences across prompting strategies for most models, underscoring the\neffectiveness of simple approaches such as zero-shot prompting for\nnarrative-driven recommendations. Overall, this work offers valuable insights\nfor recommender system researchers as well as practitioners aiming to integrate\nLLMs into real-world recommendation tools.\n","authors":["Lukas Eberhard","Thorsten Ruprechter","Denis Helic"],"pdf_url":"https://arxiv.org/pdf/2410.13604v1.pdf","comment":"Under review; 19 pages"},{"id":"http://arxiv.org/abs/2410.13562v1","updated":"2024-10-17T14:00:13Z","published":"2024-10-17T14:00:13Z","title":"Enhancing Fact Retrieval in PLMs through Truthfulness","summary":"  Pre-trained Language Models (PLMs) encode various facts about the world at\ntheir pre-training phase as they are trained to predict the next or missing\nword in a sentence. There has a been an interest in quantifying and improving\nthe amount of facts that can be extracted from PLMs, as they have been\nenvisioned to act as soft knowledge bases, which can be queried in natural\nlanguage. Different approaches exist to enhance fact retrieval from PLM. Recent\nwork shows that the hidden states of PLMs can be leveraged to determine the\ntruthfulness of the PLMs' inputs. Leveraging this finding to improve factual\nknowledge retrieval remains unexplored. In this work, we investigate the use of\na helper model to improve fact retrieval. The helper model assesses the\ntruthfulness of an input based on the corresponding hidden states\nrepresentations from the PLMs. We evaluate this approach on several masked PLMs\nand show that it enhances fact retrieval by up to 33\\%. Our findings highlight\nthe potential of hidden states representations from PLMs in improving their\nfactual knowledge retrieval.\n","authors":["Paul Youssef","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2410.13562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13553v1","updated":"2024-10-17T13:51:03Z","published":"2024-10-17T13:51:03Z","title":"Integrating Temporal Representations for Dynamic Memory Retrieval and\n  Management in Large Language Models","summary":"  Conventional dialogue agents often struggle with effective memory recall,\nleading to redundant retrieval and inadequate management of unique user\nassociations. To address this, we propose SynapticRAG, a novel approach\nintegrating synaptic dynamics into Retrieval-Augmented Generation (RAG).\nSynapticRAG integrates temporal representations into memory vectors, mimicking\nbiological synapses by differentiating events based on occurrence times and\ndynamically updating memory significance. This model employs temporal scoring\nfor memory connections and a synaptic-inspired propagation control mechanism.\nExperiments across English, Japanese, and Chinese datasets demonstrate\nSynapticRAG's superiority over existing methods, including traditional RAG,\nwith up to 14.66\\% improvement in memory retrieval accuracy. Our approach\nadvances context-aware dialogue AI systems by enhancing long-term context\nmaintenance and specific information extraction from conversations.\n","authors":["Yuki Hou","Haruki Tamoto","Homei Miyashita"],"pdf_url":"https://arxiv.org/pdf/2410.13553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16710v3","updated":"2024-10-17T13:50:46Z","published":"2024-04-25T16:20:23Z","title":"LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding","summary":"  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.\n","authors":["Mostafa Elhoushi","Akshat Shrivastava","Diana Liskovich","Basil Hosmer","Bram Wasti","Liangzhen Lai","Anas Mahmoud","Bilge Acun","Saurabh Agarwal","Ahmed Roman","Ahmed A Aly","Beidi Chen","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2404.16710v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2410.12691v2","updated":"2024-10-17T13:32:23Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12294v2","updated":"2024-10-17T13:27:43Z","published":"2024-10-16T06:51:09Z","title":"LLM-based Cognitive Models of Students with Misconceptions","summary":"  Accurately modeling student cognition is crucial for developing effective\nAI-driven educational technologies. A key challenge is creating realistic\nstudent models that satisfy two essential properties: (1) accurately\nreplicating specific misconceptions, and (2) correctly solving problems where\nthese misconceptions are not applicable. This dual requirement reflects the\ncomplex nature of student understanding, where misconceptions coexist with\ncorrect knowledge. This paper investigates whether Large Language Models (LLMs)\ncan be instruction-tuned to meet this dual requirement and effectively simulate\nstudent thinking in algebra. We introduce MalAlgoPy, a novel Python library\nthat generates datasets reflecting authentic student solution patterns through\na graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,\nwe define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned\nto faithfully emulate realistic student behavior. Our findings reveal that LLMs\ntrained on misconception examples can efficiently learn to replicate errors.\nHowever, the training diminishes the model's ability to solve problems\ncorrectly, particularly for problem types where the misconceptions are not\napplicable, thus failing to satisfy second property of CSMs. We demonstrate\nthat by carefully calibrating the ratio of correct to misconception examples in\nthe training data - sometimes as low as 0.25 - it is possible to develop CSMs\nthat satisfy both properties. Our insights enhance our understanding of\nAI-based student models and pave the way for effective adaptive learning\nsystems.\n","authors":["Shashank Sonkar","Xinghe Chen","Naiming Liu","Richard G. Baraniuk","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.12294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03857v2","updated":"2024-10-17T13:08:13Z","published":"2024-06-06T08:42:36Z","title":"MuJo: Multimodal Joint Feature Space Learning for Human Activity\n  Recognition","summary":"  Human Activity Recognition (HAR) is a longstanding problem in AI with\napplications in a broad range of areas, including healthcare, sports and\nfitness, security, and more. The performance of HAR in real-world settings is\nstrongly dependent on the type and quality of the input signal that can be\nacquired. Given an unobstructed, high-quality camera view of a scene, computer\nvision systems, in particular in conjunction with foundation models, can today\nfairly reliably distinguish complex activities. On the other hand, recognition\nusing modalities such as wearable sensors (which are often more broadly\navailable, e.g., in mobile phones and smartwatches) is a more difficult\nproblem, as the signals often contain less information and labeled training\ndata is more difficult to acquire. To alleviate the need for labeled data, we\nintroduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this\nwork, which can be used with the proposed pre-training method MuJo (Multimodal\nJoint Feature Space Learning) to enhance HAR performance across various\nmodalities. FiMAD was created using YouTube fitness videos and contains\nparallel video, language, pose, and simulated IMU sensor data. MuJo utilizes\nthis dataset to learn a joint feature space for these modalities. We show that\nclassifiers pre-trained on FiMAD can increase the performance on real HAR\ndatasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on\nMM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%\nof the training data and 0.942 when utilizing the full training set for\nclassification tasks. We have compared our approach to other self-supervised\nones and showed that, unlike them, ours can consistently improve on the\nbaseline network performance as well as provide a better data-efficiency.\n","authors":["Stefan Gerd Fritsch","Cennet Oguz","Vitor Fortes Rey","Lala Ray","Maximilian Kiefer-Emmanouilidis","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2406.03857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13517v1","updated":"2024-10-17T13:06:02Z","published":"2024-10-17T13:06:02Z","title":"Bias in the Mirror : Are LLMs opinions robust to their own adversarial\n  attacks ?","summary":"  Large language models (LLMs) inherit biases from their training data and\nalignment processes, influencing their responses in subtle ways. While many\nstudies have examined these biases, little work has explored their robustness\nduring interactions. In this paper, we introduce a novel approach where two\ninstances of an LLM engage in self-debate, arguing opposing viewpoints to\npersuade a neutral version of the model. Through this, we evaluate how firmly\nbiases hold and whether models are susceptible to reinforcing misinformation or\nshifting to harmful viewpoints. Our experiments span multiple LLMs of varying\nsizes, origins, and languages, providing deeper insights into bias persistence\nand flexibility across linguistic and cultural contexts.\n","authors":["Virgile Rennard","Christos Xypolopoulos","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2410.13517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13510v1","updated":"2024-10-17T12:56:52Z","published":"2024-10-17T12:56:52Z","title":"GeoCoder: Solving Geometry Problems by Generating Modular Code through\n  Vision-Language Models","summary":"  Geometry problem-solving demands advanced reasoning abilities to process\nmultimodal inputs and employ mathematical knowledge effectively.\nVision-language models (VLMs) have made significant progress in various\nmultimodal tasks. Yet, they still struggle with geometry problems and are\nsignificantly limited by their inability to perform mathematical operations not\nseen during pre-training, such as calculating the cosine of an arbitrary angle,\nand by difficulties in correctly applying relevant geometry formulas. To\novercome these challenges, we present GeoCoder, which leverages modular\ncode-finetuning to generate and execute code using a predefined geometry\nfunction library. By executing the code, we achieve accurate and deterministic\ncalculations, contrasting the stochastic nature of autoregressive token\nprediction, while the function library minimizes errors in formula usage. We\nalso propose a multimodal retrieval-augmented variant of GeoCoder, named\nRAG-GeoCoder, which incorporates a non-parametric memory module for retrieving\nfunctions from the geometry library, thereby reducing reliance on parametric\nmemory. Our modular code-finetuning approach enhances the geometric reasoning\ncapabilities of VLMs, yielding an average improvement of over 16% across\nvarious question complexities on the GeomVerse dataset compared to other\nfinetuning methods.\n","authors":["Aditya Sharma","Aman Dalmia","Mehran Kazemi","Amal Zouaq","Christopher J. Pal"],"pdf_url":"https://arxiv.org/pdf/2410.13510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13509v1","updated":"2024-10-17T12:53:29Z","published":"2024-10-17T12:53:29Z","title":"RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable\n  Data Rewards","summary":"  Retrieval-Augmented Generation (RAG) has proven its effectiveness in\nmitigating hallucinations in Large Language Models (LLMs) by retrieving\nknowledge from external resources. To adapt LLMs for RAG pipelines, current\napproaches use instruction tuning to optimize LLMs, improving their ability to\nutilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses\non equipping LLMs to handle diverse RAG tasks using different instructions.\nHowever, it trains RAG modules to overfit training signals and overlooks the\nvarying data preferences among agents within the RAG system. In this paper, we\npropose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG\nsystems by aligning data preferences between different RAG modules. DDR works\nby collecting the rewards to optimize each agent with a rollout method. This\nmethod prompts agents to sample some potential responses as perturbations,\nevaluates the impact of these perturbations on the whole RAG system, and\nsubsequently optimizes the agent to produce outputs that improve the\nperformance of the RAG system. Our experiments on various knowledge-intensive\ntasks demonstrate that DDR significantly outperforms the SFT method,\nparticularly for LLMs with smaller-scale parameters that depend more on the\nretrieved knowledge. Additionally, DDR exhibits a stronger capability to align\nthe data preference between RAG modules. The DDR method makes generation module\nmore effective in extracting key information from documents and mitigating\nconflicts between parametric memory and external knowledge. All codes are\navailable at https://github.com/OpenMatch/RAG-DDR.\n","authors":["Xinze Li","Sen Mei","Zhenghao Liu","Yukun Yan","Shuo Wang","Shi Yu","Zheni Zeng","Hao Chen","Ge Yu","Zhiyuan Liu","Maosong Sun","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.13509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v1","updated":"2024-10-17T12:48:14Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to problems that\nare more complex than the ones on which they have been trained. Empirical\ninvestigations of such questions are impeded by two major flaws of current\nevaluations: (i) much of the evaluation data is contaminated, in the sense that\nit has already been seen during training, and (ii) benchmark datasets do not\ncapture how problem proofs may be arbitrarily complex in various ways. As a\nstep towards addressing these issues, we present a framework for evaluating\nLLMs on problems that have arbitrarily complex arithmetic proofs, called\nMathGAP. MathGAP generates problems that follow fixed proof specifications --\nalong with chain-of-thought reasoning annotations -- enabling systematic\nstudies on generalization with respect to arithmetic proof complexity. We apply\nMathGAP to analyze how in-context learning interacts with generalization to\nproblems that have more complex proofs. We find that among the models tested,\nmost show a significant decrease in performance as proofs get deeper and wider.\nThis effect is more pronounced in complex, nonlinear proof structures, which\nare challenging even for GPT-4o. Surprisingly, providing in-context examples\nfrom the same distribution as the test set is not always beneficial for\nperformance. In particular, zero-shot prompting as well as demonstrating a\ndiverse range of examples that are less complex than the test data sometimes\nyield similar or higher accuracies.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schölkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.13498v1","updated":"2024-10-17T12:43:49Z","published":"2024-10-17T12:43:49Z","title":"Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum\n  Learning, Semi-Supervised Training, and Advanced Optimization Techniques","summary":"  Text generation is the automated process of producing written or spoken\nlanguage using computational methods. It involves generating coherent and\ncontextually relevant text based on predefined rules or learned patterns.\nHowever, challenges in text generation arise from maintaining coherence,\nensuring diversity and creativity, and avoiding biases or inappropriate\ncontent. This research paper developed a novel approach to improve text\ngeneration in the context of joint Natural Language Generation (NLG) and\nNatural Language Understanding (NLU) learning. The data is prepared by\ngathering and preprocessing annotated datasets, including cleaning,\ntokenization, stemming, and stop-word removal. Feature extraction techniques\nsuch as POS tagging, Bag of words, and Term Frequency-Inverse Document\nFrequency (TF-IDF) are applied. Transformer-based encoders and decoders,\ncapturing long range dependencies and improving source-target sequence\nmodelling. Pre-trained language models like Optimized BERT are incorporated,\nalong with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).\nReinforcement learning with policy gradient techniques, semi-supervised\ntraining, improved attention mechanisms, and differentiable approximations like\nstraight-through Gumbel SoftMax estimator are employed to fine-tune the models\nand handle complex linguistic tasks effectively. The proposed model is\nimplemented using Python.\n","authors":["Rahimanuddin Shaik","Katikela Sreeharsha Kishore"],"pdf_url":"https://arxiv.org/pdf/2410.13498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13497v1","updated":"2024-10-17T12:43:47Z","published":"2024-10-17T12:43:47Z","title":"Repetition Neurons: How Do Language Models Produce Repetitions?","summary":"  This paper introduces repetition neurons, regarded as skill neurons\nresponsible for the repetition problem in text generation tasks. These neurons\nare progressively activated more strongly as repetition continues, indicating\nthat they perceive repetition as a task to copy the previous context\nrepeatedly, similar to in-context learning. We identify these repetition\nneurons by comparing activation values before and after the onset of repetition\nin texts generated by recent pre-trained language models. We analyze the\nrepetition neurons in three English and one Japanese pre-trained language\nmodels and observe similar patterns across them.\n","authors":["Tatsuya Hiraoka","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2410.13497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17453v3","updated":"2024-10-17T12:43:21Z","published":"2024-06-25T10:44:01Z","title":"Learning to Ask Informative Questions: Enhancing LLMs with Preference\n  Optimization and Expected Information Gain","summary":"  Questions are essential tools for acquiring the necessary information to\ncomplete information-seeking tasks. However, large language models (LLMs),\nespecially open-source models, often perform poorly in generating informative\nquestions, as measured by expected information gain (EIG). In this paper, we\npropose a method to enhance the informativeness of LLM-generated questions in\n20-question game dialogues. We sample multiple questions from the same model\n(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG\nquestions to apply a Direct Preference Optimization (DPO) algorithm. Our\nresults show that this method produces more effective questions (in terms of\nEIG), even in domains different from those used to train the DPO model.\n","authors":["Davide Mazzaccara","Alberto Testoni","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.17453v3.pdf","comment":"Accepted to EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2405.02933v2","updated":"2024-10-17T12:39:05Z","published":"2024-05-05T13:42:25Z","title":"Relay Decoding: Concatenating Large Language Models for Machine\n  Translation","summary":"  Leveraging large language models for machine translation has demonstrated\npromising results. However, it does require the large language models to\npossess the capability of handling both the source and target languages in\nmachine translation. When it is challenging to find large models that support\nthe desired languages, resorting to continuous learning methods becomes a\ncostly endeavor. To mitigate these expenses, we propose an innovative approach\ncalled RD (Relay Decoding), which entails concatenating two distinct large\nmodels that individually support the source and target languages. By\nincorporating a simple mapping layer to facilitate the connection between these\ntwo models and utilizing a limited amount of parallel data for training, we\nsuccessfully achieve superior results in the machine translation task.\nExperimental results conducted on the Multi30k and WikiMatrix datasets validate\nthe effectiveness of our proposed method.\n","authors":["Chengpeng Fu","Xiaocheng Feng","Yichong Huang","Wenshuai Huo","Baohang Li","Hui Wang","Bin Qin","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2405.02933v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.13488v1","updated":"2024-10-17T12:32:00Z","published":"2024-10-17T12:32:00Z","title":"Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes","summary":"  Detecting offensive memes is crucial, yet standard deep neural network\nsystems often remain opaque. Various input attribution-based methods attempt to\ninterpret their behavior, but they face challenges with implicitly offensive\nmemes and non-causal attributions. To address these issues, we propose a\nframework based on a Structural Causal Model (SCM). In this framework,\nVisualBERT is trained to predict the class of an input meme based on both meme\ninput and causal concepts, allowing for transparent interpretation. Our\nqualitative evaluation demonstrates the framework's effectiveness in\nunderstanding model behavior, particularly in determining whether the model was\nright due to the right reason, and in identifying reasons behind\nmisclassification. Additionally, quantitative analysis assesses the\nsignificance of proposed modelling choices, such as de-confounding, adversarial\nlearning, and dynamic routing, and compares them with input attribution\nmethods. Surprisingly, we find that input attribution methods do not guarantee\ncausality within our framework, raising questions about their reliability in\nsafety-critical applications. The project page is at:\nhttps://newcodevelop.github.io/causality_adventure/\n","authors":["Dibyanayan Bandyopadhyay","Mohammed Hasanuzzaman","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2410.13488v1.pdf","comment":"Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.10850v2","updated":"2024-10-17T11:52:38Z","published":"2024-10-06T07:40:11Z","title":"On the Reliability of Large Language Models to Misinformed and\n  Demographically-Informed Prompts","summary":"  We investigate and observe the behaviour and performance of Large Language\nModel (LLM)-backed chatbots in addressing misinformed prompts and questions\nwith demographic information within the domains of Climate Change and Mental\nHealth. Through a combination of quantitative and qualitative methods, we\nassess the chatbots' ability to discern the veracity of statements, their\nadherence to facts, and the presence of bias or misinformation in their\nresponses. Our quantitative analysis using True/False questions reveals that\nthese chatbots can be relied on to give the right answers to these close-ended\nquestions. However, the qualitative insights, gathered from domain experts,\nshows that there are still concerns regarding privacy, ethical implications,\nand the necessity for chatbots to direct users to professional services. We\nconclude that while these chatbots hold significant promise, their deployment\nin sensitive areas necessitates careful consideration, ethical oversight, and\nrigorous refinement to ensure they serve as a beneficial augmentation to human\nexpertise rather than an autonomous solution.\n","authors":["Toluwani Aremu","Oluwakemi Akinwehinmi","Chukwuemeka Nwagu","Syed Ishtiaque Ahmed","Rita Orji","Pedro Arnau Del Amo","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2410.10850v2.pdf","comment":"Study conducted between August and December 2023. Under review at\n  AAAI-AI Magazine. Submitted for archival purposes only"},{"id":"http://arxiv.org/abs/2410.13464v1","updated":"2024-10-17T11:48:57Z","published":"2024-10-17T11:48:57Z","title":"IterSelectTune: An Iterative Training Framework for Efficient\n  Instruction-Tuning Data Selection","summary":"  As large language models (LLMs) continue to advance, instruction tuning has\nbecome critical for improving their ability to generate accurate and\ncontextually appropriate responses. Although numerous instruction-tuning\ndatasets have been developed to enhance LLM performance, selecting high-quality\ninstruction data from large source datasets typically demands significant human\neffort. In this work, we introduce $\\textbf{IterSelectTune}$, an efficient,\ncost-effective iterative training policy for selecting high-quality instruction\ndata with no human involvement and limited reliance on GPT-4. By fine-tuning on\napproximately 20\\% of the source data, our method consistently outperforms\nmodels fine-tuned on the full dataset across multiple benchmarks and public\ntest datasets. These results highlight the effectiveness of our approach in\nenhancing LLM performance while reducing the computational resources required\nfor instruction tuning.\n","authors":["Jielin Song","Siyu Liu","Bin Zhu","Yanghui Rao"],"pdf_url":"https://arxiv.org/pdf/2410.13464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13461v1","updated":"2024-10-17T11:46:33Z","published":"2024-10-17T11:46:33Z","title":"Progressive Mixed-Precision Decoding for Efficient LLM Inference","summary":"  In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality.\n","authors":["Hao Mark Chen","Fuwen Tan","Alexandros Kouris","Royson Lee","Hongxiang Fan","Stylianos I. Venieris"],"pdf_url":"https://arxiv.org/pdf/2410.13461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16807v2","updated":"2024-10-17T11:46:18Z","published":"2024-06-24T17:19:34Z","title":"Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback\n  for Text-to-Image Generation","summary":"  Human feedback plays a critical role in learning and refining reward models\nfor text-to-image generation, but the optimal form the feedback should take for\nlearning an accurate reward function has not been conclusively established.\nThis paper investigates the effectiveness of fine-grained feedback which\ncaptures nuanced distinctions in image quality and prompt-alignment, compared\nto traditional coarse-grained feedback (for example, thumbs up/down or ranking\nbetween a set of options). While fine-grained feedback holds promise,\nparticularly for systems catering to diverse societal preferences, we show that\ndemonstrating its superiority to coarse-grained feedback is not automatic.\nThrough experiments on real and synthetic preference data, we surface the\ncomplexities of building effective models due to the interplay of model choice,\nfeedback type, and the alignment between human judgment and computational\ninterpretation. We identify key challenges in eliciting and utilizing\nfine-grained feedback, prompting a reassessment of its assumed benefits and\npracticality. Our findings -- e.g., that fine-grained feedback can lead to\nworse models for a fixed budget, in some settings; however, in controlled\nsettings with known attributes, fine grained rewards can indeed be more helpful\n-- call for careful consideration of feedback attributes and potentially beckon\nnovel modeling approaches to appropriately unlock the potential value of\nfine-grained feedback in-the-wild.\n","authors":["Katherine M. Collins","Najoung Kim","Yonatan Bitton","Verena Rieser","Shayegan Omidshafiei","Yushi Hu","Sherol Chen","Senjuti Dutta","Minsuk Chang","Kimin Lee","Youwei Liang","Georgina Evans","Sahil Singla","Gang Li","Adrian Weller","Junfeng He","Deepak Ramachandran","Krishnamurthy Dj Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2406.16807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13460v1","updated":"2024-10-17T11:43:16Z","published":"2024-10-17T11:43:16Z","title":"Breaking the Manual Annotation Bottleneck: Creating a Comprehensive\n  Legal Case Criticality Dataset through Semi-Automated Labeling","summary":"  Predicting case criticality helps legal professionals in the court system\nmanage large volumes of case law. This paper introduces the Criticality\nPrediction dataset, a new resource for evaluating the potential influence of\nSwiss Federal Supreme Court decisions on future jurisprudence. Unlike existing\napproaches that rely on resource-intensive manual annotations, we\nsemi-automatically derive labels leading to a much larger dataset than\notherwise possible. Our dataset features a two-tier labeling system: (1) the\nLD-Label, which identifies cases published as Leading Decisions (LD), and (2)\nthe Citation-Label, which ranks cases by their citation frequency and recency.\nThis allows for a more nuanced evaluation of case importance. We evaluate\nseveral multilingual models, including fine-tuned variants and large language\nmodels, and find that fine-tuned models consistently outperform zero-shot\nbaselines, demonstrating the need for task-specific adaptation. Our\ncontributions include the introduction of this task and the release of a\nmultilingual dataset to the research community.\n","authors":["Ronja Stern","Ken Kawamura","Matthias Stürmer","Ilias Chalkidis","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2410.13460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13458v1","updated":"2024-10-17T11:38:54Z","published":"2024-10-17T11:38:54Z","title":"MedINST: Meta Dataset of Biomedical Instructions","summary":"  The integration of large language model (LLM) techniques in the field of\nmedical analysis has brought about significant advancements, yet the scarcity\nof large, diverse, and well-annotated datasets remains a major challenge.\nMedical data and tasks, which vary in format, size, and other parameters,\nrequire extensive preprocessing and standardization for effective use in\ntraining LLMs. To address these challenges, we introduce MedINST, the Meta\nDataset of Biomedical Instructions, a novel multi-domain, multi-task\ninstructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over\n7 million training samples, making it the most comprehensive biomedical\ninstruction dataset to date. Using MedINST as the meta dataset, we curate\nMedINST32, a challenging benchmark with different task difficulties aiming to\nevaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and\nevaluate on MedINST32, showcasing enhanced cross-task generalization.\n","authors":["Wenhan Han","Meng Fang","Zihan Zhang","Yu Yin","Zirui Song","Ling Chen","Mykola Pechenizkiy","Qingyu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13456v1","updated":"2024-10-17T11:34:07Z","published":"2024-10-17T11:34:07Z","title":"Unlocking Legal Knowledge: A Multilingual Dataset for Judicial\n  Summarization in Switzerland","summary":"  Legal research is a time-consuming task that most lawyers face on a daily\nbasis. A large part of legal research entails looking up relevant caselaw and\nbringing it in relation to the case at hand. Lawyers heavily rely on summaries\n(also called headnotes) to find the right cases quickly. However, not all\ndecisions are annotated with headnotes and writing them is time-consuming.\nAutomated headnote creation has the potential to make hundreds of thousands of\ndecisions more accessible for legal research in Switzerland alone. To kickstart\nthis, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a\nnovel cross-lingual resource featuring 18K court rulings from the Swiss Federal\nSupreme Court (SFSC), in German, French, and Italian, along with German\nheadnotes. We fine-tune and evaluate three mT5 variants, along with proprietary\nmodels. Our analysis highlights that while proprietary models perform well in\nzero-shot and one-shot settings, fine-tuned smaller models still provide a\nstrong competitive edge. We publicly release the dataset to facilitate further\nresearch in multilingual legal summarization and the development of assistive\ntechnologies for legal professionals\n","authors":["Luca Rolshoven","Vishvaksenan Rasiah","Srinanda Brügger Bose","Matthias Stürmer","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2410.13456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11843v2","updated":"2024-10-17T11:26:10Z","published":"2024-07-16T15:24:44Z","title":"InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive\n  Evaluation and Human Feedback","summary":"  A crucial requirement for deploying LLM-based agents in real-life\napplications is the robustness against risky or even irreversible mistakes.\nHowever, the existing research lacks a focus on preemptive evaluation of\nreasoning trajectories performed by LLM agents, leading to a gap in ensuring\nsafe and reliable operations. To explore better solutions, this paper\nintroduces InferAct, a novel approach that leverages the belief reasoning\nability of LLMs, grounded in Theory-of-Mind, to proactively detect potential\nerrors before risky actions are executed (e.g., `buy-now' in automatic online\ntrading or web shopping). InferAct acts as a human proxy, detecting unsafe\nactions and alerting users for intervention, which helps prevent irreversible\nrisks in time and enhances the actor agent's decision-making process.\nExperiments on three widely-used tasks demonstrate the effectiveness of\nInferAct, presenting a novel solution for safely developing LLM agents in\nenvironments involving critical decision-making.\n","authors":["Haishuo Fang","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.11843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13445v1","updated":"2024-10-17T11:19:44Z","published":"2024-10-17T11:19:44Z","title":"Parameter-efficient Adaptation of Multilingual Multimodal Models for\n  Low-resource ASR","summary":"  Automatic speech recognition (ASR) for low-resource languages remains a\nchallenge due to the scarcity of labeled training data. Parameter-efficient\nfine-tuning and text-only adaptation are two popular methods that have been\nused to address such low-resource settings. In this work, we investigate how\nthese techniques can be effectively combined using a multilingual multimodal\nmodel like SeamlessM4T. Multimodal models are able to leverage unlabeled text\nvia text-only adaptation with further parameter-efficient ASR fine-tuning, thus\nboosting ASR performance. We also show cross-lingual transfer from a\nhigh-resource language, achieving up to a relative 17% WER reduction over a\nbaseline in a zero-shot setting without any labeled speech.\n","authors":["Abhishek Gupta","Amruta Parulekar","Sameep Chattopadhyay","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2410.13445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13443v1","updated":"2024-10-17T11:18:23Z","published":"2024-10-17T11:18:23Z","title":"NLIP_Lab-IITH Multilingual MT System for WAT24 MT Shared Task","summary":"  This paper describes NLIP Lab's multilingual machine translation system for\nthe WAT24 shared task on multilingual Indic MT task for 22 scheduled languages\nbelonging to 4 language families. We explore pre-training for Indic languages\nusing alignment agreement objectives. We utilize bi-lingual dictionaries to\nsubstitute words from source sentences. Furthermore, we fine-tuned language\ndirection-specific multilingual translation models using small and high-quality\nseed data. Our primary submission is a 243M parameters multilingual translation\nmodel covering 22 Indic languages. In the IN22-Gen benchmark, we achieved an\naverage chrF++ score of 46.80 and 18.19 BLEU score for the En-Indic direction.\nIn the Indic-En direction, we achieved an average chrF++ score of 56.34 and\n30.82 BLEU score. In the In22-Conv benchmark, we achieved an average chrF++\nscore of 43.43 and BLEU score of 16.58 in the En-Indic direction, and in the\nIndic-En direction, we achieved an average of 52.44 and 29.77 for chrF++ and\nBLEU respectively. Our model\\footnote{Our code and models are available at\n\\url{https://github.com/maharajbrahma/WAT2024-MultiIndicMT}} is competitive\nwith IndicTransv1 (474M parameter model).\n","authors":["Maharaj Brahma","Pramit Sahoo","Maunendra Sankar Desarkar"],"pdf_url":"https://arxiv.org/pdf/2410.13443v1.pdf","comment":"WMT 24 WAT Shared Task IndicMultiMT (Best System)"},{"id":"http://arxiv.org/abs/2410.13439v1","updated":"2024-10-17T11:12:55Z","published":"2024-10-17T11:12:55Z","title":"Similarity-Dissimilarity Loss with Supervised Contrastive Learning for\n  Multi-label Classification","summary":"  Supervised contrastive learning has been explored in making use of label\ninformation for multi-label classification, but determining positive samples in\nmulti-label scenario remains challenging. Previous studies have examined\nstrategies for identifying positive samples, considering label overlap\nproportion between anchors and samples. However, they ignore various relations\nbetween given anchors and samples, as well as how to dynamically adjust the\nweights in contrastive loss functions based on different relations, leading to\ngreat ambiguity. In this paper, we introduce five distinct relations between\nmulti-label samples and propose a Similarity-Dissimilarity Loss with\ncontrastive learning for multi-label classification. Our loss function\nre-weights the loss by computing the similarity and dissimilarity between\npositive samples and a given anchor based on the introduced relations. We\nmainly conduct experiments for multi-label text classification on MIMIC\ndatasets, then further extend the evaluation on MS-COCO. The Experimental\nresults show that our proposed loss effectively improves the performance on all\nencoders under supervised contrastive learning paradigm, demonstrating its\neffectiveness and robustness.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Sheng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12298v2","updated":"2024-10-17T11:00:37Z","published":"2024-10-16T06:57:18Z","title":"Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large\n  Language Models and Knowledge Graphs","summary":"  Large Language Models (LLMs) possess impressive reasoning abilities but are\nprone to generating incorrect information, often referred to as hallucinations.\nWhile incorporating external Knowledge Graphs (KGs) can partially mitigate this\nissue, existing methods primarily treat KGs as static knowledge repositories,\noverlooking the critical disparity between KG and LLM knowledge, and failing to\nfully exploit the reasoning capabilities inherent in KGs. To address these\nlimitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for\nseamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis\nto construct a hierarchical pyramid structure. This structure is designed to\nreflect the input question and generate more validated deductive knowledge,\nthereby enhancing the alignment of LLMs and KGs and ensuring more cohesive\nintegration. Furthermore, PDA employs a recursive mechanism to harness the\nunderlying reasoning abilities of KGs, resulting in more accurate knowledge\nretrieval for question-answering tasks. Our experimental results reveal a\nsubstantial performance advantage of PDA over state-of-the-art baselines, with\nimprovements reaching 26.70% and 26.78%.\n","authors":["Lei Sun","Xinchen Wang","Youdi Li"],"pdf_url":"https://arxiv.org/pdf/2410.12298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14928v2","updated":"2024-10-17T10:30:41Z","published":"2024-06-21T07:37:19Z","title":"Autonomous Agents for Collaborative Task under Information Asymmetry","summary":"  Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great\nprogress in solving complex tasks. It performs communication among agents\nwithin the system to collaboratively solve tasks, under the premise of shared\ninformation. However, when agents' collaborations are leveraged to perform\nmulti-person tasks, a new challenge arises due to information asymmetry, since\neach agent can only access the information of its human user. Previous MAS\nstruggle to complete tasks under this condition. To address this, we propose a\nnew MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems.\nIn iAgents, the human social network is mirrored in the agent network, where\nagents proactively exchange human information necessary for task resolution,\nthereby overcoming information asymmetry. iAgents employs a novel agent\nreasoning mechanism, InfoNav, to navigate agents' communication toward\neffective information exchange. Together with InfoNav, iAgents organizes human\ninformation in a mixed memory to provide agents with accurate and comprehensive\ninformation for exchange. Additionally, we introduce InformativeBench, the\nfirst benchmark tailored for evaluating LLM agents' task-solving ability under\ninformation asymmetry. Experimental results show that iAgents can collaborate\nwithin a social network of 140 individuals and 588 relationships, autonomously\ncommunicate over 30 turns, and retrieve information from nearly 70,000 messages\nto complete tasks within 3 minutes.\n","authors":["Wei Liu","Chenxi Wang","Yifei Wang","Zihao Xie","Rennai Qiu","Yufan Dang","Zhuoyun Du","Weize Chen","Cheng Yang","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2406.14928v2.pdf","comment":"32 pages, 12 figures, 6 tables, accepted by NeurIPS 2024, see detail\n  at https://thinkwee.top/iagents"},{"id":"http://arxiv.org/abs/2410.13413v1","updated":"2024-10-17T10:23:24Z","published":"2024-10-17T10:23:24Z","title":"Think Thrice Before You Act: Progressive Thought Refinement in Large\n  Language Models","summary":"  Recent advancements in large language models (LLMs) have demonstrated that\nprogressive refinement, rather than providing a single answer, results in more\naccurate and thoughtful outputs. However, existing methods often rely heavily\non supervision signals to evaluate previous responses, making it difficult to\nassess output quality in more open-ended scenarios effectively. Additionally,\nthese methods are typically designed for specific tasks, which limits their\ngeneralization to new domains. To address these limitations, we propose\nProgressive Thought Refinement (PTR), a framework that enables LLMs to refine\ntheir responses progressively. PTR operates in two phases: (1) Thought data\nconstruction stage: We propose a weak and strong model collaborative selection\nstrategy to build a high-quality progressive refinement dataset to ensure\nlogical consistency from thought to answers, and the answers are gradually\nrefined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training\nstructure to mask the \"thought\" and adjust loss weights to encourage LLMs to\nrefine prior thought, teaching them to implicitly understand \"how to improve\"\nrather than \"what is correct.\" Experimental results show that PTR significantly\nenhances LLM performance across ten diverse tasks (avg. from 49.6% to 53.5%)\nwithout task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also\ndemonstrate substantial improvements in the quality of responses beyond mere\naccuracy, suggesting that PTR truly teaches LLMs to self-improve over time.\n","authors":["Chengyu Du","Jinyi Han","Yizhou Ying","Aili Chen","Qianyu He","Haokun Zhao","Sirui Xia","Haoran Guo","Jiaqing Liang","Zulong Chen","Liangyue Li","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.13413v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13409v1","updated":"2024-10-17T10:16:56Z","published":"2024-10-17T10:16:56Z","title":"Attr-Int: A Simple and Effective Entity Alignment Framework for\n  Heterogeneous Knowledge Graphs","summary":"  Entity alignment (EA) refers to the task of linking entities in different\nknowledge graphs (KGs). Existing EA methods rely heavily on structural\nisomorphism. However, in real-world KGs, aligned entities usually have\nnon-isomorphic neighborhood structures, which paralyses the application of\nthese structure-dependent methods. In this paper, we investigate and tackle the\nproblem of entity alignment between heterogeneous KGs. First, we propose two\nnew benchmarks to closely simulate real-world EA scenarios of heterogeneity.\nThen we conduct extensive experiments to evaluate the performance of\nrepresentative EA methods on the new benchmarks. Finally, we propose a simple\nand effective entity alignment framework called Attr-Int, in which innovative\nattribute information interaction methods can be seamlessly integrated with any\nembedding encoder for entity alignment, improving the performance of existing\nentity alignment techniques. Experiments demonstrate that our framework\noutperforms the state-of-the-art approaches on two new benchmarks.\n","authors":["Linyan Yang","Jingwei Cheng","Chuanhao Xu","Xihao Wang","Jiayi Li","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13408v1","updated":"2024-10-17T10:14:52Z","published":"2024-10-17T10:14:52Z","title":"MoR: Mixture of Ranks for Low-Rank Adaptation Tuning","summary":"  Low-Rank Adaptation (LoRA) drives research to align its performance with full\nfine-tuning. However, significant challenges remain: (1) Simply increasing the\nrank size of LoRA does not effectively capture high-rank information, which\nleads to a performance bottleneck.(2) MoE-style LoRA methods substantially\nincrease parameters and inference latency, contradicting the goals of efficient\nfine-tuning and ease of application. To address these challenges, we introduce\nMixture of Ranks (MoR), which learns rank-specific information for different\ntasks based on input and efficiently integrates multi-rank information. We\nfirstly propose a new framework that equates the integration of multiple LoRAs\nto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA\nalready captures sufficient intrinsic information, and MoR can derive high-rank\ninformation through mathematical transformations of the low-rank components.\nThus, MoR can reduces the learning difficulty of LoRA and enhances its\nmulti-task capabilities. MoR achieves impressive results, with MoR delivering a\n1.31\\% performance improvement while using only 93.93\\% of the parameters\ncompared to baseline methods.\n","authors":["Chuanyu Tang","Yilong Chen","Zhenyu Zhang","Junyuan Shang","Wenyuan Zhang","Yong Huang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13408v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.13400v1","updated":"2024-10-17T09:54:54Z","published":"2024-10-17T09:54:54Z","title":"Towards Hybrid Intelligence in Journalism: Findings and Lessons Learnt\n  from a Collaborative Analysis of Greek Political Rhetoric by ChatGPT and\n  Humans","summary":"  This chapter introduces a research project titled \"Analyzing the Political\nDiscourse: A Collaboration Between Humans and Artificial Intelligence\", which\nwas initiated in preparation for Greece's 2023 general elections. The project\nfocused on the analysis of political leaders' campaign speeches, employing\nArtificial Intelligence (AI), in conjunction with an interdisciplinary team\ncomprising journalists, a political scientist, and data scientists. The chapter\ndelves into various aspects of political discourse analysis, including\nsentiment analysis, polarization, populism, topic detection, and Named Entities\nRecognition (NER). This experimental study investigates the capabilities of\nlarge language model (LLMs), and in particular OpenAI's ChatGPT, for analyzing\npolitical speech, evaluates its strengths and weaknesses, and highlights the\nessential role of human oversight in using AI in journalism projects and\npotentially other societal sectors. The project stands as an innovative example\nof human-AI collaboration (known also as \"hybrid intelligence\") within the\nrealm of digital humanities, offering valuable insights for future initiatives.\n","authors":["Thanasis Troboukis","Kelly Kiki","Antonis Galanopoulos","Pavlos Sermpezis","Stelios Karamanidis","Ilias Dimitriadis","Athena Vakali"],"pdf_url":"https://arxiv.org/pdf/2410.13400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13396v1","updated":"2024-10-17T09:48:08Z","published":"2024-10-17T09:48:08Z","title":"Linguistically Grounded Analysis of Language Models using Shapley Head\n  Values","summary":"  Understanding how linguistic knowledge is encoded in language models is\ncrucial for improving their generalisation capabilities. In this paper, we\ninvestigate the processing of morphosyntactic phenomena, by leveraging a\nrecently proposed method for probing language models via Shapley Head Values\n(SHVs). Using the English language BLiMP dataset, we test our approach on two\nwidely used models, BERT and RoBERTa, and compare how linguistic constructions\nsuch as anaphor agreement and filler-gap dependencies are handled. Through\nquantitative pruning and qualitative clustering analysis, we demonstrate that\nattention heads responsible for processing related linguistic phenomena cluster\ntogether. Our results show that SHV-based attributions reveal distinct patterns\nacross both models, providing insights into how language models organize and\nprocess linguistic information. These findings support the hypothesis that\nlanguage models learn subnetworks corresponding to linguistic theory, with\npotential implications for cross-linguistic model analysis and interpretability\nin Natural Language Processing (NLP).\n","authors":["Marcell Fekete","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2410.13396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13394v1","updated":"2024-10-17T09:45:32Z","published":"2024-10-17T09:45:32Z","title":"Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs","summary":"  Evaluating machine-generated text remains a significant challenge in NLP,\nespecially for non-English languages. Current methodologies, including\nautomated metrics, human assessments, and LLM-based evaluations, predominantly\nfocus on English, revealing a significant gap in multilingual evaluation\nframeworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an\nextensible framework that includes evaluator LLMs (Hercule) and a novel test\nset (Recon) specifically designed for multilingual evaluation. Our test set\nfeatures 500 human-annotated instructions spanning various task capabilities\nalong with human judgment scores across six languages. This would enable\nbenchmarking of general-purpose multilingual LLMs and facilitate\nmeta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a\ncross-lingual evaluation model that addresses the scarcity of reference answers\nin the target language by learning to assign scores to responses based on\neasily available reference answers in English. Our experiments demonstrate that\nHercule aligns more closely with human judgments compared to proprietary\nmodels, demonstrating the effectiveness of such cross-lingual evaluation in low\nresource scenarios. Further, it is also effective in zero-shot evaluation on\nunseen languages. This study is the first comprehensive examination of\ncross-lingual evaluation using LLMs, presenting a scalable and effective\napproach for multilingual assessment. All code, datasets, and models will be\npublicly available to enable further research in this important area.\n","authors":["Sumanth Doddapaneni","Mohammed Safi Ur Rahman Khan","Dilip Venkatesh","Raj Dabre","Anoop Kunchukuttan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2410.13394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13392v1","updated":"2024-10-17T09:42:30Z","published":"2024-10-17T09:42:30Z","title":"Metacognitive Monitoring: A Human Ability Beyond Generative Artificial\n  Intelligence","summary":"  Large language models (LLMs) have shown impressive alignment with human\ncognitive processes, raising questions about the extent of their similarity to\nhuman cognition. This study investigates whether LLMs, specifically ChatGPT,\npossess metacognitive monitoring abilities akin to humans-particularly in\npredicting memory performance on an item-by-item basis. We employed a\ncross-agent prediction model to compare the metacognitive performance of humans\nand ChatGPT in a language-based memory task involving garden-path sentences\npreceded by either fitting or unfitting context sentences. Both humans and\nChatGPT rated the memorability of these sentences; humans then completed a\nsurprise recognition memory test. Our findings reveal a significant positive\nrelationship between humans' memorability ratings and their actual recognition\nperformance, indicating reliable metacognitive monitoring. In contrast, ChatGPT\ndid not exhibit a similar predictive capability. Bootstrapping analyses\ndemonstrated that none of the GPT models tested (GPT-3.5-turbo, GPT-4-turbo,\nGPT-4o) could accurately predict human memory performance on a per-item basis.\nThis suggests that, despite their advanced language processing abilities and\nalignment with human cognition at the object level, current LLMs lack the\nmetacognitive mechanisms that enable humans to anticipate their memory\nperformance. These results highlight a fundamental difference between human and\nAI cognition at the metacognitive level. Addressing this gap is crucial for\ndeveloping AI systems capable of effective self-monitoring and adaptation to\nhuman needs, thereby enhancing human-AI interactions across domains such as\neducation and personalized learning.\n","authors":["Markus Huff","Elanur Ulakçı"],"pdf_url":"https://arxiv.org/pdf/2410.13392v1.pdf","comment":"28 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2403.05152"},{"id":"http://arxiv.org/abs/2410.13385v1","updated":"2024-10-17T09:37:20Z","published":"2024-10-17T09:37:20Z","title":"On the Use of Audio to Improve Dialogue Policies","summary":"  With the significant progress of speech technologies, spoken goal-oriented\ndialogue systems are becoming increasingly popular. One of the main modules of\na dialogue system is typically the dialogue policy, which is responsible for\ndetermining system actions. This component usually relies only on audio\ntranscriptions, being strongly dependent on their quality and ignoring very\nimportant extralinguistic information embedded in the user's speech. In this\npaper, we propose new architectures to add audio information by combining\nspeech and text embeddings using a Double Multi-Head Attention component. Our\nexperiments show that audio embedding-aware dialogue policies outperform\ntext-based ones, particularly in noisy transcription scenarios, and that how\ntext and audio embeddings are combined is crucial to improve performance. We\nobtained a 9.8% relative improvement in the User Request Score compared to an\nonly-text-based dialogue system on the DSTC2 dataset.\n","authors":["Daniel Roncel","Federico Costa","Javier Hernando"],"pdf_url":"https://arxiv.org/pdf/2410.13385v1.pdf","comment":"IberSpeech 2024"},{"id":"http://arxiv.org/abs/2410.12532v2","updated":"2024-10-17T09:22:41Z","published":"2024-10-16T13:10:27Z","title":"MedAide: Towards an Omni Medical Aide via Specialized LLM-based\n  Multi-Agent Collaboration","summary":"  Large Language Model (LLM)-driven interactive systems currently show\npotential promise in healthcare domains. Despite their remarkable capabilities,\nLLMs typically lack personalized recommendations and diagnosis analysis in\nsophisticated medical applications, causing hallucinations and performance\nbottlenecks. To address these challenges, this paper proposes MedAide, an\nLLM-based omni medical multi-agent collaboration framework for specialized\nhealthcare services. Specifically, MedAide first performs query rewriting\nthrough retrieval-augmented generation to accomplish accurate medical intent\nunderstanding. Immediately, we devise a contextual encoder to obtain intent\nprototype embeddings, which are used to recognize fine-grained intents by\nsimilarity matching. According to the intent relevance, the activated agents\ncollaborate effectively to provide integrated decision analysis. Extensive\nexperiments are conducted on four medical benchmarks with composite intents.\nExperimental results from automated metrics and expert doctor evaluations show\nthat MedAide outperforms current LLMs and improves their medical proficiency\nand strategic reasoning.\n","authors":["Jinjie Wei","Dingkang Yang","Yanshu Li","Qingyao Xu","Zhaoyu Chen","Mingcheng Li","Yue Jiang","Xiaolu Hou","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12532v2.pdf","comment":"LLM-based Multi-Agent Collaboration for Medical Applications"},{"id":"http://arxiv.org/abs/2410.13360v1","updated":"2024-10-17T09:10:26Z","published":"2024-10-17T09:10:26Z","title":"Remember, Retrieve and Generate: Understanding Infinite Visual Concepts\n  as Your Personalized Assistant","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13352v1","updated":"2024-10-17T09:03:38Z","published":"2024-10-17T09:03:38Z","title":"LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of\n  the European Court of Human Rights","summary":"  We present Legal Argument Reasoning (LAR), a novel task designed to evaluate\nthe legal reasoning capabilities of Large Language Models (LLMs). The task\nrequires selecting the correct next statement (from multiple choice options) in\na chain of legal arguments from court proceedings, given the facts of the case.\nWe constructed a dataset (LAR-ECHR) for this task using cases from the European\nCourt of Human Rights (ECHR). We evaluated seven general-purpose LLMs on\nLAR-ECHR and found that (a) the ranking of the models is aligned with that of\nLegalBench, an established US-based legal reasoning benchmark, even though\nLAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more\nclearly, compared to LegalBench, (c) even the best model (GPT-4o) obtains 75.8%\naccuracy on LAR-ECHR, indicating significant potential for further model\nimprovement. The process followed to construct LAR-ECHR can be replicated with\ncases from other legal systems.\n","authors":["Odysseas S. Chlapanis","Dimitrios Galanis","Ion Androutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.13352v1.pdf","comment":"Published in Natural Legal Language Processing (NLLP) 2024 workshop"},{"id":"http://arxiv.org/abs/2410.13351v1","updated":"2024-10-17T09:02:28Z","published":"2024-10-17T09:02:28Z","title":"Representation Learning of Structured Data for Medical Foundation Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious domains, including healthcare. However, their ability to effectively\nrepresent structured non-textual data, such as the alphanumeric medical codes\nused in records like ICD-10 or SNOMED-CT, is limited and has been particularly\nexposed in recent research. This paper examines the challenges LLMs face in\nprocessing medical codes due to the shortcomings of current tokenization\nmethods. As a result, we introduce the UniStruct architecture to design a\nmultimodal medical foundation model of unstructured text and structured data,\nwhich addresses these challenges by adapting subword tokenization techniques\nspecifically for the structured medical codes. Our approach is validated\nthrough model pre-training on both an extensive internal medical database and a\npublic repository of structured medical records. Trained on over 1 billion\ntokens on the internal medical database, the proposed model achieves up to a\n23% improvement in evaluation metrics, with around 2% gain attributed to our\nproposed tokenization. Additionally, when evaluated on the EHRSHOT public\nbenchmark with a 1/1000 fraction of the pre-training data, the UniStruct model\nimproves performance on over 42% of the downstream tasks. Our approach not only\nenhances the representation and generalization capabilities of patient-centric\nmodels but also bridges a critical gap in representation learning models'\nability to handle complex structured medical data, alongside unstructured text.\n","authors":["Vijay Prakash Dwivedi","Viktor Schlegel","Andy T. Liu","Thanh-Tung Nguyen","Abhinav Ramesh Kashyap","Jeng Wei","Wei-Hsian Yin","Stefan Winkler","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2410.13351v1.pdf","comment":"NeurIPS 2024 Workshop on Unifying Representations in Neural Models\n  (UniReps 2024)"},{"id":"http://arxiv.org/abs/2404.11916v2","updated":"2024-10-17T09:01:27Z","published":"2024-04-18T05:43:50Z","title":"Skeleton: A New Framework for Accelerating Language Models via Task\n  Neuron Localized Prompt Tuning","summary":"  Prompt tuning methods have shown comparable performance to general training\nmethods as parameter-efficient fine-tuning (PEFT) methods in various natural\nlanguage understanding tasks. However, existing prompt tuning methods still\nutilize the entire model architecture even when solving a specific task, which\nprevents them from accelerating inference speed during the application\nprocedure. In this paper, we propose a novel prompt tuning framework called\nSkeleton to efficiently utilize a language model in terms of memory and time\ncomplexity for solving various tasks, retaining only task-relevant neurons by\nusing an explainability method. From our framework, we can efficiently solve\nvarious tasks by using only task-relevant neurons and prepending adequate\ntask-specific prompt tokens with only a single language model. Experiments\nreveal that our method significantly enhances inference efficiency (at most x\n1.73 speed up) for various widely used benchmarks, showing comparable\nperformance to the prompt tuning method. Moreover, our method is applicable\nacross various transformer-based architectures, confirming its practicality and\nscalability.\n","authors":["Nakyeong Yang","Jiwon Moon","Junseok Kim","Yunah Jang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2404.11916v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2410.13344v1","updated":"2024-10-17T08:55:18Z","published":"2024-10-17T08:55:18Z","title":"Cerberus: Efficient Inference with Adaptive Parallel Decoding and\n  Sequential Knowledge Enhancement","summary":"  Large language models (LLMs) often face a bottleneck in inference speed due\nto their reliance on auto-regressive decoding. Recently, parallel decoding has\nshown significant promise in enhancing inference efficiency. However, we have\nidentified two key issues with existing parallel decoding frameworks: (1)\ndecoding heads fail to balance prediction accuracy and the parallelism of\nexecution, and (2) parallel decoding is not a universal solution, as it can\nbring unnecessary overheads at some challenging decoding steps. To address\nthese issues, we propose Cerberus, an adaptive parallel decoding framework\nintroduces the gating mechanism to enable the LLMs to adaptively choose\nappropriate decoding approaches at each decoding step, along with introducing a\nnew paradigm of decoding heads that introduce the sequential knowledge while\nmaintaining execution parallelism. The experiment results demonstrate that the\nCerberus can achieve up to 2.12x speed up compared to auto-regressive decoding,\nand outperforms one of the leading parallel decoding frameworks, Medusa, with a\n10% - 30% increase in acceleration and superior generation quality.\n","authors":["Yuxuan Liu","Wenyuan Li","Laizhong Cui","Hailiang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07979v2","updated":"2024-10-17T08:54:37Z","published":"2024-04-11T17:57:22Z","title":"LLoCO: Learning Long Contexts Offline","summary":"  Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.\n","authors":["Sijun Tan","Xiuyu Li","Shishir Patil","Ziyang Wu","Tianjun Zhang","Kurt Keutzer","Joseph E. Gonzalez","Raluca Ada Popa"],"pdf_url":"https://arxiv.org/pdf/2404.07979v2.pdf","comment":"EMNLP 2024. The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2410.13343v1","updated":"2024-10-17T08:52:52Z","published":"2024-10-17T08:52:52Z","title":"Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges\n  in Large Language Models","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks. However, LLMs may rely on dataset biases as\nshortcuts for prediction, which can significantly impair their robustness and\ngeneralization capabilities. This paper presents Shortcut Suite, a\ncomprehensive test suite designed to evaluate the impact of shortcuts on LLMs'\nperformance, incorporating six shortcut types, five evaluation metrics, and\nfour prompting strategies. Our extensive experiments yield several key\nfindings: 1) LLMs demonstrate varying reliance on shortcuts for downstream\ntasks, significantly impairing their performance. 2) Larger LLMs are more\nlikely to utilize shortcuts under zero-shot and few-shot in-context learning\nprompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and\noutperforms other prompting strategies, while few-shot prompts generally\nunderperform compared to zero-shot prompts. 4) LLMs often exhibit\noverconfidence in their predictions, especially when dealing with datasets that\ncontain shortcuts. 5) LLMs generally have a lower explanation quality in\nshortcut-laden datasets, with errors falling into three types: distraction,\ndisguised comprehension, and logical fallacy. Our findings offer new insights\nfor evaluating robustness and generalization in LLMs and suggest potential\ndirections for mitigating the reliance on shortcuts. The code is available at\n\\url {https://github.com/yyhappier/ShortcutSuite.git}.\n","authors":["Yu Yuan","Lili Zhao","Kai Zhang","Guangting Zheng","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13339v1","updated":"2024-10-17T08:48:54Z","published":"2024-10-17T08:48:54Z","title":"Probing-RAG: Self-Probing to Guide Language Models in Selective Document\n  Retrieval","summary":"  Retrieval-Augmented Generation (RAG) enhances language models by retrieving\nand incorporating relevant external knowledge. However, traditional\nretrieve-and-generate processes may not be optimized for real-world scenarios,\nwhere queries might require multiple retrieval steps or none at all. In this\npaper, we propose a Probing-RAG, which utilizes the hidden state\nrepresentations from the intermediate layers of language models to adaptively\ndetermine the necessity of additional retrievals for a given query. By\nemploying a pre-trained prober, Probing-RAG effectively captures the model's\ninternal cognition, enabling reliable decision-making about retrieving external\ndocuments. Experimental results across five open-domain QA datasets demonstrate\nthat Probing-RAG outperforms previous methods while reducing the number of\nredundant retrieval steps.\n","authors":["Ingeol Baek","Hwan Chang","Byeongjeong Kim","Jimin Lee","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13339v1.pdf","comment":"6 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.13334v1","updated":"2024-10-17T08:46:09Z","published":"2024-10-17T08:46:09Z","title":"Do LLMs Have Political Correctness? Analyzing Ethical Biases and\n  Jailbreak Vulnerabilities in AI Systems","summary":"  Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content. To address\nthese issues, many LLM developers have implemented various safety measures to\nalign these models. This alignment involves several techniques, including data\nfiltering during pre-training, supervised fine-tuning, reinforcement learning\nfrom human feedback, and red-teaming exercises. These methods often introduce\ndeliberate and intentional biases similar to Political Correctness (PC) to\nensure the ethical behavior of LLMs. In this paper, we delve into the\nintentional biases injected into LLMs for safety purposes and examine methods\nto circumvent these safety alignment techniques. Notably, these intentional\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20% between non-binary and cisgender keywords and by 16% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of PCJailbreak, highlighting the inherent risks posed by\nthese safety-induced biases. Additionally, we propose an efficient defense\nmethod PCDefense, which prevents jailbreak attempts by injecting defense\nprompts prior to generation. PCDefense stands as an appealing alternative to\nGuard Models, such as Llama-Guard, that require additional inference cost after\ntext generation. Our findings emphasize the urgent need for LLM developers to\nadopt a more responsible approach when designing and implementing safety\nmeasures.\n","authors":["Isack Lee","Haebin Seong"],"pdf_url":"https://arxiv.org/pdf/2410.13334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13332v1","updated":"2024-10-17T08:45:02Z","published":"2024-10-17T08:45:02Z","title":"Fine-Tuning Language Models on Multiple Datasets for Citation Intention\n  Classification","summary":"  Citation intention Classification (CIC) tools classify citations by their\nintention (e.g., background, motivation) and assist readers in evaluating the\ncontribution of scientific literature. Prior research has shown that pretrained\nlanguage models (PLMs) such as SciBERT can achieve state-of-the-art performance\non CIC benchmarks. PLMs are trained via self-supervision tasks on a large\ncorpus of general text and can quickly adapt to CIC tasks via moderate\nfine-tuning on the corresponding dataset. Despite their advantages, PLMs can\neasily overfit small datasets during fine-tuning. In this paper, we propose a\nmulti-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset\nof primary interest together with multiple auxiliary CIC datasets to take\nadvantage of additional supervision signals. We develop a data-driven task\nrelation learning (TRL) method that controls the contribution of auxiliary\ndatasets to avoid negative transfer and expensive hyper-parameter tuning. We\nconduct experiments on three CIC datasets and show that fine-tuning with\nadditional datasets can improve the PLMs' generalization performance on the\nprimary dataset. PLMs fine-tuned with our proposed framework outperform the\ncurrent state-of-the-art models by 7% to 11% on small datasets while aligning\nwith the best-performing model on a large dataset.\n","authors":["Zeren Shui","Petros Karypis","Daniel S. Karls","Mingjian Wen","Saurav Manchanda","Ellad B. Tadmor","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2410.13332v1.pdf","comment":"To be appear as a Findings paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02028v2","updated":"2024-10-17T08:31:32Z","published":"2024-10-02T20:48:28Z","title":"Are Large Language Models Good Classifiers? A Study on Edit Intent\n  Classification in Scientific Document Revisions","summary":"  Classification is a core NLP task architecture with many potential\napplications. While large language models (LLMs) have brought substantial\nadvancements in text generation, their potential for enhancing classification\ntasks remains underexplored. To address this gap, we propose a framework for\nthoroughly investigating fine-tuning LLMs for classification, including both\ngeneration- and encoding-based approaches. We instantiate this framework in\nedit intent classification (EIC), a challenging and underexplored\nclassification task. Our extensive experiments and systematic comparisons with\nvarious training approaches and a representative selection of LLMs yield new\ninsights into their application for EIC. We investigate the generalizability of\nthese findings on five further classification tasks. To demonstrate the\nproposed methods and address the data shortage for empirical edit analysis, we\nuse our best-performing EIC model to create Re3-Sci2.0, a new large-scale\ndataset of 1,780 scientific document revisions with over 94k labeled edits. The\nquality of the dataset is assessed through human evaluation. The new dataset\nenables an in-depth empirical study of human editing behavior in academic\nwriting. We make our experimental framework, models and data publicly\navailable.\n","authors":["Qian Ruan","Ilia Kuznetsov","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2410.02028v2.pdf","comment":"EMNLP2024 Main"},{"id":"http://arxiv.org/abs/2410.12622v2","updated":"2024-10-17T08:28:45Z","published":"2024-10-16T14:42:23Z","title":"From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic\n  Training Data for Classifying Social Constructs","summary":"  Computational text classification is a challenging task, especially for\nmulti-dimensional social constructs. Recently, there has been increasing\ndiscussion that synthetic training data could enhance classification by\noffering examples of how these constructs are represented in texts. In this\npaper, we systematically examine the potential of theory-driven synthetic\ntraining data for improving the measurement of social constructs. In\nparticular, we explore how researchers can transfer established knowledge from\nmeasurement instruments in the social sciences, such as survey scales or\nannotation codebooks, into theory-driven generation of synthetic data. Using\ntwo studies on measuring sexism and political topics, we assess the added value\nof synthetic training data for fine-tuning text classification models. Although\nthe results of the sexism study were less promising, our findings demonstrate\nthat synthetic data can be highly effective in reducing the need for labeled\ndata in political topic classification. With only a minimal drop in\nperformance, synthetic data allows for substituting large amounts of labeled\ndata. Furthermore, theory-driven synthetic data performed markedly better than\ndata generated without conceptual information in mind.\n","authors":["Lukas Birkenmaier","Matthias Roth","Indira Sen"],"pdf_url":"https://arxiv.org/pdf/2410.12622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13321v1","updated":"2024-10-17T08:24:27Z","published":"2024-10-17T08:24:27Z","title":"Mitigating Hallucinations in Large Vision-Language Models via\n  Summary-Guided Decoding","summary":"  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in\ngenerating detailed and coherent responses from visual inputs. However, they\nare prone to generate hallucinations due to an over-reliance on language\npriors. To address this issue, we investigate the language priors in LVLMs and\nmake two key observations: (1) Even when predicting the tokens associated with\nimage-related part-of-speech (POS), models increasingly rely on linguistic\npriors as the token sequences grow, thereby amplifying hallucinations. (2)\nMethods that directly calibrate LVLM's output distribution to mitigate language\npriors can lead to a degradation in text quality or even exacerbate\nhallucinations. Based on these findings, we propose a novel method,\nSummary-Guided Decoding (SGD). This method naturally encourages the model to\nfocus more on image information by reducing the text context through summaries,\nwhile controlling only the image-related POS tokens to maintain text quality.\nThrough experiments, we demonstrate that SGD achieves state-of-the-art\nperformance on object hallucination benchmarks. Furthermore, in terms of the\ntrade-off between precision and recall, SGD achieves Pareto optimality among\nthe existing methods. Lastly, we observe that although existing methods\nstruggle to balance the reduction of object hallucinations with maintaining\ntext quality, SGD demonstrates robustness in handling this challenge.\n","authors":["Kyungmin Min","Minbeom Kim","Kang-il Lee","Dongryeol Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2410.13321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13318v1","updated":"2024-10-17T08:20:29Z","published":"2024-10-17T08:20:29Z","title":"Computational Approaches to Arabic-English Code-Switching","summary":"  Natural Language Processing (NLP) is a vital computational method for\naddressing language processing, analysis, and generation. NLP tasks form the\ncore of many daily applications, from automatic text correction to speech\nrecognition. While significant research has focused on NLP tasks for the\nEnglish language, less attention has been given to Modern Standard Arabic and\nDialectal Arabic. Globalization has also contributed to the rise of\nCode-Switching (CS), where speakers mix languages within conversations and even\nwithin individual words (intra-word CS). This is especially common in Arab\ncountries, where people often switch between dialects or between dialects and a\nforeign language they master. CS between Arabic and English is frequent in\nEgypt, especially on social media. Consequently, a significant amount of\ncode-switched content can be found online. Such code-switched data needs to be\ninvestigated and analyzed for several NLP tasks to tackle the challenges of\nthis multilingual phenomenon and Arabic language challenges. No work has been\ndone before for several integral NLP tasks on Arabic-English CS data. In this\nwork, we focus on the Named Entity Recognition (NER) task and other tasks that\nhelp propose a solution for the NER task on CS data, e.g., Language\nIdentification. This work addresses this gap by proposing and applying\nstate-of-the-art techniques for Modern Standard Arabic and Arabic-English NER.\nWe have created the first annotated CS Arabic-English corpus for the NER task.\nAlso, we apply two enhancement techniques to improve the NER tagger on CS data\nusing CS contextual embeddings and data augmentation techniques. All methods\nshowed improvements in the performance of the NER taggers on CS data. Finally,\nwe propose several intra-word language identification approaches to determine\nthe language type of a mixed text and identify whether it is a named entity or\nnot.\n","authors":["Caroline Sabty"],"pdf_url":"https://arxiv.org/pdf/2410.13318v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2403.12675v2","updated":"2024-10-17T08:14:21Z","published":"2024-03-19T12:21:20Z","title":"Pragmatic Competence Evaluation of Large Language Models for the Korean\n  Language","summary":"  Benchmarks play a significant role in the current evaluation of Large\nLanguage Models (LLMs), yet they often overlook the models' abilities to\ncapture the nuances of human language, primarily focusing on evaluating\nembedded knowledge and technical skills. To address this gap, our study\nevaluates how well LLMs understand context-dependent expressions from a\npragmatic standpoint, specifically in Korean. We use both Multiple-Choice\nQuestions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs)\nassessed by human experts. Our results show that GPT-4 leads with scores of\n81.11 in MCQs and 85.69 in OEQs, closely followed by HyperCLOVA X.\nAdditionally, while few-shot learning generally improves performance,\nChain-of-Thought (CoT) prompting tends to encourage literal interpretations,\nwhich may limit effective pragmatic inference. Our findings highlight the need\nfor LLMs to better understand and generate language that reflects human\ncommunicative norms.\n","authors":["Dojun Park","Jiwoo Lee","Hyeyun Jeong","Seohyun Park","Sungeun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.12675v2.pdf","comment":"38th Pacific Asia Conference on Language, Information and Computation"},{"id":"http://arxiv.org/abs/2410.13313v1","updated":"2024-10-17T08:10:24Z","published":"2024-10-17T08:10:24Z","title":"Mitigating Biases to Embrace Diversity: A Comprehensive Annotation\n  Benchmark for Toxic Language","summary":"  This study introduces a prescriptive annotation benchmark grounded in\nhumanities research to ensure consistent, unbiased labeling of offensive\nlanguage, particularly for casual and non-mainstream language uses. We\ncontribute two newly annotated datasets that achieve higher inter-annotator\nagreement between human and language model (LLM) annotations compared to\noriginal datasets based on descriptive instructions. Our experiments show that\nLLMs can serve as effective alternatives when professional annotators are\nunavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated\ndata outperform models trained on larger, single-source human-annotated\ndatasets. These findings highlight the value of structured guidelines in\nreducing subjective variability, maintaining performance with limited data, and\nembracing language diversity.\n  Content Warning: This article only analyzes offensive language for academic\npurposes. Discretion is advised.\n","authors":["Xinmeng Hou"],"pdf_url":"https://arxiv.org/pdf/2410.13313v1.pdf","comment":"12 pages, 9 figures, EMNLP-NLP4DH 2024"},{"id":"http://arxiv.org/abs/2406.12494v2","updated":"2024-10-17T08:09:37Z","published":"2024-06-18T10:57:27Z","title":"LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document\n  Summarization","summary":"  Open-Domain Multi-Document Summarization (ODMDS) is the task of generating\nsummaries from large document collections in response to user queries. This\ntask is crucial for efficiently addressing diverse information needs from\nusers. Traditional retrieve-then-summarize approaches fall short for open-ended\nqueries in ODMDS tasks. These queries often require broader context than\ninitially retrieved passages provide, making it challenging to retrieve all\nrelevant information in a single search. While iterative retrieval methods has\nbeen explored for multi-hop question answering (MQA), it's impractical for\nODMDS due to high latency from repeated LLM inference. Accordingly, we propose\nLightPAL, a lightweight passage retrieval method for ODMDS. LightPAL leverages\nan LLM to pre-construct a graph representing passage relationships, then\nemploys random walk during retrieval, avoiding iterative LLM inference.\nExperiments demonstrate that LightPAL outperforms naive sparse and pre-trained\ndense retrievers in both retrieval and summarization metrics, while achieving\nhigher efficiency compared to iterative MQA approaches.\n","authors":["Masafumi Enomoto","Kunihiro Takeoka","Kosuke Akimoto","Kiril Gashteovski","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2406.12494v2.pdf","comment":"15 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.13305v1","updated":"2024-10-17T08:05:02Z","published":"2024-10-17T08:05:02Z","title":"Reference-Based Post-OCR Processing with LLM for Diacritic Languages","summary":"  Extracting fine-grained OCR text from aged documents in diacritic languages\nremains challenging due to unexpected artifacts, time-induced degradation, and\nlack of datasets. While standalone spell correction approaches have been\nproposed, they show limited performance for historical documents due to\nnumerous possible OCR error combinations and differences between modern and\nclassical corpus distributions. We propose a method utilizing available\ncontent-focused ebooks as a reference base to correct imperfect OCR-generated\ntext, supported by large language models. This technique generates\nhigh-precision pseudo-page-to-page labels for diacritic languages, where small\nstrokes pose significant challenges in historical conditions. The pipeline\neliminates various types of noise from aged documents and addresses issues such\nas missing characters, words, and disordered sequences. Our post-processing\nmethod, which generated a large OCR dataset of classical Vietnamese books,\nachieved a mean grading score of 8.72 on a 10-point scale. This outperformed\nthe state-of-the-art transformer-based Vietnamese spell correction model, which\nscored 7.03 when evaluated on a sampled subset of the dataset. We also trained\na baseline OCR model to assess and compare it with well-known engines.\nExperimental results demonstrate the strength of our baseline model compared to\nwidely used open-source solutions. The resulting dataset will be released\npublicly to support future studies.\n","authors":["Thao Do"],"pdf_url":"https://arxiv.org/pdf/2410.13305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13298v1","updated":"2024-10-17T07:55:33Z","published":"2024-10-17T07:55:33Z","title":"Advancing Large Language Model Attribution through Self-Improving","summary":"  Teaching large language models (LLMs) to generate text with citations to\nevidence sources can mitigate hallucinations and enhance verifiability in\ninformation-seeking systems. However, improving this capability requires\nhigh-quality attribution data, which is costly and labor-intensive. Inspired by\nrecent advances in self-improvement that enhance LLMs without manual\nannotation, we present START, a Self-Taught AttRibuTion framework for\niteratively improving the attribution capability of LLMs. First, to prevent\nmodels from stagnating due to initially insufficient supervision signals, START\nleverages the model to self-construct synthetic training data for warming up.\nTo further self-improve the model's attribution ability, START iteratively\nutilizes fine-grained preference supervision signals constructed from its\nsampled responses to encourage robust, comprehensive, and attributable\ngeneration. Experiments on three open-domain question-answering datasets,\ncovering long-form QA and multi-step reasoning, demonstrate significant\nperformance gains of 25.13% on average without relying on human annotations and\nmore advanced models. Further analysis reveals that START excels in aggregating\ninformation across multiple sources.\n","authors":["Lei Huang","Xiaocheng Feng","Weitao Ma","Liang Zhao","Yuchun Fan","Weihong Zhong","Dongliang Xu","Qing Yang","Hongtao Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.13298v1.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2404.06666v3","updated":"2024-10-17T07:28:23Z","published":"2024-04-10T00:26:08Z","title":"SafeGen: Mitigating Sexually Explicit Content Generation in\n  Text-to-Image Models","summary":"  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited\nremarkable performance in generating high-quality images from text descriptions\nin recent years. However, text-to-image models may be tricked into generating\nnot-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.\nExisting countermeasures mostly focus on filtering inappropriate inputs and\noutputs, or suppressing improper text embeddings, which can block sexually\nexplicit content (e.g., naked) but may still be vulnerable to adversarial\nprompts -- inputs that appear innocent but are ill-intended. In this paper, we\npresent SafeGen, a framework to mitigate sexual content generation by\ntext-to-image models in a text-agnostic manner. The key idea is to eliminate\nexplicit visual representations from the model regardless of the text input. In\nthis way, the text-to-image model is resistant to adversarial prompts since\nsuch unsafe visual representations are obstructed from within. Extensive\nexperiments conducted on four datasets and large-scale user studies demonstrate\nSafeGen's effectiveness in mitigating sexually explicit content generation\nwhile preserving the high-fidelity of benign images. SafeGen outperforms eight\nstate-of-the-art baseline methods and achieves 99.4% sexual content removal\nperformance. Furthermore, our constructed benchmark of adversarial prompts\nprovides a basis for future development and evaluation of anti-NSFW-generation\nmethods.\n","authors":["Xinfeng Li","Yuchen Yang","Jiangyi Deng","Chen Yan","Yanjiao Chen","Xiaoyu Ji","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2404.06666v3.pdf","comment":"Accepted by ACM CCS 2024. Please cite this paper as \"Xinfeng Li,\n  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.\n  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image\n  Models. In Proceedings of ACM Conference on Computer and Communications\n  Security (CCS), 2024.\""},{"id":"http://arxiv.org/abs/2410.13284v1","updated":"2024-10-17T07:28:18Z","published":"2024-10-17T07:28:18Z","title":"Learning to Route with Confidence Tokens","summary":"  Large language models (LLMs) have demonstrated impressive performance on\nseveral tasks and are increasingly deployed in real-world applications.\nHowever, especially in high-stakes settings, it becomes vital to know when the\noutput of an LLM may be unreliable. Depending on whether an answer is\ntrustworthy, a system can then choose to route the question to another expert,\nor otherwise fall back on a safe default behavior. In this work, we study the\nextent to which LLMs can reliably indicate confidence in their answers, and how\nthis notion of confidence can translate into downstream accuracy gains. We\npropose Self-REF, a lightweight training strategy to teach LLMs to express\nconfidence in whether their answers are correct in a reliable manner. Self-REF\nintroduces confidence tokens into the LLM, from which a confidence score can be\nextracted. Compared to conventional approaches such as verbalizing confidence\nand examining token probabilities, we demonstrate empirically that confidence\ntokens show significant improvements in downstream routing and rejection\nlearning tasks.\n","authors":["Yu-Neng Chuang","Helen Zhou","Prathusha Kameswara Sarma","Parikshit Gopalan","John Boccio","Sara Bolouki","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2410.13284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14991v2","updated":"2024-10-17T07:23:23Z","published":"2024-06-21T09:06:45Z","title":"SpreadsheetBench: Towards Challenging Real World Spreadsheet\n  Manipulation","summary":"  We introduce SpreadsheetBench, a challenging spreadsheet manipulation\nbenchmark exclusively derived from real-world scenarios, designed to immerse\ncurrent large language models (LLMs) in the actual workflow of spreadsheet\nusers. Unlike existing benchmarks that rely on synthesized queries and\nsimplified spreadsheet files, SpreadsheetBench is built from 912 real questions\ngathered from online Excel forums, which reflect the intricate needs of users.\nThe associated spreadsheets from the forums contain a variety of tabular data\nsuch as multiple tables, non-standard relational tables, and abundant\nnon-textual elements. Furthermore, we propose a more reliable evaluation metric\nakin to online judge platforms, where multiple spreadsheet files are created as\ntest cases for each instruction, ensuring the evaluation of robust solutions\ncapable of handling spreadsheets with varying values. Our comprehensive\nevaluation of various LLMs under both single-round and multi-round inference\nsettings reveals a substantial gap between the state-of-the-art (SOTA) models\nand human performance, highlighting the benchmark's difficulty.\n","authors":["Zeyao Ma","Bohan Zhang","Jing Zhang","Jifan Yu","Xiaokang Zhang","Xiaohan Zhang","Sijia Luo","Xi Wang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2406.14991v2.pdf","comment":"Neurips 2024 (Spotlight); Homepage:\n  https://spreadsheetbench.github.io/"},{"id":"http://arxiv.org/abs/2410.13281v1","updated":"2024-10-17T07:15:15Z","published":"2024-10-17T07:15:15Z","title":"BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla","summary":"  The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.\n","authors":["Fabiha Haider","Fariha Tanjim Shifat","Md Farhan Ishmam","Deeparghya Dutta Barua","Md Sakib Ul Rahman Sourove","Md Fahim","Md Farhad Alam"],"pdf_url":"https://arxiv.org/pdf/2410.13281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13276v1","updated":"2024-10-17T07:07:09Z","published":"2024-10-17T07:07:09Z","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs","summary":"  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity limits the efficiency and scalability of LLMs, especially\nfor those with a long-context window. A promising approach addressing this\nlimitation is to leverage the sparsity in attention. However, existing\nsparsity-based solutions predominantly rely on predefined patterns or\nheuristics to approximate sparsity. This practice falls short to fully capture\nthe dynamic nature of attention sparsity in language-based tasks. This paper\nargues that attention sparsity should be learned rather than predefined. To\nthis end, we design SeerAttention, a new Attention mechanism that augments the\nconventional attention with a learnable gate that adaptively selects\nsignificant blocks in an attention map and deems the rest blocks sparse. Such\nblock-level sparsity effectively balances accuracy and speedup. To enable\nefficient learning of the gating network, we develop a customized\nFlashAttention implementation that extracts the block-level ground truth of\nattention map with minimum overhead. SeerAttention not only applies to\npost-training, but also excels in long-context fine-tuning. Our results show\nthat at post-training stages, SeerAttention significantly outperforms\nstate-of-the-art static or heuristic-based sparse attention methods, while also\nbeing more versatile and flexible to adapt to varying context lengths and\nsparsity ratios. When applied to long-context fine-tuning with YaRN,\nSeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context\nlength with minimal perplexity loss, offering a 5.67x speedup over\nFlashAttention-2.\n","authors":["Yizhao Gao","Zhichen Zeng","Dayou Du","Shijie Cao","Hayden Kwok-Hay So","Ting Cao","Fan Yang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08440v4","updated":"2024-10-17T07:00:19Z","published":"2024-07-11T12:26:55Z","title":"Beyond Instruction Following: Evaluating Inferential Rule Following of\n  Large Language Models","summary":"  Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/\n","authors":["Wangtao Sun","Chenxiang Zhang","XueYou Zhang","Xuanqing Yu","Ziyang Huang","Pei Chen","Haotian Xu","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.08440v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13274v1","updated":"2024-10-17T07:00:15Z","published":"2024-10-17T07:00:15Z","title":"Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning","summary":"  Large language models (LLMs) serve as giant information stores, often\nincluding personal or copyrighted data, and retraining them from scratch is not\na viable option. This has led to the development of various fast, approximate\nunlearning techniques to selectively remove knowledge from LLMs. Prior research\nhas largely focused on minimizing the probabilities of specific token sequences\nby reversing the language modeling objective. However, these methods still\nleave LLMs vulnerable to adversarial attacks that exploit indirect references.\nIn this work, we examine the limitations of current unlearning techniques in\neffectively erasing a particular type of indirect prompt: multi-hop queries.\nOur findings reveal that existing methods fail to completely remove multi-hop\nknowledge when one of the intermediate hops is unlearned. To address this\nissue, we propose MUNCH, a simple uncertainty-based approach that breaks down\nmulti-hop queries into subquestions and leverages the uncertainty of the\nunlearned model in final decision-making. Empirical results demonstrate the\neffectiveness of our framework, and MUNCH can be easily integrated with\nexisting unlearning techniques, making it a flexible and useful solution for\nenhancing unlearning processes.\n","authors":["Minseok Choi","ChaeHun Park","Dohyun Lee","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2410.13274v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.13268v1","updated":"2024-10-17T06:44:06Z","published":"2024-10-17T06:44:06Z","title":"Roadmap towards Superhuman Speech Understanding using Large Language\n  Models","summary":"  The success of large language models (LLMs) has prompted efforts to integrate\nspeech and audio data, aiming to create general foundation models capable of\nprocessing both textual and non-textual inputs. Recent advances, such as\nGPT-4o, highlight the potential for end-to-end speech LLMs, which preserves\nnon-semantic information and world knowledge for deeper speech understanding.\nTo guide the development of speech LLMs, we propose a five-level roadmap,\nranging from basic automatic speech recognition (ASR) to advanced superhuman\nmodels capable of integrating non-semantic information with abstract acoustic\nknowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark,\nthat standardizes critical aspects across various tasks in these five levels,\nuncovering challenges in using abstract acoustic knowledge and completeness of\ncapability. Our findings reveal gaps in handling paralinguistic cues and\nabstract acoustic knowledge, and we offer future directions. This paper\noutlines a roadmap for advancing speech LLMs, introduces a benchmark for\nevaluation, and provides key insights into their current limitations and\npotential.\n","authors":["Fan Bu","Yuhao Zhang","Xidong Wang","Benyou Wang","Qun Liu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.13268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13267v1","updated":"2024-10-17T06:43:54Z","published":"2024-10-17T06:43:54Z","title":"CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages\n  Using Large Language Models","summary":"  Challenges in managing linguistic diversity and integrating various musical\nmodalities are faced by current music information retrieval systems. These\nlimitations reduce their effectiveness in a global, multimodal music\nenvironment. To address these issues, we introduce CLaMP 2, a system compatible\nwith 101 languages that supports both ABC notation (a text-based musical\nnotation format) and MIDI (Musical Instrument Digital Interface) for music\ninformation retrieval. CLaMP 2, pre-trained on 1.5 million ABC-MIDI-text\ntriplets, includes a multilingual text encoder and a multimodal music encoder\naligned via contrastive learning. By leveraging large language models, we\nobtain refined and consistent multilingual descriptions at scale, significantly\nreducing textual noise and balancing language distribution. Our experiments\nshow that CLaMP 2 achieves state-of-the-art results in both multilingual\nsemantic search and music classification across modalities, thus establishing a\nnew standard for inclusive and global music information retrieval.\n","authors":["Shangda Wu","Yashan Wang","Ruibin Yuan","Zhancheng Guo","Xu Tan","Ge Zhang","Monan Zhou","Jing Chen","Xuefeng Mu","Yuejie Gao","Yuanliang Dong","Jiafeng Liu","Xiaobing Li","Feng Yu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.13267v1.pdf","comment":"17 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.14065v2","updated":"2024-10-17T06:37:44Z","published":"2024-09-21T08:41:08Z","title":"Temporally Consistent Factuality Probing for Large Language Models","summary":"  The prolific use of Large Language Models (LLMs) as an alternate knowledge\nbase requires them to be factually consistent, necessitating both correctness\nand consistency traits for paraphrased queries. Recently, significant attempts\nhave been made to benchmark datasets and metrics to evaluate LLMs for these\ntraits. However, structural simplicity (subject-relation-object) and\ncontemporary association in their query formulation limit the broader\ndefinition of factuality and consistency. In this study, we introduce TeCFaP, a\nnovel Temporally Consistent Factuality Probe task to expand the consistent\nfactuality probe in the temporal dimension. To this end, we propose TEMP-COFAC,\na high-quality dataset of prefix-style English query paraphrases. Subsequently,\nwe extend the definitions of existing metrics to represent consistent\nfactuality across temporal dimension. We experiment with a diverse set of LLMs\nand find most of them performing poorly on TeCFaP. Next, we propose a novel\nsolution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining\nmulti-task instruction tuning (MT-IT) with consistent-time-sensitive\nreinforcement learning (CTSRL) to improve temporally consistent factuality in\nLLMs. Our experiments demonstrate the efficacy of CoTSeLF over several\nbaselines.\n","authors":["Ashutosh Bajpai","Aaryan Goyal","Atif Anwer","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2409.14065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03087v2","updated":"2024-10-17T06:35:00Z","published":"2023-04-06T14:12:02Z","title":"Investigating Chain-of-thought with ChatGPT for Stance Detection on\n  Social Media","summary":"  Stance detection predicts attitudes towards targets in texts and has gained\nattention with the rise of social media. Traditional approaches include\nconventional machine learning, early deep neural networks, and pre-trained\nfine-tuning models. However, with the evolution of very large pre-trained\nlanguage models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face\ndeployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not\nrequiring backpropagation training, has emerged as a promising alternative.\nThis paper examines CoT's effectiveness in stance detection tasks,\ndemonstrating its superior accuracy and discussing associated challenges.\n","authors":["Bowen Zhang","Xianghua Fu","Daijun Ding","Hu Huang","Genan Dai","Nan Yin","Yangyang Li","Liwen Jing"],"pdf_url":"https://arxiv.org/pdf/2304.03087v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2212.14548"},{"id":"http://arxiv.org/abs/2410.13259v1","updated":"2024-10-17T06:31:49Z","published":"2024-10-17T06:31:49Z","title":"From Babbling to Fluency: Evaluating the Evolution of Language Models in\n  Terms of Human Language Acquisition","summary":"  We examine the language capabilities of language models (LMs) from the\ncritical perspective of human language acquisition. Building on classical\nlanguage development theories, we propose a three-stage framework to assess the\nabilities of LMs, ranging from preliminary word understanding to complex\ngrammar and complex logical reasoning. Using this framework, we evaluate the\ngenerative capacities of LMs using methods from linguistic research. Results\nindicate that although recent LMs outperform earlier models in overall\nperformance, their developmental trajectory does not strictly follow the path\nof human language acquisition. Notably, in generation tasks, LMs are more\nsimilar to human performance in areas where information is easier to extract\nfrom the corpus, such as average word length, clauses, and auxiliary verbs.\nNewer LMs did not exhibit significant progress in terms of specific dimensions,\nsuch as clauses and auxiliary verbs, where the variation across corpora is\nrelatively limited. Register theory offers a plausible explanation for these\nobservations, suggesting that the linguistic features of the training data have\na substantial impact on the models' abilities.\n","authors":["Qiyuan Yang","Pengda Wang","Luke D. Plonsky","Frederick L. Oswald","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13258v1","updated":"2024-10-17T06:30:55Z","published":"2024-10-17T06:30:55Z","title":"A Systematic Investigation of Knowledge Retrieval and Selection for\n  Retrieval Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has emerged as a powerful method for\nenhancing natural language generation by integrating external knowledge into a\nmodel's output. While prior work has demonstrated the importance of improving\nknowledge retrieval for boosting generation quality, the role of knowledge\nselection remains less clear. In this paper, we perform a comprehensive\nanalysis of how knowledge retrieval and selection influence downstream\ngeneration performance in RAG systems. By simulating different retrieval and\nselection conditions through a controlled mixture of gold and distractor\nknowledge, we assess the impact of these factors on generation outcomes. Our\nfindings indicate that the downstream generator model's capability, as well as\nthe complexity of the task and dataset, significantly influence the impact of\nknowledge retrieval and selection on the overall RAG system performance. In\ntypical scenarios, improving the knowledge recall score is key to enhancing\ngeneration outcomes, with the knowledge selector providing a limited additional\nbenefit when a strong generator model is used on clear, well-defined tasks. For\nweaker generator models or more ambiguous tasks and datasets, the knowledge F1\nscore becomes a critical factor, and the knowledge selector plays a more\nprominent role in improving overall performance.\n","authors":["Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2410.13258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13255v1","updated":"2024-10-17T06:21:38Z","published":"2024-10-17T06:21:38Z","title":"Automatic Translation Alignment Pipeline for Multilingual Digital\n  Editions of Literary Works","summary":"  This paper investigates the application of translation alignment algorithms\nin the creation of a Multilingual Digital Edition (MDE) of Alessandro Manzoni's\nItalian novel \"I promessi sposi\" (\"The Betrothed\"), with translations in eight\nlanguages (English, Spanish, French, German, Dutch, Polish, Russian and\nChinese) from the 19th and 20th centuries. We identify key requirements for the\nMDE to improve both the reader experience and support for translation studies.\nOur research highlights the limitations of current state-of-the-art algorithms\nwhen applied to the translation of literary texts and outlines an automated\npipeline for MDE creation. This pipeline transforms raw texts into web-based,\nside-by-side representations of original and translated texts with different\nrendering options. In addition, we propose new metrics for evaluating the\nalignment of literary translations and suggest visualization techniques for\nfuture analysis.\n","authors":["Maria Levchenko"],"pdf_url":"https://arxiv.org/pdf/2410.13255v1.pdf","comment":"18 pages, Computational Humanities Research Conference, December 4-6,\n  2024, Aarhus, Denmark"},{"id":"http://arxiv.org/abs/2410.13248v1","updated":"2024-10-17T06:15:00Z","published":"2024-10-17T06:15:00Z","title":"Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation","summary":"  Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. We will release our code and datasets\nupon acceptance.\n","authors":["Ryotaro Shimizu","Takashi Wada","Yu Wang","Johannes Kruse","Sean O'Brien","Sai HtaungKham","Linxin Song","Yuya Yoshikawa","Yuki Saito","Fugee Tsung","Masayuki Goto","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13246v1","updated":"2024-10-17T06:09:26Z","published":"2024-10-17T06:09:26Z","title":"Atomic Calibration of LLMs in Long-Form Generations","summary":"  Large language models (LLMs) often suffer from hallucinations, posing\nsignificant challenges for real-world applications. Confidence calibration,\nwhich estimates the underlying uncertainty of model predictions, is essential\nto enhance the LLMs' trustworthiness. Existing research on LLM calibration has\nprimarily focused on short-form tasks, providing a single confidence score at\nthe response level (macro calibration). However, this approach is insufficient\nfor long-form generations, where responses often contain more complex\nstatements and may include both accurate and inaccurate information. Therefore,\nwe introduce atomic calibration, a novel approach that evaluates factuality\ncalibration at a fine-grained level by breaking down long responses into atomic\nclaims. We classify confidence elicitation methods into discriminative and\ngenerative types and demonstrate that their combination can enhance\ncalibration. Our extensive experiments on various LLMs and datasets show that\natomic calibration is well-suited for long-form generation and can also improve\nmacro calibration results. Additionally, atomic calibration reveals insightful\npatterns in LLM confidence throughout the generation process.\n","authors":["Caiqi Zhang","Ruihan Yang","Zhisong Zhang","Xinting Huang","Sen Yang","Dong Yu","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.13246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17484v3","updated":"2024-10-17T06:04:54Z","published":"2024-06-25T12:05:56Z","title":"MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment\n  and Knowledge Aggregation","summary":"  Large language models (LLMs) have shown substantial progress in natural\nlanguage understanding and generation, proving valuable especially in the\nmedical field. Despite advancements, challenges persist due to the complexity\nand diversity inherent in medical tasks, which can be categorized as\nknowledge-intensive tasks and alignment-required tasks. Previous approaches\neither ignore the latter task or focus on a minority of tasks and hence lose\ngeneralization. To address these drawbacks, we propose a progressive\nfine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise\naggregator to encode diverse knowledge in the first stage and filter out\ndetrimental information. In the second stage, we drop the Noise Aggregator to\navoid the interference of suboptimal representation and leverage an additional\nalignment module optimized towards an orthogonal direction to the knowledge\nspace to mitigate knowledge forgetting. Based on this two-stage paradigm, we\nproposed a Medical LLM through decoupling Clinical Alignment and Knowledge\nAggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)\nperformance on over 20 medical tasks, as well as SOTA results on specific\nmedical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all\ndemonstrate significant improvements over existing models with similar model\nsizes.\n","authors":["Yusheng Liao","Shuyang Jiang","Zhe Chen","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17484v3.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.13237v1","updated":"2024-10-17T05:43:30Z","published":"2024-10-17T05:43:30Z","title":"Large Language Models are Easily Confused: A Quantitative Metric,\n  Security Implications and Typological Analysis","summary":"  Language Confusion is a phenomenon where Large Language Models (LLMs)\ngenerate text that is neither in the desired language, nor in a contextually\nappropriate language. This phenomenon presents a critical challenge in text\ngeneration by LLMs, often appearing as erratic and unpredictable behavior. We\nhypothesize that there are linguistic regularities to this inherent\nvulnerability in LLMs and shed light on patterns of language confusion across\nLLMs. We introduce a novel metric, Language Confusion Entropy, designed to\ndirectly measure and quantify this confusion, based on language distributions\ninformed by linguistic typology and lexical variation. Comprehensive\ncomparisons with the Language Confusion Benchmark (Marchisio et al., 2024)\nconfirm the effectiveness of our metric, revealing patterns of language\nconfusion across LLMs. We further link language confusion to LLM security, and\nfind patterns in the case of multilingual embedding inversion attacks. Our\nanalysis demonstrates that linguistic typology offers theoretically grounded\ninterpretation, and valuable insights into leveraging language similarities as\na prior for LLM alignment and security.\n","authors":["Yiyi Chen","Qiongxiu Li","Russa Biswas","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2410.13237v1.pdf","comment":"17 pages, 6 figures, 14 tables"},{"id":"http://arxiv.org/abs/2410.13236v1","updated":"2024-10-17T05:40:54Z","published":"2024-10-17T05:40:54Z","title":"SPIN: Self-Supervised Prompt INjection","summary":"  Large Language Models (LLMs) are increasingly used in a variety of important\napplications, yet their safety and reliability remain as major concerns.\nVarious adversarial and jailbreak attacks have been proposed to bypass the\nsafety alignment and cause the model to produce harmful responses. We introduce\nSelf-supervised Prompt INjection (SPIN) which can detect and reverse these\nvarious attacks on LLMs. As our self-supervised prompt defense is done at\ninference-time, it is also compatible with existing alignment and adds an\nadditional layer of safety for defense. Our benchmarks demonstrate that our\nsystem can reduce the attack success rate by up to 87.9%, while maintaining the\nperformance on benign user requests. In addition, we discuss the situation of\nan adaptive attacker and show that our method is still resilient against\nattackers who are aware of our defense.\n","authors":["Leon Zhou","Junfeng Yang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2410.13236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13232v1","updated":"2024-10-17T05:37:00Z","published":"2024-10-17T05:37:00Z","title":"Web Agents with World Models: Learning and Leveraging Environment\n  Dynamics in Web Navigation","summary":"  Large language models (LLMs) have recently gained much attention in building\nautonomous agents. However, the performance of current LLM-based web agents in\nlong-horizon tasks is far from optimal, often yielding errors such as\nrepeatedly buying a non-refundable flight ticket. By contrast, humans can avoid\nsuch an irreversible mistake, as we have an awareness of the potential outcomes\n(e.g., losing money) of our actions, also known as the \"world model\". Motivated\nby this, our study first starts with preliminary analyses, confirming the\nabsence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet,\netc.). Then, we present a World-model-augmented (WMA) web agent, which\nsimulates the outcomes of its actions for better decision-making. To overcome\nthe challenges in training LLMs as world models predicting next observations,\nsuch as repeated elements across observations and long HTML inputs, we propose\na transition-focused observation abstraction, where the prediction objectives\nare free-form natural language descriptions exclusively highlighting important\nstate differences between time steps. Experiments on WebArena and Mind2Web show\nthat our world models improve agents' policy selection without training and\ndemonstrate our agents' cost- and time-efficiency compared to recent\ntree-search-based agents.\n","authors":["Hyungjoo Chae","Namyoung Kim","Kai Tzu-iunn Ong","Minju Gwak","Gwanwoo Song","Jihoon Kim","Sunghwan Kim","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.13232v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.19764v2","updated":"2024-10-17T05:28:24Z","published":"2024-06-28T09:09:36Z","title":"Belief Revision: The Adaptability of Large Language Models Reasoning","summary":"  The capability to reason from text is crucial for real-world NLP\napplications. Real-world scenarios often involve incomplete or evolving data.\nIn response, individuals update their beliefs and understandings accordingly.\nHowever, most existing evaluations assume that language models (LMs) operate\nwith consistent information. We introduce Belief-R, a new dataset designed to\ntest LMs' belief revision ability when presented with new evidence. Inspired by\nhow humans suppress prior inferences, this task assesses LMs within the newly\nproposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of\npremises designed to simulate scenarios where additional information could\nnecessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across\ndiverse prompting strategies and found that LMs generally struggle to\nappropriately revise their beliefs in response to new information. Further,\nmodels adept at updating often underperformed in scenarios without necessary\nupdates, highlighting a critical trade-off. These insights underscore the\nimportance of improving LMs' adaptiveness to changing information, a step\ntoward more reliable AI systems.\n","authors":["Bryan Wilie","Samuel Cahyawijaya","Etsuko Ishii","Junxian He","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2406.19764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12897v3","updated":"2024-10-17T05:21:24Z","published":"2024-04-19T14:05:03Z","title":"Enabling Natural Zero-Shot Prompting on Encoder Models via\n  Statement-Tuning","summary":"  While Large Language Models (LLMs) exhibit remarkable capabilities in\nzero-shot and few-shot scenarios, they often require computationally\nprohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT\nand RoBERTa achieve state-of-the-art results through fine-tuning but struggle\nwith extending to few-shot and zero-shot settings due to their architectural\nconstraints. Hence, we propose Statement-Tuning, a technique that models\ndiscriminative tasks as a set of finite statements and trains an encoder model\nto discriminate between the potential statements to determine the label. We do\nStatement-Tuning on multiple tasks to enable cross-task generalization.\nExperimental results demonstrate that Statement-Tuning achieves competitive\nperformance compared to state-of-the-art LLMs with significantly fewer\nparameters. Moreover, the study investigates the impact of several design\nchoices on few-shot and zero-shot generalization, revealing that\nStatement-Tuning can achieve strong performance with modest training data and\nbenefits from task and statement diversity for unseen task generalizability.\n","authors":["Ahmed Elshabrawy","Yongxin Huang","Iryna Gurevych","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2404.12897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13224v1","updated":"2024-10-17T05:10:12Z","published":"2024-10-17T05:10:12Z","title":"Proof Flow: Preliminary Study on Generative Flow Network Language Model\n  Tuning for Formal Reasoning","summary":"  Reasoning is a fundamental substrate for solving novel and complex problems.\nDeliberate efforts in learning and developing frameworks around System 2\nreasoning have made great strides, yet problems of sufficient complexity remain\nlargely out of reach for open models. To address this gap, we examine the\npotential of Generative Flow Networks as a fine-tuning method for LLMs to\nunlock advanced reasoning capabilities. In this paper, we present a proof of\nconcept in the domain of formal reasoning, specifically in the Neural Theorem\nProving (NTP) setting, where proofs specified in a formal language such as Lean\ncan be deterministically and objectively verified. Unlike classical\nreward-maximization reinforcement learning, which frequently over-exploits\nhigh-reward actions and fails to effectively explore the state space, GFlowNets\nhave emerged as a promising approach for sampling compositional objects,\nimproving generalization, and enabling models to maintain diverse hypotheses.\nOur early results demonstrate GFlowNet fine-tuning's potential for enhancing\nmodel performance in a search setting, which is especially relevant given the\nparadigm shift towards inference time compute scaling and \"thinking slowly.\"\n","authors":["Matthew Ho","Vincent Zhu","Xiaoyin Chen","Moksh Jain","Nikolay Malkin","Edwin Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13218v1","updated":"2024-10-17T04:52:57Z","published":"2024-10-17T04:52:57Z","title":"CBT-Bench: Evaluating Large Language Models on Assisting Cognitive\n  Behavior Therapy","summary":"  There is a significant gap between patient needs and available mental health\nsupport today. In this paper, we aim to thoroughly examine the potential of\nusing Large Language Models (LLMs) to assist professional psychotherapy. To\nthis end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation\nof cognitive behavioral therapy (CBT) assistance. We include three levels of\ntasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of\nmultiple-choice questions; II: Cognitive model understanding, with the tasks of\ncognitive distortion classification, primary core belief classification, and\nfine-grained core belief classification; III: Therapeutic response generation,\nwith the task of generating responses to patient speech in CBT therapy\nsessions. These tasks encompass key aspects of CBT that could potentially be\nenhanced through AI assistance, while also outlining a hierarchy of capability\nrequirements, ranging from basic knowledge recitation to engaging in real\ntherapeutic conversations. We evaluated representative LLMs on our benchmark.\nExperimental results indicate that while LLMs perform well in reciting CBT\nknowledge, they fall short in complex real-world scenarios requiring deep\nanalysis of patients' cognitive structures and generating effective responses,\nsuggesting potential future work.\n","authors":["Mian Zhang","Xianjun Yang","Xinlu Zhang","Travis Labrum","Jamie C. Chiu","Shaun M. Eack","Fei Fang","William Yang Wang","Zhiyu Zoey Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00138v2","updated":"2024-10-17T04:43:40Z","published":"2024-08-29T17:58:38Z","title":"PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in\n  Action","summary":"  As language models (LMs) are widely utilized in personalized communication\nscenarios (e.g., sending emails, writing social media posts) and endowed with a\ncertain level of agency, ensuring they act in accordance with the contextual\nprivacy norms becomes increasingly critical. However, quantifying the privacy\nnorm awareness of LMs and the emerging privacy risk in LM-mediated\ncommunication is challenging due to (1) the contextual and long-tailed nature\nof privacy-sensitive cases, and (2) the lack of evaluation approaches that\ncapture realistic application scenarios. To address these challenges, we\npropose PrivacyLens, a novel framework designed to extend privacy-sensitive\nseeds into expressive vignettes and further into agent trajectories, enabling\nmulti-level evaluation of privacy leakage in LM agents' actions. We instantiate\nPrivacyLens with a collection of privacy norms grounded in privacy literature\nand crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM\nperformance in answering probing questions and their actual behavior when\nexecuting user instructions in an agent setup. State-of-the-art LMs, like GPT-4\nand Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even\nwhen prompted with privacy-enhancing instructions. We also demonstrate the\ndynamic nature of PrivacyLens by extending each seed into multiple trajectories\nto red-team LM privacy leakage risk. Dataset and code are available at\nhttps://github.com/SALT-NLP/PrivacyLens.\n","authors":["Yijia Shao","Tianshi Li","Weiyan Shi","Yanchen Liu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.00138v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.13216v1","updated":"2024-10-17T04:42:48Z","published":"2024-10-17T04:42:48Z","title":"Anchored Alignment for Self-Explanations Enhancement","summary":"  In this work, we introduce a methodology for alignment designed to enhance\nthe ability of large language models (LLMs) to articulate their reasoning\n(self-explanation) even in the absence of annotated rationale explanations. Our\nalignment methodology comprises three key components: explanation quality\nassessment, self-instruction dataset generation, and model alignment.\nAdditionally, we present a novel technique called Alignment with Anchor\nPreference Pairs, which improves the selection of preference pairs by\ncategorizing model outputs into three groups: consistently correct,\nconsistently incorrect, and variable. By applying tailored strategies to each\ncategory, we enhance the effectiveness of Direct Preference Optimization (DPO).\nOur experimental results demonstrate that this approach significantly improves\nexplanation quality while maintaining accuracy compared to other fine-tuning\nstrategies.\n","authors":["Luis Felipe Villa-Arenas","Ata Nizamoglu","Qianli Wang","Sebastian Möller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2410.13216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13210v1","updated":"2024-10-17T04:30:46Z","published":"2024-10-17T04:30:46Z","title":"FaithBench: A Diverse Hallucination Benchmark for Summarization by\n  Modern LLMs","summary":"  Summarization is one of the most common tasks performed by large language\nmodels (LLMs), especially in applications like Retrieval-Augmented Generation\n(RAG). However, existing evaluations of hallucinations in LLM-generated\nsummaries, and evaluations of hallucination detection models both suffer from a\nlack of diversity and recency in the LLM and LLM families considered. This\npaper introduces FaithBench, a summarization hallucination benchmark comprising\nchallenging hallucinations made by 10 modern LLMs from 8 different families,\nwith ground truth annotations by human experts. ``Challenging'' here means\nsummaries on which popular, state-of-the-art hallucination detection models,\nincluding GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and\nGPT-3.5-Turbo produce the least hallucinations. However, even the best\nhallucination detection models have near 50\\% accuracies on FaithBench,\nindicating lots of room for future improvement. The repo is\nhttps://github.com/vectara/FaithBench\n","authors":["Forrest Sheng Bao","Miaoran Li","Renyi Qu","Ge Luo","Erana Wan","Yujia Tang","Weisi Fan","Manveer Singh Tamber","Suleman Kazi","Vivek Sourabh","Mike Qi","Ruixuan Tu","Chenyu Xu","Matthew Gonzales","Ofer Mendelevitch","Amin Ahmad"],"pdf_url":"https://arxiv.org/pdf/2410.13210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13206v1","updated":"2024-10-17T04:19:26Z","published":"2024-10-17T04:19:26Z","title":"BQA: Body Language Question Answering Dataset for Video Large Language\n  Models","summary":"  A large part of human communication relies on nonverbal cues such as facial\nexpressions, eye contact, and body language. Unlike language or sign language,\nsuch nonverbal communication lacks formal rules, requiring complex reasoning\nbased on commonsense understanding. Enabling current Video Large Language\nModels (VideoLLMs) to accurately interpret body language is a crucial\nchallenge, as human unconscious actions can easily cause the model to\nmisinterpret their intent. To address this, we propose a dataset, BQA, a body\nlanguage question answering dataset, to validate whether the model can\ncorrectly interpret emotions from short clips of body language comprising 26\nemotion labels of videos of body language. We evaluated various VideoLLMs on\nBQA and revealed that understanding body language is challenging, and our\nanalyses of the wrong answers by VideoLLMs show that certain VideoLLMs made\nsignificantly biased answers depending on the age group and ethnicity of the\nindividuals in the video. The dataset is available.\n","authors":["Shintaro Ozaki","Kazuki Hayashi","Miyu Oba","Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.13206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13204v1","updated":"2024-10-17T04:12:17Z","published":"2024-10-17T04:12:17Z","title":"Measuring Free-Form Decision-Making Inconsistency of Language Models in\n  Military Crisis Simulations","summary":"  There is an increasing interest in using language models (LMs) for automated\ndecision-making, with multiple countries actively testing LMs to aid in\nmilitary crisis decision-making. To scrutinize relying on LM decision-making in\nhigh-stakes settings, we examine the inconsistency of responses in a crisis\nsimulation (\"wargame\"), similar to reported tests conducted by the US military.\nPrior work illustrated escalatory tendencies and varying levels of aggression\namong LMs but were constrained to simulations with pre-defined actions. This\nwas due to the challenges associated with quantitatively measuring semantic\ndifferences and evaluating natural language decision-making without relying on\npre-defined actions. In this work, we query LMs for free form responses and use\na metric based on BERTScore to measure response inconsistency quantitatively.\nLeveraging the benefits of BERTScore, we show that the inconsistency metric is\nrobust to linguistic variations that preserve semantic meaning in a\nquestion-answering setting across text lengths. We show that all five tested\nLMs exhibit levels of inconsistency that indicate semantic differences, even\nwhen adjusting the wargame setting, anonymizing involved conflict countries, or\nadjusting the sampling temperature parameter $T$. Further qualitative\nevaluation shows that models recommend courses of action that share few to no\nsimilarities. We also study the impact of different prompt sensitivity\nvariations on inconsistency at temperature $T = 0$. We find that inconsistency\ndue to semantically equivalent prompt variations can exceed response\ninconsistency from temperature sampling for most studied models across\ndifferent levels of ablations. Given the high-stakes nature of military\ndeployment, we recommend further consideration be taken before using LMs to\ninform military decisions or other cases of high-stakes decision-making.\n","authors":["Aryan Shrivastava","Jessica Hullman","Max Lamparth"],"pdf_url":"https://arxiv.org/pdf/2410.13204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12388v2","updated":"2024-10-17T04:09:09Z","published":"2024-10-16T09:13:23Z","title":"Prompt Compression for Large Language Models: A Survey","summary":"  Leveraging large language models (LLMs) for complex natural language tasks\ntypically requires long-form prompts to convey detailed requirements and\ninformation, which results in increased memory usage and inference costs. To\nmitigate these challenges, multiple efficient methods have been proposed, with\nprompt compression gaining significant research interest. This survey provides\nan overview of prompt compression techniques, categorized into hard prompt\nmethods and soft prompt methods. First, the technical approaches of these\nmethods are compared, followed by an exploration of various ways to understand\ntheir mechanisms, including the perspectives of attention optimization,\nParameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic\nlanguage. We also examine the downstream adaptations of various prompt\ncompression techniques. Finally, the limitations of current prompt compression\nmethods are analyzed, and several future directions are outlined, such as\noptimizing the compression encoder, combining hard and soft prompts methods,\nand leveraging insights from multimodality.\n","authors":["Zongqian Li","Yinhong Liu","Yixuan Su","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2410.12388v2.pdf","comment":null}]},"2024-10-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.14677v1","updated":"2024-10-18T17:59:57Z","published":"2024-10-18T17:59:57Z","title":"Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts","summary":"  The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.\n","authors":["German Gritsai","Anastasia Voznyuk","Andrey Grabovoy","Yury Chekhovich"],"pdf_url":"https://arxiv.org/pdf/2410.14677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14676v1","updated":"2024-10-18T17:59:51Z","published":"2024-10-18T17:59:51Z","title":"SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment","summary":"  Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.\n","authors":["Qin Liu","Fei Wang","Chaowei Xiao","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14675v1","updated":"2024-10-18T17:59:47Z","published":"2024-10-18T17:59:47Z","title":"Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts","summary":"  Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released.\n","authors":["Yukun Huang","Sanxing Chen","Hongyi Cai","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14669v1","updated":"2024-10-18T17:58:21Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v1.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2410.14668v1","updated":"2024-10-18T17:57:40Z","published":"2024-10-18T17:57:40Z","title":"MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image\n  Description and Reasoning Steps","summary":"  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.\n","authors":["Xiongtao Zhou","Jie He","Lanyu Chen","jingyu li","Haojing Chen","Victor Gutierrez Basulto","Jeff Z. Pan","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14668v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2410.14666v1","updated":"2024-10-18T17:56:11Z","published":"2024-10-18T17:56:11Z","title":"DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph","summary":"  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n","authors":["Maitreya Prafulla Chitale","Uday Bindal","Rajakrishnan Rajkumar","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06331v2","updated":"2024-10-18T17:53:46Z","published":"2024-10-08T20:12:11Z","title":"Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing","summary":"  The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.\n","authors":["Zhuoran Zhang","Yongxiang Li","Zijian Kan","Keyuan Cheng","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06331v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.14651v1","updated":"2024-10-18T17:47:11Z","published":"2024-10-18T17:47:11Z","title":"Real-time Fake News from Adversarial Feedback","summary":"  We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news.\n","authors":["Sanxing Chen","Yukun Huang","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14641v1","updated":"2024-10-18T17:41:19Z","published":"2024-10-18T17:41:19Z","title":"Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs","summary":"  Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.\n","authors":["Runchu Tian","Yanghao Li","Yuepeng Fu","Siyang Deng","Qinyu Luo","Cheng Qian","Shuo Wang","Xin Cong","Zhong Zhang","Yesai Wu","Yankai Lin","Huadong Wang","Xiaojiang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.14641v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.14635v1","updated":"2024-10-18T17:36:53Z","published":"2024-10-18T17:36:53Z","title":"GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings","summary":"  Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark.\n","authors":["Raghuveer Thirukovalluru","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14632v1","updated":"2024-10-18T17:32:22Z","published":"2024-10-18T17:32:22Z","title":"Diverging Preferences: When do Annotators Disagree and do Models Know?","summary":"  We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.\n","authors":["Michael JQ Zhang","Zhilin Wang","Jena D. Hwang","Yi Dong","Olivier Delalleau","Yejin Choi","Eunsol Choi","Xiang Ren","Valentina Pyatkin"],"pdf_url":"https://arxiv.org/pdf/2410.14632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07114v2","updated":"2024-10-18T17:30:04Z","published":"2024-09-19T19:48:31Z","title":"System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam","summary":"  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch average of 40.63 points. Neither model had access to the\nexam figures. Since there was a risk of model contamination (i.e., the\nknowledge cutoff of o1-preview and GPT-4o was after the exam was published\nonline), we repeated the procedure with a new Mathematics B exam that was\npublished after the cutoff date. The results again indicated that o1-preview\nperformed strongly (97.8th percentile), which suggests that contamination was\nnot a factor. We also show that there is some variability in the output of\no1-preview, which means that sometimes there is 'luck' (the answer is correct)\nor 'bad luck' (the output has diverged into something that is incorrect). We\ndemonstrate that a self-consistency approach, where repeated prompts are given\nand the most common answer is selected, is a useful strategy for identifying\nthe correct answer. It is concluded that while OpenAI's new model series holds\ngreat potential, certain risks must be considered.\n","authors":["Joost de Winter","Dimitra Dodou","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2410.07114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14627v1","updated":"2024-10-18T17:29:56Z","published":"2024-10-18T17:29:56Z","title":"CELI: Controller-Embedded Language Model Interactions","summary":"  We introduce Controller-Embedded Language Model Interactions (CELI), a\nframework that integrates control logic directly within language model (LM)\nprompts, facilitating complex, multi-stage task execution. CELI addresses\nlimitations of existing prompt engineering and workflow optimization techniques\nby embedding control logic directly within the operational context of language\nmodels, enabling dynamic adaptation to evolving task requirements. Our\nframework transfers control from the traditional programming execution\nenvironment to the LMs, allowing them to autonomously manage computational\nworkflows while maintaining seamless interaction with external systems and\nfunctions. CELI supports arbitrary function calls with variable arguments,\nbridging the gap between LMs' adaptive reasoning capabilities and conventional\nsoftware paradigms' structured control mechanisms. To evaluate CELI's\nversatility and effectiveness, we conducted case studies in two distinct\ndomains: code generation (HumanEval benchmark) and multi-stage content\ngeneration (Wikipedia-style articles). The results demonstrate notable\nperformance improvements across a range of domains. CELI achieved a 4.9\npercentage point improvement over the best reported score of the baseline GPT-4\nmodel on the HumanEval code generation benchmark. In multi-stage content\ngeneration, 94.4% of CELI-produced Wikipedia-style articles met or exceeded\nfirst draft quality when optimally configured, with 44.4% achieving high\nquality. These outcomes underscore CELI's potential for optimizing AI-driven\nworkflows across diverse computational domains.\n","authors":["Jan-Samuel Wagner","Dave DeCaprio","Abishek Chiffon Muthu Raja","Jonathan M. Holman","Lauren K. Brady","Sky C. Cheung","Hosein Barzekar","Eric Yang","Mark Anthony Martinez II","David Soong","Sriram Sridhar","Han Si","Brandon W. Higgs","Hisham Hamadeh","Scott Ogden"],"pdf_url":"https://arxiv.org/pdf/2410.14627v1.pdf","comment":"26 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.14626v1","updated":"2024-10-18T17:27:38Z","published":"2024-10-18T17:27:38Z","title":"You Shall Know a Tool by the Traces it Leaves: The Predictability of\n  Sentiment Analysis Tools","summary":"  If sentiment analysis tools were valid classifiers, one would expect them to\nprovide comparable results for sentiment classification on different kinds of\ncorpora and for different languages. In line with results of previous studies\nwe show that sentiment analysis tools disagree on the same dataset. Going\nbeyond previous studies we show that the sentiment tool used for sentiment\nannotation can even be predicted from its outcome, revealing an algorithmic\nbias of sentiment analysis. Based on Twitter, Wikipedia and different news\ncorpora from the English, German and French languages, our classifiers separate\nsentiment tools with an averaged F1-score of 0.89 (for the English corpora). We\ntherefore warn against taking sentiment annotations as face value and argue for\nthe need of more and systematic NLP evaluation studies.\n","authors":["Daniel Baumartz","Mevlüt Bagci","Alexander Henlein","Maxim Konca","Andy Lücking","Alexander Mehler"],"pdf_url":"https://arxiv.org/pdf/2410.14626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10989v2","updated":"2024-10-18T17:21:17Z","published":"2024-10-14T18:17:01Z","title":"Liger Kernel: Efficient Triton Kernels for LLM Training","summary":"  Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.\n","authors":["Pin-Lun Hsu","Yun Dai","Vignesh Kothapalli","Qingquan Song","Shao Tang","Siyu Zhu","Steven Shimizu","Shivam Sahni","Haowen Ning","Yanning Chen"],"pdf_url":"https://arxiv.org/pdf/2410.10989v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.02525v3","updated":"2024-10-18T17:18:24Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10101v2","updated":"2024-10-18T17:15:09Z","published":"2024-10-14T02:41:01Z","title":"Learning Linear Attention in Polynomial Time","summary":"  Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.\n","authors":["Morris Yau","Ekin Akyürek","Jiayuan Mao","Joshua B. Tenenbaum","Stefanie Jegelka","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.10101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06402v2","updated":"2024-10-18T17:10:05Z","published":"2024-03-11T03:28:13Z","title":"One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning","summary":"  In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.\n","authors":["Manish Chandra","Debasis Ganguly","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2403.06402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14609v1","updated":"2024-10-18T17:03:17Z","published":"2024-10-18T17:03:17Z","title":"DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search","summary":"  Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.\n","authors":["Simon Lupart","Mohammad Aliannejadi","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2410.14609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14596v1","updated":"2024-10-18T16:49:36Z","published":"2024-10-18T16:49:36Z","title":"Teaching Models to Balance Resisting and Accepting Persuasion","summary":"  Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up.\n","authors":["Elias Stengel-Eskin","Peter Hase","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.14596v1.pdf","comment":"Code: https://github.com/esteng/persuasion_balanced_training"},{"id":"http://arxiv.org/abs/2410.14594v1","updated":"2024-10-18T16:44:22Z","published":"2024-10-18T16:44:22Z","title":"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases","summary":"  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).\n","authors":["Elias Lumer"],"pdf_url":"https://arxiv.org/pdf/2410.14594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13191v2","updated":"2024-10-18T16:42:01Z","published":"2024-10-17T03:38:29Z","title":"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback","summary":"  Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.\n","authors":["Zonghai Yao","Aditya Parashar","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13191v2.pdf","comment":"Equal contribution for the first two authors"},{"id":"http://arxiv.org/abs/2410.14589v1","updated":"2024-10-18T16:39:42Z","published":"2024-10-18T16:39:42Z","title":"Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a\n  Continuum","summary":"  There is increasing interest in looking at dialects in NLP. However, most\nwork to date still treats dialects as discrete categories. For instance,\nevaluative work in variation-oriented NLP for English often works with Indian\nEnglish or African-American Venacular English as homogeneous categories (Faisal\net al., 2024; Ziems et al., 2023), yet even within one variety there is\nsubstantial variation. We examine within-dialect variation and show that\nperformance critically varies within categories. We measure speech-to-text\nperformance on Italian dialects, and empirically observe a geographical\nperformance disparity. This disparity correlates substantially (-0.5) with\nlinguistic similarity to the highest performing dialect variety. We\ncross-examine our results against dialectometry methods, and interpret the\nperformance disparity to be due to a bias towards dialects that are more\nsimilar to the standard variety in the speech-to-text model examined. We\nadditionally leverage geostatistical methods to predict zero-shot performance\nat unseen sites, and find the incorporation of geographical information to\nsubstantially improve prediction performance, indicating there to be\ngeographical structure in the performance distribution.\n","authors":["Ryan Soh-Eun Shim","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14582v1","updated":"2024-10-18T16:32:10Z","published":"2024-10-18T16:32:10Z","title":"Do LLMs estimate uncertainty well in instruction-following?","summary":"  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n","authors":["Juyeon Heo","Miao Xiong","Christina Heinze-Deml","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14581v1","updated":"2024-10-18T16:32:06Z","published":"2024-10-18T16:32:06Z","title":"Optimizing Attention with Mirror Descent: Generalized Max-Margin Token\n  Selection","summary":"  Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.\n","authors":["Aaron Alvarado Kristanto Julistiono","Davoud Ataee Tarzanagh","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2410.14581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14578v1","updated":"2024-10-18T16:26:45Z","published":"2024-10-18T16:26:45Z","title":"Large Language Models Are Overparameterized Text Encoders","summary":"  Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.\n","authors":["Thennal D K","Tim Fischer","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.14578v1.pdf","comment":"8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures"},{"id":"http://arxiv.org/abs/2410.14574v1","updated":"2024-10-18T16:20:22Z","published":"2024-10-18T16:20:22Z","title":"MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts","summary":"  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14574v1.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE"},{"id":"http://arxiv.org/abs/2410.14567v1","updated":"2024-10-18T16:11:29Z","published":"2024-10-18T16:11:29Z","title":"RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions","summary":"  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.\n","authors":["Zhiyuan Peng","Jinming Nian","Alexandre Evfimievski","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.14567v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.07400v2","updated":"2024-10-18T15:54:56Z","published":"2024-10-09T19:57:07Z","title":"Advocating Character Error Rate for Multilingual ASR Evaluation","summary":"  Automatic speech recognition (ASR) systems have traditionally been evaluated\nusing English datasets, with the word error rate (WER) serving as the\npredominant metric. WER's simplicity and ease of interpretation have\ncontributed to its widespread adoption, particularly for English. However, as\nASR systems expand to multilingual contexts, WER fails in various ways,\nparticularly with morphologically complex languages or those without clear word\nboundaries. Our work documents the limitations of WER as an evaluation metric\nand advocates for the character error rate (CER) as the primary metric in\nmultilingual ASR evaluation. We show that CER avoids many of the challenges WER\nfaces and exhibits greater consistency across writing systems. We support our\nproposition by conducting human evaluations of ASR transcriptions in three\nlanguages: Malayalam, English, and Arabic, which exhibit distinct morphological\ncharacteristics. We show that CER correlates more closely with human judgments\nthan WER, even for English. To facilitate further research, we release our\nhuman evaluation dataset for future benchmarking of ASR metrics. Our findings\nsuggest that CER should be prioritized, or at least supplemented, in\nmultilingual ASR evaluations to account for the varying linguistic\ncharacteristics of different languages.\n","authors":["Thennal D K","Jesin James","Deepa P Gopinath","Muhammed Ashraf K"],"pdf_url":"https://arxiv.org/pdf/2410.07400v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.15652v3","updated":"2024-10-18T15:45:39Z","published":"2024-09-24T01:29:24Z","title":"English offensive text detection using CNN based Bi-GRU model","summary":"  Over the years, the number of users of social media has increased\ndrastically. People frequently share their thoughts through social platforms,\nand this leads to an increase in hate content. In this virtual community,\nindividuals share their views, express their feelings, and post photos, videos,\nblogs, and more. Social networking sites like Facebook and Twitter provide\nplatforms to share vast amounts of content with a single click. However, these\nplatforms do not impose restrictions on the uploaded content, which may include\nabusive language and explicit images unsuitable for social media. To resolve\nthis issue, a new idea must be implemented to divide the inappropriate content.\nNumerous studies have been done to automate the process. In this paper, we\npropose a new Bi-GRU-CNN model to classify whether the text is offensive or\nnot. The combination of the Bi-GRU and CNN models outperforms the existing\nmodel.\n","authors":["Tonmoy Roy","Md Robiul Islam","Asif Ahammad Miazee","Anika Antara","Al Amin","Sunjim Hossain"],"pdf_url":"https://arxiv.org/pdf/2409.15652v3.pdf","comment":"5 pages and 6 figures"},{"id":"http://arxiv.org/abs/2405.20850v2","updated":"2024-10-18T15:43:02Z","published":"2024-05-31T14:33:07Z","title":"Improving Reward Models with Synthetic Critiques","summary":"  Reward models (RMs) play a critical role in aligning language models through\nthe process of reinforcement learning from human feedback. RMs are trained to\npredict a score reflecting human preference, which requires significant time\nand cost for human annotation. Additionally, RMs tend to quickly overfit on\nsuperficial features in the training set, hindering their generalization\nperformance on unseen distributions. We propose a novel approach using\nsynthetic natural language critiques generated by large language models to\nprovide additional feedback, evaluating aspects such as instruction following,\ncorrectness, and style. This offers richer signals and more robust features for\nRMs to assess and score on. We demonstrate that high-quality critiques improve\nthe performance and data efficiency of RMs initialized from different\npretrained models, reducing the reliance on costly human annotations.\nFurthermore, incorporating critiques improves both the interpretability and\nrobustness of RM training.\n","authors":["Zihuiwen Ye","Fraser Greenlee-Scott","Max Bartolo","Phil Blunsom","Jon Ander Campos","Matthias Gallé"],"pdf_url":"https://arxiv.org/pdf/2405.20850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14917v2","updated":"2024-10-18T15:42:06Z","published":"2024-09-23T11:13:25Z","title":"With Ears to See and Eyes to Hear: Sound Symbolism Experiments with\n  Multimodal Large Language Models","summary":"  Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have\ndemonstrated aptitude as potential substitutes for human participants in\nexperiments testing psycholinguistic phenomena. However, an understudied\nquestion is to what extent models that only have access to vision and text\nmodalities are able to implicitly understand sound-based phenomena via abstract\nreasoning from orthography and imagery alone. To investigate this, we analyse\nthe ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise\na non-arbitrary link between sounds and concepts) as well as their ability to\n\"hear\" via the interplay of the language and vision modules of open and\nclosed-source multimodal models. We perform multiple experiments, including\nreplicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism\ntasks, and comparing human judgements of linguistic iconicity with that of\nLLMs. Our results show that VLMs demonstrate varying levels of agreement with\nhuman labels, and more task information may be required for VLMs versus their\nhuman counterparts for in silico experimentation. We additionally see through\nhigher maximum agreement levels that Magnitude Symbolism is an easier pattern\nfor VLMs to identify than Shape Symbolism, and that an understanding of\nlinguistic iconicity is highly dependent on model size.\n","authors":["Tyler Loakman","Yucheng Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.14917v2.pdf","comment":"Accepted to EMNLP 2024 (Camera Ready)"},{"id":"http://arxiv.org/abs/2410.14545v1","updated":"2024-10-18T15:40:48Z","published":"2024-10-18T15:40:48Z","title":"Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization","summary":"  Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.\n","authors":["Frederic Kirstein","Terry Ruas","Robert Kratel","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2410.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02067v2","updated":"2024-10-18T15:39:15Z","published":"2024-07-02T08:55:41Z","title":"Crossroads of Continents: Automated Artifact Extraction for Cultural\n  Adaptation with Large Multimodal Models","summary":"  We present a comprehensive three-phase study to examine (1) the cultural\nunderstanding of Large Multimodal Models (LMMs) by introducing DalleStreet, a\nlarge-scale dataset generated by DALL-E 3 and validated by humans, containing\n9,935 images of 67 countries and 10 concept classes; (2) the underlying\nimplicit and potentially stereotypical cultural associations with a cultural\nartifact extraction task; and (3) an approach to adapt cultural representation\nin an image based on extracted associations using a modular pipeline,\nCultureAdapt. We find disparities in cultural understanding at geographic\nsub-region levels with both open-source (LLaVA) and closed-source (GPT-4V)\nmodels on DalleStreet and other existing benchmarks, which we try to understand\nusing over 18,000 artifacts that we identify in association to different\ncountries. Our findings reveal a nuanced picture of the cultural competence of\nLMMs, highlighting the need to develop culture-aware systems. Dataset and code\nare available at https://github.com/iamshnoo/crossroads\n","authors":["Anjishnu Mukherjee","Ziwei Zhu","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.02067v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2404.11124v2","updated":"2024-10-18T15:34:41Z","published":"2024-04-17T07:15:07Z","title":"What's under the hood: Investigating Automatic Metrics on Meeting\n  Summarization","summary":"  Meeting summarization has become a critical task considering the increase in\nonline interactions. While new techniques are introduced regularly, their\nevaluation uses metrics not designed to capture meeting-specific errors,\nundermining effective evaluation. This paper investigates what the frequently\nused automatic metrics capture and which errors they mask by correlating\nautomatic metric scores with human evaluations across a broad error taxonomy.\nWe commence with a comprehensive literature review on English meeting\nsummarization to define key challenges like speaker dynamics and contextual\nturn-taking and error types such as missing information and linguistic\ninaccuracy, concepts previously loosely defined in the field. We examine the\nrelationship between characteristic challenges and errors by using annotated\ntranscripts and summaries from Transformer-based sequence-to-sequence and\nautoregressive models from the general summary QMSum dataset. Through\nexperimental validation, we find that different model architectures respond\nvariably to challenges in meeting transcripts, resulting in different\npronounced links between challenges and errors. Current default-used metrics\nstruggle to capture observable errors, showing weak to mid-correlations, while\na third of the correlations show trends of error masking. Only a subset reacts\naccurately to specific errors, while most correlations show either\nunresponsiveness or failure to reflect the error's impact on summary quality.\n","authors":["Frederic Kirstein","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2404.11124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12874v2","updated":"2024-10-18T15:26:55Z","published":"2024-10-14T18:11:53Z","title":"On Debiasing Text Embeddings Through Context Injection","summary":"  Current advances in Natural Language Processing (NLP) have made it\nincreasingly feasible to build applications leveraging textual data. Generally,\nthe core of these applications rely on having a good semantic representation of\ntext into vectors, via embedding models. However, it has been shown that these\nembeddings capture and perpetuate biases already present in text. While a few\ntechniques have been proposed to debias embeddings, they do not take advantage\nof the recent advances in context understanding of modern embedding models. In\nthis paper, we fill this gap by conducting a review of 19 embedding models by\nquantifying their biases and how well they respond to context injection as a\nmean of debiasing. We show that higher performing models are more prone to\ncapturing biases, but are also better at incorporating context. Surprisingly,\nwe find that while models can easily embed affirmative semantics, they fail at\nembedding neutral semantics. Finally, in a retrieval task, we show that biases\nin embeddings can lead to non-desirable outcomes. We use our new-found insights\nto design a simple algorithm for top $k$ retrieval, where $k$ is dynamically\nselected. We show that our algorithm is able to retrieve all relevant gendered\nand neutral chunks.\n","authors":["Thomas Uriot"],"pdf_url":"https://arxiv.org/pdf/2410.12874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13901v3","updated":"2024-10-18T15:25:44Z","published":"2024-03-20T18:13:17Z","title":"Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases","summary":"  Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of English tongue twisters - a form of\nlanguage that is required to be conditioned on a phoneme level to maximize\nsound overlap, while maintaining semantic consistency with an input topic or\nphrase and still being grammatically correct. We present TwisterLister, a\npipeline for generating phonologically informed tongue twisters from large\nlanguage models (LLMs) that we use to generate TwistList 2.0, the largest\nannotated dataset of tongue twisters to date, consisting of 17K+ examples from\na combination of human and LLM authors. Our generation pipeline involves the\nuse of a phonologically constrained vocabulary alongside LLM prompting to\ngenerate novel, non-derivative tongue twister examples. We additionally present\nthe results of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a phoneme-aware constrained decoding\nmodule (PACD) that can be integrated into an autoregressive language model and\ndemonstrate that this method generates good quality tongue twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue twisters, primarily based on phonemic edit distance (PED)\n","authors":["Tyler Loakman","Chen Tang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13901v3.pdf","comment":"Accepted Final Version to Computational Linguistics"},{"id":"http://arxiv.org/abs/2406.11580v2","updated":"2024-10-18T15:20:43Z","published":"2024-06-17T14:20:47Z","title":"Error Span Annotation: A Balanced Approach for Human Evaluation of\n  Machine Translation","summary":"  High-quality Machine Translation (MT) evaluation relies heavily on human\njudgments. Comprehensive error classification methods, such as Multidimensional\nQuality Metrics (MQM), are expensive as they are time-consuming and can only be\ndone by experts, whose availability may be limited especially for low-resource\nlanguages. On the other hand, just assigning overall scores, like Direct\nAssessment (DA), is simpler and faster and can be done by translators of any\nlevel, but is less reliable. In this paper, we introduce Error Span Annotation\n(ESA), a human evaluation protocol which combines the continuous rating of DA\nwith the high-level error severity span marking of MQM. We validate ESA by\ncomparing it to MQM and DA for 12 MT systems and one human reference\ntranslation (English to German) from WMT23. The results show that ESA offers\nfaster and cheaper annotations than MQM at the same quality level, without the\nrequirement of expensive MQM experts.\n","authors":["Tom Kocmi","Vilém Zouhar","Eleftherios Avramidis","Roman Grundkiewicz","Marzena Karpinska","Maja Popović","Mrinmaya Sachan","Mariya Shmatova"],"pdf_url":"https://arxiv.org/pdf/2406.11580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14516v1","updated":"2024-10-18T14:55:14Z","published":"2024-10-18T14:55:14Z","title":"Do LLMs \"know\" internally when they follow instructions?","summary":"  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n","authors":["Juyeon Heo","Christina Heinze-Deml","Oussama Elachqar","Shirley Ren","Udhay Nallasamy","Andy Miller","Kwan Ho Ryan Chan","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14506v1","updated":"2024-10-18T14:38:37Z","published":"2024-10-18T14:38:37Z","title":"SignAttention: On the Interpretability of Transformer Models for Sign\n  Language Translation","summary":"  This paper presents the first comprehensive interpretability analysis of a\nTransformer-based Sign Language Translation (SLT) model, focusing on the\ntranslation from video-based Greek Sign Language to glosses and text.\nLeveraging the Greek Sign Language Dataset, we examine the attention mechanisms\nwithin the model to understand how it processes and aligns visual input with\nsequential glosses. Our analysis reveals that the model pays attention to\nclusters of frames rather than individual ones, with a diagonal alignment\npattern emerging between poses and glosses, which becomes less distinct as the\nnumber of glosses increases. We also explore the relative contributions of\ncross-attention and self-attention at each decoding step, finding that the\nmodel initially relies on video frames but shifts its focus to previously\npredicted tokens as the translation progresses. This work contributes to a\ndeeper understanding of SLT models, paving the way for the development of more\ntransparent and reliable translation systems essential for real-world\napplications.\n","authors":["Pedro Alejandro Dal Bianco","Oscar Agustín Stanchi","Facundo Manuel Quiroga","Franco Ronchetti","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2410.14506v1.pdf","comment":"Accepted at IAI Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14480v1","updated":"2024-10-18T14:03:52Z","published":"2024-10-18T14:03:52Z","title":"Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models","summary":"  As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques.\n","authors":["James Vo"],"pdf_url":"https://arxiv.org/pdf/2410.14480v1.pdf","comment":"The method is currently under experimentation"},{"id":"http://arxiv.org/abs/2410.09804v2","updated":"2024-10-18T14:03:05Z","published":"2024-10-13T11:15:38Z","title":"BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models","summary":"  While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.\n","authors":["Xinyuan Wang","Victor Shea-Jay Huang","Renmiao Chen","Hao Wang","Chengwei Pan","Lei Sha","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.09804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14346v2","updated":"2024-10-18T13:59:54Z","published":"2024-07-19T14:28:53Z","title":"Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals","summary":"  Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.\n","authors":["Akash Kumar Mohankumar","Gururaj K","Gagan Madan","Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.14346v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure"},{"id":"http://arxiv.org/abs/2406.13663v4","updated":"2024-10-18T13:16:57Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernández","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v4.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2405.11877v5","updated":"2024-10-18T13:03:05Z","published":"2024-05-20T08:41:15Z","title":"A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus","summary":"  Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.\n","authors":["Eduard Poesina","Cornelia Caragea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2405.11877v5.pdf","comment":"Accepted at ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.14442v1","updated":"2024-10-18T13:01:14Z","published":"2024-10-18T13:01:14Z","title":"A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference","summary":"  Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.\n","authors":["You Wu","Haoyi Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2410.14442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10996v2","updated":"2024-10-18T12:54:21Z","published":"2024-06-16T16:17:46Z","title":"Towards Lifelong Dialogue Agents via Relation-aware Memory Construction\n  and Timeline-augmented Response Generation","summary":"  To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior work focuses on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present Theanine, a framework for LLM-based lifelong dialogue\nagents. Theanine discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, Theanine augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts in measuring\nmemory-augmented dialogue agents. A supplementary video for Theanine and data\nfor TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.\n","authors":["Kai Tzu-iunn Ong","Namyoung Kim","Minju Gwak","Hyungjoo Chae","Taeyoon Kwon","Yohan Jo","Seung-won Hwang","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2406.10996v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2305.17819v3","updated":"2024-10-18T12:49:35Z","published":"2023-05-28T22:46:21Z","title":"Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation","summary":"  The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback.\n","authors":["Magdalena Wysocka","Oskar Wysocki","Maxime Delmas","Vincent Mutel","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2305.17819v3.pdf","comment":"Accepted at the Journal of Biomedical Informatics, Volume 158,\n  October 2024, 104724"},{"id":"http://arxiv.org/abs/2410.14425v1","updated":"2024-10-18T12:39:32Z","published":"2024-10-18T12:39:32Z","title":"Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation","summary":"  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance.\n","authors":["Shuai Zhao","Xiaobao Wu","Cong-Duy Nguyen","Meihuizi Jia","Yichao Feng","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2410.14425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v5","updated":"2024-10-18T12:27:07Z","published":"2024-06-16T12:46:40Z","title":"Multi-LLM QA with Embodied Exploration","summary":"  Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10918v5.pdf","comment":"16 pages, 9 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2406.12950v2","updated":"2024-10-18T12:19:41Z","published":"2024-06-18T12:54:47Z","title":"MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction","summary":"  Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.\n","authors":["Yuyan Liu","Sirui Ding","Sheng Zhou","Wenqi Fan","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.12950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14405v1","updated":"2024-10-18T12:08:07Z","published":"2024-10-18T12:08:07Z","title":"Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of\n  Language Models for Fact Completion","summary":"  Previous interpretations of language models (LMs) miss important distinctions\nin how these models process factual information. For example, given the query\n\"Astrid Lindgren was born in\" with the corresponding completion \"Sweden\", no\ndifference is made between whether the prediction was based on having the exact\nknowledge of the birthplace of the Swedish author or assuming that a person\nwith a Swedish-sounding name was born in Sweden. In this paper, we investigate\nfour different prediction scenarios for which the LM can be expected to show\ndistinct behaviors. These scenarios correspond to different levels of model\nreliability and types of information being processed - some being less\ndesirable for factual predictions. To facilitate precise interpretations of LMs\nfor fact completion, we propose a model-specific recipe called PrISM for\nconstructing datasets with examples of each scenario based on a set of\ndiagnostic criteria. We apply a popular interpretability method, causal tracing\n(CT), to the four prediction scenarios and find that while CT produces\ndifferent results for each scenario, aggregations over a set of mixed examples\nmay only represent the results from the scenario with the strongest measured\nsignal. In summary, we contribute tools for a more granular study of fact\ncompletion in language models and analyses that provide a more nuanced\nunderstanding of how LMs process fact-related queries.\n","authors":["Denitsa Saynova","Lovisa Hagström","Moa Johansson","Richard Johansson","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2410.14405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14399v1","updated":"2024-10-18T12:02:41Z","published":"2024-10-18T12:02:41Z","title":"SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning","summary":"  Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.\n","authors":["Magdalena Wysocka","Danilo S. Carvalho","Oskar Wysocki","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2410.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14395v1","updated":"2024-10-18T11:58:03Z","published":"2024-10-18T11:58:03Z","title":"Generative AI, Pragmatics, and Authenticity in Second Language Learning","summary":"  There are obvious benefits to integrating generative AI (artificial\nintelligence) into language learning and teaching. Those include using AI as a\nlanguage tutor, creating learning materials, or assessing learner output.\nHowever, due to how AI systems under-stand human language, based on a\nmathematical model using statistical probability, they lack the lived\nexperience to be able to use language with the same social aware-ness as\nhumans. Additionally, there are built-in linguistic and cultural biases based\non their training data which is mostly in English and predominantly from\nWestern sources. Those facts limit AI suitability for some language learning\ninteractions. Stud-ies have clearly shown that systems such as ChatGPT often do\nnot produce language that is pragmatically appropriate. The lack of linguistic\nand cultural authenticity has important implications for how AI is integrated\ninto second language acquisition as well as in instruction targeting\ndevelopment of intercultural communication compe-tence.\n","authors":["Robert Godwin-Jones`"],"pdf_url":"https://arxiv.org/pdf/2410.14395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14391v1","updated":"2024-10-18T11:52:10Z","published":"2024-10-18T11:52:10Z","title":"Analyzing Context Utilization of LLMs in Document-Level Translation","summary":"  Large language models (LLM) are increasingly strong contenders in machine\ntranslation. We study document-level translation, where some words cannot be\ntranslated without context from outside the sentence. We investigate the\nability of prominent LLMs to utilize context by analyzing models' robustness to\nperturbed and randomized document context. We find that LLMs' improved\ndocument-translation performance is not always reflected in pronoun translation\nperformance. We highlight the need for context-aware finetuning of LLMs with a\nfocus on relevant parts of the context to improve their reliability for\ndocument-level translation.\n","authors":["Wafaa Mohammed","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2410.14391v1.pdf","comment":"4 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.14387v1","updated":"2024-10-18T11:39:34Z","published":"2024-10-18T11:39:34Z","title":"How Do Multilingual Models Remember? Investigating Multilingual Factual\n  Recall Mechanisms","summary":"  Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has primarily\nfocused on English monolingual models. The question of how these processes\ngeneralize to other languages and multilingual LLMs remains unexplored. In this\npaper, we address this gap by conducting a comprehensive analysis of two highly\nmultilingual LLMs. We assess the extent to which previously identified\ncomponents and mechanisms of factual recall in English apply to a multilingual\ncontext. Then, we examine when language plays a role in the recall process,\nuncovering evidence of language-independent and language-dependent mechanisms.\n","authors":["Constanza Fierro","Negar Foroutan","Desmond Elliott","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2410.14387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14375v1","updated":"2024-10-18T11:06:23Z","published":"2024-10-18T11:06:23Z","title":"Fine-Tuning Pre-trained Language Models for Robust Causal Representation\n  Learning","summary":"  The fine-tuning of pre-trained language models (PLMs) has been shown to be\neffective across various domains. By using domain-specific supervised data, the\ngeneral-purpose representation derived from PLMs can be transformed into a\ndomain-specific representation. However, these methods often fail to generalize\nto out-of-domain (OOD) data due to their reliance on non-causal\nrepresentations, often described as spurious features. Existing methods either\nmake use of adjustments with strong assumptions about lack of hidden common\ncauses, or mitigate the effect of spurious features using multi-domain data. In\nthis work, we investigate how fine-tuned pre-trained language models aid\ngeneralizability from single-domain scenarios under mild assumptions, targeting\nmore general and practical real-world scenarios. We show that a robust\nrepresentation can be derived through a so-called causal front-door adjustment,\nbased on a decomposition assumption, using fine-tuned representations as a\nsource of data augmentation. Comprehensive experiments in both synthetic and\nreal-world settings demonstrate the superior generalizability of the proposed\nmethod compared to existing approaches. Our work thus sheds light on the domain\ngeneralization problem by introducing links between fine-tuning and causal\nmechanisms into representation learning.\n","authors":["Jialin Yu","Yuxiang Zhou","Yulan He","Nevin L. Zhang","Ricardo Silva"],"pdf_url":"https://arxiv.org/pdf/2410.14375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10203v3","updated":"2024-10-18T10:43:40Z","published":"2024-06-14T17:38:21Z","title":"A Fundamental Trade-off in Aligned Language Models and its Relation to\n  Sampling Adaptors","summary":"  The relationship between the quality of a string, as judged by a human\nreader, and its probability, $p(\\boldsymbol{y})$ under a language model\nundergirds the development of better language models. For example, many popular\nalgorithms for sampling from a language model have been conceived with the goal\nof manipulating $p(\\boldsymbol{y})$ to place higher probability on strings that\nhumans deem of high quality. In this article, we examine the\nprobability--quality relationship in language models explicitly aligned to\nhuman preferences, e.g., through reinforcement learning through human feedback.\nWe show that, when sampling corpora from an aligned language model, there\nexists a trade-off between the strings' average reward and average\nlog-likelihood under the prior language model, i.e., the same model before\nalignment with human preferences. We provide a formal treatment of this\nphenomenon and demonstrate how a choice of sampling adaptor allows for a\nselection of how much likelihood we exchange for the reward.\n","authors":["Naaman Tan","Josef Valvoda","Tianyu Liu","Anej Svete","Yanxia Qin","Kan Min-Yen","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.10203v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14361v1","updated":"2024-10-18T10:40:47Z","published":"2024-10-18T10:40:47Z","title":"Efficiently Computing Susceptibility to Context in Language Models","summary":"  One strength of modern language models is their ability to incorporate\ninformation from a user-input context when answering queries. However, they are\nnot equally sensitive to the subtle changes to that context. To quantify this,\nDu et al. (2024) gives an information-theoretic metric to measure such\nsensitivity. Their metric, susceptibility, is defined as the degree to which\ncontexts can influence a model's response to a query at a distributional level.\nHowever, exactly computing susceptibility is difficult and, thus, Du et al.\n(2024) falls back on a Monte Carlo approximation. Due to the large number of\nsamples required, the Monte Carlo approximation is inefficient in practice. As\na faster alternative, we propose Fisher susceptibility, an efficient method to\nestimate the susceptibility based on Fisher information. Empirically, we\nvalidate that Fisher susceptibility is comparable to Monte Carlo estimated\nsusceptibility across a diverse set of query domains despite its being\n$70\\times$ faster. Exploiting the improved efficiency, we apply Fisher\nsusceptibility to analyze factors affecting the susceptibility of language\nmodels. We observe that larger models are as susceptible as smaller ones.\n","authors":["Tianyu Liu","Kevin Du","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2410.14361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11244v4","updated":"2024-10-18T10:21:31Z","published":"2023-10-17T13:12:32Z","title":"Entity Matching using Large Language Models","summary":"  Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines.\n","authors":["Ralph Peeters","Aaron Steiner","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2310.11244v4.pdf","comment":"Published in Proceedings of the 28th International Conference on\n  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN\n  978-3-89318-098-1 on OpenProceedings.org"},{"id":"http://arxiv.org/abs/2409.19700v3","updated":"2024-10-18T10:15:29Z","published":"2024-09-29T13:16:37Z","title":"2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models","summary":"  Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.\n","authors":["Jia-Nan Li","Jian Guan","Wei Wu","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10859v2","updated":"2024-10-18T10:02:03Z","published":"2024-10-07T13:46:06Z","title":"FAME: Towards Factual Multi-Task Model Editing","summary":"  Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.\n","authors":["Li Zeng","Yingyu Shan","Zeming Liu","Jiashu Yao","Yuhang Guo"],"pdf_url":"https://arxiv.org/pdf/2410.10859v2.pdf","comment":"9 pages, 3 figures. This paper has been accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2302.08102v2","updated":"2024-10-18T09:58:45Z","published":"2023-02-16T06:01:31Z","title":"Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition","summary":"  Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID.\n","authors":["Minsu Kim","Hyung-Il Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2302.08102v2.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2410.13281v2","updated":"2024-10-18T09:50:41Z","published":"2024-10-17T07:15:15Z","title":"BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla","summary":"  The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.\n","authors":["Fabiha Haider","Fariha Tanjim Shifat","Md Farhan Ishmam","Deeparghya Dutta Barua","Md Sakib Ul Rahman Sourove","Md Fahim","Md Farhad Alam"],"pdf_url":"https://arxiv.org/pdf/2410.13281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14335v1","updated":"2024-10-18T09:46:38Z","published":"2024-10-18T09:46:38Z","title":"Critical Questions Generation: Motivation and Challenges","summary":"  The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask.\n","authors":["Blanca Calvo Figueras","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2410.14335v1.pdf","comment":"14 pages, 3 figures, 7 tables, to be published in the 28th Conference\n  on Computational Natural Language Learning (CoNLL 2024)"},{"id":"http://arxiv.org/abs/2410.11677v2","updated":"2024-10-18T09:41:53Z","published":"2024-10-15T15:14:22Z","title":"Understanding Likelihood Over-optimisation in Direct Alignment\n  Algorithms","summary":"  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms such as\nProximal Policy Optimisation (PPO) for aligning language models to human\npreferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred)\ncompletions while discouraging worse (non-preferred) ones, while staying close\nto the original model's behaviour. In this work, we explore the relationship\nbetween completion likelihood and model performance in state-of-the-art DAAs,\nand identify a critical issue of likelihood over-optimisation. Contrary to\nexpectations, we find that higher likelihood of better completions and larger\nmargins between better and worse completion likelihoods do not necessarily lead\nto better performance, and may even degrade it. Our analysis reveals that while\nhigher likelihood correlates with better memorisation of factual knowledge\npatterns, a slightly lower completion likelihood tends to improve output\ndiversity, thus leading to better generalisation to unseen scenarios. Moreover,\nwe identify two key indicators that signal when over-optimised output diversity\nbegins to harm performance: Decreasing Entropy over Top-k Tokens and\nDiminishing Top-k Probability Mass. Our experimental results validate that\nthese indicators are reliable signs of declining performance under different\nregularisations, helping prevent over-optimisation and improve alignment with\nhuman preferences.\n","authors":["Zhengyan Shi","Sander Land","Acyr Locatelli","Matthieu Geist","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2410.11677v2.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2403.05902v2","updated":"2024-10-18T09:39:14Z","published":"2024-03-09T12:46:53Z","title":"MaiBaam Annotation Guidelines","summary":"  This document provides the annotation guidelines for MaiBaam, a Bavarian\ncorpus manually annotated with part-of-speech (POS) tags, syntactic\ndependencies, and German lemmas. MaiBaam belongs to the Universal Dependencies\n(UD) project, and our annotations elaborate on the general and German UD\nversion 2 guidelines. In this document, we detail how to preprocess and\ntokenize Bavarian data, provide an overview of the POS tags and dependencies we\nuse, explain annotation decisions that would also apply to closely related\nlanguages like German, and lastly we introduce and motivate decisions that are\nspecific to Bavarian grammar.\n","authors":["Verena Blaschke","Barbara Kovačić","Siyao Peng","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2403.05902v2.pdf","comment":"Updated for UD v2.15 (German lemmas added)"},{"id":"http://arxiv.org/abs/2410.10291v2","updated":"2024-10-18T09:26:46Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v2.pdf","comment":"The only change in the current version update is the replacement of\n  the template with a more precise one"},{"id":"http://arxiv.org/abs/2410.14309v1","updated":"2024-10-18T09:15:35Z","published":"2024-10-18T09:15:35Z","title":"LoGU: Long-form Generation with Uncertainty Expressions","summary":"  While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.\n","authors":["Ruihan Yang","Caiqi Zhang","Zhisong Zhang","Xinting Huang","Sen Yang","Nigel Collier","Dong Yu","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05672v2","updated":"2024-10-18T09:12:45Z","published":"2023-06-09T05:04:13Z","title":"I run as fast as a rabbit, can you? A Multilingual Simile Dialogue\n  Dataset","summary":"  A simile is a figure of speech that compares two different things (called the\ntenor and the vehicle) via shared properties. The tenor and the vehicle are\nusually connected with comparator words such as \"like\" or \"as\". The simile\nphenomena are unique and complex in a real-life dialogue scene where the tenor\nand the vehicle can be verbal phrases or sentences, mentioned by different\nspeakers, exist in different sentences, or occur in reversed order. However,\nthe current simile research usually focuses on similes in a triplet tuple\n(tenor, property, vehicle) or a single sentence where the tenor and vehicle are\nusually entities or noun phrases, which could not reflect complex simile\nphenomena in real scenarios. In this paper, we propose a novel and high-quality\nmultilingual simile dialogue (MSD) dataset to facilitate the study of complex\nsimile phenomena. The MSD is the largest manually annotated simile data\n($\\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD\ndata can also be used on dialogue tasks to test the ability of dialogue systems\nwhen using similes. We design 3 simile tasks (recognition, interpretation, and\ngeneration) and 2 dialogue tasks (retrieval and generation) with MSD. For each\ntask, we provide experimental results from strong pre-trained or\nstate-of-the-art models. The experiments demonstrate the challenge of MSD and\nwe have released the data/code on GitHub.\n","authors":["Longxuan Ma","Weinan Zhang","Shuhan Zhou","Churui Sun","Changxin Ke","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2306.05672v2.pdf","comment":"13 Pages, 1 Figure, 12 Tables, ACL 2023 findings"},{"id":"http://arxiv.org/abs/2406.13632v3","updated":"2024-10-18T09:07:53Z","published":"2024-06-19T15:28:29Z","title":"Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations","summary":"  Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach.\n","authors":["Arie Cattan","Alon Jacovi","Alex Fabrikant","Jonathan Herzig","Roee Aharoni","Hannah Rashkin","Dror Marcus","Avinatan Hassidim","Yossi Matias","Idan Szpektor","Avi Caciularu"],"pdf_url":"https://arxiv.org/pdf/2406.13632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09075v3","updated":"2024-10-18T09:02:46Z","published":"2023-12-14T16:10:56Z","title":"Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection","summary":"  Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.\n","authors":["Hao Sun","Hengyi Cai","Bo Wang","Yingyan Hou","Xiaochi Wei","Shuaiqiang Wang","Yan Zhang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2312.09075v3.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.13824v2","updated":"2024-10-18T09:01:01Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12013v2","updated":"2024-10-18T08:57:30Z","published":"2024-06-26T12:33:34Z","title":"Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis","summary":"  Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.\n","authors":["Mladen Popović","Maruf A. Dhali","Lambert Schomaker","Johannes van der Plicht","Kaare Lund Rasmussen","Jacopo La Nasa","Ilaria Degano","Maria Perla Colombini","Eibert Tigchelaar"],"pdf_url":"https://arxiv.org/pdf/2407.12013v2.pdf","comment":"16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments"},{"id":"http://arxiv.org/abs/2310.14626v2","updated":"2024-10-18T08:56:18Z","published":"2023-10-23T07:00:51Z","title":"Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue","summary":"  E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.\n","authors":["Yuanxing Liu","Wei-Nan Zhang","Yifan Chen","Yuchi Zhang","Haopeng Bai","Fan Feng","Hengbin Cui","Yongbin Li","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2310.14626v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2405.20680v4","updated":"2024-10-18T08:54:37Z","published":"2024-05-31T08:22:49Z","title":"Unraveling and Mitigating Retriever Inconsistencies in\n  Retrieval-Augmented Large Language Models","summary":"  Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their\nsuperiority in terms of factuality, they do not consistently outperform the\noriginal retrieval-free Language Models (LMs). Our experiments reveal that this\nexample-level performance inconsistency exists not only between\nretrieval-augmented and retrieval-free LM but also among different retrievers.\nTo understand this phenomenon, we investigate the degeneration behavior of\nRALMs and theoretically decompose it into four categories. Further analysis\nbased on our decomposition reveals that the innate difference in knowledge\nsources and the unpredictable degeneration of the reader model contribute most\nto the inconsistency. Drawing from our analysis, we introduce Ensemble of\nRetrievers (EoR), a trainable framework that can adaptively retrieve from\ndifferent knowledge sources and effectively decrease unpredictable reader\nerrors. Our experiments on Open Domain Question Answering show that EoR\nsubstantially improves performance over the RALM with a single retriever by\nconsiderably reducing inconsistent behaviors.\n","authors":["Mingda Li","Xinyu Li","Yifan Chen","Wenfeng Xuan","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.20680v4.pdf","comment":"ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2406.15053v2","updated":"2024-10-18T08:51:55Z","published":"2024-06-21T11:00:38Z","title":"PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data","summary":"  Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs.\n","authors":["Ishaan Watts","Varun Gumma","Aditya Yadavalli","Vivek Seshadri","Manohar Swaminathan","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2406.15053v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14289v1","updated":"2024-10-18T08:49:24Z","published":"2024-10-18T08:49:24Z","title":"SwaQuAD-24: QA Benchmark Dataset in Swahili","summary":"  This paper proposes the creation of a Swahili Question Answering (QA)\nbenchmark dataset, aimed at addressing the underrepresentation of Swahili in\nnatural language processing (NLP). Drawing from established benchmarks like\nSQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing\nhigh-quality, annotated question-answer pairs that capture the linguistic\ndiversity and complexity of Swahili. The dataset is designed to support a\nvariety of applications, including machine translation, information retrieval,\nand social services like healthcare chatbots. Ethical considerations, such as\ndata privacy, bias mitigation, and inclusivity, are central to the dataset\ndevelopment. Additionally, the paper outlines future expansion plans to include\ndomain-specific content, multimodal integration, and broader crowdsourcing\nefforts. The Swahili QA dataset aims to foster technological innovation in East\nAfrica and provide an essential resource for NLP research and applications in\nlow-resource languages.\n","authors":["Alfred Malengo Kondoro"],"pdf_url":"https://arxiv.org/pdf/2410.14289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14276v1","updated":"2024-10-18T08:31:22Z","published":"2024-10-18T08:31:22Z","title":"EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding","summary":"  Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks.\n","authors":["Ching Ming Samuel Lau","Weiqi Wang","Haochen Shi","Baixuan Xu","Jiaxin Bai","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2410.14276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14273v1","updated":"2024-10-18T08:27:02Z","published":"2024-10-18T08:27:02Z","title":"REEF: Representation Encoding Fingerprints for Large Language Models","summary":"  Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF.\n","authors":["Jie Zhang","Dongrui Liu","Chen Qian","Linfeng Zhang","Yong Liu","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2410.14273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14268v1","updated":"2024-10-18T08:22:07Z","published":"2024-10-18T08:22:07Z","title":"MoDification: Mixture of Depths Made Easy","summary":"  Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.\n","authors":["Chen Zhang","Meizhi Zhong","Qimeng Wang","Xuantao Lu","Zheyu Ye","Chengqiang Lu","Yan Gao","Yao Hu","Kehai Chen","Min Zhang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2410.14268v1.pdf","comment":"12 pages, 9 figures, 5 tables, work in progress"},{"id":"http://arxiv.org/abs/2406.06572v2","updated":"2024-10-18T08:20:38Z","published":"2024-06-03T17:07:46Z","title":"Graph Neural Network Enhanced Retrieval for Question Answering of LLMs","summary":"  Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset.\n","authors":["Zijian Li","Qingyan Guo","Jiawei Shao","Lei Song","Jiang Bian","Jun Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06572v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14262v1","updated":"2024-10-18T08:18:18Z","published":"2024-10-18T08:18:18Z","title":"Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation","summary":"  This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.\n","authors":[" Edward"," Kwartler","Matthew Berman","Alan Aqrawi"],"pdf_url":"https://arxiv.org/pdf/2410.14262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14259v1","updated":"2024-10-18T08:14:10Z","published":"2024-10-18T08:14:10Z","title":"Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement","summary":"  The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-AI collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies.\n","authors":["Zihao Cheng","Li Zhou","Feng Jiang","Benyou Wang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.14259v1.pdf","comment":"Social Media, Large Language Models, LLM-generated Text Detection,\n  AI-assisted News Detection"},{"id":"http://arxiv.org/abs/2410.14255v1","updated":"2024-10-18T08:04:36Z","published":"2024-10-18T08:04:36Z","title":"Nova: An Iterative Planning and Search Approach to Enhance Novelty and\n  Diversity of LLM Generated Ideas","summary":"  Scientific innovation is pivotal for humanity, and harnessing large language\nmodels (LLMs) to generate research ideas could transform discovery. However,\nexisting LLMs often produce simplistic and repetitive suggestions due to their\nlimited ability in acquiring external knowledge for innovation. To address this\nproblem, we introduce an enhanced planning and search methodology designed to\nboost the creative potential of LLM-based systems. Our approach involves an\niterative process to purposely plan the retrieval of external knowledge,\nprogressively enriching the idea generation with broader and deeper insights.\nValidation through automated and human assessments indicates that our framework\nsubstantially elevates the quality of generated ideas, particularly in novelty\nand diversity. The number of unique novel ideas produced by our framework is\n3.4 times higher than without it. Moreover, our method outperforms the current\nstate-of-the-art, generating at least 2.5 times more top-rated ideas based on\n170 seed papers in a Swiss Tournament evaluation.\n","authors":["Xiang Hu","Hongyu Fu","Jinge Wang","Yifeng Wang","Zhikun Li","Renjun Xu","Yu Lu","Yaochu Jin","Lili Pan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2410.14255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17524v4","updated":"2024-10-18T08:03:02Z","published":"2024-04-26T16:41:00Z","title":"On the Use of Large Language Models to Generate Capability Ontologies","summary":"  Capability ontologies are increasingly used to model functionalities of\nsystems or machines. The creation of such ontological models with all\nproperties and constraints of capabilities is very complex and can only be done\nby ontology experts. However, Large Language Models (LLMs) have shown that they\ncan generate machine-interpretable models from natural language text input and\nthus support engineers / ontology experts. Therefore, this paper investigates\nhow LLMs can be used to create capability ontologies. We present a study with a\nseries of experiments in which capabilities with varying complexities are\ngenerated using different prompting techniques and with different LLMs. Errors\nin the generated ontologies are recorded and compared. To analyze the quality\nof the generated ontologies, a semi-automated approach based on RDF syntax\nchecking, OWL reasoning, and SHACL constraints is used. The results of this\nstudy are very promising because even for complex capabilities, the generated\nontologies are almost free of errors.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2404.17524v4.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.14251v1","updated":"2024-10-18T08:01:39Z","published":"2024-10-18T08:01:39Z","title":"Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation","summary":"  Post-training is essential for enabling large language models (LLMs) to\nfollow human instructions. Inspired by the recent success of using LLMs to\nsimulate human society, we leverage multi-agent simulation to automatically\ngenerate diverse text-based scenarios, capturing a wide range of real-world\nhuman needs. We propose MATRIX, a multi-agent simulator that creates realistic\nand scalable scenarios. Leveraging these outputs, we introduce a novel\nscenario-driven instruction generator MATRIX-Gen for controllable and highly\nrealistic data synthesis. Extensive experiments demonstrate that our framework\neffectively generates both general and domain-specific data. Notably, on\nAlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on\ndatasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,\noutperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M\npairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen.\n","authors":["Shuo Tang","Xianghe Pang","Zexi Liu","Bohan Tang","Rui Ye","Xiaowen Dong","Yanfeng Wang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14248v1","updated":"2024-10-18T07:52:22Z","published":"2024-10-18T07:52:22Z","title":"Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models","summary":"  Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.\n","authors":["Olga Loginova","Oleksandr Bezrukov","Alexey Kravets"],"pdf_url":"https://arxiv.org/pdf/2410.14248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14236v1","updated":"2024-10-18T07:36:57Z","published":"2024-10-18T07:36:57Z","title":"A Novel Method to Metigate Demographic and Expert Bias in ICD Coding\n  with Causal Inference","summary":"  ICD(International Classification of Diseases) coding involves assigning ICD\ncodes to patients visit based on their medical notes. Considering ICD coding as\na multi-label text classification task, researchers have developed\nsophisticated methods. Despite progress, these models often suffer from label\nimbalance and may develop spurious correlations with demographic factors.\nAdditionally, while human coders assign ICD codes, the inclusion of irrelevant\ninformation from unrelated experts introduces biases. To combat these issues,\nwe propose a novel method to mitigate Demographic and Expert biases in ICD\ncoding through Causal Inference (DECI). We provide a novel causality-based\ninterpretation in ICD Coding that models make predictions by three distinct\npathways. And based counterfactual reasoning, DECI mitigate demographic and\nexpert biases. Experimental results show that DECI outperforms state-of-the-art\nmodels, offering a significant advancement in accurate and unbiased ICD coding.\n","authors":["Bin Zhang","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07962v2","updated":"2024-10-18T07:34:39Z","published":"2024-06-12T07:41:44Z","title":"Toward a Method to Generate Capability Ontologies from Natural Language\n  Descriptions","summary":"  To achieve a flexible and adaptable system, capability ontologies are\nincreasingly leveraged to describe functions in a machine-interpretable way.\nHowever, modeling such complex ontological descriptions is still a manual and\nerror-prone task that requires a significant amount of effort and ontology\nexpertise. This contribution presents an innovative method to automate\ncapability ontology modeling using Large Language Models (LLMs), which have\nproven to be well suited for such tasks. Our approach requires only a natural\nlanguage description of a capability, which is then automatically inserted into\na predefined prompt using a few-shot prompting technique. After prompting an\nLLM, the resulting capability ontology is automatically verified through\nvarious steps in a loop with the LLM to check the overall correctness of the\ncapability ontology. First, a syntax check is performed, then a check for\ncontradictions, and finally a check for hallucinations and missing ontology\nelements. Our method greatly reduces manual effort, as only the initial natural\nlanguage description and a final human review and possible correction are\nnecessary, thereby streamlining the capability ontology generation process.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2406.07962v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.14235v1","updated":"2024-10-18T07:34:21Z","published":"2024-10-18T07:34:21Z","title":"Towards Robust Knowledge Representations in Multilingual LLMs for\n  Equivalence and Inheritance based Consistent Reasoning","summary":"  Reasoning and linguistic skills form the cornerstone of human intelligence,\nfacilitating problem-solving and decision-making. Recent advances in Large\nLanguage Models (LLMs) have led to impressive linguistic capabilities and\nemergent reasoning behaviors, fueling widespread adoption across application\ndomains. However, LLMs still struggle with complex reasoning tasks,\nhighlighting their systemic limitations. In this work, we focus on evaluating\nwhether LLMs have the requisite representations to reason using two\nfoundational relationships: \"equivalence\" and \"inheritance\". We introduce novel\ntasks and benchmarks spanning six languages and observe that current SOTA LLMs\noften produce conflicting answers to the same questions across languages in\n17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.\nTo enhance consistency across languages, we propose novel \"Compositional\nRepresentations\" where tokens are represented as composition of equivalent\ntokens across languages, with resulting conflict reduction (up to -4.7%)\nindicating benefits of shared LLM representations.\n","authors":["Gaurav Arora","Srujana Merugu","Shreya Jain","Vaibhav Saxena"],"pdf_url":"https://arxiv.org/pdf/2410.14235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14231v1","updated":"2024-10-18T07:25:00Z","published":"2024-10-18T07:25:00Z","title":"Unveiling Large Language Models Generated Texts: A Multi-Level\n  Fine-Grained Detection Framework","summary":"  Large language models (LLMs) have transformed human writing by enhancing\ngrammar correction, content expansion, and stylistic refinement. However, their\nwidespread use raises concerns about authorship, originality, and ethics, even\npotentially threatening scholarly integrity. Existing detection methods, which\nmainly rely on single-feature analysis and binary classification, often fail to\neffectively identify LLM-generated text in academic contexts. To address these\nchallenges, we propose a novel Multi-level Fine-grained Detection (MFD)\nframework that detects LLM-generated text by integrating low-level structural,\nhigh-level semantic, and deep-level linguistic features, while conducting\nsentence-level evaluations of lexicon, grammar, and syntax for comprehensive\nanalysis. To improve detection of subtle differences in LLM-generated text and\nenhance robustness against paraphrasing, we apply two mainstream evasion\ntechniques to rewrite the text. These variations, along with original texts,\nare used to train a text encoder via contrastive learning, extracting\nhigh-level semantic features of sentence to boost detection generalization.\nFurthermore, we leverage advanced LLM to analyze the entire text and extract\ndeep-level linguistic features, enhancing the model's ability to capture\ncomplex patterns and nuances while effectively incorporating contextual\ninformation. Extensive experiments on public datasets show that the MFD model\noutperforms existing methods, achieving an MAE of 0.1346 and an accuracy of\n88.56%. Our research provides institutions and publishers with an effective\nmechanism to detect LLM-generated text, mitigating risks of compromised\nauthorship. Educators and editors can use the model's predictions to refine\nverification and plagiarism prevention protocols, ensuring adherence to\nstandards.\n","authors":["Zhen Tao","Zhiyu Li","Runyu Chen","Dinghao Xi","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.14231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14225v1","updated":"2024-10-18T07:14:54Z","published":"2024-10-18T07:14:54Z","title":"Few-Shot Joint Multimodal Entity-Relation Extraction via\n  Knowledge-Enhanced Cross-modal Prompt Model","summary":"  Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task\nthat aims to extract entities and their relations from text-image pairs in\nsocial media posts. Existing methods for JMERE require large amounts of labeled\ndata. However, gathering and annotating fine-grained multimodal data for JMERE\nposes significant challenges. Initially, we construct diverse and comprehensive\nmultimodal few-shot datasets fitted to the original data distribution. To\naddress the insufficient information in the few-shot setting, we introduce the\n\\textbf{K}nowledge-\\textbf{E}nhanced \\textbf{C}ross-modal \\textbf{P}rompt\n\\textbf{M}odel (KECPM) for JMERE. This method can effectively address the\nproblem of insufficient information in the few-shot setting by guiding a large\nlanguage model to generate supplementary background knowledge. Our proposed\nmethod comprises two stages: (1) a knowledge ingestion stage that dynamically\nformulates prompts based on semantic similarity guide ChatGPT generating\nrelevant knowledge and employs self-reflection to refine the knowledge; (2) a\nknowledge-enhanced language model stage that merges the auxiliary knowledge\nwith the original input and utilizes a transformer-based model to align with\nJMERE's required output format. We extensively evaluate our approach on a\nfew-shot dataset derived from the JMERE dataset, demonstrating its superiority\nover strong baselines in terms of both micro and macro F$_1$ scores.\nAdditionally, we present qualitative analyses and case studies to elucidate the\neffectiveness of our model.\n","authors":["Li Yuan","Yi Cai","Junsheng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14225v1.pdf","comment":"accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2409.09785v3","updated":"2024-10-18T07:11:35Z","published":"2024-09-15T16:32:49Z","title":"Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","summary":"  Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.\n","authors":["Chao-Han Huck Yang","Taejin Park","Yuan Gong","Yuanchao Li","Zhehuai Chen","Yen-Ting Lin","Chen Chen","Yuchen Hu","Kunal Dhawan","Piotr Żelasko","Chao Zhang","Yun-Nung Chen","Yu Tsao","Jagadeesh Balam","Boris Ginsburg","Sabato Marco Siniscalchi","Eng Siong Chng","Peter Bell","Catherine Lai","Shinji Watanabe","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.09785v3.pdf","comment":"IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline"},{"id":"http://arxiv.org/abs/2410.09421v2","updated":"2024-10-18T07:10:38Z","published":"2024-10-12T07:56:47Z","title":"VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language\n  Models Alignment","summary":"  As large vision-language models (LVLMs) evolve rapidly, the demand for\nhigh-quality and diverse data to align these models becomes increasingly\ncrucial. However, the creation of such data with human supervision proves\ncostly and time-intensive. In this paper, we investigate the efficacy of AI\nfeedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the\nfirst large-scale vision-language feedback dataset, comprising over 82K\nmulti-modal instructions and comprehensive rationales generated by\noff-the-shelf models without human annotations. To evaluate the effectiveness\nof AI feedback for vision-language alignment, we train Silkie, an LVLM\nfine-tuned via direct preference optimization on VLFeedback. Silkie showcases\nexceptional performance regarding helpfulness, visual faithfulness, and safety\nmetrics. It outperforms its base model by 6.9\\% and 9.5\\% in perception and\ncognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits\nenhanced resilience against red-teaming attacks. Furthermore, our analysis\nunderscores the advantage of AI feedback, particularly in fostering preference\ndiversity to deliver more comprehensive improvements. Our dataset, training\ncode and models are available at https://vlf-silkie.github.io.\n","authors":["Lei Li","Zhihui Xie","Mukai Li","Shunian Chen","Peiyi Wang","Liang Chen","Yazheng Yang","Benyou Wang","Lingpeng Kong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09421v2.pdf","comment":"EMNLP 2024 Main Conference camera-ready version (fixed small typos).\n  This article supersedes arXiv:2312.10665"},{"id":"http://arxiv.org/abs/2403.04808v3","updated":"2024-10-18T07:05:57Z","published":"2024-03-06T10:55:30Z","title":"WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off","summary":"  Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.\n","authors":["Eva Giboulot","Teddy Furon"],"pdf_url":"https://arxiv.org/pdf/2403.04808v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14024v4","updated":"2024-10-18T06:59:24Z","published":"2024-06-20T06:42:27Z","title":"LLM Critics Help Catch Bugs in Mathematics: Towards a Better\n  Mathematical Verifier with Natural Language Feedback","summary":"  In recent progress, mathematical verifiers have achieved success in\nmathematical reasoning tasks by validating the correctness of solutions\ngenerated by policy models. However, existing verifiers are trained with binary\nclassification labels, which are not informative enough for the model to\naccurately assess the solutions. To mitigate the aforementioned insufficiency\nof binary labels, we introduce step-wise natural language feedback as rationale\nlabels, that is, the correctness of each step and the detailed explanations. In\nthis paper, we propose Math-Minos, a natural language feedback-enhanced\nverifier by constructing automatically generated training data and a two-stage\ntraining paradigm for effective training and efficient inference. Our\nexperiments reveal that a small set of natural language feedback can\nsignificantly boost the performance of the verifier in both verification and\nreinforcement learning. We have released the code and data for further\nexploration.\n","authors":["Bofei Gao","Zefan Cai","Runxin Xu","Peiyi Wang","Ce Zheng","Runji Lin","Keming Lu","Dayiheng Liu","Chang Zhou","Wen Xiao","Junjie Hu","Tianyu Liu","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2406.14024v4.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2410.14211v1","updated":"2024-10-18T06:57:19Z","published":"2024-10-18T06:57:19Z","title":"Paths-over-Graph: Knowledge Graph Enpowered Large Language Model\n  Reasoning","summary":"  Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.\n","authors":["Xingyu Tan","Xiaoyang Wang","Qing Liu","Xiwei Xu","Xin Yuan","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19740v2","updated":"2024-10-18T06:57:08Z","published":"2024-05-30T06:38:32Z","title":"PertEval: Unveiling Real Knowledge Capacity of LLMs with\n  Knowledge-Invariant Perturbations","summary":"  Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}.\n","authors":["Jiatong Li","Renjun Hu","Kunzhe Huang","Yan Zhuang","Qi Liu","Mengxiao Zhu","Xing Shi","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2405.19740v2.pdf","comment":"Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14\n  tables"},{"id":"http://arxiv.org/abs/2403.01976v5","updated":"2024-10-18T06:52:17Z","published":"2024-03-04T12:19:28Z","title":"SciAssess: Benchmarking LLM Proficiency in Scientific Literature\n  Analysis","summary":"  Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nscientific literature analysis. However, existing benchmarks fail to adequately\nevaluate the proficiency of LLMs in this domain, particularly in scenarios\nrequiring higher-level abilities beyond mere memorization and the handling of\nmultimodal data. In response to this gap, we introduce SciAssess, a benchmark\nspecifically designed for the comprehensive evaluation of LLMs in scientific\nliterature analysis. It aims to thoroughly assess the efficacy of LLMs by\nevaluating their capabilities in Memorization (L1), Comprehension (L2), and\nAnalysis \\& Reasoning (L3). It encompasses a variety of tasks drawn from\ndiverse scientific fields, including biology, chemistry, material, and\nmedicine. To ensure the reliability of SciAssess, rigorous quality control\nmeasures have been implemented, ensuring accuracy, anonymization, and\ncompliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting\ntheir strengths and areas for improvement. We hope this evaluation supports the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are available at\n\\url{https://github.com/sci-assess/SciAssess}.\n","authors":["Hengxing Cai","Xiaochen Cai","Junhan Chang","Sihang Li","Lin Yao","Changxin Wang","Zhifeng Gao","Hongshuai Wang","Yongge Li","Mujie Lin","Shuwen Yang","Jiankun Wang","Mingjun Xu","Jin Huang","Xi Fang","Jiaxi Zhuang","Yuqi Yin","Yaqi Li","Changhong Chen","Zheng Cheng","Zifeng Zhao","Linfeng Zhang","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2403.01976v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14208v1","updated":"2024-10-18T06:50:15Z","published":"2024-10-18T06:50:15Z","title":"Montessori-Instruct: Generate Influential Training Data Tailored for\n  Student Learning","summary":"  Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.\n","authors":["Xiaochuan Li","Zichun Yu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.14208v1.pdf","comment":"Codes and data are open-sourced at\n  https://github.com/cxcscmu/Montessori-Instruct"},{"id":"http://arxiv.org/abs/2410.14204v1","updated":"2024-10-18T06:38:22Z","published":"2024-10-18T06:38:22Z","title":"MediTOD: An English Dialogue Dataset for Medical History Taking with\n  Comprehensive Annotations","summary":"  Medical task-oriented dialogue systems can assist doctors by collecting\npatient medical history, aiding in diagnosis, or guiding treatment selection,\nthereby reducing doctor burnout and expanding access to medical services.\nHowever, doctor-patient dialogue datasets are not readily available, primarily\ndue to privacy regulations. Moreover, existing datasets lack comprehensive\nannotations involving medical slots and their different attributes, such as\nsymptoms and their onset, progression, and severity. These comprehensive\nannotations are crucial for accurate diagnosis. Finally, most existing datasets\nare non-English, limiting their utility for the larger research community.\n  In response, we introduce MediTOD, a new dataset of doctor-patient dialogues\nin English for the medical history-taking task. Collaborating with doctors, we\ndevise a questionnaire-based labeling scheme tailored to the medical domain.\nThen, medical professionals create the dataset with high-quality comprehensive\nannotations, capturing medical slots and their attributes. We establish\nbenchmarks in supervised and few-shot settings on MediTOD for natural language\nunderstanding, policy learning, and natural language generation subtasks,\nevaluating models from both TOD and biomedical domains. We make MediTOD\npublicly available for future research.\n","authors":["Vishal Vivek Saley","Goonjan Saha","Rocktim Jyoti Das","Dinesh Raghu"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2410.14204v1.pdf","comment":"EMNLP2024 Camera Ready Version"},{"id":"http://arxiv.org/abs/2410.14202v1","updated":"2024-10-18T06:35:17Z","published":"2024-10-18T06:35:17Z","title":"Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs","summary":"  Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays.\n","authors":["SeongYeub Chu","JongWoo Kim","Bryan Wong","MunYong Yi"],"pdf_url":"https://arxiv.org/pdf/2410.14202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14200v1","updated":"2024-10-18T06:31:40Z","published":"2024-10-18T06:31:40Z","title":"E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model","summary":"  The development of 3D medical vision-language models holds significant\npotential for disease diagnosis and patient treatment. However, compared to 2D\nmedical images, 3D medical images, such as CT scans, face challenges related to\nlimited training data and high dimension, which severely restrict the progress\nof 3D medical vision-language models. To address these issues, we collect a\nlarge amount of unlabeled 3D CT data and utilize self-supervised learning to\nconstruct a 3D visual foundation model for extracting 3D visual features. Then,\nwe apply 3D spatial convolutions to aggregate and project high-level image\nfeatures, reducing computational complexity while preserving spatial\ninformation. We also construct two instruction-tuning datasets based on BIMCV-R\nand CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates\nsuperior performance compared to existing methods in report generation, visual\nquestion answering, and disease diagnosis. Code and data will be made publicly\navailable soon.\n","authors":["Haoran Lai","Zihang Jiang","Qingsong Yao","Rongsheng Wang","Zhiyang He","Xiaodong Tao","Wei Wei","Weifu Lv","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14198v1","updated":"2024-10-18T06:25:27Z","published":"2024-10-18T06:25:27Z","title":"Supervised Chain of Thought","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.\n","authors":["Xiang Zhang","Dujian Ding"],"pdf_url":"https://arxiv.org/pdf/2410.14198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06833v2","updated":"2024-10-18T06:19:22Z","published":"2024-04-10T08:49:27Z","title":"Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge","summary":"  Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.\n","authors":["Li Zhou","Taelin Karidi","Wanlong Liu","Nicolas Garneau","Yong Cao","Wenyu Chen","Haizhou Li","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2404.06833v2.pdf","comment":"cultural bias analysis, cultural knowledge probing, large language\n  models, cultural NLP"},{"id":"http://arxiv.org/abs/2405.15585v3","updated":"2024-10-18T06:14:50Z","published":"2024-05-24T14:13:54Z","title":"Synergizing In-context Learning with Hints for End-to-end Task-oriented\n  Dialog Systems","summary":"  End-to-end Task-Oriented Dialog (TOD) systems typically require extensive\ntraining datasets to perform well. In contrast, large language model (LLM)\nbased TOD systems can excel even with limited data due to their ability to\nlearn tasks through in-context exemplars. However, these models lack alignment\nwith the style of responses in training data and often generate comprehensive\nresponses, making it difficult for users to grasp the information quickly. In\nresponse, we propose SyncTOD that synergizes LLMs with task-specific hints to\nimprove alignment in low-data settings. SyncTOD employs small auxiliary models\nto provide hints and select exemplars for in-context prompts. With ChatGPT,\nSyncTOD achieves superior performance compared to LLM-based baselines and SoTA\nmodels in low-data settings, while retaining competitive performance in\nfull-data settings.\n","authors":["Vishal Vivek Saley","Rocktim Jyoti Das","Dinesh Raghu"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2405.15585v3.pdf","comment":"EMNLP2024 Camera-Ready Version"},{"id":"http://arxiv.org/abs/2410.14194v1","updated":"2024-10-18T06:09:41Z","published":"2024-10-18T06:09:41Z","title":"Speciesism in Natural Language Processing Research","summary":"  Natural Language Processing (NLP) research on AI Safety and social bias in AI\nhas focused on safety for humans and social bias against human minorities.\nHowever, some AI ethicists have argued that the moral significance of nonhuman\nanimals has been ignored in AI research. Therefore, the purpose of this study\nis to investigate whether there is speciesism, i.e., discrimination against\nnonhuman animals, in NLP research. First, we explain why nonhuman animals are\nrelevant in NLP research. Next, we survey the findings of existing research on\nspeciesism in NLP researchers, data, and models and further investigate this\nproblem in this study. The findings of this study suggest that speciesism\nexists within researchers, data, and models, respectively. Specifically, our\nsurvey and experiments show that (a) among NLP researchers, even those who\nstudy social bias in AI, do not recognize speciesism or speciesist bias; (b)\namong NLP data, speciesist bias is inherent in the data annotated in the\ndatasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,\nexhibit speciesist bias by default. Finally, we discuss how we can reduce\nspeciesism in NLP research.\n","authors":["Masashi Takeshita","Rafal Rzepka"],"pdf_url":"https://arxiv.org/pdf/2410.14194v1.pdf","comment":"This article is a preprint and has not been peer-reviewed. The\n  postprint has been accepted for publication in AI and Ethics. Please cite the\n  final version of the article once it is published"},{"id":"http://arxiv.org/abs/2410.04422v4","updated":"2024-10-18T06:09:31Z","published":"2024-10-06T09:29:19Z","title":"Hyper-multi-step: The Truth Behind Difficult Long-context Tasks","summary":"  Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them.\n","authors":["Yijiong Yu","Ma Xiufa","Fang Jianwei","Zhi Xu","Su Guangyao","Wang Jiancheng","Yongfeng Huang","Zhixiao Qi","Wei Wang","Weifeng Liu","Ran Chen","Ji Pei"],"pdf_url":"https://arxiv.org/pdf/2410.04422v4.pdf","comment":"Our code is publicly available at\n  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at\n  https://huggingface.co/datasets/yuyijiong/difficult_retrieval"},{"id":"http://arxiv.org/abs/2410.14184v1","updated":"2024-10-18T05:31:13Z","published":"2024-10-18T05:31:13Z","title":"MetaAlign: Align Large Language Models with Diverse Preferences during\n  Inference Time","summary":"  Large Language Models (LLMs) acquire extensive knowledge and remarkable\nabilities from extensive text corpora, making them powerful tools for various\napplications. To make LLMs more usable, aligning them with human preferences is\nessential. Existing alignment techniques, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed\npredefined preferences directly within the model's parameters. These methods,\nhowever, often result in a static alignment that can not account for the\ndiversity of human preferences in practical applications. In response to this\nchallenge, we propose an effective method, \\textbf{MetaAlign}, which aims to\nhelp LLMs dynamically align with various explicit or implicit preferences\nspecified at inference time. Experimental results show that LLMs optimized on\nour meticulously constructed MetaAlign Dataset can effectively align with any\npreferences specified at the inference stage, validating the feasibility of\nMetaAlign. We hope that our work can provide some insights into the alignment\nof language models.\n","authors":["Mozhi Zhang","Pengyu Wang","Chenkun Tan","Mianqiu Huang","Dong Zhang","Yaqian Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.14184v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.00131v2","updated":"2024-10-18T05:22:02Z","published":"2024-09-30T18:12:18Z","title":"Fisher Information-based Efficient Curriculum Federated Learning with\n  Large Language Models","summary":"  As a promising paradigm to collaboratively train models with decentralized\ndata, Federated Learning (FL) can be exploited to fine-tune Large Language\nModels (LLMs). While LLMs correspond to huge size, the scale of the training\ndata significantly increases, which leads to tremendous amounts of computation\nand communication costs. The training data is generally non-Independent and\nIdentically Distributed (non-IID), which requires adaptive data processing\nwithin each device. Although Low Rank Adaptation (LoRA) can significantly\nreduce the scale of parameters to update in the fine-tuning process, it still\ntakes unaffordable time to transfer the low-rank parameters of all the layers\nin LLMs. In this paper, we propose a Fisher Information-based Efficient\nCurriculum Federated Learning framework (FibecFed) with two novel methods,\ni.e., adaptive federated curriculum learning and efficient sparse parameter\nupdate. First, we propose a fisher information-based method to adaptively\nsample data within each device to improve the effectiveness of the FL\nfine-tuning process. Second, we dynamically select the proper layers for global\naggregation and sparse parameters for local update with LoRA so as to improve\nthe efficiency of the FL fine-tuning process. Extensive experimental results\nbased on 10 datasets demonstrate that FibecFed yields excellent performance (up\nto 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%\nfaster) compared with 17 baseline approaches).\n","authors":["Ji Liu","Jiaxiang Ren","Ruoming Jin","Zijie Zhang","Yang Zhou","Patrick Valduriez","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2410.00131v2.pdf","comment":"27 pages, 8 figures, 14 tables, to appear in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14182v1","updated":"2024-10-18T05:21:05Z","published":"2024-10-18T05:21:05Z","title":"LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs","summary":"  Laboratory accidents pose significant risks to human life and property,\nunderscoring the importance of robust safety protocols. Despite advancements in\nsafety training, laboratory personnel may still unknowingly engage in unsafe\npractices. With the increasing reliance on large language models (LLMs) for\nguidance in various fields, including laboratory settings, there is a growing\nconcern about their reliability in critical safety-related decision-making.\nUnlike trained human researchers, LLMs lack formal lab safety education,\nraising questions about their ability to provide safe and accurate guidance.\nExisting research on LLM trustworthiness primarily focuses on issues such as\nethical compliance, truthfulness, and fairness but fails to fully cover\nsafety-critical real-world applications, like lab safety. To address this gap,\nwe propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive\nevaluation framework based on a new taxonomy aligned with Occupational Safety\nand Health Administration (OSHA) protocols. This benchmark includes 765\nmultiple-choice questions verified by human experts, assessing LLMs and vision\nlanguage models (VLMs) performance in lab safety contexts. Our evaluations\ndemonstrate that while GPT-4o outperforms human participants, it is still prone\nto critical errors, highlighting the risks of relying on LLMs in\nsafety-critical environments. Our findings emphasize the need for specialized\nbenchmarks to accurately assess the trustworthiness of LLMs in real-world\nsafety applications.\n","authors":["Yujun Zhou","Jingdong Yang","Kehan Guo","Pin-Yu Chen","Tian Gao","Werner Geyer","Nuno Moniz","Nitesh V Chawla","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14182v1.pdf","comment":"50 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.14180v1","updated":"2024-10-18T05:16:39Z","published":"2024-10-18T05:16:39Z","title":"XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting","summary":"  Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality.\n","authors":["Taha Aksu","Chenghao Liu","Amrita Saha","Sarah Tan","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.14180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14179v1","updated":"2024-10-18T05:15:50Z","published":"2024-10-18T05:15:50Z","title":"MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart\n  Problems","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nabilities across various tasks, including visual question answering and chart\ncomprehension, yet existing benchmarks for chart-related tasks fall short in\ncapturing the complexity of real-world multi-chart scenarios. Current\nbenchmarks primarily focus on single-chart tasks, neglecting the multi-hop\nreasoning required to extract and integrate information from multiple charts,\nwhich is essential in practical applications. To fill this gap, we introduce\nMultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:\ndirect question answering, parallel question answering, comparative reasoning,\nand sequential reasoning. Our evaluation of a wide range of MLLMs reveals\nsignificant performance gaps compared to humans. These results highlight the\nchallenges in multi-chart comprehension and the potential of MultiChartQA to\ndrive advancements in this field. Our code and data are available at\nhttps://github.com/Zivenzhu/Multi-chart-QA\n","authors":["Zifeng Zhu","Mengzhao Jia","Zhihan Zhang","Lang Li","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.14179v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.10270v2","updated":"2024-10-18T05:07:38Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v2.pdf","comment":"Accepted for ENLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2408.15545v3","updated":"2024-10-18T05:04:53Z","published":"2024-08-28T05:41:52Z","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","summary":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","authors":["Sihang Li","Jin Huang","Jiaxi Zhuang","Yaorui Shi","Xiaochen Cai","Mingjun Xu","Xiang Wang","Linfeng Zhang","Guolin Ke","Hengxing Cai"],"pdf_url":"https://arxiv.org/pdf/2408.15545v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13276v2","updated":"2024-10-18T05:01:11Z","published":"2024-10-17T07:07:09Z","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs","summary":"  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity limits the efficiency and scalability of LLMs, especially\nfor those with a long-context window. A promising approach addressing this\nlimitation is to leverage the sparsity in attention. However, existing\nsparsity-based solutions predominantly rely on predefined patterns or\nheuristics to approximate sparsity. This practice falls short to fully capture\nthe dynamic nature of attention sparsity in language-based tasks. This paper\nargues that attention sparsity should be learned rather than predefined. To\nthis end, we design SeerAttention, a new Attention mechanism that augments the\nconventional attention with a learnable gate that adaptively selects\nsignificant blocks in an attention map and deems the rest blocks sparse. Such\nblock-level sparsity effectively balances accuracy and speedup. To enable\nefficient learning of the gating network, we develop a customized\nFlashAttention implementation that extracts the block-level ground truth of\nattention map with minimum overhead. SeerAttention not only applies to\npost-training, but also excels in long-context fine-tuning. Our results show\nthat at post-training stages, SeerAttention significantly outperforms\nstate-of-the-art static or heuristic-based sparse attention methods, while also\nbeing more versatile and flexible to adapt to varying context lengths and\nsparsity ratios. When applied to long-context fine-tuning with YaRN,\nSeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context\nlength with minimal perplexity loss, offering a 5.67x speedup over\nFlashAttention-2.\n","authors":["Yizhao Gao","Zhichen Zeng","Dayou Du","Shijie Cao","Hayden Kwok-Hay So","Ting Cao","Fan Yang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05541v2","updated":"2024-10-18T04:52:38Z","published":"2024-08-10T12:44:49Z","title":"P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for\n  data pruning in LLM Training","summary":"  In the rapidly advancing field of Large Language Models (LLMs), effectively\nleveraging existing datasets during fine-tuning to maximize the model's\npotential is of paramount importance. This paper introduces P3, an adaptive\nframework aimed at optimizing the task-specific fine-tuning process through\niterative data pruning. P3 consists of three key components: (1) Policy-driven\nDifficulty Measurement, which dynamically assesses data difficulty based on the\nmodel's real-time performance, replacing static metrics with adaptable\nevaluations; (2) Pace-Adaptive Selection, leveraging self-paced learning to\nprogressively introduce more challenging data, thereby enhancing model\ncapability; (3) Diversity Promotion, incorporating Determinantal Point Process\n(DPP) to ensure data diversity across epochs, enriching the learning process.\nWe validate P3 on the reasoning scenarios, APPS and MATH, demonstrating\nsignificant improvements over traditional data pruning methods. By advancing\ndynamic data selection and utilization strategies, P3 contributes both a\ntheoretical framework and concrete approach to fully exploit existing data for\nLLMs' performance improvement, offering utility across diverse tasks.\n","authors":["Yingxuan Yang","Huayi Wang","Muning Wen","Xiaoyun Mo","Qiuying Peng","Jun Wang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v4","updated":"2024-10-18T04:39:35Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\n\\textit{LatentExplainer}, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\n\\textit{LatentExplainer} tackles three main challenges: inferring the meaning\nof latent variables, aligning explanations with inductive biases, and handling\nvarying degrees of explainability. Our approach perturbs latent variables,\ninterpreting changes in generated data, and uses multi-modal large language\nmodels (MLLMs) to produce human-understandable explanations. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations for\nlatent variables. The results highlight the effectiveness of incorporating\ninductive biases and uncertainty quantification, significantly enhancing model\ninterpretability.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15820v2","updated":"2024-10-18T04:38:47Z","published":"2024-09-24T07:34:50Z","title":"Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating\n  Attention Head Activation Patterns","summary":"  LLMs' performance on complex tasks is still unsatisfactory. A key issue is\nthat presently LLMs learn in a data-driven schema, while the instructions about\nthese complex tasks are both scarce and hard to collect or construct. On the\ncontrary, a prominent phenomenon is that LLMs can learn rather fast on simpler\ntasks with adequate prior knowledge captured during pretraining stage. Thus, if\nthe prerequisite and mechanism of such rapid generalization could be\nelucidated, it could enhance the efficiency and effectiveness of the LLM's\nability to learn complex tasks. Thus, in this paper, we employ a gradient-based\nmethod, to dissect the process that the SFT process adapts LLMs to downstream\ntasks via the perspective of attention patterns. We find that: (1) LLMs\nselectively activate task-specific attention heads during SFT; (2) activation\npatterns for complex tasks are combinations of basic task patterns; and (3)\nchanges in a few parameters can significantly impact activation patterns after\nSFT on a small number of samples.Based on these insights, experiments are\nconducted to actually enhance the efficiency and effectiveness of SFT.\n","authors":["Yang Zhao","Li Du","Xiao Ding","Kai Xiong","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2409.15820v2.pdf","comment":"in review"},{"id":"http://arxiv.org/abs/2407.00902v2","updated":"2024-10-18T04:37:33Z","published":"2024-07-01T01:57:21Z","title":"From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning","summary":"  Motivated by in-context learning (ICL) capabilities of Large Language models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.\n","authors":["Nan Xu","Fei Wang","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.00902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15349v2","updated":"2024-10-18T04:32:49Z","published":"2024-05-24T08:42:40Z","title":"Everything is Editable: Extend Knowledge Editing to Unstructured Data in\n  Large Language Models","summary":"  Recent knowledge editing methods have primarily focused on modifying\nstructured knowledge in large language models. However, this task setting\noverlooks the fact that a significant portion of real-world knowledge is stored\nin an unstructured format, characterized by long-form content, noise, and a\ncomplex yet comprehensive nature. Techniques like local layer key-value storage\nand term-driven optimization, as used in previous methods like MEMIT, are not\neffective for handling unstructured knowledge. To address these challenges, we\npropose a novel Unstructured Knowledge Editing method, namely UnKE, which\nextends previous assumptions in the layer dimension and token dimension.\nFirstly, in the layer dimension, we propose non-local block key-value storage\nto replace local layer key-value storage, increasing the representation ability\nof key-value pairs and incorporating attention layer knowledge. Secondly, in\nthe token dimension, we replace term-driven optimization with cause-driven\noptimization, which edits the last token directly while preserving context,\navoiding the need to locate terms and preventing the loss of context\ninformation. Results on newly proposed unstructured knowledge editing dataset\n(UnKEBench) and traditional structured datasets demonstrate that UnKE achieves\nremarkable performance, surpassing strong baselines. In addition, UnKE has\nrobust batch editing and sequential editing capabilities.\n","authors":["Jingcheng Deng","Zihao Wei","Liang Pang","Hanxing Ding","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.15349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14166v1","updated":"2024-10-18T04:17:16Z","published":"2024-10-18T04:17:16Z","title":"LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with\n  Simple Word-based Counting Problems","summary":"  Interestingly, LLMs yet struggle with some basic tasks that humans find\ntrivial to handle, e.g., counting the number of character r's in the word\n\"strawberry\". There are several popular conjectures (e.g., tokenization,\narchitecture and training data) regarding the reason for deficiency of LLMs in\nsimple word-based counting problems, sharing the similar belief that such\nfailure stems from model pretraining hence probably inevitable during\ndeployment. In this paper, we carefully design multiple evaluation settings to\ninvestigate validity of prevalent conjectures. Meanwhile, we measure\ntransferability of advanced mathematical and coding reasoning capabilities from\nspecialized LLMs to simple counting tasks. Although specialized LLMs suffer\nfrom counting problems as well, we find conjectures about inherent deficiency\nof LLMs invalid and further seek opportunities to elicit knowledge and\ncapabilities from LLMs that are beneficial to counting tasks. Compared with\nstrategies such as finetuning and in-context learning that are commonly adopted\nto enhance performance on new or challenging tasks, we show that engaging\nreasoning is the most robust and efficient way to help LLMs better perceive\ntasks with more accurate responses.\n  We hope our conjecture validation design could provide insights into the\nstudy of future critical failure modes of LLMs. Based on challenges in\ntransferring advanced capabilities to much simpler tasks, we call for more\nattention to model capability acquisition and evaluation. We also highlight the\nimportance of cultivating consciousness of \"reasoning before responding\" during\nmodel pretraining.\n","authors":["Nan Xu","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2410.14166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14165v1","updated":"2024-10-18T04:13:51Z","published":"2024-10-18T04:13:51Z","title":"Automated Genre-Aware Article Scoring and Feedback Using Large Language\n  Models","summary":"  This paper focuses on the development of an advanced intelligent article\nscoring system that not only assesses the overall quality of written work but\nalso offers detailed feature-based scoring tailored to various article genres.\nBy integrating the pre-trained BERT model with the large language model\nChat-GPT, the system gains a deep understanding of both the content and\nstructure of the text, enabling it to provide a thorough evaluation along with\ntargeted suggestions for improvement. Experimental results demonstrate that\nthis system outperforms traditional scoring methods across multiple public\ndatasets, particularly in feature-based assessments, offering a more accurate\nreflection of the quality of different article types. Moreover, the system\ngenerates personalized feedback to assist users in enhancing their writing\nskills, underscoring the potential and practical value of automated scoring\ntechnologies in educational contexts.\n","authors":["Chihang Wang","Yuxin Dong","Zhenhong Zhang","Ruotong Wang","Shuo Wang","Jiajing Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13170v2","updated":"2024-10-18T04:13:05Z","published":"2024-06-19T02:53:39Z","title":"Amphista: Bi-directional Multi-head Decoding for Accelerating LLM\n  Inference","summary":"  Large Language Models (LLMs) inherently use autoregressive decoding, which\nlacks parallelism in inference and results in significantly slow inference\nspeed. While methods such as Medusa constructs parallelized heads, they lack\nadequate information interaction across different prediction positions. To\novercome this limitation, we introduce Amphista, an enhanced speculative\ndecoding framework that builds upon Medusa. Specifically, Amphista models an\nAuto-embedding Block capable of parallel inference, incorporating\nbi-directional attention to enable interaction between different drafting\nheads. Additionally, Amphista integrates Staged Adaptation Layers, which ensure\na seamless transition of semantic information from the target model's\nautoregressive inference to the drafting heads' non-autoregressive inference,\neffectively achieving paradigm shift and feature fusion. Experimental results\non Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista\nachieves substantial acceleration while maintaining generation quality. On\nMT-Bench, Amphista delivers up to 2.75$\\times$ speedup over vanilla\nautoregressive decoding and 1.40$\\times$ over Medusa on Vicuna 33B in\nwall-clock time.\n","authors":["Zeping Li","Xinlong Yang","Ziheng Gao","Ji Liu","Guanchen Li","Zhuang Liu","Dong Li","Jinzhang Peng","Lu Tian","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2406.13170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16710v4","updated":"2024-10-18T04:02:31Z","published":"2024-04-25T16:20:23Z","title":"LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding","summary":"  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.\n","authors":["Mostafa Elhoushi","Akshat Shrivastava","Diana Liskovich","Basil Hosmer","Bram Wasti","Liangzhen Lai","Anas Mahmoud","Bilge Acun","Saurabh Agarwal","Ahmed Roman","Ahmed A Aly","Beidi Chen","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2404.16710v4.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2410.14157v1","updated":"2024-10-18T03:48:53Z","published":"2024-10-18T03:48:53Z","title":"Beyond Autoregression: Discrete Diffusion for Complex Reasoning and\n  Planning","summary":"  Autoregressive language models, despite their impressive capabilities,\nstruggle with complex reasoning and long-term planning tasks. We introduce\ndiscrete diffusion models as a novel solution to these challenges. Through the\nlens of subgoal imbalance, we demonstrate how diffusion models effectively\nlearn difficult subgoals that elude autoregressive approaches. We propose\nMulti-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on\ndifficulty during learning. On complex tasks like Countdown, Sudoku, and\nBoolean Satisfiability Problems, MDM significantly outperforms autoregressive\nmodels without using search techniques. For instance, MDM achieves 91.5\\% and\n100\\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\\% and\n20.7\\% for autoregressive models. Our work highlights the potential of\ndiffusion-based approaches in advancing AI capabilities for sophisticated\nlanguage understanding and problem-solving tasks.\n","authors":["Jiacheng Ye","Jiahui Gao","Shansan Gong","Lin Zheng","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.14157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14155v1","updated":"2024-10-18T03:45:42Z","published":"2024-10-18T03:45:42Z","title":"Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models","summary":"  Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}\n","authors":["Wei Jie Yeo","Ranjan Satapthy","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2410.14155v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14152v1","updated":"2024-10-18T03:43:42Z","published":"2024-10-18T03:43:42Z","title":"SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy\n  with LLM-based Agent","summary":"  Public scarce resource allocation plays a crucial role in economics as it\ndirectly influences the efficiency and equity in society. Traditional studies\nincluding theoretical model-based, empirical study-based and simulation-based\nmethods encounter limitations due to the idealized assumption of complete\ninformation and individual rationality, as well as constraints posed by limited\navailable data. In this work, we propose an innovative framework, SRAP-Agent\n(Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based\nAgent), which integrates Large Language Models (LLMs) into economic\nsimulations, aiming to bridge the gap between theoretical models and real-world\ndynamics. Using public housing allocation scenarios as a case study, we conduct\nextensive policy simulation experiments to verify the feasibility and\neffectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm\nwith certain optimization objectives. The source code can be found in\nhttps://github.com/jijiarui-cather/SRAPAgent_Framework\n","authors":["Jiarui Ji","Yang Li","Hongtao Liu","Zhicheng Du","Zhewei Wei","Weiran Shen","Qi Qi","Yankai Lin"],"pdf_url":"https://arxiv.org/pdf/2410.14152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14150v1","updated":"2024-10-18T03:40:45Z","published":"2024-10-18T03:40:45Z","title":"Utilizing Large Language Models for Event Deconstruction to Enhance\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  With the rapid development of the internet, the richness of User-Generated\nContentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis\n(MABSA) a research hotspot. Existing studies have achieved certain results in\nMABSA, but they have not effectively addressed the analytical challenges in\nscenarios where multiple entities and sentiments coexist. This paper\ninnovatively introduces Large Language Models (LLMs) for event decomposition\nand proposes a reinforcement learning framework for Multimodal Aspect-based\nSentiment Analysis (MABSA-RL) framework. This framework decomposes the original\ntext into a set of events using LLMs, reducing the complexity of analysis,\nintroducing reinforcement learning to optimize model parameters. Experimental\nresults show that MABSA-RL outperforms existing advanced methods on two\nbenchmark datasets. This paper provides a new research perspective and method\nfor multimodal aspect-level sentiment analysis.\n","authors":["Xiaoyong Huang","Heli Sun","Qunshu Gao","Wenjie Huang","Ruichen Cao"],"pdf_url":"https://arxiv.org/pdf/2410.14150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12151v2","updated":"2024-10-18T03:36:03Z","published":"2024-08-22T06:40:32Z","title":"A Tighter Complexity Analysis of SparseGPT","summary":"  In this work, we improved the analysis of the running time of SparseGPT\n[Frantar, Alistarh ICML 2023] from $O(d^{3})$ to $O(d^{\\omega} + d^{2+a+o(1)} +\nd^{1+\\omega(1,1,a)-a})$ for any $a \\in [0, 1]$, where $\\omega$ is the exponent\nof matrix multiplication. In particular, for the current $\\omega \\approx 2.371$\n[Alman, Duan, Williams, Xu, Xu, Zhou 2024], our running time boils down to\n$O(d^{2.53})$. This running time is due to the analysis of the lazy update\nbehavior in iterative maintenance problems such as [Deng, Song, Weinstein 2022;\nBrand, Song, Zhou ICML 2024].\n","authors":["Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2408.12151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14148v1","updated":"2024-10-18T03:34:32Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2402.13606v3","updated":"2024-10-18T03:34:12Z","published":"2024-02-21T08:20:06Z","title":"A Comprehensive Study of Multilingual Confidence Estimation on Large\n  Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2402.13606v3.pdf","comment":"Comments: n pages; Previously this version appeared as\n  arXiv:2410.12478 which was submitted as a new work by accident"},{"id":"http://arxiv.org/abs/2410.14145v1","updated":"2024-10-18T03:33:18Z","published":"2024-10-18T03:33:18Z","title":"CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using\n  Large Language Models","summary":"  Generating emotionally appropriate responses in conversations with large\nlanguage models presents a significant challenge due to the complexities of\nhuman emotions and cognitive processes, which remain largely underexplored in\ntheir critical role in social interactions. In this study, we introduce a\ntwo-stage automatic data generation framework to create CAPE, a Chinese dataset\nnamed Cognitive Appraisal theory-based Emotional corpus. This corpus\nfacilitates the generation of dialogues with contextually appropriate emotional\nresponses by accounting for diverse personal and situational factors. We\npropose two tasks utilizing this dataset: emotion prediction and next utterance\nprediction. Both automated and human evaluations demonstrate that agents\ntrained on our dataset can deliver responses that are more aligned with human\nemotional expressions. Our study shows the potential for advancing emotional\nexpression in conversational agents, paving the way for more nuanced and\nmeaningful human-computer interactions.\n","authors":["June M. Liu","He Cao","Renliang Sun","Rui Wang","Yu Li","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14144v1","updated":"2024-10-18T03:32:00Z","published":"2024-10-18T03:32:00Z","title":"A Lightweight Multi Aspect Controlled Text Generation Solution For Large\n  Language Models","summary":"  Large language models (LLMs) show remarkable abilities with instruction\ntuning. However, they fail to achieve ideal tasks when lacking high-quality\ninstruction tuning data on target tasks. Multi-Aspect Controllable Text\nGeneration (MCTG) is a representative task for this dilemma, where aspect\ndatasets are usually biased and correlated. Existing work exploits additional\nmodel structures and strategies for solutions, limiting adaptability to LLMs.\nTo activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based\non data augmentation. We analyze bias and correlations in traditional datasets,\nand address these concerns with augmented control attributes and sentences.\nAugmented datasets are feasible for instruction tuning. In our experiments,\nLLMs perform better in MCTG after data augmentation, with a 20% accuracy rise\nand less aspect correlations.\n","authors":["Chenyang Zhang","Jiayi Lin","Haibo Tong","Bingxuan Hou","Dongyu Zhang","Jialin Li","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02052v3","updated":"2024-10-18T03:27:37Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14141v1","updated":"2024-10-18T03:26:06Z","published":"2024-10-18T03:26:06Z","title":"Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents","summary":"  When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from automated evaluation, showing that our proposed system is more\npersuasive and competent in a real-world embodied agent setting.\n","authors":["Sabit Hassan","Hye-Young Chung","Xiang Zhi Tan","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.14141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12478v2","updated":"2024-10-18T03:19:51Z","published":"2024-10-16T11:46:55Z","title":"MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.12478v2.pdf","comment":"Comments: This work was intended as a replacement of arXiv:2402.13606\n  and any subsequent updates will appear there"},{"id":"http://arxiv.org/abs/2410.04717v3","updated":"2024-10-18T03:18:50Z","published":"2024-10-07T03:15:11Z","title":"$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization","summary":"  Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\n$\\textbf{only emerges}$ when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$\\textit{$\\textbf{specialist}$}$ and $\\textit{$\\textbf{generalist}$}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.\n","authors":["Dylan Zhang","Justin Wang","Francois Charton"],"pdf_url":"https://arxiv.org/pdf/2410.04717v3.pdf","comment":"Fix formatting issues"},{"id":"http://arxiv.org/abs/2410.13804v2","updated":"2024-10-18T03:15:21Z","published":"2024-10-17T17:41:15Z","title":"BenTo: Benchmark Task Reduction with In-Context Transferability","summary":"  Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.\n","authors":["Hongyu Zhao","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13804v2.pdf","comment":"https://github.com/tianyi-lab/bento"},{"id":"http://arxiv.org/abs/2409.03258v2","updated":"2024-10-18T03:11:28Z","published":"2024-09-05T05:34:16Z","title":"GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding","summary":"  Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes.\n","authors":["Yukun Cao","Shuo Han","Zengyi Gao","Zezhong Ding","Xike Xie","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.03258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13960v3","updated":"2024-10-18T03:10:13Z","published":"2024-06-20T03:02:38Z","title":"AutoPal: Autonomous Adaptation to Users for Personal AI Companionship","summary":"  Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.\n","authors":["Yi Cheng","Wenge Liu","Kaishuai Xu","Wenjun Hou","Yi Ouyang","Chak Tou Leong","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.13960v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07825v3","updated":"2024-10-18T03:06:39Z","published":"2024-03-12T17:04:28Z","title":"Efficiently Quantifying and Mitigating Ripple Effects in Model Editing","summary":"  Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy. However, editing these models, crucial for rectifying\noutdated or erroneous information, often leads to a complex issue known as the\nripple effect in the hidden space. While difficult to detect, this effect can\nsignificantly impede the efficacy of model editing tasks and deteriorate model\nperformance. This paper addresses this scientific challenge by proposing a\nnovel evaluation methodology, Graphical Impact Evaluation(GIE), which\nquantitatively evaluates the adaptations of the model and the subsequent impact\nof editing. Furthermore, we introduce the Selective Impact Revision(SIR), a\nmodel editing method designed to mitigate this ripple effect. Our comprehensive\nevaluations reveal that the ripple effect in the hidden space is a significant\nissue in all current model editing methods. However, our proposed methods, GIE\nand SIR, effectively identify and alleviate this issue, contributing to the\nadvancement of LLM editing techniques.\n","authors":["Jianchen Wang","Zhouhong Gu","Xiaoxuan Zhu","Lin Zhang","Haoning Ye","Zhuozhi Xiong","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.07825v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13408v2","updated":"2024-10-18T03:05:01Z","published":"2024-10-17T10:14:52Z","title":"MoR: Mixture of Ranks for Low-Rank Adaptation Tuning","summary":"  Low-Rank Adaptation (LoRA) drives research to align its performance with full\nfine-tuning. However, significant challenges remain: (1) Simply increasing the\nrank size of LoRA does not effectively capture high-rank information, which\nleads to a performance bottleneck.(2) MoE-style LoRA methods substantially\nincrease parameters and inference latency, contradicting the goals of efficient\nfine-tuning and ease of application. To address these challenges, we introduce\nMixture of Ranks (MoR), which learns rank-specific information for different\ntasks based on input and efficiently integrates multi-rank information. We\nfirstly propose a new framework that equates the integration of multiple LoRAs\nto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA\nalready captures sufficient intrinsic information, and MoR can derive high-rank\ninformation through mathematical transformations of the low-rank components.\nThus, MoR can reduces the learning difficulty of LoRA and enhances its\nmulti-task capabilities. MoR achieves impressive results, with MoR delivering a\n1.31\\% performance improvement while using only 93.93\\% of the parameters\ncompared to baseline methods.\n","authors":["Chuanyu Tang","Yilong Chen","Zhenyu Zhang","Junyuan Shang","Wenyuan Zhang","Yong Huang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13408v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12841v2","updated":"2024-10-18T03:03:01Z","published":"2024-10-09T17:33:15Z","title":"UniAutoML: A Human-Centered Framework for Unified Discriminative and\n  Generative AutoML with Large Language Models","summary":"  Automated Machine Learning (AutoML) has simplified complex ML processes such\nas data pre-processing, model selection, and hyper-parameter searching.\nHowever, traditional AutoML frameworks focus solely on discriminative tasks,\noften falling short in tackling AutoML for generative models. Additionally,\nthese frameworks lack interpretability and user engagement during the training\nprocess, primarily due to the absence of human-centered design. It leads to a\nlack of transparency in final decision-making and limited user control,\npotentially reducing trust and adoption of AutoML methods. To address these\nlimitations, we introduce UniAutoML, a human-centered AutoML framework that\nleverages Large Language Models (LLMs) to unify AutoML for both discriminative\n(e.g., Transformers and CNNs for classification or regression tasks) and\ngenerative tasks (e.g., fine-tuning diffusion models or LLMs). The\nhuman-centered design of UniAutoML innovatively features a conversational user\ninterface (CUI) that facilitates natural language interactions, providing users\nwith real-time guidance, feedback, and progress updates for better\ninterpretability. This design enhances transparency and user control throughout\nthe AutoML training process, allowing users to seamlessly break down or modify\nthe model being trained. To mitigate potential risks associated with LLM\ngenerated content, UniAutoML incorporates a safety guardline that filters\ninputs and censors outputs. We evaluated UniAutoML's performance and usability\nthrough experiments on eight diverse datasets and user studies involving 25\nparticipants, demonstrating that UniAutoML not only enhances performance but\nalso improves user control and trust. Our human-centered design bridges the gap\nbetween AutoML capabilities and user understanding, making ML more accessible\nto a broader audience.\n","authors":["Jiayi Guo","Zan Chen","Yingrui Ji","Liyun Zhang","Daqin Luo","Zhigang Li","Yiqin Shen"],"pdf_url":"https://arxiv.org/pdf/2410.12841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14132v1","updated":"2024-10-18T03:00:03Z","published":"2024-10-18T03:00:03Z","title":"ViConsFormer: Constituting Meaningful Phrases of Scene Texts using\n  Transformer-based Method in Vietnamese Text-based Visual Question Answering","summary":"  Text-based VQA is a challenging task that requires machines to use scene\ntexts in given images to yield the most appropriate answer for the given\nquestion. The main challenge of text-based VQA is exploiting the meaning and\ninformation from scene texts. Recent studies tackled this challenge by\nconsidering the spatial information of scene texts in images via embedding 2D\ncoordinates of their bounding boxes. In this study, we follow the definition of\nmeaning from linguistics to introduce a novel method that effectively exploits\nthe information from scene texts written in Vietnamese. Experimental results\nshow that our proposed method obtains state-of-the-art results on two\nlarge-scale Vietnamese Text-based VQA datasets. The implementation can be found\nat this link.\n","authors":["Nghia Hieu Nguyen","Tho Thanh Quan","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12847v2","updated":"2024-10-18T02:56:32Z","published":"2024-10-10T07:48:53Z","title":"ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning","summary":"  Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method\nattributed to its remarkable performance with few updated parameters on various\nlarge-scale pretrained Language Models (PLMs). Traditionally, each prompt has\nbeen considered indivisible and updated independently, leading the parameters\nincrease proportionally as prompt length grows. To address this issue, we\npropose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT).\nIn our method, we refer to the concept of product quantization (PQ), allowing\nall soft prompts to share a set of learnable codebook vectors in each subspace,\nwith each prompt differentiated by a set of adaptive weights. We achieve the\nsuperior performance on 17 diverse natural language tasks including natural\nlanguage understanding (NLU) and question answering (QA) tasks by tuning only\n0.3% of parameters of the PLMs. Our approach also excels in few-shot and large\nmodel settings, highlighting its significant potential.\n","authors":["Yu-Chen Lin","Wei-Hua Li","Jun-Cheng Chen","Chu-Song Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12847v2.pdf","comment":"EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2406.05213v2","updated":"2024-10-18T02:55:27Z","published":"2024-06-07T18:54:40Z","title":"On Subjective Uncertainty Quantification and Calibration in Natural\n  Language Generation","summary":"  Applications of large language models often involve the generation of\nfree-form responses, in which case uncertainty quantification becomes\nchallenging. This is due to the need to identify task-specific uncertainties\n(e.g., about the semantics) which appears difficult to define in general cases.\nThis work addresses these challenges from a perspective of Bayesian decision\ntheory, starting from the assumption that our utility is characterized by a\nsimilarity measure that compares a generated response with a hypothetical true\nresponse. We discuss how this assumption enables principled quantification of\nthe model's subjective uncertainty and its calibration. We further derive a\nmeasure for epistemic uncertainty, based on a missing data perspective and its\ncharacterization as an excess risk. The proposed methods can be applied to\nblack-box language models. We illustrate the methods on question answering and\nmachine translation tasks. Our experiments provide a principled evaluation of\ntask-specific calibration, and demonstrate that epistemic uncertainty offers a\npromising deferral strategy for efficient data acquisition in in-context\nlearning.\n","authors":["Ziyu Wang","Chris Holmes"],"pdf_url":"https://arxiv.org/pdf/2406.05213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13166v2","updated":"2024-10-18T02:53:14Z","published":"2024-10-17T02:47:10Z","title":"An Evolved Universal Transformer Memory","summary":"  Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.\n","authors":["Edoardo Cetin","Qi Sun","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.13166v2.pdf","comment":"29 pages, 14 figures. Preprint, under submission. Source code is\n  available at https://github.com/SakanaAI/evo-memory"}]},"2024-10-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.16267v1","updated":"2024-10-21T17:59:11Z","published":"2024-10-21T17:59:11Z","title":"xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video\n  Even in VLMs","summary":"  We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html\n","authors":["Michael S. Ryoo","Honglu Zhou","Shrikant Kendre","Can Qin","Le Xue","Manli Shu","Silvio Savarese","Ran Xu","Caiming Xiong","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2410.16267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16256v1","updated":"2024-10-21T17:56:51Z","published":"2024-10-21T17:56:51Z","title":"CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and\n  Evolution","summary":"  Efficient and accurate evaluation is crucial for the continuous improvement\nof large language models (LLMs). Among various assessment methods, subjective\nevaluation has garnered significant attention due to its superior alignment\nwith real-world usage scenarios and human preferences. However, human-based\nevaluations are costly and lack reproducibility, making precise automated\nevaluators (judgers) vital in this process. In this report, we introduce\n\\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM.\nCompassJudger-1 is a general-purpose LLM that demonstrates remarkable\nversatility. It is capable of: 1. Performing unitary scoring and two-model\ncomparisons as a reward model; 2. Conducting evaluations according to specified\nformats; 3. Generating critiques; 4. Executing diverse tasks like a general\nLLM. To assess the evaluation capabilities of different judge models under a\nunified setting, we have also established \\textbf{JudgerBench}, a new benchmark\nthat encompasses various subjective evaluation tasks and covers a wide range of\ntopics. CompassJudger-1 offers a comprehensive solution for various evaluation\ntasks while maintaining the flexibility to adapt to diverse requirements. Both\nCompassJudger and JudgerBench are released and available to the research\ncommunity athttps://github.com/open-compass/CompassJudger. We believe that by\nopen-sourcing these tools, we can foster collaboration and accelerate progress\nin LLM evaluation methodologies.\n","authors":["Maosong Cao","Alexander Lam","Haodong Duan","Hongwei Liu","Songyang Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16256v1.pdf","comment":"Technical Report, Code and Models:\n  https://github.com/open-compass/CompassJudger"},{"id":"http://arxiv.org/abs/2410.16251v1","updated":"2024-10-21T17:55:54Z","published":"2024-10-21T17:55:54Z","title":"Can Knowledge Editing Really Correct Hallucinations?","summary":"  Large Language Models (LLMs) suffer from hallucinations, referring to the\nnon-factual information in generated content, despite their superior capacities\nacross tasks. Meanwhile, knowledge editing has been developed as a new popular\nparadigm to correct the erroneous factual knowledge encoded in LLMs with the\nadvantage of avoiding retraining from scratch. However, one common issue of\nexisting evaluation datasets for knowledge editing is that they do not ensure\nLLMs actually generate hallucinated answers to the evaluation questions before\nediting. When LLMs are evaluated on such datasets after being edited by\ndifferent techniques, it is hard to directly adopt the performance to assess\nthe effectiveness of different knowledge editing methods in correcting\nhallucinations. Thus, the fundamental question remains insufficiently\nvalidated: Can knowledge editing really correct hallucinations in LLMs? We\nproposed HalluEditBench to holistically benchmark knowledge editing methods in\ncorrecting real-world hallucinations. First, we rigorously construct a massive\nhallucination dataset with 9 domains, 26 topics and more than 6,000\nhallucinations. Then, we assess the performance of knowledge editing methods in\na holistic way on five dimensions including Efficacy, Generalization,\nPortability, Locality, and Robustness. Through HalluEditBench, we have provided\nnew insights into the potentials and limitations of different knowledge editing\nmethods in correcting hallucinations, which could inspire future improvements\nand facilitate the progress in the field of knowledge editing.\n","authors":["Baixiang Huang","Canyu Chen","Xiongxiao Xu","Ali Payani","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2410.16251v1.pdf","comment":"The first two authors contributed equally to this work. The main\n  paper is 10 pages long, with 35 pages total. The code, results, dataset, and\n  additional resources are available on the project website:\n  https://llm-editing.github.io/"},{"id":"http://arxiv.org/abs/2410.16246v1","updated":"2024-10-21T17:51:41Z","published":"2024-10-21T17:51:41Z","title":"Analyzing Context Contributions in LLM-based Machine Translation","summary":"  Large language models (LLMs) have achieved state-of-the-art performance in\nmachine translation (MT) and demonstrated the ability to leverage in-context\nlearning through few-shot examples. However, the mechanisms by which LLMs use\ndifferent parts of the input context remain largely unexplored. In this work,\nwe provide a comprehensive analysis of context utilization in MT, studying how\nLLMs use various context parts, such as few-shot examples and the source text,\nwhen generating translations. We highlight several key findings: (1) the source\npart of few-shot examples appears to contribute more than its corresponding\ntargets, irrespective of translation direction; (2) finetuning LLMs with\nparallel data alters the contribution patterns of different context parts; and\n(3) there is a positional bias where earlier few-shot examples have higher\ncontributions to the translated sequence. Finally, we demonstrate that\ninspecting anomalous context contributions can potentially uncover pathological\ntranslations, such as hallucinations. Our findings shed light on the internal\nworkings of LLM-based MT which go beyond those known for standard\nencoder-decoder MT models.\n","authors":["Emmanouil Zaranis","Nuno M. Guerreiro","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2410.16246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16235v1","updated":"2024-10-21T17:41:11Z","published":"2024-10-21T17:41:11Z","title":"ToW: Thoughts of Words Improve Reasoning in Large Language Models","summary":"  We introduce thoughts of words (ToW), a novel training-time data-augmentation\nmethod for next-word prediction. ToW views next-word prediction as a core\nreasoning task and injects fine-grained thoughts explaining what the next word\nshould be and how it is related to the previous contexts in pre-training texts.\nOur formulation addresses two fundamental drawbacks of existing next-word\nprediction learning schemes: they induce factual hallucination and are\ninefficient for models to learn the implicit reasoning processes in raw texts.\nWhile there are many ways to acquire such thoughts of words, we explore the\nfirst step of acquiring ToW annotations through distilling from larger models.\nAfter continual pre-training with only 70K ToW annotations, we effectively\nimprove models' reasoning performances by 7% to 9% on average and reduce model\nhallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks\nand applications, introducing no additional biases on labels or semantics.\n","authors":["Zhikun Xu","Ming Shen","Jacob Dineen","Zhaonan Li","Xiao Ye","Shijie Lu","Aswin RRV","Chitta Baral","Ben Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.16235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16232v1","updated":"2024-10-21T17:39:49Z","published":"2024-10-21T17:39:49Z","title":"Sketch2Code: Evaluating Vision-Language Models for Interactive Web\n  Design Prototyping","summary":"  Sketches are a natural and accessible medium for UI designers to\nconceptualize early-stage ideas. However, existing research on UI/UX automation\noften requires high-fidelity inputs like Figma designs or detailed screenshots,\nlimiting accessibility and impeding efficient design iteration. To bridge this\ngap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art\nVision Language Models (VLMs) on automating the conversion of rudimentary\nsketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code\nsupports interactive agent evaluation that mimics real-world design workflows,\nwhere a VLM-based agent iteratively refines its generations by communicating\nwith a simulated user, either passively receiving feedback instructions or\nproactively asking clarification questions. We comprehensively analyze ten\ncommercial and open-source models, showing that Sketch2Code is challenging for\nexisting VLMs; even the most capable models struggle to accurately interpret\nsketches and formulate effective questions that lead to steady improvement.\nNevertheless, a user study with UI/UX experts reveals a significant preference\nfor proactive question-asking over passive feedback reception, highlighting the\nneed to develop more effective paradigms for multi-turn conversational agents.\n","authors":["Ryan Li","Yanzhe Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16232v1.pdf","comment":"preprint, 9 pages"},{"id":"http://arxiv.org/abs/2407.02273v3","updated":"2024-10-21T17:37:26Z","published":"2024-07-02T14:02:53Z","title":"Language Model Alignment in Multilingual Trolley Problems","summary":"  We evaluate the moral alignment of large language models (LLMs) with human\npreferences in multilingual trolley problems. Building on the Moral Machine\nexperiment, which captures over 40 million human judgments across 200+\ncountries, we develop a cross-lingual corpus of moral dilemma vignettes in over\n100 languages called MultiTP. This dataset enables the assessment of LLMs'\ndecision-making processes in diverse linguistic contexts. Our analysis explores\nthe alignment of 19 different LLMs with human judgments, capturing preferences\nacross six moral dimensions: species, gender, fitness, status, age, and the\nnumber of lives involved. By correlating these preferences with the demographic\ndistribution of language speakers and examining the consistency of LLM\nresponses to various prompt paraphrasings, our findings provide insights into\ncross-lingual and ethical biases of LLMs and their intersection. We discover\nsignificant variance in alignment across languages, challenging the assumption\nof uniform moral reasoning in AI systems and highlighting the importance of\nincorporating diverse perspectives in AI ethics. The results underscore the\nneed for further research on the integration of multilingual dimensions in\nresponsible AI research to ensure fair and equitable AI interactions worldwide.\nOur code and data are at https://github.com/causalNLP/moralmachine\n","authors":["Zhijing Jin","Max Kleiman-Weiner","Giorgio Piatti","Sydney Levine","Jiarui Liu","Fernando Gonzalez","Francesco Ortu","András Strausz","Mrinmaya Sachan","Rada Mihalcea","Yejin Choi","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2407.02273v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16229v1","updated":"2024-10-21T17:34:39Z","published":"2024-10-21T17:34:39Z","title":"Building A Coding Assistant via the Retrieval-Augmented Language Model","summary":"  Pretrained language models have shown strong effectiveness in code-related\ntasks, such as code retrieval, code generation, code summarization, and code\ncompletion tasks. In this paper, we propose COde assistaNt viA\nretrieval-augmeNted language model (CONAN), which aims to build a code\nassistant by mimicking the knowledge-seeking behaviors of humans during coding.\nSpecifically, it consists of a code structure aware retriever (CONAN-R) and a\ndual-view code representation-based retrieval-augmented generation model\n(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and\nMasked Entity Prediction tasks to make language models code structure-aware and\nlearn effective representations for code snippets and documentation. Then\nCONAN-G designs a dual-view code representation mechanism for implementing a\nretrieval-augmented code generation model. CONAN-G regards the code\ndocumentation descriptions as prompts, which help language models better\nunderstand the code semantics. Our experiments show that CONAN achieves\nconvincing performance on different code generation tasks and significantly\noutperforms previous retrieval augmented code generation models. Our further\nanalyses show that CONAN learns tailored representations for both code snippets\nand documentation by aligning code-documentation data pairs and capturing\nstructural semantics by masking and predicting entities in the code data.\nAdditionally, the retrieved code snippets and documentation provide necessary\ninformation from both program language and natural language to assist the code\ngeneration process. CONAN can also be used as an assistant for Large Language\nModels (LLMs), providing LLMs with external knowledge in shorter code document\nlengths to improve their effectiveness on various code tasks. It shows the\nability of CONAN to extract necessary information and help filter out the noise\nfrom retrieved code documents.\n","authors":["Xinze Li","Hanbin Wang","Zhenghao Liu","Shi Yu","Shuo Wang","Shuo Wang","Yukun Yan","Yukai Fu","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2410.16229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16221v1","updated":"2024-10-21T17:25:32Z","published":"2024-10-21T17:25:32Z","title":"On Creating an English-Thai Code-switched Machine Translation in Medical\n  Domain","summary":"  Machine translation (MT) in the medical domain plays a pivotal role in\nenhancing healthcare quality and disseminating medical knowledge. Despite\nadvancements in English-Thai MT technology, common MT approaches often\nunderperform in the medical field due to their inability to precisely translate\nmedical terminologies. Our research prioritizes not merely improving\ntranslation accuracy but also maintaining medical terminology in English within\nthe translated text through code-switched (CS) translation. We developed a\nmethod to produce CS medical translation data, fine-tuned a CS translation\nmodel with this data, and evaluated its performance against strong baselines,\nsuch as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4. Our model\ndemonstrated competitive performance in automatic metrics and was highly\nfavored in human preference evaluations. Our evaluation result also shows that\nmedical professionals significantly prefer CS translations that maintain\ncritical English terms accurately, even if it slightly compromises fluency. Our\ncode and test set are publicly available\nhttps://github.com/preceptorai-org/NLLB_CS_EM_NLP2024.\n","authors":["Parinthapat Pengpun","Krittamate Tiankanon","Amrest Chinkamol","Jiramet Kinchagawat","Pitchaya Chairuengjitjaras","Pasit Supholkhan","Pubordee Aussavavirojekul","Chiraphat Boonnag","Kanyakorn Veerakanjana","Hirunkul Phimsiri","Boonthicha Sae-jia","Nattawach Sataudom","Piyalitt Ittichaiwong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2410.16221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16215v1","updated":"2024-10-21T17:16:13Z","published":"2024-10-21T17:16:13Z","title":"Pre-training Distillation for Large Language Models: A Design Space\n  Exploration","summary":"  Knowledge distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. Previous work applying KD in the field of\nlarge language models (LLMs) typically focused on the post-training phase,\nwhere the student LLM learns directly from instructions and corresponding\nresponses generated by the teacher model. In this paper, we extend KD to the\npre-training phase of LLMs, named pre-training distillation (PD). We first\nconduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a\n1.9B parameter student LLM, validating the effectiveness of PD. Considering the\nkey impact factors of distillation, we systematically explore the design space\nof pre-training distillation across four aspects: logits processing, loss\nselection, scaling law, and offline or online logits. We conduct extensive\nexperiments to explore the design space of pre-training distillation and find\nbetter configurations and interesting conclusions, such as larger student LLMs\ngenerally benefiting more from pre-training distillation, while a larger\nteacher LLM does not necessarily guarantee better results. We hope our\nexploration of the design space will inform future practices in pre-training\ndistillation.\n","authors":["Hao Peng","Xin Lv","Yushi Bai","Zijun Yao","Jiajie Zhang","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2410.16215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16208v1","updated":"2024-10-21T17:11:21Z","published":"2024-10-21T17:11:21Z","title":"Compute-Constrained Data Selection","summary":"  Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. These experiments show the\nvalidity of this model in real-world experiments. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective.\n","authors":["Junjie Oscar Yin","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.16208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16207v1","updated":"2024-10-21T17:10:43Z","published":"2024-10-21T17:10:43Z","title":"CoT-TL: Low-Resource Temporal Knowledge Representation of Planning\n  Instructions Using Chain-of-Thought Reasoning","summary":"  Autonomous agents often face the challenge of interpreting uncertain natural\nlanguage instructions for planning tasks. Representing these instructions as\nLinear Temporal Logic (LTL) enables planners to synthesize actionable plans. We\nintroduce CoT-TL, a data-efficient in-context learning framework for\ntranslating natural language specifications into LTL representations. CoT-TL\naddresses the limitations of large language models, which typically rely on\nextensive fine-tuning data, by extending chain-of-thought reasoning and\nsemantic roles to align with the requirements of formal logic creation. This\napproach enhances the transparency and rationale behind LTL generation,\nfostering user trust. CoT-TL achieves state-of-the-art accuracy across three\ndiverse datasets in low-data scenarios, outperforming existing methods without\nfine-tuning or intermediate translations. To improve reliability and minimize\nhallucinations, we incorporate model checking to validate the syntax of the\ngenerated LTL output. We further demonstrate CoT-TL's effectiveness through\nablation studies and evaluations on unseen LTL structures and formulas in a new\ndataset. Finally, we validate CoT-TL's practicality by integrating it into a\nQuadCopter for multi-step drone planning based on natural language\ninstructions.\n","authors":["Kumar Manas","Stefan Zwicklbauer","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2410.16207v1.pdf","comment":"Accepted for publication in Proceedings of the 2024 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2024), Abu\n  Dhabi 14-18 October 2024"},{"id":"http://arxiv.org/abs/2410.16204v1","updated":"2024-10-21T17:05:50Z","published":"2024-10-21T17:05:50Z","title":"Systematic Review: Text Processing Algorithms in Machine Learning and\n  Deep Learning for Mental Health Detection on Social Media","summary":"  The global rise in depression necessitates innovative detection methods for\nearly intervention. Social media provides a unique opportunity to identify\ndepression through user-generated posts. This systematic review evaluates\nmachine learning (ML) models for depression detection on social media, focusing\non biases and methodological challenges throughout the ML lifecycle. A search\nof PubMed, IEEE Xplore, and Google Scholar identified 47 relevant studies\npublished after 2010. The Prediction model Risk Of Bias ASsessment Tool\n(PROBAST) was utilized to assess methodological quality and risk of bias.\nSignificant biases impacting model reliability and generalizability were found.\nThere is a predominant reliance on Twitter (63.8%) and English-language content\n(over 90%), with most studies focusing on users from the United States and\nEurope. Non-probability sampling methods (approximately 80%) limit\nrepresentativeness. Only 23% of studies explicitly addressed linguistic nuances\nlike negations, crucial for accurate sentiment analysis. Inconsistent\nhyperparameter tuning was observed, with only 27.7% properly tuning models.\nAbout 17% did not adequately partition data into training, validation, and test\nsets, risking overfitting. While 74.5% used appropriate evaluation metrics for\nimbalanced data, others relied on accuracy without addressing class imbalance,\npotentially skewing results. Reporting transparency varied, often lacking\ncritical methodological details. These findings highlight the need to diversify\ndata sources, standardize preprocessing protocols, ensure consistent model\ndevelopment practices, address class imbalance, and enhance reporting\ntransparency. By overcoming these challenges, future research can develop more\nrobust and generalizable ML models for depression detection on social media,\ncontributing to improved mental health outcomes globally.\n","authors":["Yuchen Cao","Jianglai Dai","Zhongyan Wang","Yeyubei Zhang","Xiaorui Shen","Yunchong Liu","Yexin Tian"],"pdf_url":"https://arxiv.org/pdf/2410.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19323v2","updated":"2024-10-21T17:05:15Z","published":"2024-05-29T17:54:22Z","title":"Are Large Language Models Chameleons? An Attempt to Simulate Social\n  Surveys","summary":"  Can large language models (LLMs) simulate social surveys? To answer this\nquestion, we conducted millions of simulations in which LLMs were asked to\nanswer subjective questions. A comparison of different LLM responses with the\nEuropean Social Survey (ESS) data suggests that the effect of prompts on bias\nand variability is fundamental, highlighting major cultural, age, and gender\nbiases. We further discussed statistical methods for measuring the difference\nbetween LLM answers and survey data and proposed a novel measure inspired by\nJaccard similarity, as LLM-generated responses are likely to have a smaller\nvariance. Our experiments also reveal that it is important to analyze the\nrobustness and variability of prompts before using LLMs to simulate social\nsurveys, as their imitation abilities are approximate at best.\n","authors":["Mingmeng Geng","Sihong He","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2405.19323v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.16196v1","updated":"2024-10-21T16:59:25Z","published":"2024-10-21T16:59:25Z","title":"Information for Conversation Generation: Proposals Utilising Knowledge\n  Graphs","summary":"  LLMs are frequently used tools for conversational generation. Without\nadditional information LLMs can generate lower quality responses due to lacking\nrelevant content and hallucinations, as well as the perception of poor\nemotional capability, and an inability to maintain a consistent character.\nKnowledge graphs are commonly used forms of external knowledge and may provide\nsolutions to these challenges. This paper introduces three proposals, utilizing\nknowledge graphs to enhance LLM generation. Firstly, dynamic knowledge graph\nembeddings and recommendation could allow for the integration of new\ninformation and the selection of relevant knowledge for response generation.\nSecondly, storing entities with emotional values as additional features may\nprovide knowledge that is better emotionally aligned with the user input.\nThirdly, integrating character information through narrative bubbles would\nmaintain character consistency, as well as introducing a structure that would\nreadily incorporate new information.\n","authors":["Alex Clay","Ernesto Jiménez-Ruiz"],"pdf_url":"https://arxiv.org/pdf/2410.16196v1.pdf","comment":"7 pages with citations, 1 figure, accepted to the ISWC 2024 Special\n  Session"},{"id":"http://arxiv.org/abs/2406.13791v3","updated":"2024-10-21T16:55:31Z","published":"2024-06-19T19:35:14Z","title":"IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards\n  for Better Well-Being","summary":"  Sustainable Development Goals (SDGs) give the UN a road map for development\nwith Agenda 2030 as a target. SDG3 \"Good Health and Well-Being\" ensures healthy\nlives and promotes well-being for all ages. Digital technologies can support\nSDG3. Burnout and even depression could be reduced by encouraging better\npreventive health. Due to the lack of patient knowledge and focus to take care\nof their health, it is necessary to help patients before it is too late. New\ntrends such as positive psychology and mindfulness are highly encouraged in the\nUSA. Digital Twins (DTs) can help with the continuous monitoring of emotion\nusing physiological signals (e.g., collected via wearables). DTs facilitate\nmonitoring and provide constant health insight to improve quality of life and\nwell-being with better personalization. Healthcare DTs challenges are\nstandardizing data formats, communication protocols, and data exchange\nmechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things\n(IoT) and DTs Working Group, with standards such as \"ISO/IEC 21823-3:2021 IoT -\nInteroperability for IoT Systems - Part 3 Semantic interoperability\", \"ISO/IEC\nCD 30178 - IoT - Data format, value and coding\". To achieve those data\nintegration and knowledge challenges, we designed the Mental Health Knowledge\nGraph (ontology and dataset) to boost mental health. As an example, explicit\nknowledge is described such as chocolate contains magnesium which is\nrecommended for depression. The Knowledge Graph (KG) acquires knowledge from\nontology-based mental health projects classified within the LOV4IoT ontology\ncatalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped\nto standards when possible. Standards from ETSI SmartM2M can be used such as\nSAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,\nW3C, NIST, and IEEE standards relevant to mental health can be considered.\n","authors":["Amelie Gyrard","Seyedali Mohammadi","Manas Gaur","Antonio Kung"],"pdf_url":"https://arxiv.org/pdf/2406.13791v3.pdf","comment":"20 pages, Book chapter, Smart Technologies for Achieving Good Health\n  and Well-Being: Towards Sustainable Development Goal, Taylor & Francis"},{"id":"http://arxiv.org/abs/2410.16186v1","updated":"2024-10-21T16:49:35Z","published":"2024-10-21T16:49:35Z","title":"Contamination Report for Multilingual Benchmarks","summary":"  Benchmark contamination refers to the presence of test datasets in Large\nLanguage Model (LLM) pre-training or post-training data. Contamination can lead\nto inflated scores on benchmarks, compromising evaluation results and making it\ndifficult to determine the capabilities of models. In this work, we study the\ncontamination of popular multilingual benchmarks in LLMs that support multiple\nlanguages. We use the Black Box test to determine whether $7$ frequently used\nmultilingual benchmarks are contaminated in $7$ popular open and closed LLMs\nand find that almost all models show signs of being contaminated with almost\nall the benchmarks we test. Our findings can help the community determine the\nbest set of benchmarks to use for multilingual evaluation.\n","authors":["Sanchit Ahuja","Varun Gumma","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.16186v1.pdf","comment":"11 pages, 2 tables"},{"id":"http://arxiv.org/abs/2410.16184v1","updated":"2024-10-21T16:48:26Z","published":"2024-10-21T16:48:26Z","title":"RM-Bench: Benchmarking Reward Models of Language Models with Subtlety\n  and Style","summary":"  Reward models are critical in techniques like Reinforcement Learning from\nHuman Feedback (RLHF) and Inference Scaling Laws, where they guide language\nmodel alignment and select optimal responses. Despite their importance,\nexisting reward model benchmarks often evaluate models by asking them to\ndistinguish between responses generated by models of varying power. However,\nthis approach fails to assess reward models on subtle but critical content\nchanges and variations in style, resulting in a low correlation with policy\nmodel performance. To this end, we introduce RM-Bench, a novel benchmark\ndesigned to evaluate reward models based on their sensitivity to subtle content\ndifferences and resistance to style biases. Extensive experiments demonstrate\nthat RM-Bench strongly correlates with policy model performance, making it a\nreliable reference for selecting reward models to align language models\neffectively. We evaluate nearly 40 reward models on RM-Bench. Our results\nreveal that even state-of-the-art models achieve an average performance of only\n46.6%, which falls short of random-level accuracy (50%) when faced with style\nbias interference. These findings highlight the significant room for\nimprovement in current reward models. Related code and data are available at\nhttps://github.com/THU-KEG/RM-Bench.\n","authors":["Yantao Liu","Zijun Yao","Rui Min","Yixin Cao","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2410.16184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07879v3","updated":"2024-10-21T16:48:18Z","published":"2023-11-14T03:18:28Z","title":"Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting\n  Volunteer Content Moderators","summary":"  Extensive efforts in automated approaches for content moderation have been\nfocused on developing models to identify toxic, offensive, and hateful content\nwith the aim of lightening the load for moderators. Yet, it remains uncertain\nwhether improvements on those tasks have truly addressed moderators' needs in\naccomplishing their work. In this paper, we surface gaps between past research\nefforts that have aimed to provide automation for aspects of content moderation\nand the needs of volunteer content moderators, regarding identifying violations\nof various moderation rules. To do so, we conduct a model review on Hugging\nFace to reveal the availability of models to cover various moderation rules and\nguidelines from three exemplar forums. We further put state-of-the-art LLMs to\nthe test, evaluating how well these models perform in flagging violations of\nplatform rules from one particular forum. Finally, we conduct a user survey\nstudy with volunteer moderators to gain insight into their perspectives on\nuseful moderation models. Overall, we observe a non-trivial gap, as missing\ndeveloped models and LLMs exhibit moderate to low performance on a significant\nportion of the rules. Moderators' reports provide guides for future work on\ndeveloping moderation assistant models.\n","authors":["Yang Trista Cao","Lovely-Frances Domingo","Sarah Ann Gilbert","Michelle Mazurek","Katie Shilton","Hal Daumé III"],"pdf_url":"https://arxiv.org/pdf/2311.07879v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16179v1","updated":"2024-10-21T16:44:51Z","published":"2024-10-21T16:44:51Z","title":"MagicPIG: LSH Sampling for Efficient LLM Generation","summary":"  Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.\n","authors":["Zhuoming Chen","Ranajoy Sadhukhan","Zihao Ye","Yang Zhou","Jianyu Zhang","Niklas Nolte","Yuandong Tian","Matthijs Douze","Leon Bottou","Zhihao Jia","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07428v2","updated":"2024-10-21T16:37:02Z","published":"2024-10-09T20:48:03Z","title":"The First VoicePrivacy Attacker Challenge Evaluation Plan","summary":"  The First VoicePrivacy Attacker Challenge is a new kind of challenge\norganized as part of the VoicePrivacy initiative and supported by ICASSP 2025\nas the SP Grand Challenge It focuses on developing attacker systems against\nvoice anonymization, which will be evaluated against a set of anonymization\nsystems submitted to the VoicePrivacy 2024 Challenge. Training, development,\nand evaluation datasets are provided along with a baseline attacker system.\nParticipants shall develop their attacker systems in the form of automatic\nspeaker verification systems and submit their scores on the development and\nevaluation data to the organizers. To do so, they can use any additional\ntraining data and models, provided that they are openly available and declared\nbefore the specified deadline. The metric for evaluation is equal error rate\n(EER). Results will be presented at the ICASSP 2025 special session to which 5\nselected top-ranked participants will be invited to submit and present their\nchallenge systems.\n","authors":["Natalia Tomashenko","Xiaoxiao Miao","Emmanuel Vincent","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2410.07428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16168v1","updated":"2024-10-21T16:33:16Z","published":"2024-10-21T16:33:16Z","title":"Exploring Pretraining via Active Forgetting for Improving Cross Lingual\n  Transfer for Decoder Language Models","summary":"  Large Language Models (LLMs) demonstrate exceptional capabilities in a\nmultitude of NLP tasks. However, the efficacy of such models to languages other\nthan English is often limited. Prior works have shown that encoder-only models\nsuch as BERT or XLM-RoBERTa show impressive cross lingual transfer of their\ncapabilities from English to other languages. In this work, we propose a\npretraining strategy that uses active forgetting to achieve similar cross\nlingual transfer in decoder-only LLMs. We show that LLMs pretrained with active\nforgetting are highly effective when adapting to new and unseen languages.\nThrough extensive experimentation, we find that LLMs pretrained with active\nforgetting are able to learn better multilingual representations which\ntranslates to better performance in many downstream tasks.\n","authors":["Divyanshu Aggarwal","Ashutosh Sathe","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.16168v1.pdf","comment":"12 pages, 11 tables, 12 figures"},{"id":"http://arxiv.org/abs/2410.16166v1","updated":"2024-10-21T16:32:41Z","published":"2024-10-21T16:32:41Z","title":"Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM\n  Pretraining","summary":"  Multimodal large language models (MLLMs) have made significant strides by\nintegrating visual and textual modalities. A critical factor in training MLLMs\nis the quality of image-text pairs within multimodal pretraining datasets.\nHowever, $\\textit {de facto}$ filter-based data quality enhancement paradigms\noften discard a substantial portion of high-quality image data due to\ninadequate semantic alignment between images and texts, leading to\ninefficiencies in data utilization and scalability. In this paper, we propose\nthe Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically\nassesses and enhances the quality of image-text pairs. AITQE employs a text\nrewriting mechanism for low-quality pairs and incorporates a negative sample\nlearning strategy to improve evaluative capabilities by integrating\ndeliberately selected low-quality samples during training. Unlike prior\napproaches that significantly alter text distributions, our method minimally\nadjusts text to preserve data volume while enhancing quality. Experimental\nresults demonstrate that AITQE surpasses existing methods on various benchmark,\neffectively leveraging raw data and scaling efficiently with increasing data\nvolumes. We hope our work will inspire future works. The code and model are\navailable at: https://github.com/hanhuang22/AITQE.\n","authors":["Han Huang","Yuqi Huo","Zijia Zhao","Haoyu Lu","Shu Wu","Bingning Wang","Qiang Liu","Weipeng Chen","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16165v1","updated":"2024-10-21T16:31:23Z","published":"2024-10-21T16:31:23Z","title":"From Tokens to Materials: Leveraging Language Models for Scientific\n  Discovery","summary":"  Exploring the predictive capabilities of language models in material science\nis an ongoing interest. This study investigates the application of language\nmodel embeddings to enhance material property prediction in materials science.\nBy evaluating various contextual embedding methods and pre-trained models,\nincluding Bidirectional Encoder Representations from Transformers (BERT) and\nGenerative Pre-trained Transformers (GPT), we demonstrate that domain-specific\nmodels, particularly MatBERT significantly outperform general-purpose models in\nextracting implicit knowledge from compound names and material properties. Our\nfindings reveal that information-dense embeddings from the third layer of\nMatBERT, combined with a context-averaging approach, offer the most effective\nmethod for capturing material-property relationships from the scientific\nliterature. We also identify a crucial \"tokenizer effect,\" highlighting the\nimportance of specialized text processing techniques that preserve complete\ncompound names while maintaining consistent token counts. These insights\nunderscore the value of domain-specific training and tokenization in materials\nscience applications and offer a promising pathway for accelerating the\ndiscovery and development of new materials through AI-driven approaches.\n","authors":["Yuwei Wan","Tong Xie","Nan Wu","Wenjie Zhang","Chunyu Kit","Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2410.16165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16162v1","updated":"2024-10-21T16:26:09Z","published":"2024-10-21T16:26:09Z","title":"Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Composite Spatial Reasoning","summary":"  Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, much of\nthe spatial reasoning in these tasks occurs in two-dimensional (2D)\nenvironments, and our evaluation reveals that state-of-the-art VLMs frequently\ngenerate implausible and incorrect responses to composite spatial reasoning\nproblems, including simple pathfinding tasks that humans can solve effortlessly\nat a glance. To address this, we explore an effective approach to enhance 2D\nspatial reasoning within VLMs by training the model on basic spatial\ncapabilities. We begin by disentangling the key components of 2D spatial\nreasoning: direction comprehension, distance estimation, and localization. Our\ncentral hypothesis is that mastering these basic spatial capabilities can\nsignificantly enhance a model's performance on composite spatial tasks\nrequiring advanced spatial understanding and combinatorial problem-solving. To\ninvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunes\nVLMs on these three basic spatial capabilities by synthetic data generation and\ntargeted supervision to form an instruction dataset for each capability. Our\nexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significant\nperformance gains, not only in the basic tasks themselves but also in\ngeneralizing to composite and out-of-distribution spatial reasoning tasks\n(e.g., improving from 13.5% to 40.0% on the shortest path problem). These\nfindings underscore the effectiveness of mastering basic spatial capabilities\nin enhancing composite spatial problem-solving, offering insights for improving\nVLMs' spatial reasoning capabilities.\n","authors":["Yihong Tang","Ao Qu","Zhaokai Wang","Dingyi Zhuang","Zhaofeng Wu","Wei Ma","Shenhao Wang","Yunhan Zheng","Zhan Zhao","Jinhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13116v4","updated":"2024-10-21T16:22:33Z","published":"2024-02-20T16:17:37Z","title":"A Survey on Knowledge Distillation of Large Language Models","summary":"  In the era of Large Language Models (LLMs), Knowledge Distillation (KD)\nemerges as a pivotal methodology for transferring advanced capabilities from\nleading proprietary LLMs, such as GPT-4, to their open-source counterparts like\nLLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a\ncrucial role in both compressing these models, and facilitating their\nself-improvement by employing themselves as teachers. This paper presents a\ncomprehensive survey of KD's role within the realm of LLM, highlighting its\ncritical function in imparting advanced knowledge to smaller models and its\nutility in model compression and self-improvement. Our survey is meticulously\nstructured around three foundational pillars: \\textit{algorithm},\n\\textit{skill}, and \\textit{verticalization} -- providing a comprehensive\nexamination of KD mechanisms, the enhancement of specific cognitive abilities,\nand their practical implications across diverse fields. Crucially, the survey\nnavigates the intricate interplay between data augmentation (DA) and KD,\nillustrating how DA emerges as a powerful paradigm within the KD framework to\nbolster LLMs' performance. By leveraging DA to generate context-rich,\nskill-specific training data, KD transcends traditional boundaries, enabling\nopen-source models to approximate the contextual adeptness, ethical alignment,\nand deep semantic insights characteristic of their proprietary counterparts.\nThis work aims to provide an insightful guide for researchers and\npractitioners, offering a detailed overview of current methodologies in KD and\nproposing future research directions. Importantly, we firmly advocate for\ncompliance with the legal terms that regulate the use of LLMs, ensuring ethical\nand lawful application of KD of LLMs. An associated Github repository is\navailable at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.\n","authors":["Xiaohan Xu","Ming Li","Chongyang Tao","Tao Shen","Reynold Cheng","Jinyang Li","Can Xu","Dacheng Tao","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.13116v4.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2410.16156v1","updated":"2024-10-21T16:21:45Z","published":"2024-10-21T16:21:45Z","title":"Limpeh ga li gong: Challenges in Singlish Annotations","summary":"  Singlish, or Colloquial Singapore English, is a language formed from oral and\nsocial communication within multicultural Singapore. In this work, we work on a\nfundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS)\ntagging of Singlish sentences. For our analysis, we build a parallel Singlish\ndataset containing direct English translations and POS tags, with translation\nand POS annotation done by native Singlish speakers. Our experiments show that\nautomatic transition- and transformer- based taggers perform with only $\\sim\n80\\%$ accuracy when evaluated against human-annotated POS labels, suggesting\nthat there is indeed room for improvement on computation analysis of the\nlanguage. We provide an exposition of challenges in Singlish annotation: its\ninconsistencies in form and semantics, the highly context-dependent particles\nof the language, its structural unique expressions, and the variation of the\nlanguage on different mediums. Our task definition, resultant labels and\nresults reflects the challenges in analysing colloquial languages formulated\nfrom a variety of dialects, and paves the way for future studies beyond POS\ntagging.\n","authors":["Lynnette Hui Xian Ng","Luo Qi Chan"],"pdf_url":"https://arxiv.org/pdf/2410.16156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16155v1","updated":"2024-10-21T16:21:24Z","published":"2024-10-21T16:21:24Z","title":"A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns","summary":"  With the development of large language models, they are widely used as agents\nin various fields. A key component of agents is memory, which stores vital\ninformation but is susceptible to jailbreak attacks. Existing research mainly\nfocuses on single-agent attacks and shared memory attacks. However, real-world\nscenarios often involve independent memory. In this paper, we propose the\nTroublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale,\nmulti-agent, multi-topology text-based attack evaluation framework. TMCHT\ninvolves one attacker agent attempting to mislead an entire society of agents.\nWe identify two major challenges in multi-agent attacks: (1) Non-complete graph\nstructure, (2) Large-scale systems. We attribute these challenges to a\nphenomenon we term toxicity disappearing. To address these issues, we propose\nan Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes\nthe retrieval suffix to make poisoned samples more easily retrieved and\noptimizes the replication suffix to make poisoned samples have contagious\nability. We demonstrate the superiority of our approach in TMCHT, with 23.51%,\n18.95%, and 52.93% improvements in line topology, star topology, and 100-agent\nsettings. Encourage community attention to the security of multi-agent systems.\n","authors":["Tianyi Men","Pengfei Cao","Zhuoran Jin","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16153v1","updated":"2024-10-21T16:19:41Z","published":"2024-10-21T16:19:41Z","title":"Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages","summary":"  Despite recent advances in multimodal large language models (MLLMs), their\ndevelopment has predominantly focused on English- and western-centric datasets\nand tasks, leaving most of the world's languages and diverse cultural contexts\nunderrepresented. This paper introduces Pangea, a multilingual multimodal LLM\ntrained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.\nPangeaIns features: 1) high-quality English instructions, 2) carefully\nmachine-translated instructions, and 3) culturally relevant multimodal tasks to\nensure cross-cultural coverage. To rigorously assess models' capabilities, we\nintroduce PangeaBench, a holistic evaluation suite encompassing 14 datasets\ncovering 47 languages. Results show that Pangea significantly outperforms\nexisting open-source models in multilingual settings and diverse cultural\ncontexts. Ablation studies further reveal the importance of English data\nproportions, language popularity, and the number of multimodal training samples\non overall performance. We fully open-source our data, code, and trained\ncheckpoints, to facilitate the development of inclusive and robust multilingual\nMLLMs, promoting equity and accessibility across a broader linguistic and\ncultural spectrum.\n","authors":["Xiang Yue","Yueqi Song","Akari Asai","Seungone Kim","Jean de Dieu Nyandwi","Simran Khanuja","Anjali Kantharuban","Lintang Sutawika","Sathyanarayanan Ramamoorthy","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16153v1.pdf","comment":"52 pages, 27 figures"},{"id":"http://arxiv.org/abs/2405.18406v2","updated":"2024-10-21T16:18:37Z","published":"2024-05-28T17:46:36Z","title":"RACCooN: A Versatile Instructional Video Editing Framework with\n  Auto-Generated Narratives","summary":"  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. (3) RACCooN also plans to imagine new objects in a\ngiven video, so users simply prompt the model to receive a detailed video\nediting plan for complex video editing. The proposed framework demonstrates\nimpressive versatile capabilities in video-to-paragraph generation, video\ncontent editing, and can be incorporated into other SoTA video generative\nmodels for further enhancement.\n","authors":["Jaehong Yoon","Shoubin Yu","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2405.18406v2.pdf","comment":"The first two authors contribute equally. Project Page:\n  https://raccoon-mllm-gen.github.io/"},{"id":"http://arxiv.org/abs/2410.16144v1","updated":"2024-10-21T16:14:57Z","published":"2024-10-21T16:14:57Z","title":"1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs","summary":"  Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shaoguang Mao","Shuming Ma","Hongyu Wang","Yan Xia","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.16144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16139v1","updated":"2024-10-21T16:05:58Z","published":"2024-10-21T16:05:58Z","title":"A Psycholinguistic Evaluation of Language Models' Sensitivity to\n  Argument Roles","summary":"  We present a systematic evaluation of large language models' sensitivity to\nargument roles, i.e., who did what to whom, by replicating psycholinguistic\nstudies on human argument role processing. In three experiments, we find that\nlanguage models are able to distinguish verbs that appear in plausible and\nimplausible contexts, where plausibility is determined through the relation\nbetween the verb and its preceding arguments. However, none of the models\ncapture the same selective patterns that human comprehenders exhibit during\nreal-time verb prediction. This indicates that language models' capacity to\ndetect verb plausibility does not arise from the same mechanism that underlies\nhuman real-time sentence processing.\n","authors":["Eun-Kyoung Rosa Lee","Sathvik Nair","Naomi Feldman"],"pdf_url":"https://arxiv.org/pdf/2410.16139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14134v2","updated":"2024-10-21T15:59:18Z","published":"2024-08-26T09:29:56Z","title":"Exploring the Potential of Large Language Models for Heterophilic Graphs","summary":"  Large language models (LLMs) have presented significant opportunities to\nenhance various machine learning applications, including graph neural networks\n(GNNs). By leveraging the vast open-world knowledge within LLMs, we can more\neffectively interpret and utilize textual data to better characterize\nheterophilic graphs, where neighboring nodes often have different labels.\nHowever, existing approaches for heterophilic graphs overlook the rich textual\ndata associated with nodes, which could unlock deeper insights into their\nheterophilic contexts. In this work, we explore the potential of LLMs for\nmodeling heterophilic graphs and propose a novel two-stage framework:\nLLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first\nstage, we fine-tune the LLM to better identify homophilic and heterophilic\nedges based on the textual content of their nodes. In the second stage, we\nadaptively manage message propagation in GNNs for different edge types based on\nnode features, structures, and heterophilic or homophilic characteristics. To\ncope with the computational demands when deploying LLMs in practical scenarios,\nwe further explore model distillation techniques to fine-tune smaller, more\nefficient models that maintain competitive performance. Extensive experiments\nvalidate the effectiveness of our framework, demonstrating the feasibility of\nusing LLMs to enhance node classification on heterophilic graphs.\n","authors":["Yuxia Wu","Shujie Li","Yuan Fang","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.14134v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.13761v2","updated":"2024-10-21T15:59:18Z","published":"2024-09-16T18:46:24Z","title":"Do Large Language Models Need a Content Delivery Network?","summary":"  As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.\n","authors":["Yihua Cheng","Kuntai Du","Jiayi Yao","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.13761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13502v2","updated":"2024-10-21T15:58:30Z","published":"2024-10-17T12:48:14Z","title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs","summary":"  Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to problems that\nare more complex than the ones on which they have been trained. Empirical\ninvestigations of such questions are impeded by two major flaws of current\nevaluations: (i) much of the evaluation data is contaminated, in the sense that\nit has already been seen during training, and (ii) benchmark datasets do not\ncapture how problem proofs may be arbitrarily complex in various ways. As a\nstep towards addressing these issues, we present a framework for evaluating\nLLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP.\nMathGAP generates problems that follow fixed proof specifications -- along with\nchain-of-thought reasoning annotations -- enabling systematic studies on\ngeneralization with respect to arithmetic proof complexity. We apply MathGAP to\nanalyze how in-context learning interacts with generalization to problems that\nhave more complex proofs. We find that among the models tested, most show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for GPT-4o. Surprisingly, providing in-context examples from\nthe same distribution as the test set is not always beneficial for performance.\nIn particular, zero-shot prompting as well as demonstrating a diverse range of\nexamples that are less complex than the test data sometimes yield similar or\nhigher accuracies.\n","authors":["Andreas Opedal","Haruki Shirakami","Bernhard Schölkopf","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.13502v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.16130v1","updated":"2024-10-21T15:55:27Z","published":"2024-10-21T15:55:27Z","title":"Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with\n  Multi-Task Assessment and Stepwise Audio Reasoning","summary":"  Recent advancements in large audio-language models (LALMs) have shown\nimpressive capabilities in understanding and reasoning about audio and speech\ninformation. However, these models still face challenges, including\nhallucinating non-existent sound events, misidentifying the order of sound\nevents, and incorrectly attributing sound sources, which undermine their\nreliability and real-world application. To systematically evaluate these\nissues, we propose three distinct tasks: object existence, temporal order, and\nobject attribute within audio. These tasks assess the models' comprehension of\ncritical audio information aspects. Our experimental results reveal limitations\nin these fundamental tasks, underscoring the need for better models in\nrecognizing specific sound events, determining event sequences, and identifying\nsound sources. To improve performance in these areas, we introduce a multi-turn\nchain-of-thought approach, which demonstrates significantly improved model\nperformance across the proposed tasks.\n","authors":["Chun-Yi Kuan","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2410.16130v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.13073v2","updated":"2024-10-21T15:54:34Z","published":"2024-10-16T22:25:15Z","title":"PromptExp: Multi-granularity Prompt Explanation of Large Language Models","summary":"  Large Language Models excel in tasks like natural language understanding and\ntext generation. Prompt engineering plays a critical role in leveraging LLM\neffectively. However, LLMs black-box nature hinders its interpretability and\neffective prompting engineering. A wide range of model explanation approaches\nhave been developed for deep learning models, However, these local explanations\nare designed for single-output tasks like classification and regression,and\ncannot be directly applied to LLMs, which generate sequences of tokens. Recent\nefforts in LLM explanation focus on natural language explanations, but they are\nprone to hallucinations and inaccuracies. To address this, we introduce\nOurTool, a framework for multi-granularity prompt explanations by aggregating\ntoken-level insights. OurTool introduces two token-level explanation\napproaches: 1.an aggregation-based approach combining local explanation\ntechniques, and 2. a perturbation-based approach with novel techniques to\nevaluate token masking impact. OurTool supports both white-box and black-box\nexplanations and extends explanations to higher granularity levels, enabling\nflexible analysis. We evaluate OurTool in case studies such as sentiment\nanalysis, showing the perturbation-based approach performs best using semantic\nsimilarity to assess perturbation impact. Furthermore, we conducted a user\nstudy to confirm OurTool's accuracy and practical value, and demonstrate its\npotential to enhance LLM interpretability.\n","authors":["Ximing Dong","Shaowei Wang","Dayi Lin","Gopi Krishnan Rajbahadur","Boquan Zhou","Shichao Liu","Ahmed E. Hassan"],"pdf_url":"https://arxiv.org/pdf/2410.13073v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2407.15711v2","updated":"2024-10-21T15:45:31Z","published":"2024-07-22T15:18:45Z","title":"AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?","summary":"  Language agents, built on top of language models (LMs), are systems that can\ninteract with complex environments, such as the open web. In this work, we\nexamine whether such agents can perform realistic and time-consuming tasks on\nthe web, e.g., monitoring real-estate markets or locating relevant nearby\nbusinesses. We introduce AssistantBench, a challenging new benchmark consisting\nof 214 realistic tasks that can be automatically evaluated, covering different\nscenarios and domains. We find that AssistantBench exposes the limitations of\ncurrent systems, including language models and retrieval-augmented language\nmodels, as no model reaches an accuracy of more than 26 points. While\nclosed-book LMs perform well in terms of accuracy, they exhibit low precision\nand tend to hallucinate facts. State-of-the-art web agents reach a score of\nnear zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that\nsignificantly outperforms previous agents, and an ensemble of SPA and\nclosed-book models reaches the best overall performance. Moreover, we analyze\nfailures of current systems and highlight that open web navigation remains a\nmajor challenge.\n","authors":["Ori Yoran","Samuel Joseph Amouyal","Chaitanya Malaviya","Ben Bogin","Ofir Press","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2407.15711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16107v1","updated":"2024-10-21T15:35:44Z","published":"2024-10-21T15:35:44Z","title":"Do LLMs write like humans? Variation in grammatical and rhetorical\n  styles","summary":"  Large language models (LLMs) are capable of writing grammatical text that\nfollows instructions, answers questions, and solves problems. As they have\nadvanced, it has become difficult to distinguish their output from\nhuman-written text. While past research has found some differences in surface\nfeatures such as word choice and punctuation, and developed classifiers to\ndetect LLM output, none has studied the rhetorical styles of LLMs.\n  Using several variants of Llama 3 and GPT-4o, we construct two parallel\ncorpora of human- and LLM-written texts from common prompts. Using Douglas\nBiber's set of lexical, grammatical, and rhetorical features, we identify\nsystematic differences between LLMs and humans and between different LLMs.\nThese differences persist when moving from smaller models to larger ones, and\nare larger for instruction-tuned models than base models. This demonstrates\nthat despite their advanced abilities, LLMs struggle to match human styles, and\nhence more advanced linguistic features can detect patterns in their behavior\nnot previously recognized.\n","authors":["Alex Reinhart","David West Brown","Ben Markey","Michael Laudenbach","Kachatad Pantusen","Ronald Yurko","Gordon Weinberg"],"pdf_url":"https://arxiv.org/pdf/2410.16107v1.pdf","comment":"29 pages, 4 figures, 11 tables"},{"id":"http://arxiv.org/abs/2409.08160v3","updated":"2024-10-21T15:22:58Z","published":"2024-09-12T15:52:22Z","title":"On the Role of Context in Reading Time Prediction","summary":"  We present a new perspective on how readers integrate context during\nreal-time language comprehension. Our proposals build on surprisal theory,\nwhich posits that the processing effort of a linguistic unit (e.g., a word) is\nan affine function of its in-context information content. We first observe that\nsurprisal is only one out of many potential ways that a contextual predictor\ncan be derived from a language model. Another one is the pointwise mutual\ninformation (PMI) between a unit and its context, which turns out to yield the\nsame predictive power as surprisal when controlling for unigram frequency.\nMoreover, both PMI and surprisal are correlated with frequency. This means that\nneither PMI nor surprisal contains information about context alone. In response\nto this, we propose a technique where we project surprisal onto the orthogonal\ncomplement of frequency, yielding a new contextual predictor that is\nuncorrelated with frequency. Our experiments show that the proportion of\nvariance in reading times explained by context is a lot smaller when context is\nrepresented by the orthogonalized predictor. From an interpretability\nstandpoint, this indicates that previous studies may have overstated the role\nthat context has in predicting reading times.\n","authors":["Andreas Opedal","Eleanor Chodroff","Ryan Cotterell","Ethan Gotlieb Wilcox"],"pdf_url":"https://arxiv.org/pdf/2409.08160v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.05072v2","updated":"2024-10-21T15:19:41Z","published":"2024-01-10T11:03:53Z","title":"Aligning Translation-Specific Understanding to General Understanding in\n  Large Language Models","summary":"  Large Language models (LLMs) have exhibited remarkable abilities in\nunderstanding complex texts, offering a promising path towards human-like\ntranslation performance. However, this study reveals the misalignment between\nthe translation-specific understanding and the general understanding inside\nLLMs. This understanding misalignment leads to LLMs mistakenly or literally\ntranslating some complicated concepts that they accurately comprehend in the\ngeneral scenarios (e.g., QA). To align the translation-specific understanding\nto the general one, we propose a novel translation process, DUAT (Difficult\nwords Understanding Aligned Translation), explicitly incorporating the general\nunderstanding on the complicated content incurring inconsistent understanding\nto guide the translation. Specifically, DUAT performs cross-lingual\ninterpretation for the difficult-to-translate words and enhances the\ntranslation with the generated interpretations. Furthermore, we reframe the\nexternal tools to improve DUAT in detecting difficult words and generating\nhelpful interpretations. We conduct experiments on the self-constructed\nbenchmark Challenge-WMT, consisting of samples that are prone to\nmistranslation. Human evaluation results on high-resource and low-resource\nlanguage pairs indicate that DUAT significantly facilitates the understanding\nalignment, which improves the translation quality (up to +3.85 COMET) and\nreduces the literality of the translation by -25% to -51%.\n","authors":["Yichong Huang","Baohang Li","Xiaocheng Feng","Chengpeng Fu","Wenshuai Huo","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2401.05072v2.pdf","comment":"EMNLP2024 (Main)"},{"id":"http://arxiv.org/abs/2410.16090v1","updated":"2024-10-21T15:12:51Z","published":"2024-10-21T15:12:51Z","title":"Analysing the Residual Stream of Language Models Under Knowledge\n  Conflicts","summary":"  Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context. Such conflicts can lead to\nundesirable model behaviour, such as reliance on outdated or incorrect\ninformation. In this work, we investigate whether LLMs can identify knowledge\nconflicts and whether it is possible to know which source of knowledge the\nmodel will rely on by analysing the residual stream of the LLM. Through probing\ntasks, we find that LLMs can internally register the signal of knowledge\nconflict in the residual stream, which can be accurately detected by probing\nthe intermediate model activations. This allows us to detect conflicts within\nthe residual stream before generating the answers without modifying the input\nor model parameters. Moreover, we find that the residual stream shows\nsignificantly different patterns when the model relies on contextual knowledge\nversus parametric knowledge to resolve conflicts. This pattern can be employed\nto estimate the behaviour of LLMs when conflict happens and prevent unexpected\nanswers before producing the answers. Our analysis offers insights into how\nLLMs internally manage knowledge conflicts and provides a foundation for\ndeveloping methods to control the knowledge selection processes.\n","authors":["Yu Zhao","Xiaotang Du","Giwon Hong","Aryo Pradipta Gema","Alessio Devoto","Hongru Wang","Xuanli He","Kam-Fai Wong","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2410.16090v1.pdf","comment":"Foundation Model Interventions Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16088v1","updated":"2024-10-21T15:12:20Z","published":"2024-10-21T15:12:20Z","title":"Fine-Tuning LLMs for Reliable Medical Question-Answering Services","summary":"  We present an advanced approach to medical question-answering (QA) services,\nusing fine-tuned Large Language Models (LLMs) to improve the accuracy and\nreliability of healthcare information. Our study focuses on optimizing models\nlike LLaMA-2 and Mistral, which have shown great promise in delivering precise,\nreliable medical answers. By leveraging comprehensive datasets, we applied\nfine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model\nperformance through a combination of decomposed model weights, varied learning\nrates for low-rank matrices, and rank stabilization, leading to improved\nefficiency. ReRAG, which integrates retrieval on demand and question rewriting,\nfurther refines the accuracy of the responses. This approach enables healthcare\nproviders to access fast, dependable information, aiding in more efficient\ndecision-making and fostering greater patient trust. Our work highlights the\npotential of fine-tuned LLMs to significantly improve the quality and\naccessibility of medical information services, ultimately contributing to\nbetter healthcare outcomes for all.\n","authors":["Ali Anaissi","Ali Braytee","Junaid Akram"],"pdf_url":"https://arxiv.org/pdf/2410.16088v1.pdf","comment":"8 pages, 10 figures, accepted and to be published in the proceedings\n  of 2024 IEEE International Conference on Data Mining Workshops (ICDMW)"},{"id":"http://arxiv.org/abs/2405.11459v2","updated":"2024-10-21T15:10:20Z","published":"2024-05-19T06:00:36Z","title":"Du-IN: Discrete units-guided mask modeling for decoding speech from\n  Intracranial Neural signals","summary":"  Invasive brain-computer interfaces with Electrocorticography (ECoG) have\nshown promise for high-performance speech decoding in medical applications, but\nless damaging methods like intracranial stereo-electroencephalography (sEEG)\nremain underexplored. With rapid advances in representation learning,\nleveraging abundant recordings to enhance speech decoding is increasingly\nattractive. However, popular methods often pre-train temporal models based on\nbrain-level tokens, overlooking that brain activities in different regions are\nhighly desynchronized during tasks. Alternatively, they pre-train\nspatial-temporal models based on channel-level tokens but fail to evaluate them\non challenging tasks like speech decoding, which requires intricate processing\nin specific language-related areas. To address this issue, we collected a\nwell-annotated Chinese word-reading sEEG dataset targeting language-related\nbrain networks from 12 subjects. Using this benchmark, we developed the Du-IN\nmodel, which extracts contextual embeddings based on region-level tokens\nthrough discrete codex-guided mask modeling. Our model achieves\nstate-of-the-art performance on the 61-word classification task, surpassing all\nbaselines. Model comparisons and ablation studies reveal that our design\nchoices, including (i) temporal modeling based on region-level tokens by\nutilizing 1D depthwise convolution to fuse channels in the lateral sensorimotor\ncortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervision\nthrough discrete codex-guided mask modeling, significantly contribute to this\nperformance. Overall, our approach -- inspired by neuroscience findings and\ncapitalizing on region-level representations from specific brain regions -- is\nsuitable for invasive brain modeling and represents a promising neuro-inspired\nAI approach in brain-computer interfaces.\n","authors":["Hui Zheng","Hai-Teng Wang","Wei-Bang Jiang","Zhong-Tao Chen","Li He","Pei-Yang Lin","Peng-Hu Wei","Guo-Guang Zhao","Yun-Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2405.11459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16077v1","updated":"2024-10-21T14:55:59Z","published":"2024-10-21T14:55:59Z","title":"CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian\n  Product Routing in Mixture-of-Experts","summary":"  Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.\n","authors":["Zhenpeng Su","Xing Wu","Zijia Lin","Yizhe Xiong","Minxuan Lv","Guangyuan Ma","Hui Chen","Songlin Hu","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2410.16077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16070v1","updated":"2024-10-21T14:48:35Z","published":"2024-10-21T14:48:35Z","title":"On-Device LLMs for SMEs: Challenges and Opportunities","summary":"  This paper presents a systematic review of the infrastructure requirements\nfor deploying Large Language Models (LLMs) on-device within the context of\nsmall and medium-sized enterprises (SMEs), focusing on both hardware and\nsoftware perspectives. From the hardware viewpoint, we discuss the utilization\nof processing units like GPUs and TPUs, efficient memory and storage solutions,\nand strategies for effective deployment, addressing the challenges of limited\ncomputational resources typical in SME settings. From the software perspective,\nwe explore framework compatibility, operating system optimization, and the use\nof specialized libraries tailored for resource-constrained environments. The\nreview is structured to first identify the unique challenges faced by SMEs in\ndeploying LLMs on-device, followed by an exploration of the opportunities that\nboth hardware innovations and software adaptations offer to overcome these\nobstacles. Such a structured review provides practical insights, contributing\nsignificantly to the community by enhancing the technological resilience of\nSMEs in integrating LLMs.\n","authors":["Jeremy Stephen Gabriel Yee Zhi Wen","Pai Chet Ng","Zhengkui Wang","Ian McLoughlin","Aik Beng Ng","Simon See"],"pdf_url":"https://arxiv.org/pdf/2410.16070v1.pdf","comment":"9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI\n  Centre"},{"id":"http://arxiv.org/abs/2410.16069v1","updated":"2024-10-21T14:47:37Z","published":"2024-10-21T14:47:37Z","title":"Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context","summary":"  Human processing of idioms relies on understanding the contextual sentences\nin which idioms occur, as well as language-intrinsic features such as frequency\nand speaker-intrinsic factors like familiarity. While LLMs have shown high\nperformance on idiomaticity detection tasks, this success may be attributed to\nreasoning shortcuts in existing datasets. To this end, we construct a novel,\ncontrolled contrastive dataset designed to test whether LLMs can effectively\nuse context to disambiguate idiomatic meaning. Additionally, we explore how\ncollocational frequency and sentence probability influence model performance.\nOur findings reveal that LLMs often fail to resolve idiomaticity when it is\nrequired to attend to the surrounding context, and that models perform better\non sentences that have higher likelihood. The collocational frequency of\nexpressions also impacts performance. We make our code and dataset publicly\navailable.\n","authors":["Maggie Mi","Aline Villavicencio","Nafise Sadat Moosavi"],"pdf_url":"https://arxiv.org/pdf/2410.16069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16062v1","updated":"2024-10-21T14:42:37Z","published":"2024-10-21T14:42:37Z","title":"Surprise! Uniform Information Density Isn't the Whole Story: Predicting\n  Surprisal Contours in Long-form Discourse","summary":"  The Uniform Information Density (UID) hypothesis posits that speakers tend to\ndistribute information evenly across linguistic units to achieve efficient\ncommunication. Of course, information rate in texts and discourses is not\nperfectly uniform. While these fluctuations can be viewed as theoretically\nuninteresting noise on top of a uniform target, another explanation is that UID\nis not the only functional pressure regulating information content in a\nlanguage. Speakers may also seek to maintain interest, adhere to writing\nconventions, and build compelling arguments. In this paper, we propose one such\nfunctional pressure; namely that speakers modulate information rate based on\nlocation within a hierarchically-structured model of discourse. We term this\nthe Structured Context Hypothesis and test it by predicting the surprisal\ncontours of naturally occurring discourses extracted from large language models\nusing predictors derived from discourse structure. We find that hierarchical\npredictors are significant predictors of a discourse's information contour and\nthat deeply nested hierarchical predictors are more predictive than shallow\nones. This work takes an initial step beyond UID to propose testable hypotheses\nfor why the information rate fluctuates in predictable ways\n","authors":["Eleftheria Tsipidi","Franz Nowak","Ryan Cotterell","Ethan Wilcox","Mario Giulianelli","Alex Warstadt"],"pdf_url":"https://arxiv.org/pdf/2410.16062v1.pdf","comment":"EMNLP 2024 (main conference)"},{"id":"http://arxiv.org/abs/2404.03881v3","updated":"2024-10-21T14:29:44Z","published":"2024-04-05T04:04:23Z","title":"A Bi-consolidating Model for Joint Relational Triple Extraction","summary":"  Current methods to extract relational triples directly make a prediction\nbased on a possible entity pair in a raw sentence without depending on entity\nrecognition. The task suffers from a serious semantic overlapping problem, in\nwhich several relation triples may share one or two entities in a sentence. In\nthis paper, based on a two-dimensional sentence representation, a\nbi-consolidating model is proposed to address this problem by simultaneously\nreinforcing the local and global semantic features relevant to a relation\ntriple. This model consists of a local consolidation component and a global\nconsolidation component. The first component uses a pixel difference\nconvolution to enhance semantic information of a possible triple representation\nfrom adjacent regions and mitigate noise in neighbouring neighbours. The second\ncomponent strengthens the triple representation based a channel attention and a\nspatial attention, which has the advantage to learn remote semantic\ndependencies in a sentence. They are helpful to improve the performance of both\nentity identification and relation type classification in relation triple\nextraction. After evaluated on several publish datasets, the bi-consolidating\nmodel achieves competitive performance. Analytical experiments demonstrate the\neffectiveness of our model for relational triple extraction and give motivation\nfor other natural language processing tasks.\n","authors":["Xiaocheng Luo","Yanping Chen","Ruixue Tang","Caiwei Yang","Ruizhang Huang","Yongbin Qin"],"pdf_url":"https://arxiv.org/pdf/2404.03881v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04684v2","updated":"2024-10-21T14:21:59Z","published":"2023-12-07T20:36:10Z","title":"Latent Skill Discovery for Chain-of-Thought Reasoning","summary":"  Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks.\n","authors":["Zifan Xu","Haozhu Wang","Dmitriy Bespalov","Xuan Wang","Peter Stone","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2312.04684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16044v1","updated":"2024-10-21T14:20:25Z","published":"2024-10-21T14:20:25Z","title":"Large Language Models Know What To Say But Not When To Speak","summary":"  Turn-taking is a fundamental mechanism in human communication that ensures\nsmooth and coherent verbal interactions. Recent advances in Large Language\nModels (LLMs) have motivated their use in improving the turn-taking\ncapabilities of Spoken Dialogue Systems (SDS), such as their ability to respond\nat appropriate times. However, existing models often struggle to predict\nopportunities for speaking -- called Transition Relevance Places (TRPs) -- in\nnatural, unscripted conversations, focusing only on turn-final TRPs and not\nwithin-turn TRPs. To address these limitations, we introduce a novel dataset of\nparticipant-labeled within-turn TRPs and use it to evaluate the performance of\nstate-of-the-art LLMs in predicting opportunities for speaking. Our experiments\nreveal the current limitations of LLMs in modeling unscripted spoken\ninteractions, highlighting areas for improvement and paving the way for more\nnaturalistic dialogue systems.\n","authors":["Muhammad Umair","Vasanth Sarathy","JP de Ruiter"],"pdf_url":"https://arxiv.org/pdf/2410.16044v1.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.16027v1","updated":"2024-10-21T14:02:40Z","published":"2024-10-21T14:02:40Z","title":"ComPO: Community Preferences for Language Model Personalization","summary":"  Conventional algorithms for training language models (LMs) with human\nfeedback rely on preferences that are assumed to account for an \"average\" user,\ndisregarding subjectivity and finer-grained variations. Recent studies have\nraised concerns that aggregating such diverse and often contradictory human\nfeedback to finetune models results in generic models that generate outputs not\npreferred by many user groups, as they tend to average out styles and norms. To\naddress this issue, we draw inspiration from recommendation systems and propose\nComPO, a method to personalize preference optimization in LMs by\ncontextualizing the probability distribution of model outputs with the\npreference provider. Focusing on group-level preferences rather than\nindividuals, we collect and release ComPRed, a question answering dataset with\ncommunity-level preferences from Reddit. This dataset facilitates studying\ndiversity in preferences without incurring privacy concerns associated with\nindividual feedback. Our experiments reveal that conditioning language models\non a community identifier (i.e., subreddit name) during preference tuning\nsubstantially enhances model performance. Conversely, replacing this context\nwith random subreddit identifiers significantly diminishes performance,\nhighlighting the effectiveness of our approach in tailoring responses to\ncommunities' preferences.\n","authors":["Sachin Kumar","Chan Young Park","Yulia Tsvetkov","Noah A. Smith","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2410.16027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15518v2","updated":"2024-10-21T14:02:00Z","published":"2024-02-11T13:41:17Z","title":"Beware of Words: Evaluating the Lexical Diversity of Conversational LLMs\n  using ChatGPT as Case Study","summary":"  The performance of conversational Large Language Models (LLMs) in general,\nand of ChatGPT in particular, is currently being evaluated on many different\ntasks, from logical reasoning or maths to answering questions on a myriad of\ntopics. Instead, much less attention is being devoted to the study of the\nlinguistic features of the texts generated by these LLMs. This is surprising\nsince LLMs are models for language, and understanding how they use the language\nis important. Indeed, conversational LLMs are poised to have a significant\nimpact on the evolution of languages as they may eventually dominate the\ncreation of new text. This means that for example, if conversational LLMs do\nnot use a word it may become less and less frequent and eventually stop being\nused altogether. Therefore, evaluating the linguistic features of the text they\nproduce and how those depend on the model parameters is the first step toward\nunderstanding the potential impact of conversational LLMs on the evolution of\nlanguages. In this paper, we consider the evaluation of the lexical richness of\nthe text generated by LLMs and how it depends on the model parameters. A\nmethodology is presented and used to conduct a comprehensive evaluation of\nlexical richness using ChatGPT as a case study. The results show how lexical\nrichness depends on the version of ChatGPT and some of its parameters, such as\nthe presence penalty, or on the role assigned to the model. The dataset and\ntools used in our analysis are released under open licenses with the goal of\ndrawing the much-needed attention to the evaluation of the linguistic features\nof LLM-generated text.\n","authors":["Gonzalo Martínez","José Alberto Hernández","Javier Conde","Pedro Reviriego","Elena Merino"],"pdf_url":"https://arxiv.org/pdf/2402.15518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16011v1","updated":"2024-10-21T13:42:19Z","published":"2024-10-21T13:42:19Z","title":"CA*: Addressing Evaluation Pitfalls in Computation-Aware Latency for\n  Simultaneous Speech Translation","summary":"  Simultaneous speech translation (SimulST) systems must balance translation\nquality with response time, making latency measurement crucial for evaluating\ntheir real-world performance. However, there has been a longstanding belief\nthat current metrics yield unrealistically high latency measurements in\nunsegmented streaming settings. In this paper, we investigate this phenomenon,\nrevealing its root cause in a fundamental misconception underlying existing\nlatency evaluation approaches. We demonstrate that this issue affects not only\nstreaming but also segment-level latency evaluation across different metrics.\nFurthermore, we propose a modification to correctly measure computation-aware\nlatency for SimulST systems, addressing the limitations present in existing\nmetrics.\n","authors":["Xi Xu","Wenda Xu","Siqi Ouyang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.16011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05770v3","updated":"2024-10-21T13:41:54Z","published":"2024-10-08T07:52:35Z","title":"Efficient Few-shot Learning for Multi-label Classification of Scientific\n  Documents with Many Classes","summary":"  Scientific document classification is a critical task and often involves many\nclasses. However, collecting human-labeled data for many classes is expensive\nand usually leads to label-scarce scenarios. Moreover, recent work has shown\nthat sentence embedding model fine-tuning for few-shot classification is\nefficient, robust, and effective. In this work, we propose FusionSent\n(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free\napproach for few-shot classification of scientific documents with many classes.\nFusionSent uses available training examples and their respective label texts to\ncontrastively fine-tune two different sentence embedding models. Afterward, the\nparameters of both fine-tuned models are fused to combine the complementary\nknowledge from the separate fine-tuning steps into a single model. Finally, the\nresulting sentence embedding model is frozen to embed the training instances,\nwhich are then used as input features to train a classification head. Our\nexperiments show that FusionSent significantly outperforms strong baselines by\nan average of $6.0$ $F_{1}$ points across multiple scientific document\nclassification datasets. In addition, we introduce a new dataset for\nmulti-label classification of scientific documents, which contains 203,961\nscientific articles and 130 classes from the arXiv category taxonomy. Code and\ndata are available at https://github.com/sebischair/FusionSent.\n","authors":["Tim Schopf","Alexander Blatzheim","Nektarios Machner","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2410.05770v3.pdf","comment":"Accepted to the 7th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2024)"},{"id":"http://arxiv.org/abs/2406.10576v2","updated":"2024-10-21T13:39:32Z","published":"2024-06-15T09:31:03Z","title":"Bypass Back-propagation: Optimization-based Structural Pruning for Large\n  Language Models via Policy Gradient","summary":"  In contrast to moderate-size neural network pruning, structural weight\npruning on the Large-Language Models (LLMs) imposes a novel challenge on the\nefficiency of the pruning algorithms, due to the heavy computation/memory\ndemands of the LLMs. Recent efficient LLM pruning methods typically operate at\nthe post-training phase without the expensive weight finetuning, however, their\npruning criteria often rely on heuristically hand-crafted metrics, potentially\nleading to suboptimal performance. We instead propose a novel\noptimization-based structural pruning that learns the pruning masks in a\nprobabilistic space directly by optimizing the loss of the pruned model. To\npreserve the efficiency, our method eliminates the back-propagation through the\nLLM per se during the optimization, requiring only the forward pass of the LLM.\nWe achieve this by learning an underlying Bernoulli distribution to sample\nbinary pruning masks, where we decouple the Bernoulli parameters from the LLM\nloss, thus facilitating an efficient optimization via a policy gradient\nestimator without back-propagation. As a result, our method is able to 1)\noperate at structural granularities of channels, heads, and layers, 2) support\nglobal and heterogeneous pruning (i.e., our method automatically determines\ndifferent redundancy for different layers), and 3) optionally initialize with a\nmetric-based method (for our Bernoulli distributions). Extensive experiments on\nLLaMA, LLaMA-2, LLaMA-3, Vicuna, and Mistral using the C4 and WikiText2\ndatasets demonstrate that our method operates for 2.7 hours with around 35GB\nmemory for the 13B models on a single A100 GPU, and our pruned models\noutperform the state-of-the-arts w.r.t. both perplexity and the majority of\nvarious zero-shot tasks. Codes will be released.\n","authors":["Yuan Gao","Zujing Liu","Weizhong Zhang","Bo Du","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2406.10576v2.pdf","comment":"Initially submitted on June 15, 2024, this version mainly changed the\n  title, and added several experiments: such as 1) experiments on LLaMA-3,\n  Mistral, 2) additional baseline methods (i.e., Bosai -- Everybody Prune Now),\n  and 3) post-pruning finetuned performance (i.e., first prune then finetune)"},{"id":"http://arxiv.org/abs/2410.16006v1","updated":"2024-10-21T13:39:03Z","published":"2024-10-21T13:39:03Z","title":"Exploring Continual Fine-Tuning for Enhancing Language Ability in Large\n  Language Model","summary":"  A common challenge towards the adaptability of Large Language Models (LLMs)\nis their ability to learn new languages over time without hampering the model's\nperformance on languages in which the model is already proficient (usually\nEnglish). Continual fine-tuning (CFT) is the process of sequentially\nfine-tuning an LLM to enable the model to adapt to downstream tasks with\nvarying data distributions and time shifts. This paper focuses on the language\nadaptability of LLMs through CFT. We study a two-phase CFT process in which an\nEnglish-only end-to-end fine-tuned LLM from Phase 1 (predominantly Task\nAbility) is sequentially fine-tuned on a multilingual dataset -- comprising\ntask data in new languages -- in Phase 2 (predominantly Language Ability). We\nobserve that the ``similarity'' of Phase 2 tasks with Phase 1 determines the\nLLM's adaptability. For similar phase-wise datasets, the LLM after Phase 2 does\nnot show deterioration in task ability. In contrast, when the phase-wise\ndatasets are not similar, the LLM's task ability deteriorates. We test our\nhypothesis on the open-source \\mis\\ and \\llm\\ models with multiple phase-wise\ndataset pairs. To address the deterioration, we analyze tailored variants of\ntwo CFT methods: layer freezing and generative replay. Our findings demonstrate\ntheir effectiveness in enhancing the language ability of LLMs while preserving\ntask performance, in comparison to relevant baselines.\n","authors":["Divyanshu Aggarwal","Sankarshan Damle","Navin Goyal","Satya Lokam","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2410.16006v1.pdf","comment":"19 pages, 6 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.15999v1","updated":"2024-10-21T13:30:47Z","published":"2024-10-21T13:30:47Z","title":"Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering","summary":"  Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$).\n","authors":["Yu Zhao","Alessio Devoto","Giwon Hong","Xiaotang Du","Aryo Pradipta Gema","Hongru Wang","Kam-Fai Wong","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2410.15999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15998v1","updated":"2024-10-21T13:29:08Z","published":"2024-10-21T13:29:08Z","title":"1024m at SMM4H 2024: Tasks 3, 5 & 6 -- Ensembles of Transformers and\n  Large Language Models for Medical Text Classification","summary":"  Social media is a great source of data for users reporting information and\nregarding their health and how various things have had an effect on them. This\npaper presents various approaches using Transformers and Large Language Models\nand their ensembles, their performance along with advantages and drawbacks for\nvarious tasks of SMM4H'24 - Classifying texts on impact of nature and outdoor\nspaces on the author's mental health (Task 3), Binary classification of tweets\nreporting their children's health disorders like Asthma, Autism, ADHD and\nSpeech disorder (task 5), Binary classification of users self-reporting their\nage (task 6).\n","authors":["Ram Mohan Rao Kadiyala","M. V. P. Chandra Sekhara Rao"],"pdf_url":"https://arxiv.org/pdf/2410.15998v1.pdf","comment":"short paper , acl 2024"},{"id":"http://arxiv.org/abs/2410.15990v1","updated":"2024-10-21T13:20:15Z","published":"2024-10-21T13:20:15Z","title":"Augmenting Legal Decision Support Systems with LLM-based NLI for\n  Analyzing Social Media Evidence","summary":"  This paper presents our system description and error analysis of our entry\nfor NLLP 2024 shared task on Legal Natural Language Inference (L-NLI)\n\\citep{hagag2024legallenssharedtask2024}. The task required classifying these\nrelationships as entailed, contradicted, or neutral, indicating any association\nbetween the review and the complaint. Our system emerged as the winning\nsubmission, significantly outperforming other entries with a substantial margin\nand demonstrating the effectiveness of our approach in legal text analysis. We\nprovide a detailed analysis of the strengths and limitations of each model and\napproach tested, along with a thorough error analysis and suggestions for\nfuture improvements. This paper aims to contribute to the growing field of\nlegal NLP by offering insights into advanced techniques for natural language\ninference in legal contexts, making it accessible to both experts and newcomers\nin the field.\n","authors":["Ram Mohan Rao Kadiyala","Siddartha Pullakhandam","Kanwal Mehreen","Subhasya Tippareddy","Ashay Srivastava"],"pdf_url":"https://arxiv.org/pdf/2410.15990v1.pdf","comment":"8 pages , accepted to emnlp 2024"},{"id":"http://arxiv.org/abs/2410.11786v2","updated":"2024-10-21T13:11:44Z","published":"2024-10-15T17:05:25Z","title":"Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.\n","authors":["Tsz Ting Chung","Leyang Cui","Lemao Liu","Xinting Huang","Shuming Shi","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.11786v2.pdf","comment":"14 pages, 5 figures, 10 tables, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15974v1","updated":"2024-10-21T13:00:09Z","published":"2024-10-21T13:00:09Z","title":"Large Language Models for Cross-lingual Emotion Detection","summary":"  This paper presents a detailed system description of our entry for the WASSA\n2024 Task 2, focused on cross-lingual emotion detection. We utilized a\ncombination of large language models (LLMs) and their ensembles to effectively\nunderstand and categorize emotions across different languages. Our approach not\nonly outperformed other submissions with a large margin, but also demonstrated\nthe strength of integrating multiple models to enhance performance.\nAdditionally, We conducted a thorough comparison of the benefits and\nlimitations of each model used. An error analysis is included along with\nsuggested areas for future improvement. This paper aims to offer a clear and\ncomprehensive understanding of advanced techniques in emotion detection, making\nit accessible even to those new to the field.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.15974v1.pdf","comment":"6 pages , accepted to acl 2024"},{"id":"http://arxiv.org/abs/2410.15970v1","updated":"2024-10-21T12:58:03Z","published":"2024-10-21T12:58:03Z","title":"Policy-driven Knowledge Selection and Response Generation for\n  Document-grounded Dialogue","summary":"  Document-grounded dialogue (DGD) uses documents as external knowledge for\ndialogue generation. Correctly understanding the dialogue context is crucial\nfor selecting knowledge from the document and generating proper responses. In\nthis paper, we propose using a dialogue policy to help the dialogue\nunderstanding in DGD. Our dialogue policy consists of two kinds of guiding\nsignals: utterance function and topic transfer intent. The utterance function\nreflects the purpose and style of an utterance, and the topic transfer intent\nreflects the topic and content of an utterance. We propose a novel framework\nexploiting our dialogue policy for two core tasks in DGD, namely knowledge\nselection (KS) and response generation (RG). The framework consists of two\nmodules: the Policy planner leverages policy-aware dialogue representation to\nselect knowledge and predict the policy of the response; the generator uses\npolicy/knowledge-aware dialogue representation for response generation. Our\npolicy-driven model gets state-of-the-art performance on three public\nbenchmarks and we provide a detailed analysis of the experimental results. Our\ncode/data will be released on GitHub.\n","authors":["Longxuan Ma","Jiapeng Li","Mingda Li","Wei-Nan Zhang","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15970v1.pdf","comment":"29 pages, 9 figures, 14 tables, TOIS 2024"},{"id":"http://arxiv.org/abs/2409.14038v3","updated":"2024-10-21T12:54:33Z","published":"2024-09-21T06:49:34Z","title":"OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching","summary":"  Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.14038v3.pdf","comment":"5 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2410.15966v1","updated":"2024-10-21T12:52:03Z","published":"2024-10-21T12:52:03Z","title":"Self-Explained Keywords Empower Large Language Models for Code\n  Generation","summary":"  Large language models (LLMs) have achieved impressive performance in code\ngeneration. However, due to the long-tail distribution of LLMs' training data,\nlow-frequency terms are typically underrepresented in the training process.\nConsequently, LLMs often misunderstand or overlook problem-specific,\nlow-frequency keywords during code generation, compromising the accuracy of the\ngenerated code. To address this, we propose a novel technique named\nSEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM\nfor better code generation by extracting and explaining the key terms in the\nproblem description with the LLM itself and ranking them based on frequency.\nComprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+),\nand APPS, with five representative LLMs, show that SEK can significantly\nimprove LLMs in code generation, yielding substantial and consistent gains. For\ninstance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to\n93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables\nthe LLMs to shift their attention from low-frequency keywords to their\ncorresponding high-frequency counterparts.\n","authors":["Lishui Fan","Mouxiang Chen","Zhongxin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19381v2","updated":"2024-10-21T12:48:58Z","published":"2024-09-28T15:12:55Z","title":"INC-Math: Integrating Natural Language and Code for Enhanced\n  Mathematical Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) are commonly used to generate solutions for\nmathematical reasoning problems in the following formats: natural language,\ncode, or a combination of both. In this paper, we explore fundamental questions\nrelated to solving mathematical reasoning problems using natural language and\ncode with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.\nOur findings show that LLMs are better at reasoning in natural language\ncompared to code. Additionally, although natural language and code serve as\ncomplementary forms of reasoning, they can affect each other in a negative way\nin certain scenarios. These insights motivate our development of a new\nprompting method, INC-Math, which leverages an LLM to dynamically select the\nmost appropriate reasoning form, resulting in improved performance over\ncomparable baselines with GPT-4o-mini.\n","authors":["Xuyuan Xiong","Simeng Han","Ziyue Zhou","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2409.19381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15962v1","updated":"2024-10-21T12:47:57Z","published":"2024-10-21T12:47:57Z","title":"Systematic Exploration of Dialogue Summarization Approaches for\n  Reproducibility, Comparative Assessment, and Methodological Innovations for\n  Advancing Natural Language Processing in Abstractive Summarization","summary":"  Reproducibility in scientific research, particularly within the realm of\nnatural language processing (NLP), is essential for validating and verifying\nthe robustness of experimental findings. This paper delves into the\nreproduction and evaluation of dialogue summarization models, focusing\nspecifically on the discrepancies observed between original studies and our\nreproduction efforts. Dialogue summarization is a critical aspect of NLP,\naiming to condense conversational content into concise and informative\nsummaries, thus aiding in efficient information retrieval and decision-making\nprocesses. Our research involved a thorough examination of several dialogue\nsummarization models using the AMI (Augmented Multi-party Interaction) dataset.\nThe models assessed include Hierarchical Memory Networks (HMNet) and various\nversions of Pointer-Generator Networks (PGN), namely PGN(DKE), PGN(DRD),\nPGN(DTS), and PGN(DALL). The primary objective was to evaluate the\ninformativeness and quality of the summaries generated by these models through\nhuman assessment, a method that introduces subjectivity and variability in the\nevaluation process. The analysis began with Dataset 1, where the sample\nstandard deviation of 0.656 indicated a moderate dispersion of data points\naround the mean.\n","authors":["Yugandhar Reddy Gogireddy","Jithendra Reddy Gogireddy"],"pdf_url":"https://arxiv.org/pdf/2410.15962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15956v1","updated":"2024-10-21T12:34:17Z","published":"2024-10-21T12:34:17Z","title":"Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs","summary":"  Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.\n","authors":["Yanzhu Guo","Simone Conia","Zelin Zhou","Min Li","Saloni Potdar","Henry Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.15956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15949v1","updated":"2024-10-21T12:30:44Z","published":"2024-10-21T12:30:44Z","title":"Findings of the Third Shared Task on Multilingual Coreference Resolution","summary":"  The paper presents an overview of the third edition of the shared task on\nmultilingual coreference resolution, held as part of the CRAC 2024 workshop.\nSimilarly to the previous two editions, the participants were challenged to\ndevelop systems capable of identifying mentions and clustering them based on\nidentity coreference.\n  This year's edition took another step towards real-world application by not\nproviding participants with gold slots for zero anaphora, increasing the task's\ncomplexity and realism. In addition, the shared task was expanded to include a\nmore diverse set of languages, with a particular focus on historical languages.\nThe training and evaluation data were drawn from version 1.2 of the\nmultilingual collection of harmonized coreference resources CorefUD,\nencompassing 21 datasets across 15 languages. 6 systems competed in this shared\ntask.\n","authors":["Michal Novák","Barbora Dohnalová","Miloslav Konopík","Anna Nedoluzhko","Martin Popel","Ondřej Pražák","Jakub Sido","Milan Straka","Zdeněk Žabokrtský","Daniel Zeman"],"pdf_url":"https://arxiv.org/pdf/2410.15949v1.pdf","comment":"Accepted to CRAC 2024"},{"id":"http://arxiv.org/abs/2410.15939v1","updated":"2024-10-21T12:12:21Z","published":"2024-10-21T12:12:21Z","title":"CausalGraph2LLM: Evaluating LLMs for Causal Queries","summary":"  Causality is essential in scientific research, enabling researchers to\ninterpret true relationships between variables. These causal relationships are\noften represented by causal graphs, which are directed acyclic graphs. With the\nrecent advancements in Large Language Models (LLMs), there is an increasing\ninterest in exploring their capabilities in causal reasoning and their\npotential use to hypothesize causal graphs. These tasks necessitate the LLMs to\nencode the causal graph effectively for subsequent downstream tasks. In this\npaper, we propose a comprehensive benchmark, \\emph{CausalGraph2LLM},\nencompassing a variety of causal graph settings to assess the causal graph\nunderstanding capability of LLMs. We categorize the causal queries into two\ntypes: graph-level and node-level queries. We benchmark both open-sourced and\nclosed models for our study. Our findings reveal that while LLMs show promise\nin this domain, they are highly sensitive to the encoding used. Even capable\nmodels like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with\ndeviations of about $60\\%$. We further demonstrate this sensitivity for\ndownstream causal intervention tasks. Moreover, we observe that LLMs can often\ndisplay biases when presented with contextual information about a causal graph,\npotentially stemming from their parametric memory.\n","authors":["Ivaxi Sheth","Bahare Fatemi","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2410.15939v1.pdf","comment":"Code - https://github.com/ivaxi0s/CausalGraph2LLM"},{"id":"http://arxiv.org/abs/2410.15929v1","updated":"2024-10-21T11:57:56Z","published":"2024-10-21T11:57:56Z","title":"Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with\n  Fine-tuning of Voice Activity Projection","summary":"  In human conversations, short backchannel utterances such as \"yeah\" and \"oh\"\nplay a crucial role in facilitating smooth and engaging dialogue. These\nbackchannels signal attentiveness and understanding without interrupting the\nspeaker, making their accurate prediction essential for creating more natural\nconversational agents. This paper proposes a novel method for real-time,\ncontinuous backchannel prediction using a fine-tuned Voice Activity Projection\n(VAP) model. While existing approaches have relied on turn-based or\nartificially balanced datasets, our approach predicts both the timing and type\nof backchannels in a continuous and frame-wise manner on unbalanced, real-world\ndatasets. We first pre-train the VAP model on a general dialogue corpus to\ncapture conversational dynamics and then fine-tune it on a specialized dataset\nfocused on backchannel behavior. Experimental results demonstrate that our\nmodel outperforms baseline methods in both timing and type prediction tasks,\nachieving robust performance in real-time environments. This research offers a\npromising step toward more responsive and human-like dialogue systems, with\nimplications for interactive spoken dialogue applications such as virtual\nassistants and robots.\n","authors":["Koji Inoue","Divesh Lala","Gabriel Skantze","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2410.15929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15926v1","updated":"2024-10-21T11:54:53Z","published":"2024-10-21T11:54:53Z","title":"Mitigating Object Hallucination via Concentric Causal Attention","summary":"  Recent Large Vision Language Models (LVLMs) present remarkable zero-shot\nconversational and reasoning capabilities given multimodal queries.\nNevertheless, they suffer from object hallucination, a phenomenon where LVLMs\nare prone to generate textual responses not factually aligned with image\ninputs. Our pilot study reveals that object hallucination is closely tied with\nRotary Position Encoding (RoPE), a widely adopted positional dependency\nmodeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs\ntend to hallucinate more when relevant visual cues are distant from instruction\ntokens in the multimodal input sequence. Additionally, we observe a similar\neffect when reversing the sequential order of visual tokens during multimodal\nalignment. Our tests indicate that long-term decay in RoPE poses challenges to\nLVLMs while capturing visual-instruction interactions across long distances. We\npropose Concentric Causal Attention (CCA), a simple yet effective positional\nalignment strategy that mitigates the impact of RoPE long-term decay in LVLMs\nby naturally reducing relative distance between visual and instruction tokens.\nWith CCA, visual tokens can better interact with instruction tokens, thereby\nenhancing model's perception capability and alleviating object hallucination.\nWithout bells and whistles, our positional alignment method surpasses existing\nhallucination mitigation strategies by large margins on multiple object\nhallucination benchmarks.\n","authors":["Yun Xing","Yiheng Li","Ivan Laptev","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2410.15926v1.pdf","comment":"To appear at NeurIPS 2024. Code is available at\n  https://github.com/xing0047/cca-llava"},{"id":"http://arxiv.org/abs/2404.12174v2","updated":"2024-10-21T11:33:33Z","published":"2024-04-18T13:31:05Z","title":"Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation\n  Guidelines?","summary":"  The increasing threat of disinformation calls for automating parts of the\nfact-checking pipeline. Identifying text segments requiring fact-checking is\nknown as claim detection (CD) and claim check-worthiness detection (CW), the\nlatter incorporating complex domain-specific criteria of worthiness and often\nframed as a ranking task. Zero- and few-shot LLM prompting is an attractive\noption for both tasks, as it bypasses the need for labeled datasets and allows\nverbalized claim and worthiness criteria to be directly used for prompting. We\nevaluate the LLMs' predictive and calibration accuracy on five CD/CW datasets\nfrom diverse domains, each utilizing a different worthiness criterion. We\ninvestigate two key aspects: (1) how best to distill factuality and worthiness\ncriteria into a prompt and (2) what amount of context to provide for each\nclaim. To this end, we experiment with varying the level of prompt verbosity\nand the amount of contextual information provided to the model. Our results\nshow that optimal prompt verbosity is domain-dependent, adding context does not\nimprove performance, and confidence scores can be directly used to produce\nreliable check-worthiness rankings.\n","authors":["Laura Majer","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2404.12174v2.pdf","comment":"Accepted to WASSA at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.15911v1","updated":"2024-10-21T11:33:18Z","published":"2024-10-21T11:33:18Z","title":"DefVerify: Do Hate Speech Models Reflect Their Dataset's Definition?","summary":"  When building a predictive model, it is often difficult to ensure that\ndomain-specific requirements are encoded by the model that will eventually be\ndeployed. Consider researchers working on hate speech detection. They will have\nan idea of what is considered hate speech, but building a model that reflects\ntheir view accurately requires preserving those ideals throughout the workflow\nof data set construction and model training. Complications such as sampling\nbias, annotation bias, and model misspecification almost always arise, possibly\nresulting in a gap between the domain specification and the model's actual\nbehavior upon deployment. To address this issue for hate speech detection, we\npropose DefVerify: a 3-step procedure that (i) encodes a user-specified\ndefinition of hate speech, (ii) quantifies to what extent the model reflects\nthe intended definition, and (iii) tries to identify the point of failure in\nthe workflow. We use DefVerify to find gaps between definition and model\nbehavior when applied to six popular hate speech benchmark datasets.\n","authors":["Urja Khurana","Eric Nalisnick","Antske Fokkens"],"pdf_url":"https://arxiv.org/pdf/2410.15911v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.04284v2","updated":"2024-10-21T11:26:20Z","published":"2024-08-08T07:43:17Z","title":"LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection","summary":"  The ease of access to large language models (LLMs) has enabled a widespread\nof machine-generated texts, and now it is often hard to tell whether a piece of\ntext was human-written or machine-generated. This raises concerns about\npotential misuse, particularly within educational and academic domains. Thus,\nit is important to develop practical systems that can automate the process.\nHere, we present one such system, LLM-DetectAIve, designed for fine-grained\ndetection. Unlike most previous work on machine-generated text detection, which\nfocused on binary classification, LLM-DetectAIve supports four categories: (i)\nhuman-written, (ii) machine-generated, (iii) machine-written, then\nmachine-humanized, and (iv) human-written, then machine-polished. Category\n(iii) aims to detect attempts to obfuscate the fact that a text was\nmachine-generated, while category (iv) looks for cases where the LLM was used\nto polish a human-written text, which is typically acceptable in academic\nwriting, but not in education. Our experiments show that LLM-DetectAIve can\neffectively identify the above four categories, which makes it a potentially\nuseful tool in education, academia, and other domains.\n  LLM-DetectAIve is publicly accessible at\nhttps://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system\nis available at https://youtu.be/E8eT_bE7k8c.\n","authors":["Mervat Abassy","Kareem Elozeiri","Alexander Aziz","Minh Ngoc Ta","Raj Vardhan Tomar","Bimarsha Adhikari","Saad El Dine Ahmed","Yuxia Wang","Osama Mohammed Afzal","Zhuohan Xie","Jonibek Mansurov","Ekaterina Artemova","Vladislav Mikhailov","Rui Xing","Jiahui Geng","Hasan Iqbal","Zain Muhammad Mujahid","Tarek Mahmoud","Akim Tsvigun","Alham Fikri Aji","Artem Shelmanov","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2408.04284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14859v2","updated":"2024-10-21T11:25:48Z","published":"2024-03-21T22:08:44Z","title":"Log Probabilities Are a Reliable Estimate of Semantic Plausibility in\n  Base and Instruction-Tuned Language Models","summary":"  Semantic plausibility (e.g. knowing that \"the actor won the award\" is more\nlikely than \"the actor won the battle\") serves as an effective proxy for\ngeneral world knowledge. Language models (LMs) capture vast amounts of world\nknowledge by learning distributional patterns in text, accessible via log\nprobabilities (LogProbs) they assign to plausible vs. implausible outputs. The\nnew generation of instruction-tuned LMs can now also provide explicit estimates\nof plausibility via prompting. Here, we evaluate the effectiveness of LogProbs\nand basic prompting to measure semantic plausibility, both in single-sentence\nminimal pairs (Experiment 1) and short context-dependent scenarios (Experiment\n2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a\nmore reliable measure of semantic plausibility than direct zero-shot prompting,\nwhich yields inconsistent and often poor results; (ii) instruction-tuning\ngenerally does not alter the sensitivity of LogProbs to semantic plausibility\n(although sometimes decreases it); (iii) across models, context mostly\nmodulates LogProbs in expected ways, as measured by three novel metrics of\ncontext-sensitive plausibility and their match to explicit human plausibility\njudgments. We conclude that, even in the era of prompt-based evaluations,\nLogProbs constitute a useful metric of semantic plausibility, both in base and\ninstruction-tuned LMs.\n","authors":["Carina Kauf","Emmanuele Chersoni","Alessandro Lenci","Evelina Fedorenko","Anna A. Ivanova"],"pdf_url":"https://arxiv.org/pdf/2403.14859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13296v2","updated":"2024-10-21T11:10:00Z","published":"2024-08-23T14:48:02Z","title":"The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An\n  Exhaustive Review of Technologies, Research, Best Practices, Applied Research\n  Challenges and Opportunities","summary":"  This report examines the fine-tuning of Large Language Models (LLMs),\nintegrating theoretical insights with practical applications. It outlines the\nhistorical evolution of LLMs from traditional Natural Language Processing (NLP)\nmodels to their pivotal role in AI. A comparison of fine-tuning methodologies,\nincluding supervised, unsupervised, and instruction-based approaches,\nhighlights their applicability to different tasks. The report introduces a\nstructured seven-stage pipeline for fine-tuning LLMs, spanning data\npreparation, model initialization, hyperparameter tuning, and model deployment.\nEmphasis is placed on managing imbalanced datasets and optimization techniques.\nParameter-efficient methods like Low-Rank Adaptation (LoRA) and Half\nFine-Tuning are explored for balancing computational efficiency with\nperformance. Advanced techniques such as memory fine-tuning, Mixture of Experts\n(MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized\nnetworks and multi-agent collaboration. The report also examines novel\napproaches like Proximal Policy Optimization (PPO) and Direct Preference\nOptimization (DPO), which align LLMs with human preferences, alongside pruning\nand routing optimizations to improve efficiency. Further sections cover\nvalidation frameworks, post-deployment monitoring, and inference optimization,\nwith attention to deploying LLMs on distributed and cloud-based platforms.\nEmerging areas such as multimodal LLMs, fine-tuning for audio and speech, and\nchallenges related to scalability, privacy, and accountability are also\naddressed. This report offers actionable insights for researchers and\npractitioners navigating LLM fine-tuning in an evolving landscape.\n","authors":["Venkatesh Balavadhani Parthasarathy","Ahtsham Zafar","Aafaq Khan","Arsalan Shahid"],"pdf_url":"https://arxiv.org/pdf/2408.13296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10621v3","updated":"2024-10-21T11:06:06Z","published":"2024-06-15T12:48:00Z","title":"StrucText-Eval: Evaluating Large Language Model's Reasoning Ability in\n  Structure-Rich Text","summary":"  The effective utilization of structured data, integral to corporate data\nstrategies, has been challenged by the rise of large language models (LLMs)\ncapable of processing unstructured information. This shift prompts the\nquestion: can LLMs interpret structured data directly in its unstructured form?\nWe propose an automatic evaluation data generation method for assessing LLMs'\nreasoning capabilities on structure-rich text to explore this. Our approach\nsupports 8 structured languages and 29 tasks, generating data with adjustable\ncomplexity through controllable nesting and structural width. We introduce\nStrucText-Eval, a benchmark containing 5,800 pre-generated and annotated\nsamples designed to evaluate how well LLMs understand and reason through\nstructured text. StrucText-Eval is divided into two suites: a regular Test\nsuite (3,712 samples) and a Test-Hard suite (2,088 samples), the latter\nemphasizing the gap between human and model performance on more complex tasks.\nExperimental results show that while open-source LLMs achieve a maximum\naccuracy of 74.9\\% on the standard dataset, their performance drops\nsignificantly to 45.8\\% on the harder dataset. In contrast, human participants\nreach an accuracy of 92.6\\% on StrucText-Eval-Hard, highlighting LLMs' current\nlimitations in handling intricate structural information. The benchmark and\ngeneration codes are open sourced in\n\\url{https://github.com/MikeGu721/StrucText-Eval}\n","authors":["Zhouhong Gu","Haoning Ye","Xingzhou Chen","Zeyang Zhou","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.10621v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03615v2","updated":"2024-10-21T11:05:27Z","published":"2024-08-07T08:16:32Z","title":"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in\n  Long-Horizon Tasks","summary":"  Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.\n","authors":["Zaijing Li","Yuquan Xie","Rui Shao","Gongwei Chen","Dongmei Jiang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2408.03615v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15884v1","updated":"2024-10-21T11:02:18Z","published":"2024-10-21T11:02:18Z","title":"Using GPT Models for Qualitative and Quantitative News Analytics in the\n  2024 US Presidental Election Process","summary":"  The paper considers an approach of using Google Search API and GPT-4o model\nfor qualitative and quantitative analyses of news through retrieval-augmented\ngeneration (RAG). This approach was applied to analyze news about the 2024 US\npresidential election process. Different news sources for different time\nperiods have been analyzed. Quantitative scores generated by GPT model have\nbeen analyzed using Bayesian regression to derive trend lines. The\ndistributions found for the regression parameters allow for the analysis of\nuncertainty in the election process. The obtained results demonstrate that\nusing the GPT models for news analysis, one can get informative analytics and\nprovide key insights that can be applied in further analyses of election\nprocesses.\n","authors":["Bohdan M. Pavlyshenko"],"pdf_url":"https://arxiv.org/pdf/2410.15884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07989v3","updated":"2024-10-21T10:54:55Z","published":"2024-04-11T17:59:45Z","title":"Any2Point: Empowering Any-modality Large Models for Efficient 3D\n  Understanding","summary":"  Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.\n","authors":["Yiwen Tang","Ray Zhang","Jiaming Liu","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Shanghang Zhang","Peng Gao","Hongsheng Li","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.07989v3.pdf","comment":"Code and models are released at\n  https://github.com/Ivan-Tang-3D/Any2Point"},{"id":"http://arxiv.org/abs/2410.15865v1","updated":"2024-10-21T10:49:54Z","published":"2024-10-21T10:49:54Z","title":"Principles of semantic and functional efficiency in grammatical\n  patterning","summary":"  Grammatical features such as number and gender serve two central functions in\nhuman languages. While they encode salient semantic attributes like numerosity\nand animacy, they also offload sentence processing cost by predictably linking\nwords together via grammatical agreement. Grammars exhibit consistent\norganizational patterns across diverse languages, invariably rooted in a\nsemantic foundation, a widely confirmed but still theoretically unexplained\nphenomenon. To explain the basis of universal grammatical patterns, we unify\ntwo fundamental properties of grammar, semantic encoding and agreement-based\npredictability, into a single information-theoretic objective under cognitive\nconstraints. Our analyses reveal that grammatical organization provably\ninherits from perceptual attributes, but that grammars empirically prioritize\nfunctional goals, promoting efficient language processing over semantic\nencoding.\n","authors":["Emily Cheng","Francesca Franzon"],"pdf_url":"https://arxiv.org/pdf/2410.15865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12284v2","updated":"2024-10-21T10:11:11Z","published":"2024-10-16T06:43:02Z","title":"Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical\n  Decision-Support Setting","summary":"  The growing capabilities of AI models are leading to their wider use,\nincluding in safety-critical domains. Explainable AI (XAI) aims to make these\nmodels safer to use by making their inference process more transparent.\nHowever, current explainability methods are seldom evaluated in the way they\nare intended to be used: by real-world end users. To address this, we conducted\na large-scale user study with 85 healthcare practitioners in the context of\nhuman-AI collaborative chest X-ray analysis. We evaluated three types of\nexplanations: visual explanations (saliency maps), natural language\nexplanations, and a combination of both modalities. We specifically examined\nhow different explanation types influence users depending on whether the AI\nadvice and explanations are factually correct. We find that text-based\nexplanations lead to significant over-reliance, which is alleviated by\ncombining them with saliency maps. We also observe that the quality of\nexplanations, that is, how much factually correct information they entail, and\nhow much this aligns with AI correctness, significantly impacts the usefulness\nof the different explanation types.\n","authors":["Maxime Kayser","Bayar Menzat","Cornelius Emde","Bogdan Bercean","Alex Novak","Abdala Espinosa","Bartlomiej W. Papiez","Susanne Gaube","Thomas Lukasiewicz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2410.12284v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2408.04167v2","updated":"2024-10-21T09:48:08Z","published":"2024-08-08T02:28:32Z","title":"mbrs: A Library for Minimum Bayes Risk Decoding","summary":"  Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs\n","authors":["Hiroyuki Deguchi","Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2408.04167v2.pdf","comment":"Accepted at EMNLP2024 System Demonstration track"},{"id":"http://arxiv.org/abs/2402.14296v3","updated":"2024-10-21T09:42:55Z","published":"2024-02-22T05:17:49Z","title":"Mitigating Biases of Large Language Models in Stance Detection with\n  Counterfactual Augmented Calibration","summary":"  Stance detection is critical for understanding the underlying position or\nattitude expressed toward a topic. Large language models (LLMs) have\ndemonstrated significant advancements across various natural language\nprocessing tasks including stance detection, however, their performance in\nstance detection is limited by biases and spurious correlations inherent due to\ntheir data-driven nature. Our statistical experiment reveals that LLMs are\nprone to generate biased stances due to sentiment-stance spurious correlations\nand preference towards certain individuals and topics. Furthermore, the results\ndemonstrate a strong negative correlation between stance bias and stance\ndetection performance, underscoring the importance of mitigating bias to\nenhance the utility of LLMs in stance detection. Therefore, in this paper, we\npropose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel\ncalibration network is devised to calibrate potential bias in the stance\nprediction of LLMs. Further, to address the challenge of effectively learning\nbias representations and the difficulty in the generalizability of debiasing,\nwe construct counterfactual augmented data. This approach enhances the\ncalibration network, facilitating the debiasing and out-of-domain\ngeneralization. Experimental results on in-target and zero-shot stance\ndetection tasks show that the proposed FACTUAL can effectively mitigate biases\nof LLMs, achieving state-of-the-art results.\n","authors":["Ang Li","Jingqian Zhao","Bin Liang","Lin Gui","Hui Wang","Xi Zeng","Xingwei Liang","Kam-Fai Wong","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2402.14296v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15825v1","updated":"2024-10-21T09:42:13Z","published":"2024-10-21T09:42:13Z","title":"Did somebody say \"Gest-IT\"? A pilot exploration of multimodal data\n  management","summary":"  The paper presents a pilot exploration of the construction, management and\nanalysis of a multimodal corpus. Through a three-layer annotation that provides\northographic, prosodic, and gestural transcriptions, the Gest-IT resource\nallows to investigate the variation of gesture-making patterns in conversations\nbetween sighted people and people with visual impairment. After discussing the\ntranscription methods and technical procedures employed in our study, we\npropose a unified CoNLL-U corpus and indicate our future steps\n","authors":["Ludovica Pannitto","Lorenzo Albanesi","Laura Marion","Federica Maria Martines","Carmelo Caruso","Claudia S. Bianchini","Francesca Masini","Caterina Mauri"],"pdf_url":"https://arxiv.org/pdf/2410.15825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05846v2","updated":"2024-10-21T09:38:03Z","published":"2024-03-09T09:11:49Z","title":"Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines","summary":"  Text-to-image diffusion models (T2I) use a latent representation of a text\nprompt to guide the image generation process. However, the process by which the\nencoder produces the text representation is unknown. We propose the Diffusion\nLens, a method for analyzing the text encoder of T2I models by generating\nimages from its intermediate representations. Using the Diffusion Lens, we\nperform an extensive analysis of two recent T2I models. Exploring compound\nprompts, we find that complex scenes describing multiple objects are composed\nprogressively and more slowly compared to simple scenes; Exploring knowledge\nretrieval, we find that representation of uncommon concepts requires further\ncomputation compared to common concepts, and that knowledge retrieval is\ngradual across layers. Overall, our findings provide valuable insights into the\ntext encoder component in T2I pipelines.\n","authors":["Michael Toker","Hadas Orgad","Mor Ventura","Dana Arad","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.05846v2.pdf","comment":"Published in: ACL 2024 Project webpage:\n  tokeron.github.io/DiffusionLensWeb"},{"id":"http://arxiv.org/abs/2410.12691v3","updated":"2024-10-21T09:28:12Z","published":"2024-10-16T15:51:18Z","title":"Building Better: Avoiding Pitfalls in Developing Language Resources when\n  Data is Scarce","summary":"  Language is a symbolic capital that affects people's lives in many ways\n(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,\ncultures, traditions, and societies in general. Hence, data in a given language\nshould be viewed as more than a collection of tokens. Good data collection and\nlabeling practices are key to building more human-centered and socially aware\ntechnologies. While there has been a rising interest in mid- to low-resource\nlanguages within the NLP community, work in this space has to overcome unique\nchallenges such as data scarcity and access to suitable annotators. In this\npaper, we collect feedback from those directly involved in and impacted by NLP\nartefacts for mid- to low-resource languages. We conduct a quantitative and\nqualitative analysis of the responses and highlight the main issues related to\n(1) data quality such as linguistic and cultural data suitability; and (2) the\nethics of common annotation practices such as the misuse of online community\nservices. Based on these findings, we make several recommendations for the\ncreation of high-quality language artefacts that reflect the cultural milieu of\nits speakers, while simultaneously respecting the dignity and labor of data\nworkers.\n","authors":["Nedjma Ousidhoum","Meriem Beloucif","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2410.12691v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15801v1","updated":"2024-10-21T09:18:30Z","published":"2024-10-21T09:18:30Z","title":"Improve Dense Passage Retrieval with Entailment Tuning","summary":"  Retrieval module can be plugged into many downstream NLP tasks to improve\ntheir performance, such as open-domain question answering and\nretrieval-augmented generation. The key to a retrieval system is to calculate\nrelevance scores to query and passage pairs. However, the definition of\nrelevance is often ambiguous. We observed that a major class of relevance\naligns with the concept of entailment in NLI tasks. Based on this observation,\nwe designed a method called entailment tuning to improve the embedding of dense\nretrievers. Specifically, we unify the form of retrieval data and NLI data\nusing existence claim as a bridge. Then, we train retrievers to predict the\nclaims entailed in a passage with a variant task of masked prediction. Our\nmethod can be efficiently plugged into current dense retrieval methods, and\nexperiments show the effectiveness of our method.\n","authors":["Lu Dai","Hao Liu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.15801v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2408.10724v2","updated":"2024-10-21T08:58:39Z","published":"2024-08-20T10:45:36Z","title":"Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian","summary":"  In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.\n","authors":["Cem Üyük","Danica Rovó","Shaghayegh Kolli","Rabia Varol","Georg Groh","Daryna Dementieva"],"pdf_url":"https://arxiv.org/pdf/2408.10724v2.pdf","comment":"EMNLP 2024 NLP4PI Workshop"},{"id":"http://arxiv.org/abs/2407.12831v2","updated":"2024-10-21T08:55:49Z","published":"2024-07-03T13:01:54Z","title":"Truth is Universal: Robust Detection of Lies in LLMs","summary":"  Large Language Models (LLMs) have revolutionised natural language processing,\nexhibiting impressive human-like capabilities. In particular, LLMs are capable\nof \"lying\", knowingly outputting false statements. Hence, it is of interest and\nimportance to develop methods to detect when LLMs lie. Indeed, several authors\ntrained classifiers to detect LLM lies based on their internal model\nactivations. However, other researchers showed that these classifiers may fail\nto generalise, for example to negated statements. In this work, we aim to\ndevelop a robust method to detect when an LLM is lying. To this end, we make\nthe following key contributions: (i) We demonstrate the existence of a\ntwo-dimensional subspace, along which the activation vectors of true and false\nstatements can be separated. Notably, this finding is universal and holds for\nvarious LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our\nanalysis explains the generalisation failures observed in previous studies and\nsets the stage for more robust lie detection; (ii) Building upon (i), we\nconstruct an accurate LLM lie detector. Empirically, our proposed classifier\nachieves state-of-the-art performance, attaining 94% accuracy in both\ndistinguishing true from false factual statements and detecting lies generated\nin real-world scenarios.\n","authors":["Lennart Bürger","Fred A. Hamprecht","Boaz Nadler"],"pdf_url":"https://arxiv.org/pdf/2407.12831v2.pdf","comment":"NeurIPS 2024 poster"},{"id":"http://arxiv.org/abs/2405.20648v2","updated":"2024-10-21T08:52:10Z","published":"2024-05-31T07:30:24Z","title":"Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision\n  Models For Video Captioning and Summarization","summary":"  Video is an increasingly prominent and information-dense medium, yet it poses\nsubstantial challenges for language models. A typical video consists of a\nsequence of shorter segments, or shots, that collectively form a coherent\nnarrative. Each shot is analogous to a word in a sentence where multiple data\nstreams of information (such as visual and auditory data) must be processed\nsimultaneously. Comprehension of the entire video requires not only\nunderstanding the visual-audio information of each shot but also requires that\nthe model links the ideas between each shot to generate a larger,\nall-encompassing story. Despite significant progress in the field, current\nworks often overlook videos' more granular shot-by-shot semantic information.\nIn this project, we propose a family of efficient large language vision models\n(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By\nleveraging better pretraining and data collection strategies, we extend the\nabilities of existing small LLVMs from being able to understand a picture to\nbeing able to understand a sequence of frames. Specifically, we show that\nShotluck Holmes achieves better performance than state-of-the-art results on\nthe Shot2Story video captioning and summary task with significantly smaller and\nmore computationally efficient models.\n","authors":["Richard Luo","Austin Peng","Adithya Vasudev","Rishabh Jain"],"pdf_url":"https://arxiv.org/pdf/2405.20648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08703v2","updated":"2024-10-21T08:49:18Z","published":"2024-10-11T10:47:02Z","title":"On the token distance modeling ability of higher RoPE attention\n  dimension","summary":"  Length extrapolation algorithms based on Rotary position embedding (RoPE)\nhave shown promising results in extending the context length of language\nmodels. However, understanding how position embedding can capture longer-range\ncontextual information remains elusive. Based on the intuition that different\ndimensions correspond to different frequency of changes in RoPE encoding, we\nconducted a dimension-level analysis to investigate the correlation between a\nhidden dimension of an attention head and its contribution to capturing\nlong-distance dependencies. Using our correlation metric, we identified a\nparticular type of attention heads, which we named Positional Heads, from\nvarious length-extrapolated models. These heads exhibit a strong focus on\nlong-range information interaction and play a pivotal role in long input\nprocessing, as evidence by our ablation. We further demonstrate the correlation\nbetween the efficiency of length extrapolation and the extension of the\nhigh-dimensional attention allocation of these heads. The identification of\nPositional Heads provides insights for future research in long-text\ncomprehension.\n","authors":["Xiangyu Hong","Che Jiang","Biqing Qi","Fandong Meng","Mo Yu","Bowen Zhou","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.08703v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2407.18698v2","updated":"2024-10-21T08:43:41Z","published":"2024-07-26T12:23:54Z","title":"Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended\n  Text Generation","summary":"  Decoding from the output distributions of large language models to produce\nhigh-quality text is a complex challenge in language modeling. Various\napproaches, such as beam search, sampling with temperature, $k-$sampling,\nnucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive\nsearch, have been proposed to address this problem, aiming to improve\ncoherence, diversity, as well as resemblance to human-generated text. In this\nstudy, we introduce adaptive contrastive search, a novel decoding strategy\nextending contrastive search by incorporating an adaptive degeneration penalty,\nguided by the estimated uncertainty of the model at each generation step. This\nstrategy is designed to enhance both the creativity and diversity of the\nlanguage modeling process while at the same time producing coherent and\nhigh-quality generated text output. Our findings indicate performance\nenhancement in both aspects, across different model architectures and datasets,\nunderscoring the effectiveness of our method in text generation tasks. Our code\nbase, datasets, and models are publicly available.\n","authors":["Esteban Garces Arias","Julian Rodemann","Meimingwei Li","Christian Heumann","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2407.18698v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15761v1","updated":"2024-10-21T08:21:00Z","published":"2024-10-21T08:21:00Z","title":"Learning-to-Defer for Extractive Question Answering","summary":"  Pre-trained language models have profoundly impacted the field of extractive\nquestion-answering, leveraging large-scale textual corpora to enhance\ncontextual language understanding. Despite their success, these models struggle\nin complex scenarios that demand nuanced interpretation or inferential\nreasoning beyond immediate textual cues. Furthermore, their size poses\ndeployment challenges on resource-constrained devices. Addressing these\nlimitations, we introduce an adapted two-stage Learning-to-Defer mechanism that\nenhances decision-making by enabling selective deference to human experts or\nlarger models without retraining language models in the context of\nquestion-answering. This approach not only maintains computational efficiency\nbut also significantly improves model reliability and accuracy in ambiguous\ncontexts. We establish the theoretical soundness of our methodology by proving\nBayes and $(\\mathcal{H}, \\mathcal{R})$--consistency of our surrogate loss\nfunction, guaranteeing the optimality of the final solution. Empirical\nevaluations on the SQuADv2 dataset illustrate performance gains from\nintegrating human expertise and leveraging larger models. Our results further\ndemonstrate that deferring a minimal number of queries allows the smaller model\nto achieve performance comparable to their larger counterparts while preserving\ncomputing efficiency, thus broadening the applicability of pre-trained language\nmodels in diverse operational environments.\n","authors":["Montreuil Yannis","Carlier Axel","Ng Lai Xing","Ooi Wei Tsang"],"pdf_url":"https://arxiv.org/pdf/2410.15761v1.pdf","comment":"25 pages, 17 main paper"},{"id":"http://arxiv.org/abs/2410.10270v3","updated":"2024-10-21T08:13:45Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v3.pdf","comment":"Accepted for EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2408.10188v4","updated":"2024-10-21T08:12:42Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models \\qinghao{by co-designing the\nalgorithm and system. For model training, we upgrade existing VLMs to support\nlong video understanding by incorporating two additional stages, {\\em i.e.},\nlong context extension and long video supervised fine-tuning. However, training\non long video is computationally and memory intensive. We introduce the\nlong-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently\nparallelizes long video training and inference, enabling 2M context length\ntraining on 256 GPUs without any gradient checkpointing. LongVILA efficiently\nextends the number of video frames of VILA from 8 to 2048, improving the long\nvideo captioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy\nin 6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%\nwith subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence\nparallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and\ntensor parallelism. Moreover, it seamlessly integrates with Hugging Face\nTransformers.\n","authors":["Fuzhao Xue","Yukang Chen","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v4.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/blob/main/LongVILA.md"},{"id":"http://arxiv.org/abs/2410.15753v1","updated":"2024-10-21T08:11:47Z","published":"2024-10-21T08:11:47Z","title":"Natural Language Querying System Through Entity Enrichment","summary":"  This paper focuses on a domain expert querying system over databases. It\npresents a solution designed for a French enterprise interested in offering a\nnatural language interface for its clients. The approach, based on entity\nenrichment, aims at translating natural language queries into database queries.\nIn this paper, the database is treated through a logical paradigm, suggesting\nthe adaptability of our approach to different database models. The good\nprecision of our method is shown through some preliminary experiments.\n","authors":["Joshua Amavi","Mirian Halfeld Ferrari","Nicolas Hiot"],"pdf_url":"https://arxiv.org/pdf/2410.15753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15743v1","updated":"2024-10-21T08:01:46Z","published":"2024-10-21T08:01:46Z","title":"Toeing the Party Line: Election Manifestos as a Key to Understand\n  Political Discourse on Twitter","summary":"  Political discourse on Twitter is a moving target: politicians continuously\nmake statements about their positions. It is therefore crucial to track their\ndiscourse on social media to understand their ideological positions and goals.\nHowever, Twitter data is also challenging to work with since it is ambiguous\nand often dependent on social context, and consequently, recent work on\npolitical positioning has tended to focus strongly on manifestos (parties'\nelectoral programs) rather than social media.\n  In this paper, we extend recently proposed methods to predict pairwise\npositional similarities between parties from the manifesto case to the Twitter\ncase, using hashtags as a signal to fine-tune text representations, without the\nneed for manual annotation. We verify the efficacy of fine-tuning and conduct a\nseries of experiments that assess the robustness of our method for low-resource\nscenarios. We find that our method yields stable positioning reflective of\nmanifesto positioning, both in scenarios with all tweets of candidates across\nyears available and when only smaller subsets from shorter time periods are\navailable. This indicates that it is possible to reliably analyze the relative\npositioning of actors forgoing manual annotation, even in the noisier context\nof social media.\n","authors":["Maximilian Maurer","Tanise Ceron","Sebastian Padó","Gabriella Lapesa"],"pdf_url":"https://arxiv.org/pdf/2410.15743v1.pdf","comment":"9 pages, accepted at EMNLP (Findings) 2024"},{"id":"http://arxiv.org/abs/2410.15737v1","updated":"2024-10-21T07:56:45Z","published":"2024-10-21T07:56:45Z","title":"Who's Who: Large Language Models Meet Knowledge Conflicts in Practice","summary":"  Retrieval-augmented generation (RAG) methods are viable solutions for\naddressing the static memory limits of pre-trained language models.\nNevertheless, encountering conflicting sources of information within the\nretrieval context is an inevitable practical challenge. In such situations, the\nlanguage models are recommended to transparently inform users about the\nconflicts rather than autonomously deciding what to present based on their\ninherent biases. To analyze how current large language models (LLMs) align with\nour recommendation, we introduce WhoQA, a public benchmark dataset to examine\nmodel's behavior in knowledge conflict situations. We induce conflicts by\nasking about a common property among entities having the same name, resulting\nin questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K\nquestions across 13 Wikidata property types and 150K Wikipedia entities. Our\nexperiments show that despite the simplicity of WhoQA questions, knowledge\nconflicts significantly degrades LLMs' performance in RAG settings.\n","authors":["Quang Hieu Pham","Hoang Ngo","Anh Tuan Luu","Dat Quoc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.15737v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15726v1","updated":"2024-10-21T07:44:01Z","published":"2024-10-21T07:44:01Z","title":"Reducing annotator bias by belief elicitation","summary":"  Crowdsourced annotations of data play a substantial role in the development\nof Artificial Intelligence (AI). It is broadly recognised that annotations of\ntext data can contain annotator bias, where systematic disagreement in\nannotations can be traced back to differences in the annotators' backgrounds.\nBeing unaware of such annotator bias can lead to representational bias against\nminority group perspectives and therefore several methods have been proposed\nfor recognising bias or preserving perspectives. These methods typically\nrequire either a substantial number of annotators or annotations per data\ninstance. In this study, we propose a simple method for handling bias in\nannotations without requirements on the number of annotators or instances.\nInstead, we ask annotators about their beliefs of other annotators' judgements\nof an instance, under the hypothesis that these beliefs may provide more\nrepresentative and less biased labels than judgements. The method was examined\nin two controlled, survey-based experiments involving Democrats and Republicans\n(n=1,590) asked to judge statements as arguments and then report beliefs about\nothers' judgements. The results indicate that bias, defined as systematic\ndifferences between the two groups of annotators, is consistently reduced when\nasking for beliefs instead of judgements. Our proposed method therefore has the\npotential to reduce the risk of annotator bias, thereby improving the\ngeneralisability of AI systems and preventing harm to unrepresented\nsocio-demographic groups, and we highlight the need for further studies of this\npotential in other tasks and downstream applications.\n","authors":["Terne Sasha Thorn Jakobsen","Andreas Bjerre-Nielsen","Robert Böhm"],"pdf_url":"https://arxiv.org/pdf/2410.15726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15702v1","updated":"2024-10-21T07:19:19Z","published":"2024-10-21T07:19:19Z","title":"Mitigating Hallucinations of Large Language Models in Medical\n  Information Extraction via Contrastive Decoding","summary":"  The impressive capabilities of large language models (LLMs) have attracted\nextensive interests of applying LLMs to medical field. However, the complex\nnature of clinical environments presents significant hallucination challenges\nfor LLMs, hindering their widespread adoption. In this paper, we address these\nhallucination issues in the context of Medical Information Extraction (MIE)\ntasks by introducing ALternate Contrastive Decoding (ALCD). We begin by\nredefining MIE tasks as an identify-and-classify process. We then separate the\nidentification and classification functions of LLMs by selectively masking the\noptimization of tokens during fine-tuning. During the inference stage, we\nalternately contrast output distributions derived from sub-task models. This\napproach aims to selectively enhance the identification and classification\ncapabilities while minimizing the influence of other inherent abilities in\nLLMs. Additionally, we propose an alternate adaptive constraint strategy to\nmore effectively adjust the scale and scope of contrastive tokens. Through\ncomprehensive experiments on two different backbones and six diverse medical\ninformation extraction tasks, ALCD demonstrates significant improvements in\nresolving hallucination issues compared to conventional decoding methods.\n","authors":["Derong Xu","Ziheng Zhang","Zhihong Zhu","Zhenxi Lin","Qidong Liu","Xian Wu","Tong Xu","Xiangyu Zhao","Yefeng Zheng","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15702v1.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.15700v1","updated":"2024-10-21T07:18:23Z","published":"2024-10-21T07:18:23Z","title":"InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert\n  Iteration on Large-Scale LEAN Problems","summary":"  Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. The\nmajor learning paradigm is expert iteration, which necessitates a pre-defined\ndataset comprising numerous mathematical problems. In this process, LLMs\nattempt to prove problems within the dataset and iteratively refine their\ncapabilities through self-training on the proofs they discover. We propose to\nuse large scale LEAN problem datasets Lean-workbook for expert iteration with\nmore than 20,000 CPU days. During expert iteration, we found log-linear trends\nbetween solved problem amount with proof length and CPU usage. We train a\ncritic model to select relatively easy problems for policy models to make\ntrials and guide the model to search for deeper proofs. InternLM2.5-StepProver\nachieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,\nand Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the\nMiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus\nwhich shows a significant improvement compared to only 9.5% of problems proved\nwhen Lean-Workbook-Plus was released. We open-source our models and searched\nproofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.\n","authors":["Zijian Wu","Suozhi Huang","Zhejian Zhou","Huaiyuan Ying","Jiayu Wang","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19170v2","updated":"2024-10-21T07:17:20Z","published":"2024-06-27T13:44:03Z","title":"The Illusion of Competence: Evaluating the Effect of Explanations on\n  Users' Mental Models of Visual Question Answering Systems","summary":"  We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's\nlimitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.\nHowever, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.\n","authors":["Judith Sieker","Simeon Junker","Ronja Utescher","Nazia Attari","Heiko Wersing","Hendrik Buschmeier","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2406.19170v2.pdf","comment":"17 pages (including Appendix). Accepted at EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2410.13699v2","updated":"2024-10-21T07:12:26Z","published":"2024-10-17T16:04:07Z","title":"Unconstrained Model Merging for Enhanced LLM Reasoning","summary":"  Recent advancements in building domain-specific large language models (LLMs)\nhave shown remarkable success, especially in tasks requiring reasoning\nabilities like logical inference over complex relationships and multi-step\nproblem solving. However, creating a powerful all-in-one LLM remains\nchallenging due to the need for proprietary data and vast computational\nresources. As a resource-friendly alternative, we explore the potential of\nmerging multiple expert models into a single LLM. Existing studies on model\nmerging mainly focus on generalist LLMs instead of domain experts, or the LLMs\nunder the same architecture and size. In this work, we propose an unconstrained\nmodel merging framework that accommodates both homogeneous and heterogeneous\nmodel architectures with a focus on reasoning tasks. A fine-grained layer-wise\nweight merging strategy is designed for homogeneous models merging, while\nheterogeneous model merging is built upon the probabilistic distribution\nknowledge derived from instruction-response fine-tuning data. Across 7\nbenchmarks and 9 reasoning-optimized LLMs, we reveal key findings that\ncombinatorial reasoning emerges from merging which surpasses simple additive\neffects. We propose that unconstrained model merging could serve as a\nfoundation for decentralized LLMs, marking a notable progression from the\nexisting centralized LLM framework. This evolution could enhance wider\nparticipation and stimulate additional advancement in the field of artificial\nintelligence, effectively addressing the constraints posed by centralized\nmodels.\n","authors":["Yiming Zhang","Baoyi He","Shengyu Zhang","Yuhao Fu","Qi Zhou","Zhijie Sang","Zijin Hong","Kejing Yang","Wenjun Wang","Jianbo Yuan","Guanghan Ning","Linyi Li","Chunlin Ji","Fei Wu","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13699v2.pdf","comment":"Under review, correct typos"},{"id":"http://arxiv.org/abs/2410.15696v1","updated":"2024-10-21T07:10:07Z","published":"2024-10-21T07:10:07Z","title":"Tokenization as Finite-State Transduction","summary":"  Tokenization is the first step in modern neural language model pipelines\nwhere an input text is converted to a sequence of subword tokens. We introduce\nfrom first principles a finite-state transduction framework which can\nefficiently encode all possible tokenizations of a regular language. We then\nconstructively show that Byte-Pair Encoding (BPE) and MaxMatch (WordPiece), two\npopular tokenization schemes, fit within this framework. For BPE, this is\nparticularly surprising given its resemblance to context-free grammar and the\nfact that it does not tokenize strings from left to right.\n  An application of this is to guided generation, where the outputs of a\nlanguage model are constrained to match some pattern. Here, patterns are\nencoded at the character level, which creates a mismatch between the\nconstraints and the model's subword vocabulary. While past work has focused\nonly on constraining outputs without regard to the underlying tokenization\nalgorithm, our framework allows for simultaneously constraining the model\noutputs to match a specified pattern while also adhering to the underlying\ntokenizer's canonical tokenization.\n","authors":["Marco Cognetta","Naoaki Okazaki"],"pdf_url":"https://arxiv.org/pdf/2410.15696v1.pdf","comment":"10 pages + 5 pages in appendix"},{"id":"http://arxiv.org/abs/2406.05392v2","updated":"2024-10-21T07:08:11Z","published":"2024-06-08T07:55:01Z","title":"Deconstructing The Ethics of Large Language Models from Long-standing\n  Issues to New-emerging Dilemmas: A Survey","summary":"  Large Language Models (LLMs) have achieved unparalleled success across\ndiverse language modeling tasks in recent years. However, this progress has\nalso intensified ethical concerns, impacting the deployment of LLMs in everyday\ncontexts. This paper provides a comprehensive survey of ethical challenges\nassociated with LLMs, from longstanding issues such as copyright infringement,\nsystematic bias, and data privacy, to emerging problems like truthfulness and\nsocial norms. We critically analyze existing research aimed at understanding,\nexamining, and mitigating these ethical risks. Our survey underscores\nintegrating ethical standards and societal values into the development of LLMs,\nthereby guiding the development of responsible and ethically aligned language\nmodels.\n","authors":["Chengyuan Deng","Yiqun Duan","Xin Jin","Heng Chang","Yijun Tian","Han Liu","Yichen Wang","Kuofeng Gao","Henry Peng Zou","Yiqiao Jin","Yijia Xiao","Shenghao Wu","Zongxing Xie","Weimin Lyu","Sihong He","Lu Cheng","Haohan Wang","Jun Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.05392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15690v1","updated":"2024-10-21T07:01:25Z","published":"2024-10-21T07:01:25Z","title":"Efficient Terminology Integration for LLM-based Translation in\n  Specialized Domains","summary":"  Traditional machine translation methods typically involve training models\ndirectly on large parallel corpora, with limited emphasis on specialized\nterminology. However, In specialized fields such as patent, finance, or\nbiomedical domains, terminology is crucial for translation, with many terms\nthat needs to be translated following agreed-upon conventions. In this paper we\nintroduce a methodology that efficiently trains models with a smaller amount of\ndata while preserving the accuracy of terminology translation. We achieve this\nthrough a systematic process of term extraction and glossary creation using the\nTrie Tree algorithm, followed by data reconstruction to teach the LLM how to\nintegrate these specialized terms. This methodology enhances the model's\nability to handle specialized terminology and ensures high-quality\ntranslations, particularly in fields where term consistency is crucial. Our\napproach has demonstrated exceptional performance, achieving the highest\ntranslation score among participants in the WMT patent task to date, showcasing\nits effectiveness and broad applicability in specialized translation domains\nwhere general methods often fall short.\n","authors":["Sejoon Kim","Mingi Sung","Jeonghwan Lee","Hyunkuk Lim","Jorge Froilan Gimenez Perez"],"pdf_url":"https://arxiv.org/pdf/2410.15690v1.pdf","comment":"Accepted to WMT 2024"},{"id":"http://arxiv.org/abs/2402.06900v4","updated":"2024-10-21T06:56:26Z","published":"2024-02-10T07:55:27Z","title":"Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric","summary":"  In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.\n","authors":["Hyukhun Koh","Dohyung Kim","Minwoo Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2402.06900v4.pdf","comment":"8 page long"},{"id":"http://arxiv.org/abs/2410.15687v1","updated":"2024-10-21T06:55:35Z","published":"2024-10-21T06:55:35Z","title":"DomainSum: A Hierarchical Benchmark for Fine-Grained Domain Shift in\n  Abstractive Text Summarization","summary":"  Most research on abstractive summarization focuses on single-domain\napplications, often neglecting how domain shifts between documents affect\nperformance and the generalization ability of summarization models. To address\nthis issue, we introduce DomainSum, a hierarchical benchmark designed to\ncapture fine-grained domain shifts in abstractive summarization. We categorize\nthese shifts into three levels: genre, style, and topic, and demonstrate\nthrough comprehensive benchmark analysis that they follow a hierarchical\nstructure. Furthermore, we evaluate the domain generalization capabilities of\ncommonly used pre-trained language models (PLMs) and large language models\n(LLMs) in in-domain and cross-domain settings.\n","authors":["Haohan Yuan","Haopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15678v1","updated":"2024-10-21T06:42:11Z","published":"2024-10-21T06:42:11Z","title":"Revealing and Mitigating the Local Pattern Shortcuts of Mamba","summary":"  Large language models (LLMs) have advanced significantly due to the attention\nmechanism, but their quadratic complexity and linear memory demands limit their\nperformance on long-context tasks. Recently, researchers introduced Mamba, an\nadvanced model built upon State Space Models(SSMs) that offers linear\ncomplexity and constant memory. Although Mamba is reported to match or surpass\nthe performance of attention-based models, our analysis reveals a performance\ngap: Mamba excels in tasks that involve localized key information but faces\nchallenges with tasks that require handling distributed key information. Our\ncontrolled experiments suggest that this inconsistency arises from Mamba's\nreliance on local pattern shortcuts, which enable the model to remember local\nkey information within its limited memory but hinder its ability to retain more\ndispersed information. Therefore, we introduce a global selection module into\nthe Mamba model to address this issue. Experiments on both existing and\nproposed synthetic tasks, as well as real-world tasks, demonstrate the\neffectiveness of our method. Notably, with the introduction of only 4M extra\nparameters, our approach enables the Mamba model(130M) to achieve a significant\nimprovement on tasks with distributed information, increasing its performance\nfrom 0 to 80.54 points.\n","authors":["Wangjie You","Zecheng Tang","Juntao Li","Lili Yao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02050v3","updated":"2024-10-21T06:33:13Z","published":"2024-06-04T07:31:06Z","title":"Analyzing Social Biases in Japanese Large Language Models","summary":"  With the development of Large Language Models (LLMs), social biases in the\nLLMs have become a crucial issue. While various benchmarks for social biases\nhave been provided across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, and analyze social biases in Japanese LLMs. The\nresults show that while current open Japanese LLMs improve their accuracies on\nJBBQ by setting larger parameters, their bias scores become larger. In\naddition, prompts with warnings about social biases and Chain-of-Thought\nprompting reduce the effect of biases in model outputs, but there is room for\nimprovement in the consistency of reasoning.\n","authors":["Hitomi Yanaka","Namgi Han","Ryoma Kumon","Jie Lu","Masashi Takeshita","Ryo Sekizawa","Taisei Kato","Hiromi Arai"],"pdf_url":"https://arxiv.org/pdf/2406.02050v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16756v2","updated":"2024-10-21T06:30:07Z","published":"2024-08-29T17:54:14Z","title":"How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities\n  of Large Language Models","summary":"  The rapid evolution of large language models (LLMs) has transformed the\ncompetitive landscape in natural language processing (NLP), particularly for\nEnglish and other data-rich languages. However, underrepresented languages like\nCantonese, spoken by over 85 million people, face significant development gaps,\nwhich is particularly concerning given the economic significance of the\nGuangdong-Hong Kong-Macau Greater Bay Area, and in substantial\nCantonese-speaking populations in places like Singapore and North America.\nDespite its wide use, Cantonese has scant representation in NLP research,\nespecially compared to other languages from similarly developed regions. To\nbridge these gaps, we outline current Cantonese NLP methods and introduce new\nbenchmarks designed to evaluate LLM performance in factual generation,\nmathematical logic, complex reasoning, and general knowledge in Cantonese,\nwhich aim to advance open-source Cantonese LLM technology. We also propose\nfuture research directions and recommended models to enhance Cantonese LLM\ndevelopment.\n","authors":["Jiyue Jiang","Pengan Chen","Liheng Chen","Sheng Wang","Qinghang Bao","Lingpeng Kong","Yu Li","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.16756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00608v2","updated":"2024-10-21T06:26:03Z","published":"2024-09-01T04:23:48Z","title":"TinyAgent: Function Calling at the Edge","summary":"  Recent large language models (LLMs) have enabled the development of advanced\nagentic systems that can integrate various tools and APIs to fulfill user\nqueries through function calling. However, the deployment of these LLMs on the\nedge has not been explored since they typically require cloud-based\ninfrastructure due to their substantial model size and computational demands.\nTo this end, we present TinyAgent, an end-to-end framework for training and\ndeploying task-specific small language model agents capable of function calling\nfor driving agentic systems at the edge. We first show how to enable accurate\nfunction calling for open-source models via the LLMCompiler framework. We then\nsystematically curate a high-quality dataset for function calling, which we use\nto fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient\ninference, we introduce a novel tool retrieval method to reduce the input\nprompt length and utilize quantization to further accelerate the inference\nspeed. As a driving application, we demonstrate a local Siri-like system for\nApple's MacBook that can execute user commands through text or voice input. Our\nresults show that our models can achieve, and even surpass, the\nfunction-calling capabilities of larger models like GPT-4-Turbo, while being\nfully deployed at the edge. We open-source our dataset, models, and installable\npackage and provide a demo video for our MacBook assistant agent.\n","authors":["Lutfi Eren Erdogan","Nicholas Lee","Siddharth Jha","Sehoon Kim","Ryan Tabrizi","Suhong Moon","Coleman Hooper","Gopala Anumanchipalli","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2409.00608v2.pdf","comment":"EMNLP 2024 Demo"},{"id":"http://arxiv.org/abs/2410.15669v1","updated":"2024-10-21T06:22:51Z","published":"2024-10-21T06:22:51Z","title":"Learning to Generate and Evaluate Fact-checking Explanations with\n  Transformers","summary":"  In an era increasingly dominated by digital platforms, the spread of\nmisinformation poses a significant challenge, highlighting the need for\nsolutions capable of assessing information veracity. Our research contributes\nto the field of Explainable Artificial Antelligence (XAI) by developing\ntransformer-based fact-checking models that contextualise and justify their\ndecisions by generating human-accessible explanations. Importantly, we also\ndevelop models for automatic evaluation of explanations for fact-checking\nverdicts across different dimensions such as \\texttt{(self)-contradiction},\n\\texttt{hallucination}, \\texttt{convincingness} and \\texttt{overall quality}.\nBy introducing human-centred evaluation methods and developing specialised\ndatasets, we emphasise the need for aligning Artificial Intelligence\n(AI)-generated explanations with human judgements. This approach not only\nadvances theoretical knowledge in XAI but also holds practical implications by\nenhancing the transparency, reliability and users' trust in AI-driven\nfact-checking systems. Furthermore, the development of our metric learning\nmodels is a first step towards potentially increasing efficiency and reducing\nreliance on extensive manual assessment. Based on experimental results, our\nbest performing generative model \\textsc{ROUGE-1} score of 47.77, demonstrating\nsuperior performance in generating fact-checking explanations, particularly\nwhen provided with high-quality evidence. Additionally, the best performing\nmetric learning model showed a moderately strong correlation with human\njudgements on objective dimensions such as \\texttt{(self)-contradiction and\n\\texttt{hallucination}, achieving a Matthews Correlation Coefficient (MCC) of\naround 0.7.}\n","authors":["Darius Feher","Abdullah Khered","Hao Zhang","Riza Batista-Navarro","Viktor Schlegel"],"pdf_url":"https://arxiv.org/pdf/2410.15669v1.pdf","comment":"Forthcoming in Engineering Applications of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2407.11417v2","updated":"2024-10-21T06:17:56Z","published":"2024-07-16T06:18:21Z","title":"SPINACH: SPARQL-Based Information Navigation for Challenging Real-World\n  Questions","summary":"  Large Language Models (LLMs) have led to significant improvements in the\nKnowledge Base Question Answering (KBQA) task. However, datasets used in KBQA\nstudies do not capture the true complexity of KBQA tasks. They either have\nsimple questions, use synthetically generated logical forms, or are based on\nsmall knowledge base (KB) schemas.\n  We introduce the SPINACH dataset, an expert-annotated KBQA dataset collected\nfrom discussions on Wikidata's \"Request a Query\" forum with 320\ndecontextualized question-SPARQL pairs. The complexity of these in-the-wild\nqueries calls for a KBQA system that can dynamically explore large and often\nincomplete schemas and reason about them, as it is infeasible to create a\ncomprehensive training dataset.\n  We also introduce an in-context learning KBQA agent, also called SPINACH,\nthat mimics how a human expert would write SPARQLs to handle challenging\nquestions. SPINACH achieves a new state of the art on the QALD-7, QALD-9 Plus\nand QALD-10 datasets by 31.0%, 27.0%, and 10.0% in $F_1$, respectively, and\ncoming within 1.6% of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On\nour new SPINACH dataset, the SPINACH agent outperforms all baselines, including\nthe best GPT-4-based KBQA agent, by at least 38.1% in $F_1$.\n","authors":["Shicheng Liu","Sina J. Semnani","Harold Triedman","Jialiang Xu","Isaac Dan Zhao","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2407.11417v2.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.15667v1","updated":"2024-10-21T06:11:38Z","published":"2024-10-21T06:11:38Z","title":"RAC: Efficient LLM Factuality Correction with Retrieval Augmentation","summary":"  Large Language Models (LLMs) exhibit impressive results across a wide range\nof natural language processing (NLP) tasks, yet they can often produce\nfactually incorrect outputs. This paper introduces a simple but effective\nlow-latency post-correction method, \\textbf{Retrieval Augmented Correction\n(RAC)}, aimed at enhancing the factual performance of LLMs without requiring\nadditional fine-tuning. Our method is general and can be used with any\ninstruction-tuned LLM, and has greatly reduced latency compared to prior\napproaches. RAC decomposes the LLM's output into atomic facts and applies a\nfine-grained verification and correction process with retrieved content to\nverify and correct the LLM-generated output. Our extensive experiments show\nthat RAC yields up to 30\\% improvements over state-of-the-art baselines across\ntwo popular factuality evaluation datasets, validating its efficacy and\nrobustness in both with and without the integration of Retrieval-Augmented\nGeneration (RAG) across different LLMs.\\footnote{Our code is at\n\\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}\n","authors":["Changmao Li","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2410.15667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15661v1","updated":"2024-10-21T06:03:49Z","published":"2024-10-21T06:03:49Z","title":"Scalable Data Ablation Approximations for Language Models through\n  Modular Training and Merging","summary":"  Training data compositions for Large Language Models (LLMs) can significantly\naffect their downstream performance. However, a thorough data ablation study\nexploring large sets of candidate data mixtures is typically prohibitively\nexpensive since the full effect is seen only after training the models; this\ncan lead practitioners to settle for sub-optimal data mixtures. We propose an\nefficient method for approximating data ablations which trains individual\nmodels on subsets of a training corpus and reuses them across evaluations of\ncombinations of subsets. In continued pre-training experiments, we find that,\ngiven an arbitrary evaluation set, the perplexity score of a single model\ntrained on a candidate set of data is strongly correlated with perplexity\nscores of parameter averages of models trained on distinct partitions of that\ndata. From this finding, we posit that researchers and practitioners can\nconduct inexpensive simulations of data ablations by maintaining a pool of\nmodels that were each trained on partitions of a large training corpus, and\nassessing candidate data mixtures by evaluating parameter averages of\ncombinations of these models. This approach allows for substantial improvements\nin amortized training efficiency -- scaling only linearly with respect to new\ndata -- by enabling reuse of previous training computation, opening new avenues\nfor improving model performance through rigorous, incremental data assessment\nand mixing.\n","authors":["Clara Na","Ian Magnusson","Ananya Harsh Jha","Tom Sherborne","Emma Strubell","Jesse Dodge","Pradeep Dasigi"],"pdf_url":"https://arxiv.org/pdf/2410.15661v1.pdf","comment":"EMNLP 2024. 17 pages"},{"id":"http://arxiv.org/abs/2410.15657v1","updated":"2024-10-21T05:51:51Z","published":"2024-10-21T05:51:51Z","title":"CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision\n  Large Language Models","summary":"  Human-object interaction (HOI) detection has seen advancements with Vision\nLanguage Models (VLMs), but these methods often depend on extensive manual\nannotations. Vision Large Language Models (VLLMs) can inherently recognize and\nreason about interactions at the image level but are computationally heavy and\nnot designed for instance-level HOI detection. To overcome these limitations,\nwe propose a Cross-Level HOI distillation (CL-HOI) framework, which distills\ninstance-level HOIs from VLLMs image-level understanding without the need for\nmanual annotations. Our approach involves two stages: context distillation,\nwhere a Visual Linguistic Translator (VLT) converts visual information into\nlinguistic form, and interaction distillation, where an Interaction Cognition\nNetwork (ICN) reasons about spatial, visual, and context relations. We design\ncontrastive distillation losses to transfer image-level context and interaction\nknowledge from the teacher to the student model, enabling instance-level HOI\ndetection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our\nCL-HOI surpasses existing weakly supervised methods and VLLM supervised\nmethods, showing its efficacy in detecting HOIs without manual labels.\n","authors":["Jianjun Gao","Chen Cai","Ruoyu Wang","Wenyang Liu","Kim-Hui Yap","Kratika Garg","Boon-Siew Han"],"pdf_url":"https://arxiv.org/pdf/2410.15657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15642v1","updated":"2024-10-21T05:08:18Z","published":"2024-10-21T05:08:18Z","title":"Resource-Efficient Medical Report Generation using Large Language Models","summary":"  Medical report generation is the task of automatically writing radiology\nreports for chest X-ray images. Manually composing these reports is a\ntime-consuming process that is also prone to human errors. Generating medical\nreports can therefore help reduce the burden on radiologists. In other words,\nwe can promote greater clinical automation in the medical domain. In this work,\nwe propose a new framework leveraging vision-enabled Large Language Models\n(LLM) for the task of medical report generation. We introduce a lightweight\nsolution that achieves better or comparative performance as compared to\nprevious solutions on the task of medical report generation. We conduct\nextensive experiments exploring different model sizes and enhancement\napproaches, such as prefix tuning to improve the text generation abilities of\nthe LLMs. We evaluate our approach on a prominent large-scale radiology report\ndataset - MIMIC-CXR. Our results demonstrate the capability of our\nresource-efficient framework to generate patient-specific reports with strong\nmedical contextual understanding and high precision.\n","authors":[" Abdullah","Ameer Hamza","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.15642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15641v1","updated":"2024-10-21T05:01:50Z","published":"2024-10-21T05:01:50Z","title":"SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical\n  Synthesis","summary":"  The increasing integration of large language models (LLMs) across various\nfields has heightened concerns about their potential to propagate dangerous\ninformation. This paper specifically explores the security vulnerabilities of\nLLMs within the field of chemistry, particularly their capacity to provide\ninstructions for synthesizing hazardous substances. We evaluate the\neffectiveness of several prompt injection attack methods, including\nred-teaming, explicit prompting, and implicit prompting. Additionally, we\nintroduce a novel attack technique named SMILES-prompting, which uses the\nSimplified Molecular-Input Line-Entry System (SMILES) to reference chemical\nsubstances. Our findings reveal that SMILES-prompting can effectively bypass\ncurrent safety mechanisms. These findings highlight the urgent need for\nenhanced domain-specific safeguards in LLMs to prevent misuse and improve their\npotential for positive social impact.\n","authors":["Aidan Wong","He Cao","Zijing Liu","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.15641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15639v1","updated":"2024-10-21T04:57:09Z","published":"2024-10-21T04:57:09Z","title":"Can Large Language Models Invent Algorithms to Improve Themselves?","summary":"  Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and evaluates model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. In mathematical reasoning tasks, Self-Developing not only creates\nmodels that surpass the seed model but also consistently outperforms models\ncreated using human-designed algorithms. Additionally, these LLM-discovered\nalgorithms demonstrate strong effectiveness, including transferability to\nout-of-domain models.\n","authors":["Yoichi Ishibashi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2410.15639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02355v2","updated":"2024-10-21T04:32:56Z","published":"2024-10-03T10:06:27Z","title":"AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models","summary":"  Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.\n","authors":["Junfeng Fang","Houcheng Jiang","Kun Wang","Yunshan Ma","Xiang Wang","Xiangnan He","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.02355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15633v1","updated":"2024-10-21T04:30:53Z","published":"2024-10-21T04:30:53Z","title":"Selecting Influential Samples for Long Context Alignment via Homologous\n  Models' Guidance and Contextual Awareness Measurement","summary":"  The expansion of large language models to effectively handle instructions\nwith extremely long contexts has yet to be fully investigated. The primary\nobstacle lies in constructing a high-quality long instruction-following dataset\ndevised for long context alignment. Existing studies have attempted to scale up\nthe available data volume by synthesizing long instruction-following samples.\nHowever, indiscriminately increasing the quantity of data without a\nwell-defined strategy for ensuring data quality may introduce low-quality\nsamples and restrict the final performance. To bridge this gap, we aim to\naddress the unique challenge of long-context alignment, i.e., modeling the\nlong-range dependencies for handling instructions and lengthy input contexts.\nWe propose GATEAU, a novel framework designed to identify the influential and\nhigh-quality samples enriched with long-range dependency relations by utilizing\ncrafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement\n(CAM). Specifically, HMG attempts to measure the difficulty of generating\ncorresponding responses due to the long-range dependencies, using the\nperplexity scores of the response from two homologous models with different\ncontext windows. Also, the role of CAM is to measure the difficulty of\nunderstanding the long input contexts due to long-range dependencies by\nevaluating whether the model's attention is focused on important segments.\nBuilt upon both proposed methods, we select the most challenging samples as the\ninfluential data to effectively frame the long-range dependencies, thereby\nachieving better performance of LLMs. Comprehensive experiments indicate that\nGATEAU effectively identifies samples enriched with long-range dependency\nrelations and the model trained on these selected samples exhibits better\ninstruction-following and long-context understanding capabilities.\n","authors":["Shuzheng Si","Haozhe Zhao","Gang Chen","Yunshui Li","Kangyang Luo","Chuancheng Lv","Kaikai An","Fanchao Qi","Baobao Chang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.15633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09603v4","updated":"2024-10-21T04:16:09Z","published":"2023-11-16T06:22:17Z","title":"Self-Contradictory Reasoning Evaluation and Detection","summary":"  In a plethora of recent work, large language models (LLMs) demonstrated\nimpressive reasoning ability, but many proposed downstream reasoning tasks only\nfocus on final answers. Two fundamental questions persist: 1) how consistent is\nthe reasoning, and 2) can models detect unreliable reasoning? In this paper, we\ninvestigate self-contradictory (Self-Contra) reasoning, where the model\nreasoning does not support its answers. To answer 1), we define and assess the\nSelf-Contra rate across three datasets and delve into finer-grained categories\nof Self-Contra reasoning. We find that LLMs often contradict themselves in\nreasoning tasks involving contextual information understanding or commonsense.\nThe model may generate correct answers by taking shortcuts in reasoning or\noverlooking contextual evidence, leading to compromised reasoning. For 2), we\ntask the state-of-the-art model GPT-4 with identifying Self-Contra reasoning\nand finer-grained fallacies. We find that finer-grained categories enhanced\ndetection can improve GPT-4's ability to detect Self-Contra. However, it is\nonly able to detect Self-Contra with a 52.2% F1 score, much lower compared to\n66.7% for humans. Our results indicate that current LLMs lack the robustness\nnecessary for reliable reasoning and we emphasize the urgent need for\nestablishing best practices in comprehensive reasoning evaluations beyond pure\nperformance-based metrics.\n","authors":["Ziyi Liu","Soumya Sanyal","Isabelle Lee","Yongkang Du","Rahul Gupta","Yang Liu","Jieyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.09603v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03744v2","updated":"2024-10-21T04:10:50Z","published":"2024-02-06T06:23:12Z","title":"INSIDE: LLMs' Internal States Retain the Power of Hallucination\n  Detection","summary":"  Knowledge hallucination have raised widespread concerns for the security and\nreliability of deployed LLMs. Previous efforts in detecting hallucinations have\nbeen employed at logit-level uncertainty estimation or language-level\nself-consistency evaluation, where the semantic information is inevitably lost\nduring the token-decoding procedure. Thus, we propose to explore the dense\nsemantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates\nfor halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular,\na simple yet effective \\textbf{EigenScore} metric is proposed to better\nevaluate responses' self-consistency, which exploits the eigenvalues of\nresponses' covariance matrix to measure the semantic consistency/diversity in\nthe dense embedding space. Furthermore, from the perspective of self-consistent\nhallucination detection, a test time feature clipping approach is explored to\ntruncate extreme activations in the internal states, which reduces\noverconfident generations and potentially benefits the detection of\noverconfident hallucinations. Extensive experiments and ablation studies are\nperformed on several popular LLMs and question-answering (QA) benchmarks,\nshowing the effectiveness of our proposal.\n","authors":["Chao Chen","Kai Liu","Ze Chen","Yi Gu","Yue Wu","Mingyuan Tao","Zhihang Fu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2402.03744v2.pdf","comment":"Accepted by ICLR-2024"},{"id":"http://arxiv.org/abs/2406.10786v2","updated":"2024-10-21T04:09:33Z","published":"2024-06-16T02:52:32Z","title":"Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems\n  at once","summary":"  Recent studies have proposed placing multiple problems in a single prompt to\nimprove input token utilization for a more efficient LLM inference. We call\nthis MPP, in contrast to conventional SPP that prompts an LLM with a single\nproblem at a time. While MPP has been shown to work comparably well or even\nbetter than SPP under few-shot settings, its zero-shot performance is\nunderexplored, which better reveals the innate multiple problem handling\ncapabilities of LLMs. To address that, we study the zero-shot MPP performance\nof various LLMs on 6 classification and 12 reasoning benchmarks and confirm\nthat LLMs are competent zero-shot multi-problem solvers. We also examine the\nconditions of effectiveness of zero-shot MPP and explore several model-level\nfactors that may enable MPP. We observe that LLMs consistently perform worse\nwith selecting indices of texts of a given class label and with multiple\nmixed-source reasoning problems, indicating a lack of true understanding. We\nalso find that instruction tuning is an important factor than enhances MPP.\n","authors":["Zhengxiang Wang","Jordan Kodner","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2406.10786v2.pdf","comment":"26 pages, 11 figures, 16 tables"},{"id":"http://arxiv.org/abs/2410.15625v1","updated":"2024-10-21T04:08:37Z","published":"2024-10-21T04:08:37Z","title":"Improving Parallel Program Performance Through DSL-Driven Code\n  Generation with LLM Optimizers","summary":"  Mapping computations to processors and assigning data to memory are critical\nfor maximizing performance in parallel programming. These mapping decisions are\nmanaged through the development of specialized low-level system code, called\nmappers, crafted by performance engineers. Each mapper is tailored to a\nspecific application and optimized for the underlying machine architecture, a\nprocess that requires days of refinement and tuning from an expert. Despite\nadvances in system research, automating mapper generation remains a challenge\ndue to the complexity of making millions of decisions to find the optimal\nsolution and generate the solution as code. We introduce an approach that\nleverages recent advances in LLM-based optimizers for mapper design. In under\nten minutes, our method automatically discovers mappers that surpass human\nexpert designs in scientific applications by up to 1.34X speedup. For parallel\nmatrix multiplication algorithms, our mapper achieves up to 1.31X of the\nexpert-designed solution. To achieve this, we simplify the complexity of\nlow-level code generation by introducing a domain-specific language (DSL) that\nabstracts the low-level system programming details and defines a structured\nsearch space for LLMs to explore. To maximize the application performance, we\nuse an LLM optimizer to improve an agentic system that generates the mapper\ncode. As a result, this approach significantly reduces the workload for\nperformance engineers while achieving substantial performance gains across\ndiverse applications. Finally, our results demonstrate the effectiveness of\nLLM-based optimization in system design and suggest its potential for\naddressing other complex system challenges.\n","authors":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2410.15625v1.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.15623v1","updated":"2024-10-21T04:08:16Z","published":"2024-10-21T04:08:16Z","title":"Guardians of Discourse: Evaluating LLMs on Multilingual Offensive\n  Language Detection","summary":"  Identifying offensive language is essential for maintaining safety and\nsustainability in the social media era. Though large language models (LLMs)\nhave demonstrated encouraging potential in social media analytics, they lack\nthorough evaluation when in offensive language detection, particularly in\nmultilingual environments. We for the first time evaluate multilingual\noffensive language detection of LLMs in three languages: English, Spanish, and\nGerman with three LLMs, GPT-3.5, Flan-T5, and Mistral, in both monolingual and\nmultilingual settings. We further examine the impact of different prompt\nlanguages and augmented translation data for the task in non-English contexts.\nFurthermore, we discuss the impact of the inherent bias in LLMs and the\ndatasets in the mispredictions related to sensitive topics.\n","authors":["Jianfei He","Lilin Wang","Jiaying Wang","Zhenyu Liu","Hongbin Na","Zimu Wang","Wei Wang","Qi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15623v1.pdf","comment":"Accepted at UIC 2024 proceedings. Accepted version"},{"id":"http://arxiv.org/abs/2409.08098v2","updated":"2024-10-21T04:02:23Z","published":"2024-09-12T14:51:43Z","title":"The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal","summary":"  This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.\n","authors":["Huiyuan Xie","Felix Steffek","Joana Ribeiro de Faria","Christine Carter","Jonathan Rutherford"],"pdf_url":"https://arxiv.org/pdf/2409.08098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10678v3","updated":"2024-10-21T03:55:36Z","published":"2022-12-20T22:41:24Z","title":"Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias","summary":"  Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias.\n","authors":["Yuen Chen","Vethavikashini Chithrra Raghuram","Justus Mattern","Rada Mihalcea","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2212.10678v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15620v1","updated":"2024-10-21T03:48:23Z","published":"2024-10-21T03:48:23Z","title":"Acoustic Model Optimization over Multiple Data Sources: Merging and\n  Valuation","summary":"  Due to the rising awareness of privacy protection and the voluminous scale of\nspeech data, it is becoming infeasible for Automatic Speech Recognition (ASR)\nsystem developers to train the acoustic model with complete data as before. For\nexample, the data may be owned by different curators, and it is not allowed to\nshare with others. In this paper, we propose a novel paradigm to solve salient\nproblems plaguing the ASR field. In the first stage, multiple acoustic models\nare trained based upon different subsets of the complete speech data, while in\nthe second phase, two novel algorithms are utilized to generate a high-quality\nacoustic model based upon those trained on data subsets. We first propose the\nGenetic Merge Algorithm (GMA), which is a highly specialized algorithm for\noptimizing acoustic models but suffers from low efficiency. We further propose\nthe SGD-Based Optimizational Merge Algorithm (SOMA), which effectively\nalleviates the efficiency bottleneck of GMA and maintains superior model\naccuracy. Extensive experiments on public data show that the proposed methods\ncan significantly outperform the state-of-the-art. Furthermore, we introduce\nShapley Value to estimate the contribution score of the trained models, which\nis useful for evaluating the effectiveness of the data and providing fair\nincentives to their curators.\n","authors":["Victor Junqiu Wei","Weicheng Wang","Di Jiang","Conghui Tan","Rongzhong Lian"],"pdf_url":"https://arxiv.org/pdf/2410.15620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13185v2","updated":"2024-10-21T03:36:05Z","published":"2024-10-17T03:26:37Z","title":"Chain of Ideas: Revolutionizing Research in Novel Idea Development with\n  LLM Agents","summary":"  Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.\n","authors":["Long Li","Weiwen Xu","Jiayan Guo","Ruochen Zhao","Xinxuan Li","Yuqian Yuan","Boqiang Zhang","Yuming Jiang","Yifei Xin","Ronghao Dang","Deli Zhao","Yu Rong","Tian Feng","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.13185v2.pdf","comment":"10 pages,5 figures, conference"},{"id":"http://arxiv.org/abs/2410.14180v2","updated":"2024-10-21T03:19:40Z","published":"2024-10-18T05:16:39Z","title":"XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting","summary":"  Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality.\n","authors":["Taha Aksu","Chenghao Liu","Amrita Saha","Sarah Tan","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.14180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15609v1","updated":"2024-10-21T03:13:22Z","published":"2024-10-21T03:13:22Z","title":"Interventional Speech Noise Injection for ASR Generalizable Spoken\n  Language Understanding","summary":"  Recently, pre-trained language models (PLMs) have been increasingly adopted\nin spoken language understanding (SLU). However, automatic speech recognition\n(ASR) systems frequently produce inaccurate transcriptions, leading to noisy\ninputs for SLU models, which can significantly degrade their performance. To\naddress this, our objective is to train SLU models to withstand ASR errors by\nexposing them to noises commonly observed in ASR systems, referred to as\nASR-plausible noises. Speech noise injection (SNI) methods have pursued this\nobjective by introducing ASR-plausible noises, but we argue that these methods\nare inherently biased towards specific ASR systems, or ASR-specific noises. In\nthis work, we propose a novel and less biased augmentation method of\nintroducing the noises that are plausible to any ASR system, by cutting off the\nnon-causal effect of noises. Experimental results and analyses demonstrate the\neffectiveness of our proposed methods in enhancing the robustness and\ngeneralizability of SLU models against unseen ASR systems by introducing more\ndiverse and plausible ASR noises in advance.\n","authors":["Yeonjoon Jung","Jaeseong Lee","Seungtaek Choi","Dohyeon Lee","Minsoo Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.15609v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.15608v1","updated":"2024-10-21T03:13:20Z","published":"2024-10-21T03:13:20Z","title":"Moonshine: Speech Recognition for Live Transcription and Voice Commands","summary":"  This paper introduces Moonshine, a family of speech recognition models\noptimized for live transcription and voice command processing. Moonshine is\nbased on an encoder-decoder transformer architecture and employs Rotary\nPosition Embedding (RoPE) instead of traditional absolute position embeddings.\nThe model is trained on speech segments of various lengths, but without using\nzero-padding, leading to greater efficiency for the encoder during inference\ntime. When benchmarked against OpenAI's Whisper tiny.en, Moonshine Tiny\ndemonstrates a 5x reduction in compute requirements for transcribing a\n10-second speech segment while incurring no increase in word error rates across\nstandard evaluation datasets. These results highlight Moonshine's potential for\nreal-time and resource-constrained applications.\n","authors":["Nat Jeffries","Evan King","Manjunath Kudlur","Guy Nicholson","James Wang","Pete Warden"],"pdf_url":"https://arxiv.org/pdf/2410.15608v1.pdf","comment":"7 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.03421v2","updated":"2024-10-21T02:43:50Z","published":"2024-10-04T13:31:09Z","title":"One2set + Large Language Model: Best Partners for Keyphrase Generation","summary":"  Keyphrase generation (KPG) aims to automatically generate a collection of\nphrases representing the core concepts of a given document. The dominant\nparadigms in KPG include one2seq and one2set. Recently, there has been\nincreasing interest in applying large language models (LLMs) to KPG. Our\npreliminary experiments reveal that it is challenging for a single model to\nexcel in both recall and precision. Further analysis shows that: 1) the one2set\nparadigm owns the advantage of high recall, but suffers from improper\nassignments of supervision signals during training; 2) LLMs are powerful in\nkeyphrase selection, but existing selection methods often make redundant\nselections. Given these observations, we introduce a generate-then-select\nframework decomposing KPG into two steps, where we adopt a one2set-based model\nas generator to produce candidates and then use an LLM as selector to select\nkeyphrases from these candidates. Particularly, we make two important\nimprovements on our generator and selector: 1) we design an Optimal\nTransport-based assignment strategy to address the above improper assignments;\n2) we model the keyphrase selection as a sequence labeling task to alleviate\nredundant selections. Experimental results on multiple benchmark datasets show\nthat our framework significantly surpasses state-of-the-art models, especially\nin absent keyphrase prediction.\n","authors":["Liangying Shao","Liang Zhang","Minlong Peng","Guoqi Ma","Hao Yue","Mingming Sun","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2410.03421v2.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.05021v3","updated":"2024-10-21T02:41:21Z","published":"2024-10-07T13:24:24Z","title":"DEPT: Decoupled Embeddings for Pre-training Language Models","summary":"  Language model pre-training benefits from diverse data to enhance performance\nacross domains and languages. However, training on such heterogeneous corpora\nrequires extensive and costly efforts. Since these data sources vary lexically,\nsyntactically, and semantically, they cause negative interference or the\n``curse of multilinguality''. We propose a novel pre-training framework to\nalleviate this curse. Our method, DEPT, decouples embeddings from the\ntransformer body while simultaneously training the latter in multiple contexts.\nDEPT enables training without a shared global vocabulary and: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces\ntoken embedding parameters by up to 80% and the communication costs by 675x for\nbillion-scale models, (3) enhances model generalization and plasticity in\nadapting to new languages and domains, and (4) permits training with custom\noptimized vocabularies per data source. We demonstrate DEPT's potential via the\nfirst vocabulary-agnostic federated multilingual pre-training of a 1.3\nbillion-parameter model, limiting its embedding size to 102.4 million instead\nof 512 million.\n","authors":["Alex Iacob","Lorenzo Sani","Meghdad Kurmanji","William F. Shen","Xinchi Qiu","Dongqi Cai","Yan Gao","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2410.05021v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15595v1","updated":"2024-10-21T02:27:24Z","published":"2024-10-21T02:27:24Z","title":"A Comprehensive Survey of Datasets, Theories, Variants, and Applications\n  in Direct Preference Optimization","summary":"  With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community.\n","authors":["Wenyi Xiao","Zechuan Wang","Leilei Gan","Shuai Zhao","Wanggui He","Luu Anh Tuan","Long Chen","Hao Jiang","Zhou Zhao","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15592v1","updated":"2024-10-21T02:21:56Z","published":"2024-10-21T02:21:56Z","title":"CPE-Pro: A Structure-Sensitive Deep Learning Model for Protein\n  Representation and Origin Evaluation","summary":"  Protein structures are important for understanding their functions and\ninteractions. Currently, many protein structure prediction methods are\nenriching the structure database. Discriminating the origin of structures is\ncrucial for distinguishing between experimentally resolved and computationally\npredicted structures, evaluating the reliability of prediction methods, and\nguiding downstream biological studies. Building on works in structure\nprediction, We developed a structure-sensitive supervised deep learning model,\nCrystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent\nand discriminate the origin of protein structures. CPE-Pro learns the\nstructural information of proteins and captures inter-structural differences to\nachieve accurate traceability on four data classes, and is expected to be\nextended to more. Simultaneously, we utilized Foldseek to encode protein\nstructures into \"structure-sequence\" and trained a protein Structural Sequence\nLanguage Model, SSLM. Preliminary experiments demonstrated that, compared to\nlarge-scale protein language models pre-trained on vast amounts of amino acid\nsequences, the \"structure-sequences\" enable the language model to learn more\ninformative protein features, enhancing and optimizing structural\nrepresentations. We have provided the code, model weights, and all related\nmaterials on https://github.com/GouWenrui/CPE-Pro-main.git.\n","authors":["Wenrui Gou","Wenhui Ge"," YangTan","Guisheng Fan","Mingchen Li","Huiqun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.15592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15591v1","updated":"2024-10-21T02:19:24Z","published":"2024-10-21T02:19:24Z","title":"AMPLE: Emotion-Aware Multimodal Fusion Prompt Learning for Fake News\n  Detection","summary":"  Detecting fake news in large datasets is challenging due to its diversity and\ncomplexity, with traditional approaches often focusing on textual features\nwhile underutilizing semantic and emotional elements. Current methods also rely\nheavily on large annotated datasets, limiting their effectiveness in more\nnuanced analysis. To address these challenges, this paper introduces\nEmotion-\\textbf{A}ware \\textbf{M}ultimodal Fusion \\textbf{P}rompt\n\\textbf{L}\\textbf{E}arning (\\textbf{AMPLE}) framework to address the above\nissue by combining text sentiment analysis with multimodal data and hybrid\nprompt templates. This framework extracts emotional elements from texts by\nleveraging sentiment analysis tools. It then employs Multi-Head Cross-Attention\n(MCA) mechanisms and similarity-aware fusion methods to integrate multimodal\ndata. The proposed AMPLE framework demonstrates strong performance on two\npublic datasets in both few-shot and data-rich settings, with results\nindicating the potential of emotional aspects in fake news detection.\nFurthermore, the study explores the impact of integrating large language models\nwith this method for text sentiment extraction, revealing substantial room for\nfurther improvement. The code can be found at\n:\\url{https://github.com/xxm1215/MMM2025_few-shot/\n","authors":["Xiaoman Xu","Xiangrun Li","Taihang Wang","Ye Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.15591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15580v1","updated":"2024-10-21T01:57:16Z","published":"2024-10-21T01:57:16Z","title":"Language Models are Symbolic Learners in Arithmetic","summary":"  Large Language Models (LLMs) are thought to struggle with arithmetic learning\ndue to the inherent differences between language modeling and numerical\ncomputation, but concrete evidence has been lacking. This work responds to this\nclaim through a two-side experiment. We first investigate whether LLMs leverage\npartial products during arithmetic learning. We find that although LLMs can\nidentify some partial products after learning, they fail to leverage them for\narithmetic tasks, conversely. We then explore how LLMs approach arithmetic\nsymbolically by breaking tasks into subgroups, hypothesizing that difficulties\narise from subgroup complexity and selection. Our results show that when\nsubgroup complexity is fixed, LLMs treat a collection of different arithmetic\noperations similarly. By analyzing position-level accuracy across different\ntraining sizes, we further observe that it follows a U-shaped pattern: LLMs\nquickly learn the easiest patterns at the first and last positions, while\nprogressively learning the more difficult patterns in the middle positions.\nThis suggests that LLMs select subgroup following an easy-to-hard paradigm\nduring learning. Our work confirms that LLMs are pure symbolic learners in\narithmetic tasks and underscores the importance of understanding them deeply\nthrough subgroup-level quantification.\n","authors":["Chunyuan Deng","Zhiqi Li","Roy Xie","Ruidi Chang","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15578v1","updated":"2024-10-21T01:55:52Z","published":"2024-10-21T01:55:52Z","title":"Generalized Probabilistic Attention Mechanism in Transformers","summary":"  The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.\n","authors":["DongNyeong Heo","Heeyoul Choi"],"pdf_url":"https://arxiv.org/pdf/2410.15578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15576v1","updated":"2024-10-21T01:54:46Z","published":"2024-10-21T01:54:46Z","title":"A Survey of Conversational Search","summary":"  As a cornerstone of modern information access, search engines have become\nindispensable in everyday life. With the rapid advancements in AI and natural\nlanguage processing (NLP) technologies, particularly large language models\n(LLMs), search engines have evolved to support more intuitive and intelligent\ninteractions between users and systems. Conversational search, an emerging\nparadigm for next-generation search engines, leverages natural language\ndialogue to facilitate complex and precise information retrieval, thus\nattracting significant attention. Unlike traditional keyword-based search\nengines, conversational search systems enhance user experience by supporting\nintricate queries, maintaining context over multi-turn interactions, and\nproviding robust information integration and processing capabilities. Key\ncomponents such as query reformulation, search clarification, conversational\nretrieval, and response generation work in unison to enable these sophisticated\ninteractions. In this survey, we explore the recent advancements and potential\nfuture directions in conversational search, examining the critical modules that\nconstitute a conversational search system. We highlight the integration of LLMs\nin enhancing these systems and discuss the challenges and opportunities that\nlie ahead in this dynamic field. Additionally, we provide insights into\nreal-world applications and robust evaluations of current conversational search\nsystems, aiming to guide future research and development in conversational\nsearch.\n","authors":["Fengran Mo","Kelong Mao","Ziliang Zhao","Hongjin Qian","Haonan Chen","Yiruo Cheng","Xiaoxi Li","Yutao Zhu","Zhicheng Dou","Jian-Yun Nie"],"pdf_url":"https://arxiv.org/pdf/2410.15576v1.pdf","comment":"35 pages, 8 figures, continue to update"},{"id":"http://arxiv.org/abs/2410.15575v1","updated":"2024-10-21T01:50:59Z","published":"2024-10-21T01:50:59Z","title":"Neural Search Space in Gboard Decoder","summary":"  Gboard Decoder produces suggestions by looking for paths that best match\ninput touch points on the context aware search space, which is backed by the\nlanguage Finite State Transducers (FST). The language FST is currently an\nN-gram language model (LM). However, N-gram LMs, limited in context length, are\nknown to have sparsity problem under device model size constraint. In this\npaper, we propose \\textbf{Neural Search Space} which substitutes the N-gram LM\nwith a Neural Network LM (NN-LM) and dynamically constructs the search space\nduring decoding. Specifically, we integrate the long range context awareness of\nNN-LM into the search space by converting its outputs given context, into the\nlanguage FST at runtime. This involves language FST structure redesign, pruning\nstrategy tuning, and data structure optimizations. Online experiments\ndemonstrate improved quality results, reducing Words Modified Ratio by [0.26\\%,\n1.19\\%] on various locales with acceptable latency increases. This work opens\nnew avenues for further improving keyboard decoding quality by enhancing neural\nLM more directly.\n","authors":["Yanxiang Zhang","Yuanbo Zhang","Haicheng Sun","Yun Wang","Billy Dou","Gary Sivek","Shumin Zhai"],"pdf_url":"https://arxiv.org/pdf/2410.15575v1.pdf","comment":"10 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.15573v1","updated":"2024-10-21T01:36:42Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v1.pdf","comment":"Resources: https://github.com/mzhaojp22/openmu"},{"id":"http://arxiv.org/abs/2410.15572v1","updated":"2024-10-21T01:36:08Z","published":"2024-10-21T01:36:08Z","title":"Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka\n  Chatbots: Design Insights and User Perceptions","summary":"  In an era where cultural preservation is increasingly intertwined with\ntechnological innovation, this study introduces a groundbreaking approach to\npromoting and safeguarding the rich heritage of Taiwanese Hakka culture through\nthe development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.\nTraditional large language models (LLMs), while powerful, often fall short in\ndelivering accurate and contextually rich responses, particularly in culturally\nspecific domains. By integrating external databases with generative AI models,\nRAG technology bridges this gap, empowering chatbots to not only provide\nprecise answers but also resonate deeply with the cultural nuances that are\ncrucial for authentic interactions. This study delves into the intricate\nprocess of augmenting the chatbot's knowledge base with targeted cultural data,\nspecifically curated to reflect the unique aspects of Hakka traditions,\nlanguage, and practices. Through dynamic information retrieval, the\nRAG-enhanced chatbot becomes a versatile tool capable of handling complex\ninquiries that demand an in-depth understanding of Hakka cultural context. This\nis particularly significant in an age where digital platforms often dilute\ncultural identities, making the role of culturally aware AI systems more\ncritical than ever. System usability studies conducted as part of our research\nreveal a marked improvement in both user satisfaction and engagement,\nhighlighting the chatbot's effectiveness in fostering a deeper connection with\nHakka culture. The feedback underscores the potential of RAG technology to not\nonly enhance user experience but also to serve as a vital instrument in the\nbroader mission of ethnic mainstreaming and cultural celebration.\n","authors":["Chen-Chi Chang","Han-Pi Chang","Hung-Shin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.15572v1.pdf","comment":"Accepted to IEEE RASSE 2024"},{"id":"http://arxiv.org/abs/2410.15570v1","updated":"2024-10-21T01:27:29Z","published":"2024-10-21T01:27:29Z","title":"Stacking Small Language Models for Generalizability","summary":"  Recent advances show that large language models (LLMs) generalize strong\nperformance across different natural language benchmarks. However, the large\nsize of LLMs makes training and inference expensive and impractical to run in\nresource-limited settings. This paper introduces a new approach called\nfine-tuning stacks of language models (FSLM), which involves stacking small\nlanguage models (SLM) as an alternative to LLMs. By fine-tuning each SLM to\nperform a specific task, this approach breaks down high level reasoning into\nmultiple lower-level steps that specific SLMs are responsible for. As a result,\nFSLM allows for lower training and inference costs, and also improves model\ninterpretability as each SLM communicates with the subsequent one through\nnatural language. By evaluating FSLM on common natural language benchmarks,\nthis paper highlights promising early results toward generalizable performance\nusing FSLM as a cost-effective alternative to LLMs.\n","authors":["Laurence Liang"],"pdf_url":"https://arxiv.org/pdf/2410.15570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15567v1","updated":"2024-10-21T01:23:34Z","published":"2024-10-21T01:23:34Z","title":"Pruning Foundation Models for High Accuracy without Retraining","summary":"  Despite the superior performance, it is challenging to deploy foundation\nmodels or large language models (LLMs) due to their massive parameters and\ncomputations. While pruning is a promising technique to reduce model size and\naccelerate the inference, the traditional pruning techniques can hardly be\napplied for LLMs as they need to finetune the model on the full dataset with\nmultiple epochs consuming massive data and hardware resources. To deal with\nthis problem, post-training pruning methods are proposed to prune LLMs in\none-shot without retraining. However, their accuracy after pruning may suffer\nfrom certain performance degradation due to the lack of retraining with massive\ndata. To address this issue, in this paper, we first formulate the\npost-training problem for layer-wise LLM compression to simultaneously prune\nmultiple weights in LLMs. Next, we provide an optimal solution for this problem\nand design our post-training pruning algorithm for both unstructured and\nsemi-structured sparsity. Our extensive experiments demonstrate the superior\nperformance of the proposed methods in comparison to SOTA baselines across\nvarious LLM families including transformer-based LLMs and Mamba-based LLMs.\nCode link: https://github.com/piuzha/APT\n","authors":["Pu Zhao","Fei Sun","Xuan Shen","Pinrui Yu","Zhenglun Kong","Yanzhi Wang","Xue Lin"],"pdf_url":"https://arxiv.org/pdf/2410.15567v1.pdf","comment":"Accepted by EMNLP 2024 findings"},{"id":"http://arxiv.org/abs/2410.14211v2","updated":"2024-10-21T01:22:16Z","published":"2024-10-18T06:57:19Z","title":"Paths-over-Graph: Knowledge Graph Empowered Large Language Model\n  Reasoning","summary":"  Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.\n","authors":["Xingyu Tan","Xiaoyang Wang","Qing Liu","Xiwei Xu","Xin Yuan","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09662v2","updated":"2024-10-21T23:58:45Z","published":"2024-06-14T02:21:53Z","title":"Learning Language Structures through Grounding","summary":"  Language is highly structured, with syntactic and semantic structures, to\nsome extent, agreed upon by speakers of the same language. With implicit or\nexplicit awareness of such structures, humans can learn and use language\nefficiently and generalize to sentences that contain unseen words. Motivated by\nhuman language learning, in this dissertation, we consider a family of machine\nlearning tasks that aim to learn language structures through grounding. We seek\ndistant supervision from other data sources (i.e., grounds), including but not\nlimited to other modalities (e.g., vision), execution results of programs, and\nother languages.\n  We demonstrate the potential of this task formulation and advocate for its\nadoption through three schemes. In Part I, we consider learning syntactic\nparses through visual grounding. We propose the task of visually grounded\ngrammar induction, present the first models to induce syntactic structures from\nvisually grounded text and speech, and find that the visual grounding signals\ncan help improve the parsing quality over language-only models. As a side\ncontribution, we propose a novel evaluation metric that enables the evaluation\nof speech parsing without text or automatic speech recognition systems\ninvolved. In Part II, we propose two execution-aware methods to map sentences\ninto corresponding semantic structures (i.e., programs), significantly\nimproving compositional generalization and few-shot program synthesis. In Part\nIII, we propose methods that learn language structures from annotations in\nother languages. Specifically, we propose a method that sets a new state of the\nart on cross-lingual word alignment. We then leverage the learned word\nalignments to improve the performance of zero-shot cross-lingual dependency\nparsing, by proposing a novel substructure-based projection method that\npreserves structural knowledge learned from the source language.\n","authors":["Freda Shi"],"pdf_url":"https://arxiv.org/pdf/2406.09662v2.pdf","comment":"Ph.D. Thesis"},{"id":"http://arxiv.org/abs/2410.13804v3","updated":"2024-10-21T23:37:48Z","published":"2024-10-17T17:41:15Z","title":"BenTo: Benchmark Task Reduction with In-Context Transferability","summary":"  Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.\n","authors":["Hongyu Zhao","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13804v3.pdf","comment":"https://github.com/tianyi-lab/bento"},{"id":"http://arxiv.org/abs/2405.18400v5","updated":"2024-10-21T22:56:06Z","published":"2024-05-28T17:40:48Z","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass","summary":"  Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.\n","authors":["Ethan Shen","Alan Fan","Sarah M. Pratt","Jae Sung Park","Matthew Wallingford","Sham M. Kakade","Ari Holtzman","Ranjay Krishna","Ali Farhadi","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2405.18400v5.pdf","comment":"23 pages, 16 figures, accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16560v1","updated":"2024-10-21T22:39:52Z","published":"2024-10-21T22:39:52Z","title":"Raising the Stakes: Performance Pressure Improves AI-Assisted Decision\n  Making","summary":"  AI systems are used in many domains to assist with decision making, and\nalthough the potential for AI systems to assist with decision making is much\ndiscussed, human-AI collaboration often underperforms. Investigation into why\nthe performance potential is not realized has revealed many factors, including\n(mis)trust in the AI system and mental models of AI capabilities on subjective\ntasks. Performance pressure is known to influence human decision making\nbehavior, yet how it interacts with human-AI decision making is understudied.\nIn this work, we show the effects of performance pressure on AI advice reliance\nwhen laypeople (Amazon Mechanical Turk crowdworkers) complete a common\nAI-assisted task (fake review detection) and thus have inherently low\nperformance pressure. We manipulate performance pressure by leveraging people's\nloss aversion towards potential monetary gains when completing a task. We find\nthat when the stakes are high, people use AI advice more appropriately than\nwhen stakes are lower, regardless of the presence of an AI explanation.\nFurthermore, when the AI system gives incorrect advice, people correctly\ndiscount the poor advice more often when the stakes are higher than when they\nare lower. We conclude by discussing the implications of how performance\npressure influences AI-assisted decision making and encourage future research\nto incorporate performance pressure analysis.\n","authors":["Nikita Haduong","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2410.16560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16540v1","updated":"2024-10-21T22:07:20Z","published":"2024-10-21T22:07:20Z","title":"A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and\n  Error-Aware Demonstration","summary":"  Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance\nin improving the reasoning capabilities of large language models (LLMs). While\ntheoretical investigations have been conducted to understand CoT, the\nunderlying transformer used in these studies isolates the CoT reasoning process\ninto separated in-context learning steps (Stepwise ICL). In this work, we\ntheoretically show that, compared to Stepwise ICL, the transformer gains better\nerror correction ability and more accurate predictions if the reasoning from\nearlier steps (Coherent CoT) is integrated. Given that this coherent reasoning\nchanges the behavior of the transformer, we further investigate the sensitivity\nof the transformer with Coherent CoT when the demonstration examples are\ncorrupted at the inference stage. Our theoretical results indicate that the\ntransformer is more sensitive to errors in intermediate reasoning steps than\nthe final outcome. Building upon this observation, we propose an improvement on\nCoT by incorporating both correct and incorrect reasoning paths in the\ndemonstration. Our experiments validate the effectiveness of the proposed\napproach.\n","authors":["Yingqian Cui","Pengfei He","Xianfeng Tang","Qi He","Chen Luo","Jiliang Tang","Yue Xing"],"pdf_url":"https://arxiv.org/pdf/2410.16540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11627v2","updated":"2024-10-21T21:49:33Z","published":"2024-10-15T14:14:19Z","title":"Tokenization and Morphology in Multilingual Language Models: A\n  Comparative Analysis of mT5 and ByT5","summary":"  Morphology is a crucial factor for multilingual language modeling as it poses\ndirect challenges for tokenization. Here, we seek to understand how\ntokenization influences the morphological knowledge encoded in multilingual\nlanguage models. Specifically, we capture the impact of tokenization by\ncontrasting two multilingual language models: mT5 and ByT5. The two models\nshare the same architecture, training objective, and training data and only\ndiffer in their tokenization strategies: subword tokenization vs.\\@\ncharacter-level tokenization. Probing the morphological knowledge encoded in\nthese models on four tasks and 17 languages, our analyses show that the models\nlearn the morphological systems of some languages better than others and that\nmorphological information is encoded in the middle and late layers. Finally, we\nshow that languages with more irregularities benefit more from having a higher\nshare of the pre-training data.\n","authors":["Thao Anh Dang","Limor Raviv","Lukas Galke"],"pdf_url":"https://arxiv.org/pdf/2410.11627v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.16533v1","updated":"2024-10-21T21:48:24Z","published":"2024-10-21T21:48:24Z","title":"Large Body Language Models","summary":"  As virtual agents become increasingly prevalent in human-computer\ninteraction, generating realistic and contextually appropriate gestures in\nreal-time remains a significant challenge. While neural rendering techniques\nhave made substantial progress with static scripts, their applicability to\nhuman-computer interactions remains limited. To address this, we introduce\nLarge Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM\narchitecture that combines a Transformer-XL large language model with a\nparallelized diffusion model to generate human-like gestures from multimodal\ninputs (text, audio, and video). LBLM-AVA incorporates several key components\nenhancing its gesture generation capabilities, such as multimodal-to-pose\nembeddings, enhanced sequence-to-sequence mapping with redefined attention\nmechanisms, a temporal smoothing module for gesture sequence coherence, and an\nattention-based refinement module for enhanced realism. The model is trained on\nour large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves\nstate-of-the-art performance in generating lifelike and contextually\nappropriate gestures with a 30% reduction in Fr\\'echet Gesture Distance (FGD),\nand a 25% improvement in Fr\\'echet Inception Distance compared to existing\napproaches.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16531v1","updated":"2024-10-21T21:45:22Z","published":"2024-10-21T21:45:22Z","title":"Bayesian scaling laws for in-context learning","summary":"  In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety.\n","authors":["Aryaman Arora","Dan Jurafsky","Christopher Potts","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2410.16531v1.pdf","comment":"10 pages main text, 26 pages total"},{"id":"http://arxiv.org/abs/2410.14668v2","updated":"2024-10-21T21:42:46Z","published":"2024-10-18T17:57:40Z","title":"MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image\n  Description and Reasoning Steps","summary":"  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.\n","authors":["Xiongtao Zhou","Jie He","Lanyu Chen","Jingyu Li","Haojing Chen","Victor Gutierrez Basulto","Jeff Z. Pan","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14668v2.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2406.08598v2","updated":"2024-10-21T21:32:51Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","summary":"  As Large Language Models (LLMs) continue to evolve, the search for efficient\nand meaningful evaluation methods is ongoing. Many recent evaluations use LLMs\nas judges to score outputs from other LLMs, often relying on a single large\nmodel like GPT-4o. However, using a single LLM judge is prone to intra-model\nbias, and many tasks - such as those related to emotional intelligence,\ncreative writing, and persuasiveness - may be too subjective for a single model\nto judge fairly. We introduce the Language Model Council (LMC), where a group\nof LLMs collaborate to create tests, respond to them, and evaluate each other's\nresponses to produce a ranking in a democratic fashion. Unlike previous\napproaches that focus on reducing cost or bias by using a panel of smaller\nmodels, our work examines the benefits and nuances of a fully inclusive LLM\nevaluation system. In a detailed case study on emotional intelligence, we\ndeploy a council of 20 recent LLMs to rank each other on open-ended responses\nto interpersonal conflicts. Our results show that the LMC produces rankings\nthat are more separable and more robust, and through a user study, we show that\nthey are more consistent with human evaluations than any individual LLM judge.\nUsing all LLMs for judging can be costly, however, so we use Monte Carlo\nsimulations and hand-curated sub-councils to study hypothetical council\ncompositions and discuss the value of the incremental LLM judge.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Benjie Genchel","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16520v1","updated":"2024-10-21T21:21:29Z","published":"2024-10-21T21:21:29Z","title":"AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context","summary":"  As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.\n","authors":["Naba Rizvi","Harper Strickland","Daniel Gitelman","Tristan Cooper","Alexis Morales-Flores","Michael Golden","Aekta Kallepalli","Akshat Alurkar","Haaset Owens","Saleha Ahmedi","Isha Khirwadkar","Imani Munyaka","Nedjma Ousidhoum"],"pdf_url":"https://arxiv.org/pdf/2410.16520v1.pdf","comment":"9 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2405.13274v2","updated":"2024-10-21T21:09:22Z","published":"2024-05-22T01:10:39Z","title":"DiffNorm: Self-Supervised Normalization for Non-autoregressive\n  Speech-to-speech Translation","summary":"  Non-autoregressive Transformers (NATs) are recently applied in direct\nspeech-to-speech translation systems, which convert speech across different\nlanguages without intermediate text data. Although NATs generate high-quality\noutputs and offer faster inference than autoregressive models, they tend to\nproduce incoherent and repetitive results due to complex data distribution\n(e.g., acoustic and linguistic variations in speech). In this work, we\nintroduce DiffNorm, a diffusion-based normalization strategy that simplifies\ndata distributions for training NAT models. After training with a\nself-supervised noise estimation objective, DiffNorm constructs normalized\ntarget data by denoising synthetically corrupted speech features. Additionally,\nwe propose to regularize NATs with classifier-free guidance, improving model\nrobustness and translation quality by randomly dropping out source information\nduring training. Our strategies result in a notable improvement of about +7\nASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr)\ntranslations on the CVSS benchmark, while attaining over 14x speedup for En-Es\nand 5x speedup for En-Fr translations compared to autoregressive baselines.\n","authors":["Weiting Tan","Jingyu Zhang","Lingfeng Shen","Daniel Khashabi","Philipp Koehn"],"pdf_url":"https://arxiv.org/pdf/2405.13274v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16509v1","updated":"2024-10-21T21:00:47Z","published":"2024-10-21T21:00:47Z","title":"Learning from others' mistakes: Finetuning machine translation models\n  with span-level error annotations","summary":"  Despite growing interest in incorporating feedback to improve language\nmodels, most efforts focus only on sequence-level annotations. In this work, we\nexplore the potential of utilizing fine-grained span-level annotations from\noffline datasets to improve model quality. We develop a simple finetuning\nalgorithm, called Training with Annotations (TWA), to directly train machine\ntranslation models on such annotated data. TWA utilizes targeted span-level\nerror information while also flexibly learning what to penalize within a span.\nMoreover, TWA considers the overall trajectory of a sequence when deciding\nwhich non-error spans to utilize as positive signals. Experiments on\nEnglish-German and Chinese-English machine translation show that TWA\noutperforms baselines such as Supervised FineTuning on sequences filtered for\nquality and Direct Preference Optimization on pairs constructed from the same\ndata.\n","authors":["Lily H. Zhang","Hamid Dadkhahi","Mara Finkelstein","Firas Trabelsi","Jiaming Luo","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2410.16509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11414v2","updated":"2024-10-21T20:58:43Z","published":"2024-02-18T01:03:25Z","title":"Fine-grained and Explainable Factuality Evaluation for Multimodal\n  Summarization","summary":"  Multimodal summarization aims to generate a concise summary based on the\ninput text and image. However, the existing methods potentially suffer from\nunfactual output. To evaluate the factuality of multimodal summarization\nmodels, we propose two fine-grained and explainable evaluation frameworks\n(FALLACIOUS) for different application scenarios, i.e. reference-based\nfactuality evaluation framework and reference-free factuality evaluation\nframework. Notably, the reference-free factuality evaluation framework doesn't\nneed ground truth and hence it has a wider application scenario. To evaluate\nthe effectiveness of the proposed frameworks, we compute the correlation\nbetween our frameworks and the other metrics. The experimental results show the\neffectiveness of our proposed method. We will release our code and dataset via\ngithub.\n","authors":["Yue Zhang","Jingxuan Zuo","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2402.11414v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16503v1","updated":"2024-10-21T20:50:51Z","published":"2024-10-21T20:50:51Z","title":"Allo-AVA: A Large-Scale Multimodal Conversational AI Dataset for\n  Allocentric Avatar Gesture Animation","summary":"  The scarcity of high-quality, multimodal training data severely hinders the\ncreation of lifelike avatar animations for conversational AI in virtual\nenvironments. Existing datasets often lack the intricate synchronization\nbetween speech, facial expressions, and body movements that characterize\nnatural human communication. To address this critical gap, we introduce\nAllo-AVA, a large-scale dataset specifically designed for text and audio-driven\navatar gesture animation in an allocentric (third person point-of-view)\ncontext. Allo-AVA consists of $\\sim$1,250 hours of diverse video content,\ncomplete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely\nmaps these keypoints to precise timestamps, enabling accurate replication of\nhuman movements (body and facial gestures) in synchronization with speech. This\ncomprehensive resource enables the development and evaluation of more natural,\ncontext-aware avatar animation models, potentially transforming applications\nranging from virtual reality to digital assistants.\n","authors":["Saif Punjwani","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2410.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16502v1","updated":"2024-10-21T20:48:16Z","published":"2024-10-21T20:48:16Z","title":"Rulebreakers Challenge: Revealing a Blind Spot in Large Language Models'\n  Reasoning with Formal Logic","summary":"  Formal logic has long been applied to natural language reasoning, but this\napproach can sometimes lead to conclusions that, while logically entailed, are\nfactually inconsistent with the premises or are not typically inferred by\nhumans. This study introduces the concept of \"rulebreakers\", which refers to\ninstances where logical entailment diverges from factually acceptable\ninference. We present RULEBREAKERS, a novel dataset for evaluating Large\nLanguage Models' (LLMs) ability to distinguish between rulebreakers and\nnon-rulebreakers. Focusing on modus tollens and disjunctive syllogism, we\nassess six state-of-the-art LLMs using RULEBREAKERS, measuring their\nperformance in terms of token-level exact accuracy and model confidence. Our\nfindings reveal that while most models perform poorly to moderately in\nrecognizing rulebreakers, they demonstrate a latent ability to distinguish\nrulebreakers when assessed by their confidence levels. Further analysis\nsuggests that the failure to recognize rulebreakers is potentially associated\nwith the models' world knowledge and their attention distribution patterns.\nThis research highlights the limitation of LLMs' reasoning capabilities, and\ncontributes to the ongoing discussion on reasoning in LLMs.\n","authors":["Jason Chan","Robert Gaizauskas","Zhixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16502v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.16498v1","updated":"2024-10-21T20:41:00Z","published":"2024-10-21T20:41:00Z","title":"Natural Language Processing for Human Resources: A Survey","summary":"  The domain of human resources (HR) includes a broad spectrum of tasks related\nto natural language processing (NLP) techniques. Recent breakthroughs in NLP\nhave generated significant interest in its industrial applications in this\ndomain and potentially alleviate challenges such as the difficulty of resource\nacquisition and the complexity of problems. At the same time, the HR domain can\nalso present unique challenges that drive state-of-the-art in NLP research. To\nsupport this, we provide NLP researchers and practitioners with an overview of\nkey HR tasks from an NLP perspective, illustrating how specific sub-tasks\n(e.g., skill extraction) contribute to broader objectives (e.g., job matching).\nThrough this survey, we identify opportunities in NLP for HR and suggest\ndirections for future exploration.\n","authors":["Naoki Otani","Nikita Bhutani","Estevam Hruschka"],"pdf_url":"https://arxiv.org/pdf/2410.16498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16491v1","updated":"2024-10-21T20:32:27Z","published":"2024-10-21T20:32:27Z","title":"BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded\n  Data","summary":"  In this work, we tackle the challenge of embedding realistic human\npersonality traits into LLMs. Previous approaches have primarily focused on\nprompt-based methods that describe the behavior associated with the desired\npersonality traits, suffering from realism and validity issues. To address\nthese limitations, we introduce BIG5-CHAT, a large-scale dataset containing\n100,000 dialogues designed to ground models in how humans express their\npersonality in text. Leveraging this dataset, we explore Supervised Fine-Tuning\nand Direct Preference Optimization as training-based methods to align LLMs more\nnaturally with human personality patterns. Our methods outperform prompting on\npersonality assessments such as BFI and IPIP-NEO, with trait correlations more\nclosely matching human data. Furthermore, our experiments reveal that models\ntrained to exhibit higher conscientiousness, higher agreeableness, lower\nextraversion, and lower neuroticism display better performance on reasoning\ntasks, aligning with psychological findings on how these traits impact human\ncognitive performance. To our knowledge, this work is the first comprehensive\nstudy to demonstrate how training-based methods can shape LLM personalities\nthrough learning from real human behaviors.\n","authors":["Wenkai Li","Jiarui Liu","Andy Liu","Xuhui Zhou","Mona Diab","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2410.16491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16473v1","updated":"2024-10-21T20:01:06Z","published":"2024-10-21T20:01:06Z","title":"Multi-head Sequence Tagging Model for Grammatical Error Correction","summary":"  To solve the Grammatical Error Correction (GEC) problem , a mapping between a\nsource sequence and a target one is needed, where the two differ only on few\nspans. For this reason, the attention has been shifted to the\nnon-autoregressive or sequence tagging models. In which, the GEC has been\nsimplified from Seq2Seq to labeling the input tokens with edit commands chosen\nfrom a large edit space. Due to this large number of classes and the limitation\nof the available datasets, the current sequence tagging approaches still have\nsome issues handling a broad range of grammatical errors just by being\nlaser-focused on one single task. To this end, we simplified the GEC further by\ndividing it into seven related subtasks: Insertion, Deletion, Merge,\nSubstitution, Transformation, Detection, and Correction, with Correction being\nour primary focus. A distinct classification head is dedicated to each of these\nsubtasks. the novel multi-head and multi-task learning model is proposed to\neffectively utilize training data and harness the information from related task\ntraining signals. To mitigate the limited number of available training samples,\na new denoising autoencoder is used to generate a new synthetic dataset to be\nused for pretraining. Additionally, a new character-level transformation is\nproposed to enhance the sequence-to-edit function and improve the model's\nvocabulary coverage. Our single/ensemble model achieves an F0.5 of 74.4/77.0,\nand 68.6/69.1 on BEA-19 (test) and CoNLL-14 (test) respectively. Moreover,\nevaluated on JFLEG test set, the GLEU scores are 61.6 and 61.7 for the single\nand ensemble models, respectively. It mostly outperforms recently published\nstate-of-the-art results by a considerable margin.\n","authors":["Kamal Al-Sabahi","Kang Yang","Wangwang Liu","Guanyu Jiang","Xian Li","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16472v1","updated":"2024-10-21T19:59:04Z","published":"2024-10-21T19:59:04Z","title":"DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding","summary":"  Document structure editing involves manipulating localized textual, visual,\nand layout components in document images based on the user's requests. Past\nworks have shown that multimodal grounding of user requests in the document\nimage and identifying the accurate structural components and their associated\nattributes remain key challenges for this task. To address these, we introduce\nthe DocEdit-v2, a novel framework that performs end-to-end document editing by\nleveraging Large Multimodal Models (LMMs). It consists of three novel\ncomponents: (1) Doc2Command, which simultaneously localizes edit regions of\ninterest (RoI) and disambiguates user edit requests into edit commands; (2)\nLLM-based Command Reformulation prompting to tailor edit commands originally\nintended for specialized software into edit instructions suitable for\ngeneralist LMMs. (3) Moreover, DocEdit-v2 processes these outputs via Large\nMultimodal Models like GPT-4V and Gemini, to parse the document layout, execute\nedits on grounded Region of Interest (RoI), and generate the edited document\nimage. Extensive experiments on the DocEdit dataset show that DocEdit-v2\nsignificantly outperforms strong baselines on edit command generation (2-33%),\nRoI bounding box detection (12-31%), and overall document editing (1-12\\%)\ntasks.\n","authors":["Manan Suri","Puneet Mathur","Franck Dernoncourt","Rajiv Jain","Vlad I Morariu","Ramit Sawhney","Preslav Nakov","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2410.16472v1.pdf","comment":"EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2409.01344v2","updated":"2024-10-21T19:49:41Z","published":"2024-09-02T15:58:24Z","title":"Pairing Analogy-Augmented Generation with Procedural Memory for\n  Procedural Q&A","summary":"  Large language models struggle to synthesize disparate pieces of information\ninto a coherent plan when approaching a complex procedural task. In this work,\nwe introduce a novel formalism and structure for such procedural knowledge.\nBased on this formalism, we present a novel procedural knowledge dataset called\nLCStep, which we created from LangChain tutorials. To leverage this procedural\nknowledge to solve new tasks, we propose analogy-augmented generation (AAG),\nwhich draws inspiration from the human ability to assimilate past experiences\nto solve unfamiliar problems. AAG uses a custom procedure memory store to\nretrieve and adapt specialized domain knowledge to answer new procedural tasks.\nWe demonstrate that AAG outperforms few-shot and RAG baselines on LCStep,\nRecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation,\ncorroborated by human evaluation in the case of RecipeNLG.\n","authors":["K Roth","Rushil Gupta","Simon Halle","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16464v1","updated":"2024-10-21T19:46:06Z","published":"2024-10-21T19:46:06Z","title":"Beyond Browsing: API-Based Web Agents","summary":"  Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by browsing agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-based agents outperform web browsing agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n20.0% absolute improvement over web browsing alone, achieving a success rate of\n35.8%, achiving the SOTA performance among task-agnostic agents. These results\nstrongly suggest that when APIs are available, they present an attractive\nalternative to relying on web browsing alone.\n","authors":["Yueqi Song","Frank Xu","Shuyan Zhou","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16464v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16461v1","updated":"2024-10-21T19:40:05Z","published":"2024-10-21T19:40:05Z","title":"Comparative Study of Multilingual Idioms and Similes in Large Language\n  Models","summary":"  This study addresses the gap in the literature concerning the comparative\nperformance of LLMs in interpreting different types of figurative language\nacross multiple languages. By evaluating LLMs using two multilingual datasets\non simile and idiom interpretation, we explore the effectiveness of various\nprompt engineering strategies, including chain-of-thought, few-shot, and\nEnglish translation prompts. We extend the language of these datasets to\nPersian as well by building two new evaluation sets. Our comprehensive\nassessment involves both closed-source (GPT-3.5, GPT-4o mini, Gemini 1.5), and\nopen-source models (Llama 3.1, Qwen2), highlighting significant differences in\nperformance across languages and figurative types. Our findings reveal that\nwhile prompt engineering methods are generally effective, their success varies\nby figurative type, language, and model. We also observe that open-source\nmodels struggle particularly with low-resource languages in similes.\nAdditionally, idiom interpretation is nearing saturation for many languages,\nnecessitating more challenging evaluations.\n","authors":["Paria Khoshtab","Danial Namazifard","Mostafa Masoudi","Ali Akhgary","Samin Mahdizadeh Sani","Yadollah Yaghoobzadeh"],"pdf_url":"https://arxiv.org/pdf/2410.16461v1.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.05094v2","updated":"2024-10-21T19:34:27Z","published":"2023-05-08T23:43:15Z","title":"Interactive Concept Learning for Uncovering Latent Themes in Large Text\n  Collections","summary":"  Experts across diverse disciplines are often interested in making sense of\nlarge text collections. Traditionally, this challenge is approached either by\nnoisy unsupervised techniques such as topic models, or by following a manual\ntheme discovery process. In this paper, we expand the definition of a theme to\naccount for more than just a word distribution, and include generalized\nconcepts deemed relevant by domain experts. Then, we propose an interactive\nframework that receives and encodes expert feedback at different levels of\nabstraction. Our framework strikes a balance between automation and manual\ncoding, allowing experts to maintain control of their study while reducing the\nmanual effort required.\n","authors":["Maria Leonor Pacheco","Tunazzina Islam","Lyle Ungar","Ming Yin","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2305.05094v2.pdf","comment":"Accepted to Findings of ACL: ACL 2023"},{"id":"http://arxiv.org/abs/2410.16456v1","updated":"2024-10-21T19:30:05Z","published":"2024-10-21T19:30:05Z","title":"To the Globe (TTG): Towards Language-Driven Guaranteed Travel Planning","summary":"  Travel planning is a challenging and time-consuming task that aims to find an\nitinerary which satisfies multiple, interdependent constraints regarding\nflights, accommodations, attractions, and other travel arrangements. In this\npaper, we propose To the Globe (TTG), a real-time demo system that takes\nnatural language requests from users, translates it to symbolic form via a\nfine-tuned Large Language Model, and produces optimal travel itineraries with\nMixed Integer Linear Programming solvers. The overall system takes ~5 seconds\nto reply to the user request with guaranteed itineraries. To train TTG, we\ndevelop a synthetic data pipeline that generates user requests, flight and\nhotel information in symbolic form without human annotations, based on the\nstatistics of real-world datasets, and fine-tune an LLM to translate NL user\nrequests to their symbolic form, which is sent to the symbolic solver to\ncompute optimal itineraries. Our NL-symbolic translation achieves ~91% exact\nmatch in a backtranslation metric (i.e., whether the estimated symbolic form of\ngenerated natural language matches the groundtruth), and its returned\nitineraries have a ratio of 0.979 compared to the optimal cost of the ground\ntruth user request. When evaluated by users, TTG achieves consistently high Net\nPromoter Scores (NPS) of 35-40% on generated itinerary.\n","authors":["Da JU","Song Jiang","Andrew Cohen","Aaron Foss","Sasha Mitts","Arman Zharmagambetov","Brandon Amos","Xian Li","Justine T Kao","Maryam Fazel-Zarandi","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.16456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16454v1","updated":"2024-10-21T19:28:37Z","published":"2024-10-21T19:28:37Z","title":"Does your LLM truly unlearn? An embarrassingly simple approach to\n  recover unlearned knowledge","summary":"  Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. Based on our empirical findings, we provide a\ntheoretical explanation for the observed phenomenon and propose a\nquantization-robust unlearning strategy to mitigate this intricate issue...\n","authors":["Zhiwei Zhang","Fali Wang","Xiaomin Li","Zongyu Wu","Xianfeng Tang","Hui Liu","Qi He","Wenpeng Yin","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16454v1.pdf","comment":"21 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.16451v1","updated":"2024-10-21T19:25:31Z","published":"2024-10-21T19:25:31Z","title":"Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between\n  Ghana and the U.S","summary":"  Recent work has highlighted the culturally-contingent nature of commonsense\nknowledge. We introduce AMAMMER${\\epsilon}$, a test set of 525 multiple-choice\nquestions designed to evaluate the commonsense knowledge of English LLMs,\nrelative to the cultural contexts of Ghana and the United States. To create\nAMAMMER${\\epsilon}$, we select a set of multiple-choice questions (MCQs) from\nexisting commonsense datasets and rewrite them in a multi-stage process\ninvolving surveys of Ghanaian and U.S. participants. In three rounds of\nsurveys, participants from both pools are solicited to (1) write correct and\nincorrect answer choices, (2) rate individual answer choices on a 5-point\nLikert scale, and (3) select the best answer choice from the newly-constructed\nMCQ items, in a final validation step. By engaging participants at multiple\nstages, our procedure ensures that participant perspectives are incorporated\nboth in the creation and validation of test items, resulting in high levels of\nagreement within each pool. We evaluate several off-the-shelf English LLMs on\nAMAMMER${\\epsilon}$. Uniformly, models prefer answers choices that align with\nthe preferences of U.S. annotators over Ghanaian annotators. Additionally, when\ntest items specify a cultural context (Ghana or the U.S.), models exhibit some\nability to adapt, but performance is consistently better in U.S. contexts than\nGhanaian. As large resources are devoted to the advancement of English LLMs,\nour findings underscore the need for culturally adaptable models and\nevaluations to meet the needs of diverse English-speaking populations around\nthe world.\n","authors":["Christabel Acquaye","Haozhe An","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2410.16451v1.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.16443v1","updated":"2024-10-21T19:12:33Z","published":"2024-10-21T19:12:33Z","title":"Improving Neuron-level Interpretability with White-box Language Models","summary":"  Neurons in auto-regressive language models like GPT-2 can be interpreted by\nanalyzing their activation patterns. Recent studies have shown that techniques\nsuch as dictionary learning, a form of post-hoc sparse coding, enhance this\nneuron-level interpretability. In our research, we are driven by the goal to\nfundamentally improve neural network interpretability by embedding sparse\ncoding directly within the model architecture, rather than applying it as an\nafterthought. In our study, we introduce a white-box transformer-like\narchitecture named Coding RAte TransformEr (CRATE), explicitly engineered to\ncapture sparse, low-dimensional structures within data distributions. Our\ncomprehensive experiments showcase significant improvements (up to 103%\nrelative improvement) in neuron-level interpretability across a variety of\nevaluation metrics. Detailed investigations confirm that this enhanced\ninterpretability is steady across different layers irrespective of the model\nsize, underlining CRATE's robust performance in enhancing neural network\ninterpretability. Further analysis shows that CRATE's increased\ninterpretability comes from its enhanced ability to consistently and\ndistinctively activate on relevant tokens. These findings point towards a\npromising direction for creating white-box foundation models that excel in\nneuron-level interpretation.\n","authors":["Hao Bai","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2410.16443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16407v1","updated":"2024-10-21T18:19:09Z","published":"2024-10-21T18:19:09Z","title":"Enhancing Multimodal Affective Analysis with Learned Live Comment\n  Features","summary":"  Live comments, also known as Danmaku, are user-generated messages that are\nsynchronized with video content. These comments overlay directly onto streaming\nvideos, capturing viewer emotions and reactions in real-time. While prior work\nhas leveraged live comments in affective analysis, its use has been limited due\nto the relative rarity of live comments across different video platforms. To\naddress this, we first construct the Live Comment for Affective Analysis\n(LCAffect) dataset which contains live comments for English and Chinese videos\nspanning diverse genres that elicit a wide spectrum of emotions. Then, using\nthis dataset, we use contrastive learning to train a video encoder to produce\nsynthetic live comment features for enhanced multimodal affective content\nanalysis. Through comprehensive experimentation on a wide range of affective\nanalysis tasks (sentiment, emotion recognition, and sarcasm detection) in both\nEnglish and Chinese, we demonstrate that these synthetic live comment features\nsignificantly improve performance over state-of-the-art methods.\n","authors":["Zhaoyuan Deng","Amith Ananthram","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2410.16407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19261v2","updated":"2024-10-21T18:12:44Z","published":"2024-05-29T16:55:08Z","title":"Faster Cascades via Speculative Decoding","summary":"  Cascades and speculative decoding are two common approaches to improving\nlanguage models' inference efficiency. Both approaches involve interleaving\nmodels of different sizes, but via fundamentally distinct mechanisms: cascades\nemploy a deferral rule that invokes the larger model only for \"hard\" inputs,\nwhile speculative decoding uses speculative execution to primarily invoke the\nlarger model in parallel verification mode. These mechanisms offer different\nbenefits: empirically, cascades offer better cost-quality trade-offs, often\neven outperforming the large model, while theoretically, speculative decoding\noffers a guarantee of quality-neutrality. In this paper, we leverage the best\nof both these approaches by designing new speculative cascading techniques that\nimplement their deferral rule through speculative execution. We characterize\nthe optimal deferral rule for our speculative cascades, and employ a plug-in\napproximation to the optimal rule. Experiments with Gemma and T5 models on a\nrange of language benchmarks show that our approach yields better cost quality\ntrade-offs than cascading and speculative decoding baselines.\n","authors":["Harikrishna Narasimhan","Wittawat Jitkrittum","Ankit Singh Rawat","Seungyeon Kim","Neha Gupta","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2405.19261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16400v1","updated":"2024-10-21T18:10:26Z","published":"2024-10-21T18:10:26Z","title":"VipAct: Visual-Perception Enhancement via Specialized VLM Agent\n  Collaboration and Tool-use","summary":"  While vision-language models (VLMs) have demonstrated remarkable performance\nacross various tasks combining textual and visual information, they continue to\nstruggle with fine-grained visual perception tasks that require detailed\npixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs\non such intricate visual elements remains an open challenge. In this paper, we\npresent VipAct, an agent framework that enhances VLMs by integrating\nmulti-agent collaboration and vision expert models, enabling more precise\nvisual understanding and comprehensive reasoning. VipAct consists of an\norchestrator agent, which manages task requirement analysis, planning, and\ncoordination, along with specialized agents that handle specific tasks such as\nimage captioning and vision expert models that provide high-precision\nperceptual information. This multi-agent approach allows VLMs to better perform\nfine-grained visual perception tasks by synergizing planning, reasoning, and\ntool use. We evaluate VipAct on benchmarks featuring a diverse set of visual\nperception tasks, with experimental results demonstrating significant\nperformance improvements over state-of-the-art baselines across all tasks.\nFurthermore, comprehensive ablation studies reveal the critical role of\nmulti-agent collaboration in eliciting more detailed System-2 reasoning and\nhighlight the importance of image input for task planning. Additionally, our\nerror analysis identifies patterns of VLMs' inherent limitations in visual\nperception, providing insights into potential future improvements. VipAct\noffers a flexible and extensible framework, paving the way for more advanced\nvisual perception systems across various real-world applications.\n","authors":["Zhehao Zhang","Ryan Rossi","Tong Yu","Franck Dernoncourt","Ruiyi Zhang","Jiuxiang Gu","Sungchul Kim","Xiang Chen","Zichao Wang","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2410.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16392v1","updated":"2024-10-21T18:06:25Z","published":"2024-10-21T18:06:25Z","title":"LLM-based Optimization of Compound AI Systems: A Survey","summary":"  In a compound AI system, components such as an LLM call, a retriever, a code\ninterpreter, or tools are interconnected. The system's behavior is primarily\ndriven by parameters such as instructions or tool definitions. Recent\nadvancements enable end-to-end optimization of these parameters using an LLM.\nNotably, leveraging an LLM as an optimizer is particularly efficient because it\navoids gradient computation and can generate complex code and instructions.\nThis paper presents a survey of the principles and emerging trends in LLM-based\noptimization of compound AI systems. It covers archetypes of compound AI\nsystems, approaches to LLM-based end-to-end optimization, and insights into\nfuture directions and broader impacts. Importantly, this survey uses concepts\nfrom program analysis to provide a unified view of how an LLM optimizer is\nprompted to optimize a compound AI system. The exhaustive list of paper is\nprovided at\nhttps://github.com/linyuhongg/LLM-based-Optimization-of-Compound-AI-Systems.\n","authors":["Matthieu Lin","Jenny Sheng","Andrew Zhao","Shenzhi Wang","Yang Yue","Yiran Wu","Huan Liu","Jun Liu","Gao Huang","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.16392v1.pdf","comment":null}]},"2024-10-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.17251v1","updated":"2024-10-22T17:59:57Z","published":"2024-10-22T17:59:57Z","title":"Altogether: Image Captioning via Re-aligning Alt-text","summary":"  This paper focuses on creating synthetic data to improve the quality of image\ncaptions. Existing works typically have two shortcomings. First, they caption\nimages from scratch, ignoring existing alt-text metadata, and second, lack\ntransparency if the captioners' training data (e.g. GPT) is unknown. In this\npaper, we study a principled approach Altogether based on the key idea to edit\nand re-align existing alt-texts associated with the images. To generate\ntraining data, we perform human annotation where annotators start with the\nexisting alt-text and re-align it to the image content in multiple rounds,\nconsequently constructing captions with rich visual concepts. This differs from\nprior work that carries out human annotation as a one-time description task\nsolely based on images and annotator knowledge. We train a captioner on this\ndata that generalizes the process of re-aligning alt-texts at scale. Our\nresults show our Altogether approach leads to richer image captions that also\nimprove text-to-image generation and zero-shot image classification tasks.\n","authors":["Hu Xu","Po-Yao Huang","Xiaoqing Ellen Tan","Ching-Feng Yeh","Jacob Kahn","Christine Jou","Gargi Ghosh","Omer Levy","Luke Zettlemoyer","Wen-tau Yih","Shang-Wen Li","Saining Xie","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2410.17251v1.pdf","comment":"accepted by EMNLP 2024; MetaCLIPv2"},{"id":"http://arxiv.org/abs/2410.17250v1","updated":"2024-10-22T17:59:56Z","published":"2024-10-22T17:59:56Z","title":"JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation","summary":"  Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.\n","authors":["Shota Onohara","Atsuyuki Miyai","Yuki Imajuku","Kazuki Egashira","Jeonghun Baek","Xiang Yue","Graham Neubig","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2410.17250v1.pdf","comment":"Project page: https://mmmu-japanese-benchmark.github.io/JMMMU/"},{"id":"http://arxiv.org/abs/2410.17247v1","updated":"2024-10-22T17:59:53Z","published":"2024-10-22T17:59:53Z","title":"PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction","summary":"  In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. We hope that the insights and approach\nintroduced by PyramidDrop will inspire future research to further investigate\nthe role of image tokens in LVLMs.\n","authors":["Long Xing","Qidong Huang","Xiaoyi Dong","Jiajie Lu","Pan Zhang","Yuhang Zang","Yuhang Cao","Conghui He","Jiaqi Wang","Feng Wu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17247v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.17245v1","updated":"2024-10-22T17:59:39Z","published":"2024-10-22T17:59:39Z","title":"Towards Reliable Evaluation of Behavior Steering Interventions in LLMs","summary":"  Representation engineering methods have recently shown promise for enabling\nefficient steering of model behavior. However, evaluation pipelines for these\nmethods have primarily relied on subjective demonstrations, instead of\nquantitative, objective metrics. We aim to take a step towards addressing this\nissue by advocating for four properties missing from current evaluations: (i)\ncontexts sufficiently similar to downstream tasks should be used for assessing\nintervention quality; (ii) model likelihoods should be accounted for; (iii)\nevaluations should allow for standardized comparisons across different target\nbehaviors; and (iv) baseline comparisons should be offered. We introduce an\nevaluation pipeline grounded in these criteria, offering both a quantitative\nand visual analysis of how effectively a given method works. We use this\npipeline to evaluate two representation engineering methods on how effectively\nthey can steer behaviors such as truthfulness and corrigibility, finding that\nsome interventions are less effective than previously reported.\n","authors":["Itamar Pres","Laura Ruis","Ekdeep Singh Lubana","David Krueger"],"pdf_url":"https://arxiv.org/pdf/2410.17245v1.pdf","comment":"Accepted to the NeurIPS 2024 - Workshop on Foundation Model\n  Interventions"},{"id":"http://arxiv.org/abs/2410.17238v1","updated":"2024-10-22T17:56:08Z","published":"2024-10-22T17:56:08Z","title":"SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning","summary":"  Automated Machine Learning (AutoML) approaches encompass traditional methods\nthat optimize fixed pipelines for model selection and ensembling, as well as\nnewer LLM-based frameworks that autonomously build pipelines. While LLM-based\nagents have shown promise in automating machine learning tasks, they often\ngenerate low-diversity and suboptimal code, even after multiple iterations. To\novercome these limitations, we introduce Tree-Search Enhanced LLM Agents\n(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline configurations\nas trees, our framework enables agents to conduct experiments intelligently and\niteratively refine their strategies, facilitating a more effective exploration\nof the machine learning solution space. This novel approach allows SELA to\ndiscover optimal pathways based on experimental feedback, improving the overall\nquality of the solutions. In an extensive evaluation across 20 machine learning\ndatasets, we compare the performance of traditional and agent-based AutoML\nmethods, demonstrating that SELA achieves a win rate of 65% to 80% against each\nbaseline across all datasets. These results underscore the significant\npotential of agent-based strategies in AutoML, offering a fresh perspective on\ntackling complex machine learning challenges.\n","authors":["Yizhou Chi","Yizhang Lin","Sirui Hong","Duyi Pan","Yaying Fei","Guanghao Mei","Bangbang Liu","Tianqi Pang","Jacky Kwok","Ceyao Zhang","Bang Liu","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17238v1.pdf","comment":"The code is available at https://github.com/geekan/MetaGPT"},{"id":"http://arxiv.org/abs/2410.17236v1","updated":"2024-10-22T17:54:45Z","published":"2024-10-22T17:54:45Z","title":"Large Language Models Empowered Personalized Web Agents","summary":"  Web agents have emerged as a promising direction to automate Web task\ncompletion based on user instructions, significantly enhancing user experience.\nRecently, Web agents have evolved from traditional agents to Large Language\nModels (LLMs)-based Web agents. Despite their success, existing LLM-based Web\nagents overlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users' personalized\ninstructions and executing customized actions. To overcome the limitation, we\nfirst formulate the task of LLM-empowered personalized Web agents, which\nintegrate personalized data and user instructions to personalize instruction\ncomprehension and action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent Benchmark\n(PersonalWAB), featuring user instructions, personalized user data, Web\nfunctions, and two evaluation paradigms across three personalized Web tasks.\nMoreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)\nframework to adapt LLMs to the personalized Web agent task. PUMA utilizes a\nmemory bank with a task-specific retrieval strategy to filter relevant\nhistorical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for\npersonalized action execution through fine-tuning and direct preference\noptimization. Extensive experiments validate the superiority of PUMA over\nexisting Web agents on PersonalWAB.\n","authors":["Hongru Cai","Yongqi Li","Wenjie Wang","Fengbin Zhu","Xiaoyu Shen","Wenjie Li","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.17236v1.pdf","comment":"The code and data are available on the project website\n  https://hongrucai.github.io/PersonalWAB/"},{"id":"http://arxiv.org/abs/2410.17235v1","updated":"2024-10-22T17:54:07Z","published":"2024-10-22T17:54:07Z","title":"Automated Spinal MRI Labelling from Reports Using a Large Language Model","summary":"  We propose a general pipeline to automate the extraction of labels from\nradiology reports using large language models, which we validate on spinal MRI\nreports. The efficacy of our labelling method is measured on five distinct\nconditions: spinal cancer, stenosis, spondylolisthesis, cauda equina\ncompression and herniation. Using open-source models, our method equals or\nsurpasses GPT-4 on a held-out set of reports. Furthermore, we show that the\nextracted labels can be used to train imaging models to classify the identified\nconditions in the accompanying MR scans. All classifiers trained using\nautomated labels achieve comparable performance to models trained using scans\nmanually annotated by clinicians. Code can be found at\nhttps://github.com/robinyjpark/AutoLabelClassifier.\n","authors":["Robin Y. Park","Rhydian Windsor","Amir Jamaludin","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2410.17235v1.pdf","comment":"Accepted to Medical Image Computing and Computer Assisted\n  Intervention (MICCAI 2024, Spotlight). 11 pages plus appendix"},{"id":"http://arxiv.org/abs/2410.17234v1","updated":"2024-10-22T17:54:03Z","published":"2024-10-22T17:54:03Z","title":"Fine-Tuning Large Language Models to Appropriately Abstain with Semantic\n  Entropy","summary":"  Large Language Models (LLMs) are known to hallucinate, whereby they generate\nplausible but inaccurate text. This phenomenon poses significant risks in\ncritical applications, such as medicine or law, necessitating robust\nhallucination mitigation strategies. While recent works have proposed\nfine-tuning methods to teach LLMs to abstain from answering questions beyond\ntheir knowledge or capabilities, these methods rely on the existence of\nground-truth labels or are limited to short-form responses. To address these\nlimitations, we propose fine-tuning using semantic entropy, an uncertainty\nmeasure derived from introspection into the model which does not require\nexternal labels. We demonstrate that our approach matches or outperforms models\nfine-tuned using prior work and achieves strong performance for both short and\nlong-form generations on a range of datasets.\n","authors":["Benedict Aaron Tjandra","Muhammed Razzak","Jannik Kossen","Kunal Handa","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2410.17234v1.pdf","comment":"Accepted to NeurIPS Safe Generative AI Workshop 2024"},{"id":"http://arxiv.org/abs/2407.12883v2","updated":"2024-10-22T17:49:31Z","published":"2024-07-16T17:58:27Z","title":"BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive\n  Retrieval","summary":"  Existing retrieval benchmarks primarily consist of information-seeking\nqueries (e.g., aggregated questions from search engines) where keyword or\nsemantic-based retrieval is usually sufficient. However, many complex\nreal-world queries require in-depth reasoning to identify relevant documents\nthat go beyond surface form matching. For example, finding documentation for a\ncoding question requires understanding the logic and syntax of the functions\ninvolved. To better benchmark retrieval on such challenging queries, we\nintroduce BRIGHT, the first text retrieval benchmark that requires intensive\nreasoning to retrieve relevant documents. Our dataset consists of 1,384\nreal-world queries spanning diverse domains, such as economics, psychology,\nmathematics, and coding. These queries are drawn from naturally occurring and\ncarefully curated human data. Extensive evaluation reveals that even\nstate-of-the-art retrieval models perform poorly on BRIGHT. The leading model\non the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of\n59.0 nDCG@10, produces a score of nDCG@10 of 18.3 on BRIGHT. We show that\nincorporating explicit reasoning about the query improves retrieval performance\nby up to 12.2 points. Moreover, incorporating retrieved documents from the\ntop-performing retriever boosts question-answering performance by over 6.6\npoints. We believe that BRIGHT paves the way for future research on retrieval\nsystems in more realistic and challenging settings.\n","authors":["Hongjin Su","Howard Yen","Mengzhou Xia","Weijia Shi","Niklas Muennighoff","Han-yu Wang","Haisu Liu","Quan Shi","Zachary S. Siegel","Michael Tang","Ruoxi Sun","Jinsung Yoon","Sercan O. Arik","Danqi Chen","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2407.12883v2.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2410.17225v1","updated":"2024-10-22T17:47:05Z","published":"2024-10-22T17:47:05Z","title":"Dhoroni: Exploring Bengali Climate Change and Environmental Views with a\n  Multi-Perspective News Dataset and Natural Language Processing","summary":"  Climate change poses critical challenges globally, disproportionately\naffecting low-income countries that often lack resources and linguistic\nrepresentation on the international stage. Despite Bangladesh's status as one\nof the most vulnerable nations to climate impacts, research gaps persist in\nBengali-language studies related to climate change and NLP. To address this\ndisparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and\nenvironmental news dataset, comprising a 2300 annotated Bangla news articles,\noffering multiple perspectives such as political influence,\nscientific/statistical data, authenticity, stance detection, and stakeholder\ninvolvement. Furthermore, we present an in-depth exploratory analysis of\nDhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family\nfor climate and environmental opinion detection in Bangla, fine-tuned on our\ndataset. This research contributes significantly to enhancing accessibility and\nanalysis of climate discourse in Bengali (Bangla), addressing crucial\ncommunication and research gaps in climate-impacted regions like Bangladesh\nwith 180 million people.\n","authors":["Azmine Toushik Wasi","Wahid Faisal","Taj Ahmad","Abdur Rahman","Mst Rafia Islam"],"pdf_url":"https://arxiv.org/pdf/2410.17225v1.pdf","comment":"In Review"},{"id":"http://arxiv.org/abs/2410.17222v1","updated":"2024-10-22T17:45:47Z","published":"2024-10-22T17:45:47Z","title":"Context-aware Prompt Tuning: Advancing In-Context Learning with\n  Adversarial Methods","summary":"  Fine-tuning Large Language Models (LLMs) typically involves updating at least\na few billions of parameters. A more parameter-efficient approach is Prompt\nTuning (PT), which updates only a few learnable tokens, and differently,\nIn-Context Learning (ICL) adapts the model to a new task by simply including\nexamples in the input without any training. When applying optimization-based\nmethods, such as fine-tuning and PT for few-shot learning, the model is\nspecifically adapted to the small set of training examples, whereas ICL leaves\nthe model unchanged. This distinction makes traditional learning methods more\nprone to overfitting; in contrast, ICL is less sensitive to the few-shot\nscenario. While ICL is not prone to overfitting, it does not fully extract the\ninformation that exists in the training examples. This work introduces\nContext-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and\nadversarial attacks. We build on the ICL strategy of concatenating examples\nbefore the input, but we extend this by PT-like learning, refining the context\nembedding through iterative optimization to extract deeper insights from the\ntraining examples. We carefully modify specific context tokens, considering the\nunique structure of input and output formats. Inspired by adversarial attacks,\nwe adjust the input based on the labels present in the context, focusing on\nminimizing, rather than maximizing, the loss. Moreover, we apply a projected\ngradient descent algorithm to keep token embeddings close to their original\nvalues, under the assumption that the user-provided data is inherently\nvaluable. Our method has been shown to achieve superior accuracy across\nmultiple classification tasks using various LLM models.\n","authors":["Tsachi Blau","Moshe Kimhi","Yonatan Belinkov","Alexander Bronstein","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2410.17222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17218v1","updated":"2024-10-22T17:43:39Z","published":"2024-10-22T17:43:39Z","title":"Creativity in AI: Progresses and Challenges","summary":"  Creativity is the ability to produce novel, useful, and surprising ideas, and\nhas been widely studied as a crucial aspect of human cognition. Machine\ncreativity on the other hand has been a long-standing challenge. With the rise\nof advanced generative AI, there has been renewed interest and debate regarding\nAI's creative capabilities. Therefore, it is imperative to revisit the state of\ncreativity in AI and identify key progresses and remaining challenges. In this\nwork, we survey leading works studying the creative capabilities of AI systems,\nfocusing on creative problem-solving, linguistic, artistic, and scientific\ncreativity. Our review suggests that while the latest AI models are largely\ncapable of producing linguistically and artistically creative outputs such as\npoems, images, and musical pieces, they struggle with tasks that require\ncreative problem-solving, abstract thinking and compositionality and their\ngenerations suffer from a lack of diversity, originality, long-range\nincoherence and hallucinations. We also discuss key questions concerning\ncopyright and authorship issues with generative models. Furthermore, we\nhighlight the need for a comprehensive evaluation of creativity that is\nprocess-driven and considers several dimensions of creativity. Finally, we\npropose future research directions to improve the creativity of AI outputs,\ndrawing inspiration from cognitive science and psychology.\n","authors":["Mete Ismayilzada","Debjit Paul","Antoine Bosselut","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2410.17218v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2410.17215v1","updated":"2024-10-22T17:40:32Z","published":"2024-10-22T17:40:32Z","title":"MiniPLM: Knowledge Distillation for Pre-Training Language Models","summary":"  Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces challenges in efficiency,\nflexibility, and effectiveness. Existing methods either incur high\ncomputational costs due to online teacher inference, require tokenization\nmatching between teacher and student LMs, or risk losing the difficulty and\ndiversity of the teacher-generated training data. To address these issues, we\npropose MiniPLM, a KD framework for pre-training LMs by refining the training\ndata distribution with the teacher's knowledge. For efficiency, MiniPLM\nperforms offline teacher LM inference, allowing KD for multiple student LMs\nwithout adding training-time costs. For flexibility, MiniPLM operates solely on\nthe training corpus, enabling KD across model families. For effectiveness,\nMiniPLM leverages the differences between large and small LMs to enhance the\ndifficulty and diversity of the training data, helping student LMs acquire\nversatile and sophisticated knowledge. Extensive experiments demonstrate that\nMiniPLM boosts the student LMs' performance on 9 widely used downstream tasks,\nimproves the language modeling capabilities, and reduces pre-training\ncomputation. The benefit of MiniPLM extends to large pre-training scales,\nevidenced by the extrapolation of the scaling curves. Further analysis reveals\nthat MiniPLM supports KD across model families and enhances the utilization of\npre-training data. Our model, code, and data are available at\nhttps://github.com/thu-coai/MiniPLM.\n","authors":["Yuxian Gu","Hao Zhou","Fandong Meng","Jie Zhou","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.17215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10796v2","updated":"2024-10-22T17:35:03Z","published":"2024-10-14T17:57:09Z","title":"Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance","summary":"  A standard practice when using large language models is for users to\nsupplement their instruction with an input context containing new information\nfor the model to process. However, models struggle to reliably follow the input\ncontext, especially when it conflicts with their parametric knowledge from\npretraining. In-principle, one would expect models to adapt to the user context\nbetter after instruction finetuning, particularly when handling knowledge\nconflicts. However, we observe a surprising failure mode: during instruction\ntuning, the context reliance under knowledge conflicts initially increases as\nexpected, but then gradually decreases as instruction finetuning progresses.\nThis happens while the performance on standard benchmarks keeps on increasing\nfar after this drop. We call this phenomenon context-parametric inversion and\nobserve it across multiple general purpose instruction tuning datasets such as\nTULU, Alpaca and Ultrachat, across different model families like Llama,\nMistral, and Pythia. We perform various controlled studies and theoretical\nanalysis to show that context-parametric inversion occurs due to examples in\nthe instruction finetuning data where the input context provides information\nthat aligns with model's parametric knowledge. Our analysis suggests some\nnatural mitigation strategies with limited but insightful gains, and serves as\na useful starting point in addressing this deficiency in instruction\nfinetuning.\n","authors":["Sachin Goyal","Christina Baek","J. Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2410.10796v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17210v1","updated":"2024-10-22T17:34:59Z","published":"2024-10-22T17:34:59Z","title":"Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh\n  through Large Language Modeling","summary":"  Purpose: Bangladesh's legal system struggles with major challenges like\ndelays, complexity, high costs, and millions of unresolved cases, which deter\nmany from pursuing legal action due to lack of knowledge or financial\nconstraints. This research seeks to develop a specialized Large Language Model\n(LLM) to assist in the Bangladeshi legal system. Methods: We created\nUKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and\nscraping data on various legal acts. We fine-tuned the GPT-2 model on this\ndataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance\nin English. Results: The model was rigorously evaluated using semantic\nassessments, including case studies supported by expert opinions. The\nevaluation provided promising results, demonstrating the potential for the\nmodel to assist in legal matters within Bangladesh. Conclusion: Our work\nrepresents the first structured effort toward building an AI-based legal\nassistant for Bangladesh. While the results are encouraging, further\nrefinements are necessary to improve the model's accuracy, credibility, and\nsafety. This is a significant step toward creating a legal AI capable of\nserving the needs of a population of 180 million.\n","authors":["Azmine Toushik Wasi","Wahid Faisal","Mst Rafia Islam","Mahathir Mohammad Bappy"],"pdf_url":"https://arxiv.org/pdf/2410.17210v1.pdf","comment":"In Review"},{"id":"http://arxiv.org/abs/2410.17209v1","updated":"2024-10-22T17:31:37Z","published":"2024-10-22T17:31:37Z","title":"Audio-to-Score Conversion Model Based on Whisper methodology","summary":"  This thesis develops a Transformer model based on Whisper, which extracts\nmelodies and chords from music audio and records them into ABC notation. A\ncomprehensive data processing workflow is customized for ABC notation,\nincluding data cleansing, formatting, and conversion, and a mutation mechanism\nis implemented to increase the diversity and quality of training data. This\nthesis innovatively introduces the \"Orpheus' Score\", a custom notation system\nthat converts music information into tokens, designs a custom vocabulary\nlibrary, and trains a corresponding custom tokenizer. Experiments show that\ncompared to traditional algorithms, the model has significantly improved\naccuracy and performance. While providing a convenient audio-to-score tool for\nmusic enthusiasts, this work also provides new ideas and tools for research in\nmusic information processing.\n","authors":["Hongyao Zhang","Bohang Sun"],"pdf_url":"https://arxiv.org/pdf/2410.17209v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17196v1","updated":"2024-10-22T17:15:20Z","published":"2024-10-22T17:15:20Z","title":"VoiceBench: Benchmarking LLM-Based Voice Assistants","summary":"  Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.\n","authors":["Yiming Chen","Xianghu Yue","Chen Zhang","Xiaoxue Gao","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.17196v1.pdf","comment":"Work in progress. Data is available at\n  https://github.com/MatthewCYM/VoiceBench"},{"id":"http://arxiv.org/abs/2410.17195v1","updated":"2024-10-22T17:13:38Z","published":"2024-10-22T17:13:38Z","title":"Language Model Non-myopic Generation for Reasoning and Planning","summary":"  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n","authors":["Chang Ma","Haiteng Zhao","Junlei Zhang","Junxian He","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08213v2","updated":"2024-10-22T17:07:14Z","published":"2024-03-13T03:22:02Z","title":"Can Large Language Models Identify Authorship?","summary":"  The ability to accurately identify authorship is crucial for verifying\ncontent authenticity and mitigating misinformation. Large Language Models\n(LLMs) have demonstrated an exceptional capacity for reasoning and\nproblem-solving. However, their potential in authorship analysis remains\nunder-explored. Traditional studies have depended on hand-crafted stylistic\nfeatures, whereas state-of-the-art approaches leverage text embeddings from\npre-trained language models. These methods, which typically require fine-tuning\non labeled data, often suffer from performance degradation in cross-domain\napplications and provide limited explainability. This work seeks to address\nthree research questions: (1) Can LLMs perform zero-shot, end-to-end authorship\nverification effectively? (2) Are LLMs capable of accurately attributing\nauthorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs\nprovide explainability in authorship analysis, particularly through the role of\nlinguistic features? Moreover, we investigate the integration of explicit\nlinguistic features to guide LLMs in their reasoning processes. Our assessment\ndemonstrates LLMs' proficiency in both tasks without the need for\ndomain-specific fine-tuning, providing explanations into their decision making\nvia a detailed analysis of linguistic features. This establishes a new\nbenchmark for future research on LLM-based authorship analysis.\n","authors":["Baixiang Huang","Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2403.08213v2.pdf","comment":"Accepted to EMNLP 2024 Findings. The main paper is 9 pages long, with\n  16 pages total. The code, results, dataset, and additional resources are\n  available on the project website: https://llm-authorship.github.io/"},{"id":"http://arxiv.org/abs/2409.13686v2","updated":"2024-10-22T17:06:17Z","published":"2024-09-20T17:54:16Z","title":"The Impact of Large Language Models in Academia: from Writing to\n  Speaking","summary":"  Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.\n","authors":["Mingmeng Geng","Caixi Chen","Yanru Wu","Dongping Chen","Yao Wan","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.13686v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2405.06643v2","updated":"2024-10-22T17:05:17Z","published":"2024-03-06T23:02:30Z","title":"Levels of AI Agents: from Rules to Large Language Models","summary":"  AI agents are defined as artificial entities to perceive the environment,\nmake decisions and take actions. Inspired by the 6 levels of autonomous driving\nby Society of Automotive Engineers, the AI agents are also categorized based on\nutilities and strongness, as the following levels: L0, no AI, with tools taking\ninto account perception plus actions; L1, using rule-based AI; L2, making\nrule-based AI replaced by IL/RL-based AI, with additional reasoning & decision\nmaking; L3, applying LLM-based AI instead of IL/RL-based AI, additionally\nsetting up memory & reflection; L4, based on L3, facilitating autonomous\nlearning & generalization; L5, based on L4, appending personality of emotion\nand character and collaborative behavior with multi-agents.\n","authors":["Yu Huang"],"pdf_url":"https://arxiv.org/pdf/2405.06643v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14344v2","updated":"2024-10-22T16:59:12Z","published":"2024-07-19T14:28:07Z","title":"LLMs left, right, and center: Assessing GPT's capabilities to label\n  political bias from web domains","summary":"  This research investigates whether OpenAI's GPT-4, a state-of-the-art large\nlanguage model, can accurately classify the political bias of news sources\nbased solely on their URLs. Given the subjective nature of political labels,\nthird-party bias ratings like those from Ad Fontes Media, AllSides, and Media\nBias/Fact Check (MBFC) are often used in research to analyze news source\ndiversity. This study aims to determine if GPT-4 can replicate these human\nratings on a seven-degree scale (\"far-left\" to \"far-right\"). The analysis\ncompares GPT-4's classifications against MBFC's, and controls for website\npopularity using Open PageRank scores. Findings reveal a high correlation\n($\\text{Spearman's } \\rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and\nMBFC's ratings, indicating the model's potential reliability. However, GPT-4\nabstained from classifying approximately $\\frac{2}{3}$ of the dataset. It is\nmore likely to abstain from rating unpopular websites, which also suffer from\nless accurate assessments. The LLM tends to avoid classifying sources that MBFC\nconsiders to be centrist, resulting in more polarized outputs. Finally, this\nanalysis shows a slight leftward skew in GPT's classifications compared to\nMBFC's. Therefore, while this paper suggests that while GPT-4 can be a\nscalable, cost-effective tool for political bias classification of news\nwebsites, its use should be as a complement to human judgment to mitigate\nbiases.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2407.14344v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.17174v1","updated":"2024-10-22T16:51:27Z","published":"2024-10-22T16:51:27Z","title":"From Attention to Activation: Unravelling the Enigmas of Large Language\n  Models","summary":"  We study two strange phenomena in auto-regressive Transformers: (1) the\ndominance of the first token in attention heads; (2) the occurrence of large\noutlier activations in the hidden states. We find that popular large language\nmodels, such as Llama attend maximally to the first token in 98% of attention\nheads, a behaviour we attribute to the softmax function. To mitigate this\nissue, we propose a reformulation of softmax to softmax-1. Furthermore, we\nidentify adaptive optimisers, e.g. Adam, as the primary contributor to the\nlarge outlier activations and introduce OrthoAdam, a novel optimiser that\nutilises orthogonal matrices to transform gradients, to address this issue.\nFinally, not only do our methods prevent these phenomena from occurring, but\nadditionally, they enable Transformers to sustain their performance when\nquantised using basic algorithms, something that standard methods are unable to\ndo. In summary, our methods reduce the attention proportion on the first token\nfrom 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to\n3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3.\n","authors":["Prannay Kaul","Chengcheng Ma","Ismail Elezi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2410.17174v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.17170v1","updated":"2024-10-22T16:50:00Z","published":"2024-10-22T16:50:00Z","title":"Self-calibration for Language Model Quantization and Pruning","summary":"  Quantization and pruning are fundamental approaches for model compression,\nenabling efficient inference for language models. In a post-training setting,\nstate-of-the-art quantization and pruning methods require calibration data, a\nsmall set of unlabeled examples. Conventionally, randomly sampled web text is\nused, aiming to reflect the model training data. However, this poses two key\nproblems: (1) unrepresentative calibration examples can harm model performance,\nand (2) organizations increasingly avoid releasing model training data. In this\npaper, we propose self-calibration as a solution. Our approach requires no\nexternal data, instead leveraging the model itself to generate synthetic\ncalibration data as a better approximation of the pre-training data\ndistribution. We extensively compare the performance of self-calibration with\nseveral baselines, across a variety of models, compression methods, and tasks.\nOur approach proves consistently competitive in maximizing downstream task\nperformance, frequently outperforming even using real data.\n","authors":["Miles Williams","George Chrysostomou","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2410.17170v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.17161v1","updated":"2024-10-22T16:34:36Z","published":"2024-10-22T16:34:36Z","title":"Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence","summary":"  We propose a novel approach for learning interchangeable tokens in language\nmodels to obtain an extendable vocabulary that can generalize to new tokens.\nOur method is designed to address alpha-equivalence, the principle that\nrenaming bound variables in a syntactic expression preserves semantics. This\nproperty arises in many formal languages such as temporal logics, in which all\nproposition symbols represent the same concept but are distinguishable from\neach other. To handle such tokens, we develop a dual-part embedding approach.\nThe first part is shared across all interchangeable tokens, thereby enforcing\nthat they represent the same core concept. The second part is randomly\ngenerated for each token, which enables distinguishability. We evaluate our\nmethod in a Transformer encoder-decoder model on two tasks: solving linear\ntemporal logic formulae and copying with extendable vocabulary. Our method\ndemonstrates promising generalization capabilities in addition to introducing a\nfavorable inductive bias for alpha-equivalence.\n","authors":["İlker Işık","Ramazan Gokberk Cinbis","Ebru Aydin Gol"],"pdf_url":"https://arxiv.org/pdf/2410.17161v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.17152v1","updated":"2024-10-22T16:29:33Z","published":"2024-10-22T16:29:33Z","title":"Improving Pinterest Search Relevance Using Large Language Models","summary":"  To improve relevance scoring on Pinterest Search, we integrate Large Language\nModels (LLMs) into our search relevance model, leveraging carefully designed\ntext representations to predict the relevance of Pins effectively. Our approach\nuses search queries alongside content representations that include captions\nextracted from a generative visual language model. These are further enriched\nwith link-based text data, historically high-quality engaged queries,\nuser-curated boards, Pin titles and Pin descriptions, creating robust models\nfor predicting search relevance. We use a semi-supervised learning approach to\nefficiently scale up the amount of training data, expanding beyond the\nexpensive human labeled data available. By utilizing multilingual LLMs, our\nsystem extends training data to include unseen languages and domains, despite\ninitial data and annotator expertise being confined to English. Furthermore, we\ndistill from the LLM-based model into real-time servable model architectures\nand features. We provide comprehensive offline experimental validation for our\nproposed techniques and demonstrate the gains achieved through the final\ndeployed system at scale.\n","authors":["Han Wang","Mukuntha Narayanan Sundararaman","Onur Gungor","Yu Xu","Krishna Kamath","Rakesh Chalasani","Kurchi Subhra Hazra","Jinfeng Rao"],"pdf_url":"https://arxiv.org/pdf/2410.17152v1.pdf","comment":"CIKM 2024 Workshop on Industrial Recommendation Systems"},{"id":"http://arxiv.org/abs/2410.14594v2","updated":"2024-10-22T16:27:12Z","published":"2024-10-18T16:44:22Z","title":"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases","summary":"  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).\n","authors":["Elias Lumer","Vamse Kumar Subbiah","James A. Burke","Pradeep Honaganahalli Basavaraju","Austin Huber"],"pdf_url":"https://arxiv.org/pdf/2410.14594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17145v1","updated":"2024-10-22T16:26:03Z","published":"2024-10-22T16:26:03Z","title":"Can General-Purpose Large Language Models Generalize to English-Thai\n  Machine Translation ?","summary":"  Large language models (LLMs) perform well on common tasks but struggle with\ngeneralization in low-resource and low-computation settings. We examine this\nlimitation by testing various LLMs and specialized translation models on\nEnglish-Thai machine translation and code-switching datasets. Our findings\nreveal that under more strict computational constraints, such as 4-bit\nquantization, LLMs fail to translate effectively. In contrast, specialized\nmodels, with comparable or lower computational requirements, consistently\noutperform LLMs. This underscores the importance of specialized models for\nmaintaining performance under resource constraints.\n","authors":["Jirat Chiaranaipanich","Naiyarat Hanmatheekuna","Jitkapat Sawatphol","Krittamate Tiankanon","Jiramet Kinchagawat","Amrest Chinkamol","Parinthapat Pengpun","Piyalitt Ittichaiwong","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2410.17145v1.pdf","comment":"Accepted in GenBench EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14669v2","updated":"2024-10-22T16:07:22Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v2.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench ; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2410.17131v1","updated":"2024-10-22T16:04:03Z","published":"2024-10-22T16:04:03Z","title":"Aligning Large Language Models via Self-Steering Optimization","summary":"  Automated alignment develops alignment systems with minimal human\nintervention. The key to automated alignment lies in providing learnable and\naccurate preference signals for preference learning without human annotation.\nIn this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm\nthat autonomously generates high-quality preference signals based on predefined\nprinciples during iterative training, eliminating the need for manual\nannotation. $SSO$ maintains the accuracy of signals by ensuring a consistent\ngap between chosen and rejected responses while keeping them both on-policy to\nsuit the current policy model's learning capacity. $SSO$ can benefit the online\nand offline training of the policy model, as well as enhance the training of\nreward models. We validate the effectiveness of $SSO$ with two foundation\nmodels, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy\npreference signals throughout iterative training. Without any manual annotation\nor external models, $SSO$ leads to significant performance improvements across\nsix subjective or objective benchmarks. Besides, the preference data generated\nby $SSO$ significantly enhanced the performance of the reward model on\nRewardbench. Our work presents a scalable approach to preference optimization,\npaving the way for more efficient and effective automated alignment.\n","authors":["Hao Xiang","Bowen Yu","Hongyu Lin","Keming Lu","Yaojie Lu","Xianpei Han","Le Sun","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17127v1","updated":"2024-10-22T16:00:26Z","published":"2024-10-22T16:00:26Z","title":"PAPILLON: PrivAcy Preservation from Internet-based and Local Language\n  MOdel ENsembles","summary":"  Users can divulge sensitive information to proprietary LLM providers, raising\nsignificant privacy concerns. While open-source models, hosted locally on the\nuser's machine, alleviate some concerns, models that users can host locally are\noften less capable than proprietary frontier models. Toward preserving user\nprivacy while retaining the best quality, we propose Privacy-Conscious\nDelegation, a novel task for chaining API-based and local models. We utilize\nrecent public collections of user-LLM interactions to construct a natural\nbenchmark called PUPA, which contains personally identifiable information\n(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM\npipeline that uses prompt optimization to address a simpler version of our\ntask. Our best pipeline maintains high response quality for 85.5% of user\nqueries while restricting privacy leakage to only 7.5%. We still leave a large\nmargin to the generation quality of proprietary LLMs for future work. Our data\nand code will be available at https://github.com/siyan-sylvia-li/PAPILLON.\n","authors":["Li Siyan","Vethavikashini Chithrra Raghuram","Omar Khattab","Julia Hirschberg","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.17127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17126v1","updated":"2024-10-22T15:59:58Z","published":"2024-10-22T15:59:58Z","title":"Exploring RL-based LLM Training for Formal Language Tasks with\n  Programmed Rewards","summary":"  Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning\nfrom Human Feedback to align large language models (LLMs) with downstream\ntasks. This paper investigates the feasibility of using PPO for direct\nreinforcement learning (RL) from explicitly programmed reward signals, as\nopposed to indirect learning from human feedback via an intermediary reward\nmodel. We focus on tasks expressed through formal languages, such as\nmathematics and programming, where explicit reward functions can be programmed\nto automatically assess the quality of generated outputs. We apply this\napproach to a sentiment alignment task, a simple arithmetic task, and a more\ncomplex game synthesis task. The sentiment alignment task replicates prior\nresearch and serves to validate our experimental setup. Our results show that\npure RL-based training for the two formal language tasks is challenging, with\nsuccess being limited even for the simple arithmetic task. We propose a novel\nbatch-entropy regularization term to aid exploration, although training is not\nyet entirely stable. Our findings suggest that direct RL training of LLMs may\nbe more suitable for relatively minor changes, such as alignment, than for\nlearning new tasks altogether, even if an informative reward signal can be\nexpressed programmatically.\n","authors":["Alexander G. Padula","Dennis J. N. J. Soemers"],"pdf_url":"https://arxiv.org/pdf/2410.17126v1.pdf","comment":"Accepted at BNAIC 2024"},{"id":"http://arxiv.org/abs/2410.17112v1","updated":"2024-10-22T15:37:46Z","published":"2024-10-22T15:37:46Z","title":"Enhancing Answer Attribution for Faithful Text Generation with Large\n  Language Models","summary":"  The increasing popularity of Large Language Models (LLMs) in recent years has\nchanged the way users interact with and pose questions to AI-based\nconversational systems. An essential aspect for increasing the trustworthiness\nof generated LLM answers is the ability to trace the individual claims from\nresponses back to relevant sources that support them, the process known as\nanswer attribution. While recent work has started exploring the task of answer\nattribution in LLMs, some challenges still remain. In this work, we first\nperform a case study analyzing the effectiveness of existing answer attribution\nmethods, with a focus on subtasks of answer segmentation and evidence\nretrieval. Based on the observed shortcomings, we propose new methods for\nproducing more independent and contextualized claims for better retrieval and\nattribution. The new methods are evaluated and shown to improve the performance\nof answer attribution components. We end with a discussion and outline of\nfuture directions for the task.\n","authors":["Juraj Vladika","Luca Mülln","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2410.17112v1.pdf","comment":"Accepted to KDIR 2024 (part of IC3K 2024)"},{"id":"http://arxiv.org/abs/2410.17099v1","updated":"2024-10-22T15:22:58Z","published":"2024-10-22T15:22:58Z","title":"Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations","summary":"  The quality is a crucial issue for crowd annotations. Answer aggregation is\nan important type of solution. The aggregated answers estimated from multiple\ncrowd answers to the same instance are the eventually collected annotations,\nrather than the individual crowd answers themselves. Recently, the capability\nof Large Language Models (LLMs) on data annotation tasks has attracted interest\nfrom researchers. Most of the existing studies mainly focus on the average\nperformance of individual crowd workers; several recent works studied the\nscenarios of aggregation on categorical labels and LLMs used as label creators.\nHowever, the scenario of aggregation on text answers and the role of LLMs as\naggregators are not yet well-studied. In this paper, we investigate the\ncapability of LLMs as aggregators in the scenario of close-ended crowd text\nanswer aggregation. We propose a human-LLM hybrid text answer aggregation\nmethod with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We\nmake the experiments based on public crowdsourcing datasets. The results show\nthe effectiveness of our approach based on the collaboration of crowd workers\nand LLMs.\n","authors":["Jiyi Li"],"pdf_url":"https://arxiv.org/pdf/2410.17099v1.pdf","comment":"Accepted in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14516v2","updated":"2024-10-22T15:20:00Z","published":"2024-10-18T14:55:14Z","title":"Do LLMs \"know\" internally when they follow instructions?","summary":"  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n","authors":["Juyeon Heo","Christina Heinze-Deml","Oussama Elachqar","Shirley Ren","Udhay Nallasamy","Andy Miller","Kwan Ho Ryan Chan","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13023v2","updated":"2024-10-22T15:18:55Z","published":"2022-09-26T20:49:18Z","title":"Lex2Sent: A bagging approach to unsupervised sentiment analysis","summary":"  Unsupervised text classification, with its most common form being sentiment\nanalysis, used to be performed by counting words in a text that were stored in\na lexicon, which assigns each word to one class or as a neutral word. In recent\nyears, these lexicon-based methods fell out of favor and were replaced by\ncomputationally demanding fine-tuning techniques for encoder-only models such\nas BERT and zero-shot classification using decoder-only models such as GPT-4.\nIn this paper, we propose an alternative approach: Lex2Sent, which provides\nimprovement over classic lexicon methods but does not require any GPU or\nexternal hardware. To classify texts, we train embedding models to determine\nthe distances between document embeddings and the embeddings of the parts of a\nsuitable lexicon. We employ resampling, which results in a bagging effect,\nboosting the performance of the classification. We show that our model\noutperforms lexica and provides a basis for a high performing few-shot\nfine-tuning approach in the task of binary sentiment analysis.\n","authors":["Kai-Robin Lange","Jonas Rieger","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2209.13023v2.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.14582v2","updated":"2024-10-22T15:16:14Z","published":"2024-10-18T16:32:10Z","title":"Do LLMs estimate uncertainty well in instruction-following?","summary":"  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n","authors":["Juyeon Heo","Miao Xiong","Christina Heinze-Deml","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17088v1","updated":"2024-10-22T15:14:54Z","published":"2024-10-22T15:14:54Z","title":"Science Out of Its Ivory Tower: Improving Accessibility with\n  Reinforcement Learning","summary":"  A vast amount of scholarly work is published daily, yet much of it remains\ninaccessible to the general public due to dense jargon and complex language. To\naddress this challenge in science communication, we introduce a reinforcement\nlearning framework that fine-tunes a language model to rewrite scholarly\nabstracts into more comprehensible versions. Guided by a carefully balanced\ncombination of word- and sentence-level accessibility rewards, our language\nmodel effectively substitutes technical terms with more accessible\nalternatives, a task which models supervised fine-tuned or guided by\nconventional readability measures struggle to accomplish. Our best model\nadjusts the readability level of scholarly abstracts by approximately six U.S.\ngrade levels -- in other words, from a postgraduate to a high school level.\nThis translates to roughly a 90% relative boost over the supervised fine-tuning\nbaseline, all while maintaining factual accuracy and high-quality language. An\nin-depth analysis of our approach shows that balanced rewards lead to\nsystematic modifications in the base model, likely contributing to smoother\noptimization and superior performance. We envision this work as a step toward\nbridging the gap between scholarly research and the general public,\nparticularly younger readers and those without a college degree.\n","authors":["Haining Wang","Jason Clark","Hannah McKelvey","Leila Sterman","Zheng Gao","Zuoyu Tian","Sandra Kübler","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16264v3","updated":"2024-10-22T15:09:58Z","published":"2024-06-24T02:03:57Z","title":"One Thousand and One Pairs: A \"novel\" challenge for long-context\n  language models","summary":"  Synthetic long-context LLM benchmarks (e.g., \"needle-in-the-haystack\") test\nonly surface-level retrieval capabilities, but how well can long-context LLMs\nretrieve, synthesize, and reason over information across book-length inputs? We\naddress this question by creating NoCha, a dataset of 1,001 minimally different\npairs of true and false claims about 67 recently-published English fictional\nbooks, written by human readers of those books. In contrast to existing\nlong-context benchmarks, our annotators confirm that the largest share of pairs\nin NoCha require global reasoning over the entire book to verify. Our\nexperiments show that while human readers easily perform this task, it is\nenormously challenging for all ten long-context LLMs that we evaluate: no\nopen-weight model performs above random chance (despite their strong\nperformance on synthetic benchmarks), while GPT-4o achieves the highest\naccuracy at 55.8%. Further analysis reveals that (1) on average, models perform\nmuch better on pairs that require only sentence-level retrieval vs. global\nreasoning; (2) model-generated explanations for their decisions are often\ninaccurate even for correctly-labeled claims; and (3) models perform\nsubstantially worse on speculative fiction books that contain extensive\nworld-building. The methodology proposed in NoCha allows for the evolution of\nthe benchmark dataset and the easy analysis of future models.\n","authors":["Marzena Karpinska","Katherine Thai","Kyle Lo","Tanya Goyal","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.16264v3.pdf","comment":"EMNLP 2024, camera ready"},{"id":"http://arxiv.org/abs/2408.10943v2","updated":"2024-10-22T15:07:35Z","published":"2024-08-20T15:33:16Z","title":"SysBench: Can Large Language Models Follow System Messages?","summary":"  Large Language Models (LLMs) have become instrumental across various\napplications, with the customization of these models to specific scenarios\nbecoming increasingly critical. System message, a fundamental component of\nLLMs, is consist of carefully crafted instructions that guide the behavior of\nmodel to meet intended goals. Despite the recognized potential of system\nmessages to optimize AI-driven solutions, there is a notable absence of a\ncomprehensive benchmark for evaluating how well LLMs follow system messages. To\nfill this gap, we introduce SysBench, a benchmark that systematically analyzes\nsystem message following ability in terms of three limitations of existing\nLLMs: constraint violation, instruction misjudgement and multi-turn\ninstability. Specifically, we manually construct evaluation dataset based on\nsix prevalent types of constraints, including 500 tailor-designed system\nmessages and multi-turn user conversations covering various interaction\nrelationships. Additionally, we develop a comprehensive evaluation protocol to\nmeasure model performance. Finally, we conduct extensive evaluation across\nvarious existing LLMs, measuring their ability to follow specified constraints\ngiven in system messages. The results highlight both the strengths and\nweaknesses of existing models, offering key insights and directions for future\nresearch. The open source library SysBench is available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/SysBench.\n","authors":["Yanzhao Qin","Tao Zhang","Tao Zhang","Yanjun Shen","Wenjing Luo","Haoze Sun","Yan Zhang","Yujing Qiao","Weipeng Chen","Zenan Zhou","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2408.10943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17081v1","updated":"2024-10-22T15:02:37Z","published":"2024-10-22T15:02:37Z","title":"Continuous Speech Tokenizer in Text To Speech","summary":"  The fusion of speech and language in the era of large language models has\ngarnered significant attention. Discrete speech token is often utilized in\ntext-to-speech tasks for speech compression and portability, which is\nconvenient for joint training with text and have good compression efficiency.\nHowever, we found that the discrete speech tokenizer still suffers from\ninformation loss. Therefore, we propose a simple yet effective continuous\nspeech tokenizer and a text-to-speech model based on continuous speech tokens.\nOur results show that the speech language model based on the continuous speech\ntokenizer has better continuity and higher estimated Mean Opinion Scores (MoS).\nThis enhancement is attributed to better information preservation rate of the\ncontinuous speech tokenizer across both low and high frequencies in the\nfrequency domain.\n","authors":["Yixing Li","Ruobing Xie","Xingwu Sun","Yu Cheng","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2410.17081v1.pdf","comment":"4 pages. Under review"},{"id":"http://arxiv.org/abs/2410.17051v1","updated":"2024-10-22T14:30:40Z","published":"2024-10-22T14:30:40Z","title":"Data-driven Coreference-based Ontology Building","summary":"  While coreference resolution is traditionally used as a component in\nindividual document understanding, in this work we take a more global view and\nexplore what can we learn about a domain from the set of all document-level\ncoreference relations that are present in a large corpus. We derive coreference\nchains from a corpus of 30 million biomedical abstracts and construct a graph\nbased on the string phrases within these chains, establishing connections\nbetween phrases if they co-occur within the same coreference chain. We then use\nthe graph structure and the betweeness centrality measure to distinguish\nbetween edges denoting hierarchy, identity and noise, assign directionality to\nedges denoting hierarchy, and split nodes (strings) that correspond to multiple\ndistinct concepts. The result is a rich, data-driven ontology over concepts in\nthe biomedical domain, parts of which overlaps significantly with\nhuman-authored ontologies. We release the coreference chains and resulting\nontology under a creative-commons license, along with the code.\n","authors":["Shir Ashury-Tahan","Amir David Nissan Cohen","Nadav Cohen","Yoram Louzoun","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2410.17051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17050v1","updated":"2024-10-22T14:30:03Z","published":"2024-10-22T14:30:03Z","title":"UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs","summary":"  The key components of machine learning are data samples for training, model\nfor learning patterns, and loss function for optimizing accuracy. Analogously,\nunlearning can potentially be achieved through anti-data samples (or\nanti-samples), unlearning method, and reversed loss function. While prior\nresearch has explored unlearning methods and reversed loss functions, the\npotential of anti-samples remains largely untapped. In this paper, we introduce\nUnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language\nmodels (LLMs). Our contributions are threefold; first, we propose a novel\nconcept of anti-sample-induced unlearning; second, we generate anti-samples by\nleveraging misleading rationales, which help reverse learned associations and\naccelerate the unlearning process; and third, we enable fine-grained targeted\nunlearning, allowing for the selective removal of specific associations without\nimpacting related knowledge - something not achievable by previous works.\nResults demonstrate that anti-samples offer an efficient, targeted unlearning\nstrategy for LLMs, opening new avenues for privacy-preserving machine learning\nand model modification.\n","authors":["Yash Sinha","Murari Mandal","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2410.17050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17040v1","updated":"2024-10-22T14:12:43Z","published":"2024-10-22T14:12:43Z","title":"Arabic Dataset for LLM Safeguard Evaluation","summary":"  The growing use of large language models (LLMs) has raised concerns regarding\ntheir safety. While many studies have focused on English, the safety of LLMs in\nArabic, with its linguistic and cultural complexities, remains under-explored.\nHere, we aim to bridge this gap. In particular, we present an\nArab-region-specific safety evaluation dataset consisting of 5,799 questions,\nincluding direct attacks, indirect attacks, and harmless requests with\nsensitive words, adapted to reflect the socio-cultural context of the Arab\nworld. To uncover the impact of different stances in handling sensitive and\ncontroversial topics, we propose a dual-perspective evaluation framework. It\nassesses the LLM responses from both governmental and opposition viewpoints.\nExperiments over five leading Arabic-centric and multilingual LLMs reveal\nsubstantial disparities in their safety performance. This reinforces the need\nfor culturally specific datasets to ensure the responsible deployment of LLMs.\n","authors":["Yasser Ashraf","Yuxia Wang","Bin Gu","Preslav Nakov","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2410.17040v1.pdf","comment":"17 pages, 6 figures, 10 tables"},{"id":"http://arxiv.org/abs/2404.18923v4","updated":"2024-10-22T14:08:52Z","published":"2024-04-29T17:58:36Z","title":"Holmes: A Benchmark to Assess the Linguistic Competence of Language\n  Models","summary":"  We introduce Holmes, a new benchmark designed to assess language models (LMs)\nlinguistic competence - their unconscious understanding of linguistic\nphenomena. Specifically, we use classifier-based probing to examine LMs'\ninternal representations regarding distinct linguistic phenomena (e.g.,\npart-of-speech tagging). As a result, we meet recent calls to disentangle LMs'\nlinguistic competence from other cognitive abilities, such as following\ninstructions in prompting-based evaluations. Composing Holmes, we review over\n270 probing studies and include more than 200 datasets to assess syntax,\nmorphology, semantics, reasoning, and discourse phenomena. Analyzing over 50\nLMs reveals that, aligned with known trends, their linguistic competence\ncorrelates with model size. However, surprisingly, model architecture and\ninstruction tuning also significantly influence performance, particularly in\nmorphology and syntax. Finally, we propose FlashHolmes, a streamlined version\nthat reduces the computation load while maintaining high-ranking precision.\n","authors":["Andreas Waldis","Yotam Perlitz","Leshem Choshen","Yufang Hou","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2404.18923v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13948v2","updated":"2024-10-22T14:07:57Z","published":"2024-04-22T07:49:36Z","title":"Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by\n  Simulating Documents in the Wild via Low-level Perturbations","summary":"  The robustness of recent Large Language Models (LLMs) has become increasingly\ncrucial as their applicability expands across various domains and real-world\napplications. Retrieval-Augmented Generation (RAG) is a promising solution for\naddressing the limitations of LLMs, yet existing studies on the robustness of\nRAG often overlook the interconnected relationships between RAG components or\nthe potential threats prevalent in real-world databases, such as minor textual\nerrors. In this work, we investigate two underexplored aspects when assessing\nthe robustness of RAG: 1) vulnerability to noisy documents through low-level\nperturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we\nintroduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}),\nwhich targets these aspects. Specifically, GARAG is designed to reveal\nvulnerabilities within each component and test the overall system functionality\nagainst noisy documents. We validate RAG robustness by applying our\n\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and\nLLMs. The experimental results show that GARAG consistently achieves high\nattack success rates. Also, it significantly devastates the performance of each\ncomponent and their synergy, highlighting the substantial risk that minor\ntextual inaccuracies pose in disrupting RAG systems in the real world.\n","authors":["Sukmin Cho","Soyeong Jeong","Jeongyeon Seo","Taeho Hwang","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2404.13948v2.pdf","comment":"Findings of EMNLP Camera-ready version"},{"id":"http://arxiv.org/abs/2410.17035v1","updated":"2024-10-22T14:06:31Z","published":"2024-10-22T14:06:31Z","title":"DIRI: Adversarial Patient Reidentification with Large Language Models\n  for Evaluating Clinical Text Anonymization","summary":"  Sharing protected health information (PHI) is critical for furthering\nbiomedical research. Before data can be distributed, practitioners often\nperform deidentification to remove any PHI contained in the text. Contemporary\ndeidentification methods are evaluated on highly saturated datasets (tools\nachieve near-perfect accuracy) which may not reflect the full variability or\ncomplexity of real-world clinical text and annotating them is resource\nintensive, which is a barrier to real-world applications. To address this gap,\nwe developed an adversarial approach using a large language model (LLM) to\nre-identify the patient corresponding to a redacted clinical note and evaluated\nthe performance with a novel De-Identification/Re-Identification (DIRI) method.\nOur method uses a large language model to reidentify the patient corresponding\nto a redacted clinical note. We demonstrate our method on medical data from\nWeill Cornell Medicine anonymized with three deidentification tools: rule-based\nPhilter and two deep-learning-based models, BiLSTM-CRF and ClinicalBERT.\nAlthough ClinicalBERT was the most effective, masking all identified PII, our\ntool still reidentified 9% of clinical notes Our study highlights significant\nweaknesses in current deidentification technologies while providing a tool for\niterative development and improvement.\n","authors":["John X. Morris","Thomas R. Campion","Sri Laasya Nutheti","Yifan Peng","Akhil Raj","Ramin Zabih","Curtis L. Cole"],"pdf_url":"https://arxiv.org/pdf/2410.17035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15608v2","updated":"2024-10-22T13:55:26Z","published":"2024-10-21T03:13:20Z","title":"Moonshine: Speech Recognition for Live Transcription and Voice Commands","summary":"  This paper introduces Moonshine, a family of speech recognition models\noptimized for live transcription and voice command processing. Moonshine is\nbased on an encoder-decoder transformer architecture and employs Rotary\nPosition Embedding (RoPE) instead of traditional absolute position embeddings.\nThe model is trained on speech segments of various lengths, but without using\nzero-padding, leading to greater efficiency for the encoder during inference\ntime. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny\ndemonstrates a 5x reduction in compute requirements for transcribing a\n10-second speech segment while incurring no increase in word error rates across\nstandard evaluation datasets. These results highlight Moonshine's potential for\nreal-time and resource-constrained applications.\n","authors":["Nat Jeffries","Evan King","Manjunath Kudlur","Guy Nicholson","James Wang","Pete Warden"],"pdf_url":"https://arxiv.org/pdf/2410.15608v2.pdf","comment":"7 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.17028v1","updated":"2024-10-22T13:52:51Z","published":"2024-10-22T13:52:51Z","title":"Can a Machine Distinguish High and Low Amount of Social Creak in Speech?","summary":"  Objectives: ncreased prevalence of social creak particularly among female\nspeakers has been reported in several studies. The study of social creak has\nbeen previously conducted by combining perceptual evaluation of speech with\nconventional acoustical parameters such as the harmonic-to-noise ratio and\ncepstral peak prominence. In the current study, machine learning (ML) was used\nto automatically distinguish speech of low amount of social creak from speech\nof high amount of social creak.\n  Methods: The amount of creak in continuous speech samples produced in Finnish\nby 90 female speakers was first perceptually assessed by two voice specialists.\nBased on their assessments, the speech samples were divided into two categories\n(low $vs$. high amount of creak). Using the speech signals and their creak\nlabels, seven different ML models were trained. Three spectral representations\nwere used as feature for each model.\n  Results: The results show that the best performance (accuracy of 71.1\\%) was\nobtained by the following two systems: an Adaboost classifier using the\nmel-spectrogram feature and a decision tree classifier using the mel-frequency\ncepstral coefficient feature.\n  Conclusions: The study of social creak is becoming increasingly popular in\nsociolinguistic and vocological research. The conventional human perceptual\nassessment of the amount of creak is laborious and therefore ML technology\ncould be used to assist researchers studying social creak. The classification\nsystems reported in this study could be considered as baselines in future\nML-based studies on social creak.\n","authors":["Anne-Maria Laukkanen","Sudarsana Reddy Kadiri","Shrikanth Narayanan","Paavo Alku"],"pdf_url":"https://arxiv.org/pdf/2410.17028v1.pdf","comment":"Accepted in Journal of Voice"},{"id":"http://arxiv.org/abs/2410.17021v1","updated":"2024-10-22T13:47:38Z","published":"2024-10-22T13:47:38Z","title":"SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop\n  Question Answering Based on Finite State Machine","summary":"  Large Language Models with chain-of-thought prompting, such as OpenAI-o1,\nhave shown impressive capabilities in natural language inference tasks.\nHowever, Multi-hop Question Answering (MHQA) remains challenging for many\nexisting models due to issues like hallucination, error propagation, and\nlimited context length. To address these challenges and enhance LLMs'\nperformance on MHQA, we propose the Self-Guiding prompting Finite State Machine\n(SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike\ntraditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively\nbreaking down complex questions into sub-questions, correcting itself to\nimprove accuracy. It processes one sub-question at a time, dynamically deciding\nthe next step based on the current context and results, functioning much like\nan automaton. Experiments across various benchmarks demonstrate the\neffectiveness of our approach, outperforming strong baselines on challenging\ndatasets such as Musique. SG-FSM reduces hallucination, enabling recovery of\nthe correct final answer despite intermediate errors. It also improves\nadherence to specified output formats, simplifying evaluation significantly.\n","authors":["Xiaochen Wang","Junqing He","Liang Chen","Reza Haf Zhe Yang","Yiru Wang","Xiangdi Meng","Kunhao Pan","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2410.17021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16070v2","updated":"2024-10-22T13:40:18Z","published":"2024-10-21T14:48:35Z","title":"On-Device LLMs for SMEs: Challenges and Opportunities","summary":"  This paper presents a systematic review of the infrastructure requirements\nfor deploying Large Language Models (LLMs) on-device within the context of\nsmall and medium-sized enterprises (SMEs), focusing on both hardware and\nsoftware perspectives. From the hardware viewpoint, we discuss the utilization\nof processing units like GPUs and TPUs, efficient memory and storage solutions,\nand strategies for effective deployment, addressing the challenges of limited\ncomputational resources typical in SME settings. From the software perspective,\nwe explore framework compatibility, operating system optimization, and the use\nof specialized libraries tailored for resource-constrained environments. The\nreview is structured to first identify the unique challenges faced by SMEs in\ndeploying LLMs on-device, followed by an exploration of the opportunities that\nboth hardware innovations and software adaptations offer to overcome these\nobstacles. Such a structured review provides practical insights, contributing\nsignificantly to the community by enhancing the technological resilience of\nSMEs in integrating LLMs.\n","authors":["Jeremy Stephen Gabriel Yee","Pai Chet Ng","Zhengkui Wang","Ian McLoughlin","Aik Beng Ng","Simon See"],"pdf_url":"https://arxiv.org/pdf/2410.16070v2.pdf","comment":"9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI\n  Centre"},{"id":"http://arxiv.org/abs/2410.17018v1","updated":"2024-10-22T13:39:47Z","published":"2024-10-22T13:39:47Z","title":"Exploring Forgetting in Large Language Model Pre-Training","summary":"  Catastrophic forgetting remains a formidable obstacle to building an\nomniscient model in large language models (LLMs). Despite the pioneering\nresearch on task-level forgetting in LLM fine-tuning, there is scant focus on\nforgetting during pre-training. We systematically explored the existence and\nmeasurement of forgetting in pre-training, questioning traditional metrics such\nas perplexity (PPL) and introducing new metrics to better detect entity memory\nretention. Based on our revised assessment of forgetting metrics, we explored\nlow-cost, straightforward methods to mitigate forgetting during the\npre-training phase. Further, we carefully analyzed the learning curves,\noffering insights into the dynamics of forgetting. Extensive evaluations and\nanalyses on forgetting of pre-training could facilitate future research on\nLLMs.\n","authors":["Chonghua Liao","Ruobing Xie","Xingwu Sun","Haowen Sun","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2410.17018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10851v2","updated":"2024-10-22T13:08:02Z","published":"2024-10-06T12:53:07Z","title":"LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis","summary":"  In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.\n","authors":["Haozhou Pang","Tianwei Ding","Lanshan He","Ming Tao","Lu Zhang","Qi Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16977v1","updated":"2024-10-22T12:56:04Z","published":"2024-10-22T12:56:04Z","title":"IPL: Leveraging Multimodal Large Language Models for Intelligent Product\n  Listing","summary":"  Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g.,\nAmazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are\nmainly targeting individual sellers who usually lack sufficient experience in\ne-commerce. Individual sellers often struggle to compose proper descriptions\nfor selling products. With the recent advancement of Multimodal Large Language\nModels (MLLMs), we attempt to integrate such state-of-the-art generative AI\ntechnologies into the product listing process. To this end, we develop IPL, an\nIntelligent Product Listing tool tailored to generate descriptions using\nvarious product attributes such as category, brand, color, condition, etc. IPL\nenables users to compose product descriptions by merely uploading photos of the\nselling product. More importantly, it can imitate the content style of our C2C\nplatform Xianyu. This is achieved by employing domain-specific instruction\ntuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation\n(RAG) process. A comprehensive empirical evaluation demonstrates that the\nunderlying model of IPL significantly outperforms the base model in\ndomain-specific tasks while producing less hallucination. IPL has been\nsuccessfully deployed in our production system, where 72% of users have their\npublished product listings based on the generated content, and those product\nlistings are shown to have a quality score 5.6% higher than those without AI\nassistance.\n","authors":["Kang Chen","Qingheng Zhang","Chengbao Lian","Yixin Ji","Xuwei Liu","Shuguang Han","Guoqiang Wu","Fei Huang","Jufeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16973v1","updated":"2024-10-22T12:51:51Z","published":"2024-10-22T12:51:51Z","title":"Learning Mathematical Rules with Large Language Models","summary":"  In this paper, we study the ability of large language models to learn\nspecific mathematical rules such as distributivity or simplifying equations. We\npresent an empirical analysis of their ability to generalize these rules, as\nwell as to reuse them in the context of word problems. For this purpose, we\nprovide a rigorous methodology to build synthetic data incorporating such\nrules, and perform fine-tuning of large language models on such data. Our\nexperiments show that our model can learn and generalize these rules to some\nextent, as well as suitably reuse them in the context of word problems.\n","authors":["Antoine Gorceix","Bastien Le Chenadec","Ahmad Rammal","Nelson Vadori","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2410.16973v1.pdf","comment":"4th MATH-AI Workshop at NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.16930v1","updated":"2024-10-22T12:00:58Z","published":"2024-10-22T12:00:58Z","title":"Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities\n  Using Only Forward Passes","summary":"  Math reasoning is a highly active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence. However, few works have\nexplored how math reasoning is encoded within LLM parameters and if it is a\nskill that can be isolated within a model. Doing so could allow targeted\nintervention to improve math performance without altering non-math behavior and\nfoster understanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a method for isolating math-specific parameters in\nLLMs using only forward passes. MathNeuro builds on existing work by using\nweights and activations to calculate parameter importance, but isolates\nmath-specific parameters by removing those important for general language\ntasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning\nability without destroying its general language ability. Scaling these\nparameters by a small constant improves a pretrained or instruction-tuned LLM's\nperformance by 4-17% on GSM8K while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.\n","authors":["Bryan R. Christ","Zack Gottesman","Jonathan Kropko","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2410.16930v1.pdf","comment":"21 pages, 29 figures"},{"id":"http://arxiv.org/abs/2410.07114v3","updated":"2024-10-22T11:55:46Z","published":"2024-09-19T19:48:31Z","title":"System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam","summary":"  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch average of 40.63 points. Neither model had access to the\nexam figures. Since there was a risk of model contamination (i.e., the\nknowledge cutoff of o1-preview and GPT-4o was after the exam was published\nonline), we repeated the procedure with a new Mathematics B exam that was\npublished after the cutoff date. The results again indicated that o1-preview\nperformed strongly (97.8th percentile), which suggests that contamination was\nnot a factor. We also show that there is some variability in the output of\no1-preview, which means that sometimes there is 'luck' (the answer is correct)\nor 'bad luck' (the output has diverged into something that is incorrect). We\ndemonstrate that a self-consistency approach, where repeated prompts are given\nand the most common answer is selected, is a useful strategy for identifying\nthe correct answer. It is concluded that while OpenAI's new model series holds\ngreat potential, certain risks must be considered.\n","authors":["Joost de Winter","Dimitra Dodou","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2410.07114v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16919v1","updated":"2024-10-22T11:52:22Z","published":"2024-10-22T11:52:22Z","title":"EnvBridge: Bridging Diverse Environments with Cross-Environment\n  Knowledge Transfer for Embodied AI","summary":"  In recent years, Large Language Models (LLMs) have demonstrated high\nreasoning capabilities, drawing attention for their applications as agents in\nvarious decision-making processes. One notably promising application of LLM\nagents is robotic manipulation. Recent research has shown that LLMs can\ngenerate text planning or control code for robots, providing substantial\nflexibility and interaction capabilities. However, these methods still face\nchallenges in terms of flexibility and applicability across different\nenvironments, limiting their ability to adapt autonomously. Current approaches\ntypically fall into two categories: those relying on environment-specific\npolicy training, which restricts their transferability, and those generating\ncode actions based on fixed prompts, which leads to diminished performance when\nconfronted with new environments. These limitations significantly constrain the\ngeneralizability of agents in robotic manipulation. To address these\nlimitations, we propose a novel method called EnvBridge. This approach involves\nthe retention and transfer of successful robot control codes from source\nenvironments to target environments. EnvBridge enhances the agent's\nadaptability and performance across diverse settings by leveraging insights\nfrom multiple environments. Notably, our approach alleviates environmental\nconstraints, offering a more flexible and generalizable solution for robotic\nmanipulation tasks. We validated the effectiveness of our method using robotic\nmanipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments\ndemonstrate that LLM agents can successfully leverage diverse knowledge sources\nto solve complex tasks. Consequently, our approach significantly enhances the\nadaptability and robustness of robotic manipulation agents in planning across\ndiverse environments.\n","authors":["Tomoyuki Kagaya","Yuxuan Lou","Thong Jing Yuan","Subramanian Lakshmi","Jayashree Karlekar","Sugiri Pranata","Natsuki Murakami","Akira Kinose","Koki Oguri","Felix Wick","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.16919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07457v3","updated":"2024-10-22T10:54:15Z","published":"2024-07-10T08:20:47Z","title":"GLBench: A Comprehensive Benchmark for Graph with Large Language Models","summary":"  The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.\n","authors":["Yuhan Li","Peisong Wang","Xiao Zhu","Aochuan Chen","Haiyun Jiang","Deng Cai","Victor Wai Kin Chan","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.07457v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15689v2","updated":"2024-10-22T10:49:57Z","published":"2024-08-28T10:25:53Z","title":"TempoFormer: A Transformer for Temporally-aware Representations in\n  Change Detection","summary":"  Dynamic representation learning plays a pivotal role in understanding the\nevolution of linguistic content over time. On this front both context and time\ndynamics as well as their interplay are of prime importance. Current approaches\nmodel context via pre-trained representations, which are typically temporally\nagnostic. Previous work on modelling context and temporal dynamics has used\nrecurrent methods, which are slow and prone to overfitting. Here we introduce\nTempoFormer, the first task-agnostic transformer-based and temporally-aware\nmodel for dynamic representation learning. Our approach is jointly trained on\ninter and intra context dynamics and introduces a novel temporal variation of\nrotary positional embeddings. The architecture is flexible and can be used as\nthe temporal representation foundation of other models or applied to different\ntransformer-based architectures. We show new SOTA performance on three\ndifferent real-time change detection tasks.\n","authors":["Talia Tseriotou","Adam Tsakalidis","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2408.15689v2.pdf","comment":"Accepted by EMNLP Main 2024"},{"id":"http://arxiv.org/abs/2405.15319v2","updated":"2024-10-22T10:31:59Z","published":"2024-05-24T08:00:00Z","title":"Stacking Your Transformers: A Closer Look at Model Growth for Efficient\n  LLM Pre-Training","summary":"  LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to\naccelerate the training of larger ones. However, the viability of these model\ngrowth methods in efficient LLM pre-training remains underexplored. This work\nidentifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1)\nlack of comprehensive evaluation, ($\\textit{O}$2) untested viability for\nscaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle\n$\\textit{O}$1, we summarize existing approaches into four atomic growth\noperators and systematically evaluate them in a standardized LLM pre-training\nsetting. Our findings reveal that a depthwise stacking operator, called\n$G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to\ndecreased loss and improved overall performance on eight standard NLP\nbenchmarks compared to strong baselines. Motivated by these promising results,\nwe conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to\naddress $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested\nscalability), our study shows that $G_{\\text{stack}}$ is scalable and\nconsistently performs well, with experiments up to 7B LLMs after growth and\npre-training LLMs with 750B tokens. For example, compared to a conventionally\ntrained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to\nthe same loss with 194B tokens, resulting in a 54.6\\% speedup. We further\naddress $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines\nto determine growth timing and growth factor for $G_{\\text{stack}}$, making it\npractical in general LLM pre-training. We also provide in-depth discussions and\ncomprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained\nmodel are available at https://llm-stacking.github.io.\n","authors":["Wenyu Du","Tongxu Luo","Zihan Qiu","Zeyu Huang","Yikang Shen","Reynold Cheng","Yike Guo","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2405.15319v2.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.14262v2","updated":"2024-10-22T10:12:00Z","published":"2024-10-18T08:18:18Z","title":"Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation","summary":"  This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.\n","authors":["Ted Kwartler","Matthew Berman","Alan Aqrawi"],"pdf_url":"https://arxiv.org/pdf/2410.14262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16855v1","updated":"2024-10-22T09:43:39Z","published":"2024-10-22T09:43:39Z","title":"Tracing the Development of the Virtual Particle Concept Using Semantic\n  Change Detection","summary":"  Virtual particles are peculiar objects. They figure prominently in much of\ntheoretical and experimental research in elementary particle physics. But\nexactly what they are is far from obvious. In particular, to what extent they\nshould be considered \"real\" remains a matter of controversy in philosophy of\nscience. Also their origin and development has only recently come into focus of\nscholarship in the history of science. In this study, we propose using the\nintriguing case of virtual particles to discuss the efficacy of Semantic Change\nDetection (SCD) based on contextualized word embeddings from a domain-adapted\nBERT model in studying specific scientific concepts. We find that the SCD\nmetrics align well with qualitative research insights in the history and\nphilosophy of science, as well as with the results obtained from Dependency\nParsing to determine the frequency and connotations of the term \"virtual.\"\nStill, the metrics of SCD provide additional insights over and above the\nqualitative research and the Dependency Parsing. Among other things, the\nmetrics suggest that the concept of the virtual particle became more stable\nafter 1950 but at the same time also more polysemous.\n","authors":["Michael Zichert","Adrian Wüthrich"],"pdf_url":"https://arxiv.org/pdf/2410.16855v1.pdf","comment":"CHR 2024: Computational Humanities Research Conference"},{"id":"http://arxiv.org/abs/2410.16077v2","updated":"2024-10-22T09:37:45Z","published":"2024-10-21T14:55:59Z","title":"CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian\n  Product Routing in Mixture-of-Experts","summary":"  Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.\n","authors":["Zhenpeng Su","Xing Wu","Zijia Lin","Yizhe Xiong","Minxuan Lv","Guangyuan Ma","Hui Chen","Songlin Hu","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2410.16077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16848v1","updated":"2024-10-22T09:35:42Z","published":"2024-10-22T09:35:42Z","title":"ETHIC: Evaluating Large Language Models on Long-Context Tasks with High\n  Information Coverage","summary":"  Recent advancements in large language models (LLM) capable of processing\nextremely long texts highlight the need for a dedicated evaluation benchmark to\nassess their long-context capabilities. However, existing methods, like the\nneedle-in-a-haystack test, do not effectively assess whether these models fully\nutilize contextual information, raising concerns about the reliability of\ncurrent evaluation techniques. To thoroughly examine the effectiveness of\nexisting benchmarks, we introduce a new metric called information coverage\n(IC), which quantifies the proportion of the input context necessary for\nanswering queries. Our findings indicate that current benchmarks exhibit low\nIC; although the input context may be extensive, the actual usable context is\noften limited. To address this, we present ETHIC, a novel benchmark designed to\nassess LLMs' ability to leverage the entire context. Our benchmark comprises\n2,648 test instances spanning four long-context tasks with high IC scores in\nthe domains of books, debates, medicine, and law. Our evaluations reveal\nsignificant performance drops in contemporary LLMs, highlighting a critical\nchallenge in managing long contexts. Our benchmark is available at\nhttps://github.com/dmis-lab/ETHIC.\n","authors":["Taewhoo Lee","Chanwoong Yoon","Kyochul Jang","Donghyeon Lee","Minju Song","Hyunjae Kim","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2410.16848v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.16843v1","updated":"2024-10-22T09:25:21Z","published":"2024-10-22T09:25:21Z","title":"Trustworthy Alignment of Retrieval-Augmented Large Language Models via\n  Reinforcement Learning","summary":"  Trustworthiness is an essential prerequisite for the real-world application\nof large language models. In this paper, we focus on the trustworthiness of\nlanguage models with respect to retrieval augmentation. Despite being supported\nwith external evidence, retrieval-augmented generation still suffers from\nhallucinations, one primary cause of which is the conflict between contextual\nand parametric knowledge. We deem that retrieval-augmented language models have\nthe inherent capabilities of supplying response according to both contextual\nand parametric knowledge. Inspired by aligning language models with human\npreference, we take the first step towards aligning retrieval-augmented\nlanguage models to a status where it responds relying merely on the external\nevidence and disregards the interference of parametric knowledge. Specifically,\nwe propose a reinforcement learning based algorithm Trustworthy-Alignment,\ntheoretically and experimentally demonstrating large language models'\ncapability of reaching a trustworthy status without explicit supervision on how\nto respond. Our work highlights the potential of large language models on\nexploring its intrinsic abilities by its own and expands the application\nscenarios of alignment from fulfilling human preference to creating trustworthy\nagents.\n","authors":["Zongmeng Zhang","Yufeng Shi","Jinhua Zhu","Wengang Zhou","Xiang Qi","Peng Zhang","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2410.16843v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2410.16842v1","updated":"2024-10-22T09:25:04Z","published":"2024-10-22T09:25:04Z","title":"Assessment of Transformer-Based Encoder-Decoder Model for Human-Like\n  Summarization","summary":"  In recent times, extracting valuable information from large text is making\nsignificant progress. Especially in the current era of social media, people\nexpect quick bites of information. Automatic text summarization seeks to tackle\nthis by slimming large texts down into more manageable summaries. This\nimportant research area can aid in decision-making by digging out salient\ncontent from large text. With the progress in deep learning models, significant\nwork in language models has emerged. The encoder-decoder framework in deep\nlearning has become the central approach for automatic text summarization. This\nwork leverages transformer-based BART model for human-like summarization which\nis an open-ended problem with many challenges. On training and fine-tuning the\nencoder-decoder model, it is tested with diverse sample articles and the\nquality of summaries of diverse samples is assessed based on human evaluation\nparameters. Further, the finetuned model performance is compared with the\nbaseline pretrained model based on evaluation metrics like ROUGE score and\nBERTScore. Additionally, domain adaptation of the model is required for\nimproved performance of abstractive summarization of dialogues between\ninterlocutors. On investigating, the above popular evaluation metrics are found\nto be insensitive to factual errors. Further investigation of the summaries\ngenerated by finetuned model is done using the contemporary evaluation metrics\nof factual consistency like WeCheck and SummaC. Empirical results on BBC News\narticles highlight that the gold standard summaries written by humans are more\nfactually consistent by 17% than the abstractive summaries generated by\nfinetuned model.\n","authors":["Sindhu Nair","Y. S. Rao","Radha Shankarmani"],"pdf_url":"https://arxiv.org/pdf/2410.16842v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2404.04846v2","updated":"2024-10-22T09:16:49Z","published":"2024-04-07T07:39:45Z","title":"F-MALLOC: Feed-forward Memory Allocation for Continual Learning in\n  Neural Machine Translation","summary":"  In the evolving landscape of Neural Machine Translation (NMT), the\npretrain-then-finetune paradigm has yielded impressive results. However, the\npersistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While\nprevious work has introduced Continual Learning (CL) methods to address CF,\nthese approaches grapple with the delicate balance between avoiding forgetting\nand maintaining system extensibility. To address this, we propose a CL method,\nnamed $\\textbf{F-MALLOC}$ ($\\textbf{F}$eed-forward $\\textbf{M}$emory\n$\\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting\nthat feed-forward layers emulate neural memories and encapsulate crucial\ntranslation knowledge. It decomposes feed-forward layers into discrete memory\ncells and allocates these memories to different tasks. By learning to allocate\nand safeguard these memories, our method effectively alleviates CF while\nensuring robust extendability. Besides, we propose a comprehensive assessment\nprotocol for multi-stage CL of NMT systems. Experiments conducted following\nthis new protocol showcase the superior performance of F-MALLOC, evidenced by\nhigher BLEU scores and almost zero forgetting.\n","authors":["Junhong Wu","Yuchen Liu","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2404.04846v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2410.16834v1","updated":"2024-10-22T09:14:21Z","published":"2024-10-22T09:14:21Z","title":"Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation","summary":"  The correlation between NLG automatic evaluation metrics and human evaluation\nis often regarded as a critical criterion for assessing the capability of an\nevaluation metric. However, different grouping methods and correlation\ncoefficients result in various types of correlation measures used in\nmeta-evaluation. In specific evaluation scenarios, prior work often directly\nfollows conventional measure settings, but the characteristics and differences\nbetween these measures have not gotten sufficient attention. Therefore, this\npaper analyzes 12 common correlation measures using a large amount of\nreal-world data from six widely-used NLG evaluation datasets and 32 evaluation\nmetrics, revealing that different measures indeed impact the meta-evaluation\nresults. Furthermore, we propose three perspectives that reflect the capability\nof meta-evaluation and find that the measure using global grouping and Pearson\ncorrelation exhibits the best overall performance, involving the discriminative\npower, ranking consistency, and sensitivity to score granularity.\n","authors":["Mingqi Gao","Xinyu Hu","Li Lin","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2410.16834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07563v2","updated":"2024-10-22T09:06:38Z","published":"2024-10-10T02:59:36Z","title":"PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency","summary":"  We introduce PLaMo-100B, a large-scale language model designed for Japanese\nproficiency. The model was trained from scratch using 2 trillion tokens, with\narchitecture such as QK Normalization and Z-Loss to ensure training stability\nduring the training process. Post-training techniques, including Supervised\nFine-Tuning and Direct Preference Optimization, were applied to refine the\nmodel's performance. Benchmark evaluations suggest that PLaMo-100B performs\nwell, particularly in Japanese-specific tasks, achieving results that are\ncompetitive with frontier models like GPT-4. The base model is available at\nhttps://huggingface.co/pfnet/plamo-100b.\n","authors":["Preferred Elements"," :","Kenshin Abe","Kaizaburo Chubachi","Yasuhiro Fujita","Yuta Hirokawa","Kentaro Imajo","Toshiki Kataoka","Hiroyoshi Komatsu","Hiroaki Mikami","Tsuguo Mogami","Shogo Murai","Kosuke Nakago","Daisuke Nishino","Toru Ogawa","Daisuke Okanohara","Yoshihiko Ozaki","Shotaro Sano","Shuji Suzuki","Tianqi Xu","Toshihiko Yanase"],"pdf_url":"https://arxiv.org/pdf/2410.07563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14710v2","updated":"2024-10-22T09:00:19Z","published":"2024-09-23T05:12:13Z","title":"ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning","summary":"  Role-playing is an emerging application in the field of Human-Computer\nInteraction (HCI), primarily implemented through the alignment training of a\nlarge language model (LLM) with assigned characters. Despite significant\nprogress, role-playing agents (RPLAs) still struggle with maintaining\nrole-consistency across conversations, particularly when confronted with\nboundary queries subtly related to character attributes. In this paper, we\npresent ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities\nthrough boundary-aware learning. ERABAL encompasses a generation pipeline for\nrole-specific dialogues and a concomitant methodology for alignment training.\nThrough comprehensive evaluations, we demonstrate that ERABAL is both efficient\nand effective. By training with significantly fewer dialogues than those used\nin leading approaches, ERABAL achieves notable improvements across\nWikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared\nto the generalist baseline models. Our code and datasets will be made publicly\navailable to support further research.\n","authors":["Yihong Tang","Jiao Ou","Che Liu","Fuzheng Zhang","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2409.14710v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.10618"},{"id":"http://arxiv.org/abs/2407.04185v3","updated":"2024-10-22T08:53:02Z","published":"2024-07-04T23:26:56Z","title":"HAF-RM: A Hybrid Alignment Framework for Reward Model Training","summary":"  The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nTheoretical justifications and experiment results on five datasets show the\nvalidity and effectiveness of our proposed hybrid framework for training a\nhigh-quality reward model. By decoupling the reward modeling procedure and\nincorporating hybrid supervision, our HaF-RM framework offers a principled and\neffective approach to enhancing the performance and alignment of reward models,\na critical component in the responsible development of powerful language\nmodels. We release our code at https://haf-rm.github.io.\n","authors":["Shujun Liu","Xiaoyu Shen","Yuhang Lai","Siyuan Wang","Shengbin Yue","Zengfeng Huang","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.04185v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16812v1","updated":"2024-10-22T08:38:50Z","published":"2024-10-22T08:38:50Z","title":"Optimizing Chain-of-Thought Reasoning: Tackling Arranging Bottleneck via\n  Plan Augmentation","summary":"  Multi-step reasoning ability of large language models is crucial in tasks\nsuch as math and tool utilization. Current researches predominantly focus on\nenhancing model performance in these multi-step reasoning tasks through\nfine-tuning with Chain-of-Thought (CoT) steps, yet these methods tend to be\nheuristic, without exploring nor resolving the bottleneck. In this study, we\nsubdivide CoT reasoning into two parts: arranging and executing, and identify\nthat the bottleneck of models mainly lies in arranging rather than executing.\nBased on this finding, we propose a plan-based training and reasoning method\nthat guides models to generate arranging steps through abstract plans. We\nexperiment on both math (GSM8k) and tool utilization (ToolBench) benchmarks.\nResults show that compared to fine-tuning directly with CoT data, our approach\nachieves a better performance on alleviating arranging bottleneck, particularly\nexcelling in long-distance reasoning generalization.\n","authors":["Yuli Qiu","Jiashu Yao","Heyan Huang","Yuhang Guo"],"pdf_url":"https://arxiv.org/pdf/2410.16812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16803v1","updated":"2024-10-22T08:28:05Z","published":"2024-10-22T08:28:05Z","title":"Context-aware Inductive Knowledge Graph Completion with Latent Type\n  Constraints and Subgraph Reasoning","summary":"  Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%.\n","authors":["Muzhi Li","Cehao Yang","Chengjin Xu","Zixing Song","Xuhui Jiang","Jian Guo","Ho-fung Leung","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2410.16803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16801v1","updated":"2024-10-22T08:27:23Z","published":"2024-10-22T08:27:23Z","title":"Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models","summary":"  Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsubspace regularization method on LoRA structure. Aiming to reduce the scale of\noutput change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix null space. Experimental\nresults on commonly used LLM finetuning tasks reveal that CLoRA significantly\noutperforms existing LoRA subsequent methods on both in-domain and outdomain\nevaluations, highlighting the superority of CLoRA as a effective\nparameter-efficient finetuning method with catastrophic forgetting mitigating.\nFurther investigation for model parameters indicates that CLoRA effectively\nbalances the trade-off between model capacity and degree of forgetting.\n","authors":["Yuheng Lu","Bingshuo Qian","Caixia Yuan","Huixing Jiang","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16788v1","updated":"2024-10-22T08:04:32Z","published":"2024-10-22T08:04:32Z","title":"Correct after Answer: Enhancing Multi-Span Question Answering with\n  Post-Processing Method","summary":"  Multi-Span Question Answering (MSQA) requires models to extract one or\nmultiple answer spans from a given context to answer a question. Prior work\nmainly focuses on designing specific methods or applying heuristic strategies\nto encourage models to predict more correct predictions. However, these models\nare trained on gold answers and fail to consider the incorrect predictions.\nThrough a statistical analysis, we observe that models with stronger abilities\ndo not predict less incorrect predictions compared with other models. In this\nwork, we propose Answering-Classifying-Correcting (ACC) framework, which\nemploys a post-processing strategy to handle incorrect predictions.\nSpecifically, the ACC framework first introduces a classifier to classify the\npredictions into three types and exclude \"wrong predictions\", then introduces a\ncorrector to modify \"partially correct predictions\". Experiments on several\nMSQA datasets show that ACC framework significantly improves the Exact Match\n(EM) scores, and further analysis demostrates that ACC framework efficiently\nreduces the number of incorrect predictions, improving the quality of\npredictions.\n","authors":["Jiayi Lin","Chenyang Zhang","Haibo Tong","Dongyu Zhang","Qingqing Hong","Bingxuan Hou","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16788v1.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.16780v1","updated":"2024-10-22T07:53:41Z","published":"2024-10-22T07:53:41Z","title":"Beyond Retrieval: Generating Narratives in Conversational Recommender\n  Systems","summary":"  The recent advances in Large Language Model's generation and reasoning\ncapabilities present an opportunity to develop truly conversational\nrecommendation systems. However, effectively integrating recommender system\nknowledge into LLMs for natural language generation which is tailored towards\nrecommendation tasks remains a challenge. This paper addresses this challenge\nby making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation\ntasks in conversational recommendations. REGEN (Reviews Enhanced with\nGEnerative Narratives) extends the Amazon Product Reviews dataset with rich\nuser narratives, including personalized explanations of product preferences,\nproduct endorsements for recommended items, and summaries of user purchase\nhistory. REGEN is made publicly available to facilitate further research.\nFurthermore, we establish benchmarks using well-known generative metrics, and\nperform an automated evaluation of the new dataset using a rater LLM. Second,\nthe paper introduces a fusion architecture (CF model with an LLM) which serves\nas a baseline for REGEN. And to the best of our knowledge, represents the first\nattempt to analyze the capabilities of LLMs in understanding recommender\nsignals and generating rich narratives. We demonstrate that LLMs can\neffectively learn from simple fusion architectures utilizing interaction-based\nCF embeddings, and this can be further enhanced using the metadata and\npersonalization data associated with items. Our experiments show that combining\nCF and content embeddings leads to improvements of 4-12% in key language\nmetrics compared to using either type of embedding individually. We also\nprovide an analysis to interpret how CF and content embeddings contribute to\nthis new generative task.\n","authors":["Krishna Sayana","Raghavendra Vasudeva","Yuri Vasilevski","Kun Su","Liam Hebert","Hubert Pham","Ambarish Jash","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2410.16780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16775v1","updated":"2024-10-22T07:45:18Z","published":"2024-10-22T07:45:18Z","title":"Context-Aware LLM Translation System Using Conversation Summarization\n  and Dialogue History","summary":"  Translating conversational text, particularly in customer support contexts,\npresents unique challenges due to its informal and unstructured nature. We\npropose a context-aware LLM translation system that leverages conversation\nsummarization and dialogue history to enhance translation quality for the\nEnglish-Korean language pair. Our approach incorporates the two most recent\ndialogues as raw data and a summary of earlier conversations to manage context\nlength effectively. We demonstrate that this method significantly improves\ntranslation accuracy, maintaining coherence and consistency across\nconversations. This system offers a practical solution for customer support\ntranslation tasks, addressing the complexities of conversational text.\n","authors":["Mingi Sung","Seungmin Lee","Jiwon Kim","Sejoon Kim"],"pdf_url":"https://arxiv.org/pdf/2410.16775v1.pdf","comment":"Accepted to WMT 2024"},{"id":"http://arxiv.org/abs/2410.14748v2","updated":"2024-10-22T07:19:40Z","published":"2024-10-17T19:38:55Z","title":"ETF: An Entity Tracing Framework for Hallucination Detection in Code\n  Summaries","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their ability to understand both natural language and code, driving\ntheir use in tasks like natural language-to-code (NL2Code) and code\nsummarization. However, LLMs are prone to hallucination-outputs that stray from\nintended meanings. Detecting hallucinations in code summarization is especially\ndifficult due to the complex interplay between programming and natural\nlanguages. We introduce a first-of-its-kind dataset with $\\sim$10K samples,\ncurated specifically for hallucination detection in code summarization. We\nfurther propose a novel Entity Tracing Framework (ETF) that a) utilizes static\nprogram analysis to identify code entities from the program and b) uses LLMs to\nmap and verify these entities and their intents within generated code\nsummaries. Our experimental analysis demonstrates the effectiveness of the\nframework, leading to a 0.73 F1 score. This approach provides an interpretable\nmethod for detecting hallucinations by grounding entities, allowing us to\nevaluate summary accuracy.\n","authors":["Kishan Maharaj","Vitobha Munigala","Srikanth G. Tamilselvam","Prince Kumar","Sayandeep Sen","Palani Kodeswaran","Abhijit Mishra","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2410.14748v2.pdf","comment":"11 pages, 6 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2407.02273v4","updated":"2024-10-22T06:48:54Z","published":"2024-07-02T14:02:53Z","title":"Language Model Alignment in Multilingual Trolley Problems","summary":"  We evaluate the moral alignment of large language models (LLMs) with human\npreferences in multilingual trolley problems. Building on the Moral Machine\nexperiment, which captures over 40 million human judgments across 200+\ncountries, we develop a cross-lingual corpus of moral dilemma vignettes in over\n100 languages called MultiTP. This dataset enables the assessment of LLMs'\ndecision-making processes in diverse linguistic contexts. Our analysis explores\nthe alignment of 19 different LLMs with human judgments, capturing preferences\nacross six moral dimensions: species, gender, fitness, status, age, and the\nnumber of lives involved. By correlating these preferences with the demographic\ndistribution of language speakers and examining the consistency of LLM\nresponses to various prompt paraphrasings, our findings provide insights into\ncross-lingual and ethical biases of LLMs and their intersection. We discover\nsignificant variance in alignment across languages, challenging the assumption\nof uniform moral reasoning in AI systems and highlighting the importance of\nincorporating diverse perspectives in AI ethics. The results underscore the\nneed for further research on the integration of multilingual dimensions in\nresponsible AI research to ensure fair and equitable AI interactions worldwide.\nOur code and data are at https://github.com/causalNLP/moralmachine\n","authors":["Zhijing Jin","Max Kleiman-Weiner","Giorgio Piatti","Sydney Levine","Jiarui Liu","Fernando Gonzalez","Francesco Ortu","András Strausz","Mrinmaya Sachan","Rada Mihalcea","Yejin Choi","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2407.02273v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14059v2","updated":"2024-10-22T06:47:43Z","published":"2024-10-17T22:03:52Z","title":"UCFE: A User-Centric Financial Expertise Benchmark for Large Language\n  Models","summary":"  This paper introduces the UCFE: User-Centric Financial Expertise benchmark,\nan innovative framework designed to evaluate the ability of large language\nmodels (LLMs) to handle complex real-world financial tasks. UCFE benchmark\nadopts a hybrid approach that combines human expert evaluations with dynamic,\ntask-specific interactions to simulate the complexities of evolving financial\nscenarios. Firstly, we conducted a user study involving 804 participants,\ncollecting their feedback on financial tasks. Secondly, based on this feedback,\nwe created our dataset that encompasses a wide range of user intents and\ninteractions. This dataset serves as the foundation for benchmarking 12 LLM\nservices using the LLM-as-Judge methodology. Our results show a significant\nalignment between benchmark scores and human preferences, with a Pearson\ncorrelation coefficient of 0.78, confirming the effectiveness of the UCFE\ndataset and our evaluation approach. UCFE benchmark not only reveals the\npotential of LLMs in the financial sector but also provides a robust framework\nfor assessing their performance and user satisfaction. The benchmark dataset\nand evaluation code are available.\n","authors":["Yuzhe Yang","Yifei Zhang","Yan Hu","Yilin Guo","Ruoli Gan","Yueru He","Mingcong Lei","Xiao Zhang","Haining Wang","Qianqian Xie","Jimin Huang","Honghai Yu","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16736v1","updated":"2024-10-22T06:43:28Z","published":"2024-10-22T06:43:28Z","title":"Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through\n  Failure-Inducing Exploration","summary":"  Large language models (LLMs) have significantly benefited from training on\ndiverse, high-quality task-specific data, leading to impressive performance\nacross a range of downstream applications. Current methods often rely on\nhuman-annotated data or predefined task templates to direct powerful LLMs in\nsynthesizing task-relevant data for effective model training. However, this\ndependence on manually designed components may constrain the scope of generated\ndata, potentially overlooking critical edge cases or novel scenarios that could\nchallenge the model. In this paper, we present a novel approach, ReverseGen,\ndesigned to automatically generate effective training samples that expose the\nweaknesses of LLMs. Specifically, we introduce a dedicated proposer trained to\nproduce queries that lead target models to generate unsatisfactory responses.\nThese failure-inducing queries are then used to construct training data,\nhelping to address the models' shortcomings and improve overall performance.\nOur approach is flexible and can be applied to models of various scales (3B,\n7B, and 8B). We evaluate ReverseGen on three key applications (safety, honesty,\nand math), demonstrating that our generated data is both highly effective and\ndiverse. Models fine-tuned with ReverseGen-generated data consistently\noutperform those trained on human-annotated or general model-generated data,\noffering a new perspective on data synthesis for task-specific LLM enhancement.\n","authors":["Qintong Li","Jiahui Gao","Sheng Wang","Renjie Pi","Xueliang Zhao","Chuan Wu","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.16736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12074v3","updated":"2024-10-22T06:38:07Z","published":"2024-06-17T20:20:47Z","title":"COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for\n  Aligning Large Language Models to Online Communities","summary":"  Social scientists use surveys to probe the opinions and beliefs of\npopulations, but these methods are slow, costly, and prone to biases. Recent\nadvances in large language models (LLMs) enable the creating of computational\nrepresentations or \"digital twins\" of populations that generate human-like\nresponses mimicking the population's language, styles, and attitudes. We\nintroduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs\nto online communities to elicit their beliefs. Given a corpus of a community's\nonline discussions, Community-Cross-Instruct automatically generates\ninstruction-output pairs by an advanced LLM to (1) finetune a foundational LLM\nto faithfully represent that community, and (2) evaluate the alignment of the\nfinetuned model to the community. We demonstrate the method's utility in\naccurately representing political and diet communities on Reddit. Unlike prior\nmethods requiring human-authored instructions, Community-Cross-Instruct\ngenerates instructions in a fully unsupervised manner, enhancing scalability\nand generalization across domains. This work enables cost-effective and\nautomated surveying of diverse online communities.\n","authors":["Zihao He","Minh Duc Chu","Rebecca Dorn","Siyi Guo","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2406.12074v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17378v2","updated":"2024-10-22T06:32:10Z","published":"2024-06-25T08:55:12Z","title":"A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens","summary":"  Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nembedding LLMs, the obtained text embedding will be able to be aligned with the\nkey tokens in the input text. We first fully analyze this phenomenon on eight\nembedding LLMs and show that this phenomenon is universal and is not affected\nby model architecture, training strategy, and embedding method. With a deeper\nanalysis, we then find that the main change in embedding space between the\nembedding LLMs and their original generative LLMs is in the first principal\ncomponent. By adjusting the first principal component, we can align text\nembedding with the key tokens. Finally, we give several examples to demonstrate\nthe vast application potential of this finding: (1) we propose a simple and\npractical sparse retrieval method based on the aligned tokens, which can\nachieve 80\\% of the dense retrieval effect of the same model while reducing the\ncomputation significantly; (2) we show that our findings provide a fresh\nperspective to help understand fuzzy concepts (e.g., semantic relatedness vs.\nsemantic similarity) and emerging technologies (e.g., instruction-following\nembedding) in this field.\n","authors":["Zhijie Nie","Richong Zhang","Zhanyu Wu"],"pdf_url":"https://arxiv.org/pdf/2406.17378v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.16726v1","updated":"2024-10-22T06:25:16Z","published":"2024-10-22T06:25:16Z","title":"Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap","summary":"  While automatic speech recognition (ASR) systems have achieved remarkable\nperformance with large-scale datasets, their efficacy remains inadequate in\nlow-resource settings, encompassing dialects, accents, minority languages, and\nlong-tail hotwords, domains with significant practical relevance. With the\nadvent of versatile and powerful text-to-speech (TTS) models, capable of\ngenerating speech with human-level naturalness, expressiveness, and diverse\nspeaker profiles, leveraging TTS for ASR data augmentation provides a\ncost-effective and practical approach to enhancing ASR performance.\nComprehensive experiments on an unprecedentedly rich variety of low-resource\ndatasets demonstrate consistent and substantial performance improvements,\nproving that the proposed method of enhancing low-resource ASR through a\nversatile TTS model is highly effective and has broad application prospects.\nFurthermore, we delve deeper into key characteristics of synthesized speech\ndata that contribute to ASR improvement, examining factors such as text\ndiversity, speaker diversity, and the volume of synthesized data, with text\ndiversity being studied for the first time in this work. We hope our findings\nprovide helpful guidance and reference for the practical application of\nTTS-based data augmentation and push the advancement of low-resource ASR one\nstep further.\n","authors":["Guanrou Yang","Fan Yu","Ziyang Ma","Zhihao Du","Zhifu Gao","Shiliang Zhang","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16635v3","updated":"2024-10-22T06:19:20Z","published":"2024-01-30T00:17:37Z","title":"Improving Reinforcement Learning from Human Feedback with Efficient\n  Reward Model Ensemble","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a widely adopted\napproach for aligning large language models with human values. However, RLHF\nrelies on a reward model that is trained with a limited amount of human\npreference data, which could lead to inaccurate predictions. As a result, RLHF\nmay produce outputs that are misaligned with human values. To mitigate this\nissue, we contribute a reward ensemble method that allows the reward model to\nmake more accurate predictions. As using an ensemble of large language\nmodel-based reward models can be computationally and resource-expensive, we\nexplore efficient ensemble methods including linear-layer ensemble and\nLoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy\nOptimization with our ensembled reward models, and verify that our ensemble\nmethods help improve the alignment performance of RLHF outputs.\n","authors":["Shun Zhang","Zhenfang Chen","Sunli Chen","Yikang Shen","Zhiqing Sun","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2401.16635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16714v1","updated":"2024-10-22T05:51:34Z","published":"2024-10-22T05:51:34Z","title":"Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Models Alignment","summary":"  Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment.\n","authors":["Mingzhi Wang","Chengdong Ma","Qizhi Chen","Linjian Meng","Yang Han","Jiancong Xiao","Zhaowei Zhang","Jing Huo","Weijie J. Su","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16714v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.11216v2","updated":"2024-10-22T05:45:46Z","published":"2024-04-17T10:00:56Z","title":"Position Engineering: Boosting Large Language Models through Positional\n  Information Manipulation","summary":"  The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.\n","authors":["Zhiyuan He","Huiqiang Jiang","Zilong Wang","Yuqing Yang","Luna Qiu","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02246v3","updated":"2024-10-22T05:44:04Z","published":"2024-03-04T17:34:34Z","title":"PHAnToM: Persona-based Prompting Has An Effect on Theory-of-Mind\n  Reasoning in Large Language Models","summary":"  The use of LLMs in natural language reasoning has shown mixed results,\nsometimes rivaling or even surpassing human performance in simpler\nclassification tasks while struggling with social-cognitive reasoning, a domain\nwhere humans naturally excel. These differences have been attributed to many\nfactors, such as variations in prompting and the specific LLMs used. However,\nno reasons appear conclusive, and no clear mechanisms have been established in\nprior work. In this study, we empirically evaluate how role-playing prompting\ninfluences Theory-of-Mind (ToM) reasoning capabilities. Grounding our rsearch\nin psychological theory, we propose the mechanism that, beyond the inherent\nvariance in the complexity of reasoning tasks, performance differences arise\nbecause of socially-motivated prompting differences. In an era where prompt\nengineering with role-play is a typical approach to adapt LLMs to new contexts,\nour research advocates caution as models that adopt specific personas might\npotentially result in errors in social-cognitive reasoning.\n","authors":["Fiona Anting Tan","Gerard Christopher Yeo","Kokil Jaidka","Fanyou Wu","Weijie Xu","Vinija Jain","Aman Chadha","Yang Liu","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.02246v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.16712v1","updated":"2024-10-22T05:39:24Z","published":"2024-10-22T05:39:24Z","title":"DENOASR: Debiasing ASRs through Selective Denoising","summary":"  Automatic Speech Recognition (ASR) systems have been examined and shown to\nexhibit biases toward particular groups of individuals, influenced by factors\nsuch as demographic traits, accents, and speech styles. Noise can\ndisproportionately impact speakers with certain accents, dialects, or speaking\nstyles, leading to biased error rates. In this work, we introduce a novel\nframework DENOASR, which is a selective denoising technique to reduce the\ndisparity in the word error rates between the two gender groups, male and\nfemale. We find that a combination of two popular speech denoising techniques,\nviz. DEMUCS and LE, can be effectively used to mitigate ASR disparity without\ncompromising their overall performance. Experiments using two state-of-the-art\nopen-source ASRs - OpenAI WHISPER and NVIDIA NEMO - on multiple benchmark\ndatasets, including TIE, VOX-POPULI, TEDLIUM, and FLEURS, show that there is a\npromising reduction in the average word error rate gap across the two gender\ngroups. For a given dataset, the denoising is selectively applied on speech\nsamples having speech intelligibility below a certain threshold, estimated\nusing a small validation sample, thus ameliorating the need for large-scale\nhuman-written ground-truth transcripts. Our findings suggest that selective\ndenoising can be an elegant approach to mitigate biases in present-day ASR\nsystems.\n","authors":["Anand Kumar Rai","Siddharth D Jaiswal","Shubham Prakash","Bendi Pragnya Sree","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.16712v1.pdf","comment":"Paper accepted at IEEE ICKG 2024"},{"id":"http://arxiv.org/abs/2410.16710v1","updated":"2024-10-22T05:32:40Z","published":"2024-10-22T05:32:40Z","title":"Influential Language Data Selection via Gradient Trajectory Pursuit","summary":"  Curating a desirable dataset for training has been the core of building\nhighly capable large language models (Touvron et al., 2023; Achiam et al.,\n2023; Team et al.,2024). Gradient influence scores (Pruthi et al., 2020; Xia et\nal., 2024) are shown to be correlated with model performance and are commonly\nused as the criterion for data selection. However, existing methods are built\nupon either individual sample rankings or inefficient matching process, leading\nto suboptimal performance or scaling up issues.In this paper, we propose\nGradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of\ngradient trajectories via jointly selecting data points under an L0-norm\nregularized objective. The proposed algorithm highlights: (1) joint selection\ninstead of independent top-k selection, which automatically de-duplicates\nsamples; (2) higher efficiency with compressive sampling processes, which can\nbe further sped up using a distributed framework. In the experiments, we\ndemonstrate the algorithm in both in-domain and target-domain selection\nbenchmarks and show that it outperforms top-k selection and competitive\nalgorithms consistently, for example, our algorithm chooses as low as 0.5% data\nto achieve full performance on the targeted instruction tuning tasks\n","authors":["Zhiwei Deng","Tao Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2410.16710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16708v1","updated":"2024-10-22T05:25:54Z","published":"2024-10-22T05:25:54Z","title":"Atomic Fact Decomposition Helps Attributed Question Answering","summary":"  Attributed Question Answering (AQA) aims to provide both a trustworthy answer\nand a reliable attribution report for a given question. Retrieval is a widely\nadopted approach, including two general paradigms: Retrieval-Then-Read (RTR)\nand post-hoc retrieval. Recently, Large Language Models (LLMs) have shown\nremarkable proficiency, prompting growing interest in AQA among researchers.\nHowever, RTR-based AQA often suffers from irrelevant knowledge and rapidly\nchanging information, even when LLMs are adopted, while post-hoc\nretrieval-based AQA struggles with comprehending long-form answers with complex\nlogic, and precisely identifying the content needing revision and preserving\nthe original intent. To tackle these problems, this paper proposes an Atomic\nfact decomposition-based Retrieval and Editing (ARE) framework, which\ndecomposes the generated long-form answers into molecular clauses and atomic\nfacts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are\nfine-tuned using a well-constructed dataset, generated from large scale\nKnowledge Graphs (KGs). This process involves extracting one-hop neighbors from\na given set of entities and transforming the result into coherent long-form\ntext. Subsequently, ARE leverages a search engine to retrieve evidences related\nto atomic facts, inputting these evidences into an LLM-based verifier to\ndetermine whether the facts require expansion for re-retrieval or editing.\nFurthermore, the edited facts are backtracked into the original answer, with\nevidence aggregated based on the relationship between molecular clauses and\natomic facts. Extensive evaluations demonstrate the superior performance of our\nproposed method over the state-of-the-arts on several datasets, with an\nadditionally proposed new metric $Attr_{p}$ for evaluating the precision of\nevidence attribution.\n","authors":["Zhichao Yan","Jiapu Wang","Jiaoyan Chen","Xiaoli Li","Ru Li","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16703v1","updated":"2024-10-22T05:16:19Z","published":"2024-10-22T05:16:19Z","title":"PLDR-LLM: Large Language Model from Power Law Decoder Representations","summary":"  We present the Large Language Model from Power Law Decoder Representations\n(PLDR-LLM), a language model that leverages non-linear and linear\ntransformations through Power Law Graph Attention mechanism to generate\nwell-defined deductive and inductive outputs. We pretrain the PLDR-LLMs of\nvarying layer sizes with a small batch size of 32 and $\\sim$8B tokens from the\nRefinedWeb dataset, and show that they achieve competitive performance in\nzero-shot and few-shot settings compared to scaled dot-product LLMs of similar\nmodel size reported in the literature. We show that deductive outputs of\nPLDR-LLMs can be used to compare model characteristics or improve the\nperformance by introducing the Directed Acyclic Graph (DAG) loss as a metric\nand regularizer. Our results indicate that the initial maximum learning rate\nand warm-up steps have a lasting impact on deductive outputs throughout the\npretraining. We provide a detailed description of PLDR-LLM architecture, its\nimplementation and the pretraining procedure.\n","authors":["Burc Gokden"],"pdf_url":"https://arxiv.org/pdf/2410.16703v1.pdf","comment":"22 pages, 4 figures, 10 tables"},{"id":"http://arxiv.org/abs/2407.00219v2","updated":"2024-10-22T05:13:15Z","published":"2024-06-28T20:06:30Z","title":"Evaluating Human Alignment and Model Faithfulness of LLM Rationale","summary":"  We study how well large language models (LLMs) explain their generations\nthrough rationales -- a set of tokens extracted from the input text that\nreflect the decision-making process of LLMs. Specifically, we systematically\nstudy rationales derived using two approaches: (1) popular prompting-based\nmethods, where prompts are used to guide LLMs in generating rationales, and (2)\ntechnical attribution-based methods, which leverage attention or gradients to\nidentify important tokens. Our analysis spans three classification datasets\nwith annotated rationales, encompassing tasks with varying performance levels.\nWhile prompting-based self-explanations are widely used, our study reveals that\nthese explanations are not always as \"aligned\" with the human rationale as\nattribution-based explanations. Even more so, fine-tuning LLMs to enhance\nclassification task accuracy does not enhance the alignment of prompting-based\nrationales. Still, it does considerably improve the alignment of\nattribution-based methods (e.g., InputXGradient). More importantly, we show\nthat prompting-based self-explanation is also less \"faithful\" than\nattribution-based explanations, failing to provide a reliable account of the\nmodel's decision-making process. To evaluate faithfulness, unlike prior studies\nthat excluded misclassified examples, we evaluate all instances and also\nexamine the impact of fine-tuning and accuracy on alignment and faithfulness.\nOur findings suggest that inconclusive faithfulness results reported in earlier\nstudies may stem from low classification accuracy. These findings underscore\nthe importance of more rigorous and comprehensive evaluations of LLM\nrationales.\n","authors":["Mohsen Fayyaz","Fan Yin","Jiao Sun","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2407.00219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16682v1","updated":"2024-10-22T04:27:03Z","published":"2024-10-22T04:27:03Z","title":"Methods of improving LLM training stability","summary":"  Training stability of large language models(LLMs) is an important research\ntopic. Reproducing training instabilities can be costly, so we use a small\nlanguage model with 830M parameters and experiment with higher learning rates\nto force models to diverge. One of the sources of training instability is the\ngrowth of logits in attention layers. We extend the focus of the previous work\nand look not only at the magnitude of the logits but at all outputs of linear\nlayers in the Transformer block. We observe that with a high learning rate the\nL2 norm of all linear layer outputs can grow with each training step and the\nmodel diverges. Specifically we observe that QKV, Proj and FC2 layers have the\nlargest growth of the output magnitude. This prompts us to explore several\noptions: 1) apply layer normalization not only after QK layers but also after\nProj and FC2 layers too; 2) apply layer normalization after the QKV layer (and\nremove pre normalization). 3) apply QK layer normalization together with\nsoftmax capping. We show that with the last two methods we can increase\nlearning rate by 1.5x (without model divergence) in comparison to an approach\nbased on QK layer normalization only. Also we observe significant perplexity\nimprovements for all three methods in comparison to the baseline model.\n","authors":["Oleg Rybakov","Mike Chrzanowski","Peter Dykas","Jinze Xue","Ben Lanir"],"pdf_url":"https://arxiv.org/pdf/2410.16682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16676v1","updated":"2024-10-22T04:18:19Z","published":"2024-10-22T04:18:19Z","title":"Improving Causal Reasoning in Large Language Models: A Survey","summary":"  Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning.\n","authors":["Siheng Xiong","Delin Chen","Qingyang Wu","Longxuan Yu","Qingzhen Liu","Dawei Li","Zhikai Chen","Xiaoze Liu","Liangming Pan"],"pdf_url":"https://arxiv.org/pdf/2410.16676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08777v3","updated":"2024-10-22T04:14:08Z","published":"2024-02-13T20:21:29Z","title":"DNABERT-S: Pioneering Species Differentiation with Species-Aware DNA\n  Embeddings","summary":"  We introduce DNABERT-S, a tailored genome model that develops species-aware\nembeddings to naturally cluster and segregate DNA sequences of different\nspecies in the embedding space. Differentiating species from genomic sequences\n(i.e., DNA and RNA) is vital yet challenging, since many real-world species\nremain uncharacterized, lacking known genomes for reference. Embedding-based\nmethods are therefore used to differentiate species in an unsupervised manner.\nDNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To\nencourage effective embeddings to error-prone long-read DNA sequences, we\nintroduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes\nthe hidden representations of DNA sequences at randomly selected layers and\ntrains the model to recognize and differentiate these mixed proportions at the\noutput layer. We further enhance it with the proposed Curriculum Contrastive\nLearning (C$^2$LR) strategy. Empirical results on 23 diverse datasets show\nDNABERT-S's effectiveness, especially in realistic label-scarce scenarios. For\nexample, it identifies twice more species from a mixture of unlabeled genomic\nsequences, doubles the Adjusted Rand Index (ARI) in species clustering, and\noutperforms the top baseline's performance in 10-shot species classification\nwith just a 2-shot training. Model, codes, and data are publicly available at\n\\url{https://github.com/MAGICS-LAB/DNABERT_S}.\n","authors":["Zhihan Zhou","Weimin Wu","Harrison Ho","Jiayi Wang","Lizhen Shi","Ramana V Davuluri","Zhong Wang","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2402.08777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05286v3","updated":"2024-10-22T03:58:20Z","published":"2024-03-08T13:10:59Z","title":"LLM4Decompile: Decompiling Binary Code with Large Language Models","summary":"  Decompilation aims to convert binary code to high-level source code, but\ntraditional tools like Ghidra often produce results that are difficult to read\nand execute. Motivated by the advancements in Large Language Models (LLMs), we\npropose LLM4Decompile, the first and largest open-source LLM series (1.3B to\n33B) trained to decompile binary code. We optimize the LLM training process and\nintroduce the LLM4Decompile-End models to decompile binary directly. The\nresulting models significantly outperform GPT-4o and Ghidra on the HumanEval\nand ExeBench benchmarks by over 100% in terms of re-executability rate.\nAdditionally, we improve the standard refinement approach to fine-tune the\nLLM4Decompile-Ref models, enabling them to effectively refine the decompiled\ncode from Ghidra and achieve a further 16.2% improvement over the\nLLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to\nrevolutionize binary code decompilation, delivering remarkable improvements in\nreadability and executability while complementing conventional tools for\noptimal results. Our code, dataset, and models are released at\nhttps://github.com/albertan017/LLM4Decompile\n","authors":["Hanzhuo Tan","Qi Luo","Jing Li","Yuqun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05286v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16665v1","updated":"2024-10-22T03:38:37Z","published":"2024-10-22T03:38:37Z","title":"SafetyAnalyst: Interpretable, transparent, and steerable LLM safety\n  moderation","summary":"  The ideal LLM content moderation system would be both structurally\ninterpretable (so its decisions can be explained to users) and steerable (to\nreflect a community's values or align to safety standards). However, current\nsystems fall short on both of these dimensions. To address this gap, we present\nSafetyAnalyst, a novel LLM safety moderation framework. Given a prompt,\nSafetyAnalyst creates a structured \"harm-benefit tree,\" which identifies 1) the\nactions that could be taken if a compliant response were provided, 2) the\nharmful and beneficial effects of those actions (along with their likelihood,\nseverity, and immediacy), and 3) the stakeholders that would be impacted by\nthose effects. It then aggregates this structured representation into a\nharmfulness score based on a parameterized set of safety preferences, which can\nbe transparently aligned to particular values. Using extensive harm-benefit\nfeatures generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM\nto specialize in generating harm-benefit trees through symbolic knowledge\ndistillation. On a comprehensive set of prompt safety benchmarks, we show that\nour system (average F1=0.75) outperforms existing LLM safety moderation systems\n(average F1$<$0.72) on prompt harmfulness classification, while offering the\nadditional advantages of interpretability and steerability.\n","authors":["Jing-Jing Li","Valentina Pyatkin","Max Kleiman-Weiner","Liwei Jiang","Nouha Dziri","Anne G. E. Collins","Jana Schaich Borg","Maarten Sap","Yejin Choi","Sydney Levine"],"pdf_url":"https://arxiv.org/pdf/2410.16665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01812v3","updated":"2024-10-22T03:30:01Z","published":"2024-09-14T02:35:29Z","title":"From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice","summary":"  Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.\n","authors":["Qian Niu","Keyu Chen","Ming Li","Pohsun Feng","Ziqian Bi","Lawrence KQ Yan","Yichao Zhang","Caitlyn Heqi Yin","Cheng Fei","Junyu Liu","Benji Peng"],"pdf_url":"https://arxiv.org/pdf/2410.01812v3.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.16659v1","updated":"2024-10-22T03:21:59Z","published":"2024-10-22T03:21:59Z","title":"RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary\n  Detection in Partially Machine Generated Texts","summary":"  With increasing usage of generative models for text generation and widespread\nuse of machine generated texts in various domains, being able to distinguish\nbetween human written and machine generated texts is a significant challenge.\nWhile existing models and proprietary systems focus on identifying whether\ngiven text is entirely human written or entirely machine generated, only a few\nsystems provide insights at sentence or paragraph level at likelihood of being\nmachine generated at a non reliable accuracy level, working well only for a set\nof domains and generators. This paper introduces few reliable approaches for\nthe novel task of identifying which part of a given text is machine generated\nat a word level while comparing results from different approaches and methods.\nWe present a comparison with proprietary systems , performance of our model on\nunseen domains' and generators' texts. The findings reveal significant\nimprovements in detection accuracy along with comparison on other aspects of\ndetection capabilities. Finally we discuss potential avenues for improvement\nand implications of our work. The proposed model is also well suited for\ndetecting which parts of a text are machine generated in outputs of Instruct\nvariants of many LLMs.\n","authors":["Ram Mohan Rao Kadiyala"],"pdf_url":"https://arxiv.org/pdf/2410.16659v1.pdf","comment":"published at naacl 2024"},{"id":"http://arxiv.org/abs/2410.16658v1","updated":"2024-10-22T03:19:16Z","published":"2024-10-22T03:19:16Z","title":"Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent","summary":"  Adsorption energy is a key reactivity descriptor in catalysis, enabling the\nefficient screening of potential catalysts. However, determining adsorption\nenergy involves comparing the energies of multiple adsorbate-catalyst\nconfigurations, which is computationally demanding due to a large number of\npossible configurations. Current algorithmic approaches typically enumerate\nadsorption sites and configurations without leveraging theoretical insights to\nguide the initial setup. In this work, we present Adsorb-Agent, a Large\nLanguage Model (LLM) agent designed to efficiently derive system-specific\nstable adsorption configurations with minimal human intervention. Adsorb-Agent\nleverages built-in knowledge and emergent reasoning capabilities, significantly\nreducing the number of initial configurations required while improving accuracy\nin predicting the minimum adsorption energy. We demonstrate its performance\nusing two example systems, NNH-CuPd3 (111) and NNH-Mo3Pd (111), for the\nNitrogen Reduction Reaction (NRR), a sustainable alternative to the Haber-Bosch\nprocess. Adsorb-Agent outperforms conventional \"heuristic\" and \"random\"\nalgorithms by identifying lower-energy configurations with fewer initial\nsetups, reducing computational costs while enhancing accuracy. This highlights\nits potential to accelerate catalyst discovery.\n","authors":["Janghoon Ock","Tirtha Vinchurkar","Yayati Jadhav","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2410.16658v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.15639v2","updated":"2024-10-22T03:14:46Z","published":"2024-10-21T04:57:09Z","title":"Can Large Language Models Invent Algorithms to Improve Themselves?","summary":"  Large Language Models (LLMs) have shown remarkable performance improvements\nand are rapidly gaining adoption in industry. However, the methods for\nimproving LLMs are still designed by humans, which restricts the invention of\nnew model-improving algorithms to human expertise and imagination. To address\nthis, we propose the Self-Developing framework, which enables LLMs to\nautonomously generate and learn model-improvement algorithms. In this\nframework, the seed model generates, applies, and learns model-improving\nalgorithms, continuously improving both the seed model and the algorithms\nthemselves. In mathematical reasoning tasks, Self-Developing not only creates\nmodels that surpass the seed model but also consistently outperforms models\ncreated using human-designed algorithms. Additionally, these LLM-discovered\nalgorithms demonstrate strong effectiveness, including transferability to\nout-of-domain models.\n","authors":["Yoichi Ishibashi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2410.15639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16645v1","updated":"2024-10-22T02:45:09Z","published":"2024-10-22T02:45:09Z","title":"Chatting with Bots: AI, Speech Acts, and the Edge of Assertion","summary":"  This paper addresses the question of whether large language model-powered\nchatbots are capable of assertion. According to what we call the Thesis of\nChatbot Assertion (TCA), chatbots are the kinds of things that can assert, and\nat least some of the output produced by current-generation chatbots qualifies\nas assertion. We provide some motivation for TCA, arguing that it ought to be\ntaken seriously and not simply dismissed. We also review recent objections to\nTCA, arguing that these objections are weighty. We thus confront the following\ndilemma: how can we do justice to both the considerations for and against TCA?\nWe consider two influential responses to this dilemma - the first appeals to\nthe notion of proxy-assertion; the second appeals to fictionalism - and argue\nthat neither is satisfactory. Instead, reflecting on the ontogenesis of\nassertion, we argue that we need to make space for a category of\nproto-assertion. We then apply the category of proto-assertion to chatbots,\narguing that treating chatbots as proto-assertors provides a satisfactory\nresolution to the dilemma of chatbot assertion.\n","authors":["Iwan Williams","Tim Bayne"],"pdf_url":"https://arxiv.org/pdf/2410.16645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16640v1","updated":"2024-10-22T02:38:48Z","published":"2024-10-22T02:38:48Z","title":"A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs","summary":"  Large language models (LLMs) such as ChatGPT, GPT-4, Claude-3, and Llama are\nbeing integrated across a variety of industries. Despite this rapid\nproliferation, experts are calling for caution in the interpretation and\nadoption of LLMs, owing to numerous associated ethical concerns. Research has\nalso uncovered shortcomings in LLMs' reasoning and logical abilities, raising\nquestions on the potential of LLMs as evaluation tools. In this paper, we\ninvestigate LLMs' self-evaluation capabilities on a novel proverb reasoning\ntask. We introduce a novel proverb database consisting of 300 proverb pairs\nthat are similar in intent but different in wordings, across topics spanning\ngender, wisdom, and society. We propose tests to evaluate textual consistencies\nas well as numerical consistencies across similar proverbs, and demonstrate the\neffectiveness of our method and dataset in identifying failures in LLMs'\nself-evaluation which in turn can highlight issues related to gender\nstereotypes and lack of cultural understanding in LLMs.\n","authors":["Ryosuke Sonoda","Ramya Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2410.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16167v3","updated":"2024-10-22T02:29:22Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16638v1","updated":"2024-10-22T02:27:57Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16633v1","updated":"2024-10-22T02:21:42Z","published":"2024-10-22T02:21:42Z","title":"Graph-Structured Trajectory Extraction from Travelogues","summary":"  Previous studies on sequence-based extraction of human movement trajectories\nhave an issue of inadequate trajectory representation. Specifically, a pair of\nlocations may not be lined up in a sequence especially when one location\nincludes the other geographically. In this study, we propose a graph\nrepresentation that retains information on the geographic hierarchy as well as\nthe temporal order of visited locations, and have constructed a benchmark\ndataset for graph-structured trajectory extraction. The experiments with our\nbaselines have demonstrated that it is possible to accurately predict visited\nlocations and the order among them, but it remains a challenge to predict the\nhierarchical relations.\n","authors":["Aitaro Yamamoto","Hiroyuki Otomo","Hiroki Ouchi","Shohei Higashiyama","Hiroki Teranishi","Hiroyuki Shindo","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.16633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10523v2","updated":"2024-10-22T01:58:40Z","published":"2024-05-17T04:05:05Z","title":"Adaptable and Reliable Text Classification using Large Language Models","summary":"  Text classification is fundamental in Natural Language Processing (NLP), and\nthe advent of Large Language Models (LLMs) has revolutionized the field. This\npaper introduces an adaptable and reliable text classification paradigm, which\nleverages LLMs as the core component to address text classification tasks. Our\nsystem simplifies the traditional text classification workflows, reducing the\nneed for extensive preprocessing and domain-specific expertise to deliver\nadaptable and reliable text classification results. We evaluated the\nperformance of several LLMs, machine learning algorithms, and neural\nnetwork-based architectures on four diverse datasets. Results demonstrate that\ncertain LLMs surpass traditional methods in sentiment analysis, spam SMS\ndetection, and multi-label classification. Furthermore, it is shown that the\nsystem's performance can be further enhanced through few-shot or fine-tuning\nstrategies, making the fine-tuned model the top performer across all datasets.\nSource code and datasets are available in this GitHub repository:\nhttps://github.com/yeyimilk/llm-zero-shot-classifiers.\n","authors":["Zhiqiang Wang","Yiran Pang","Yanbin Lin","Xingquan Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.10523v2.pdf","comment":"ICDM Workshop ARRL 2024"},{"id":"http://arxiv.org/abs/2406.16030v2","updated":"2024-10-22T01:31:31Z","published":"2024-06-23T06:38:56Z","title":"Zero-Shot Cross-Lingual NER Using Phonemic Representations for\n  Low-Resource Languages","summary":"  Existing zero-shot cross-lingual NER approaches require substantial prior\nknowledge of the target language, which is impractical for low-resource\nlanguages. In this paper, we propose a novel approach to NER using phonemic\nrepresentation based on the International Phonetic Alphabet (IPA) to bridge the\ngap between representations of different languages. Our experiments show that\nour method significantly outperforms baseline models in extremely low-resource\nlanguages, with the highest average F1 score (46.38%) and lowest standard\ndeviation (12.67), particularly demonstrating its robustness with non-Latin\nscripts. Our codes are available at\nhttps://github.com/Gabriel819/zeroshot_ner.git\n","authors":["Jimin Sohn","Haeji Jung","Alex Cheng","Jooeon Kang","Yilin Du","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2406.16030v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.14978v2","updated":"2024-10-22T01:29:36Z","published":"2024-10-19T05:01:56Z","title":"Subversive Characters and Stereotyping Readers: Characterizing Queer\n  Relationalities with Dialogue-Based Relation Extraction","summary":"  Television is often seen as a site for subcultural identification and\nsubversive fantasy, including in queer cultures. How might we measure\nsubversion, or the degree to which the depiction of social relationship between\na dyad (e.g. two characters who are colleagues) deviates from its typical\nrepresentation on TV? To explore this question, we introduce the task of\nstereotypic relationship extraction. Built on cognitive stylistics, linguistic\nanthropology, and dialogue relation extraction, in this paper, we attempt to\nmodel the cognitive process of stereotyping TV characters in dialogic\ninteractions. Given a dyad, we want to predict: what social relationship do the\nspeakers exhibit through their words? Subversion is then characterized by the\ndiscrepancy between the distribution of the model's predictions and the ground\ntruth labels. To demonstrate the usefulness of this task and gesture at a\nmethodological intervention, we enclose four case studies to characterize the\nrepresentation of queer relationalities in the Big Bang Theory, Frasier, and\nGilmore Girls, as we explore the suspicious and reparative modes of reading\nwith our computational methods.\n","authors":["Kent K. Chang","Anna Ho","David Bamman"],"pdf_url":"https://arxiv.org/pdf/2410.14978v2.pdf","comment":"CHR 2024: Computational Humanities Research Conference"},{"id":"http://arxiv.org/abs/2409.13705v2","updated":"2024-10-22T01:01:56Z","published":"2024-09-05T14:35:35Z","title":"Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble","summary":"  Increasing use of large language models (LLMs) demand performant guardrails\nto ensure the safety of inputs and outputs of LLMs. When these safeguards are\ntrained on imbalanced data, they can learn the societal biases. We present a\nlight-weight, post-processing method for mitigating counterfactual fairness in\nclosed-source text safety classifiers. Our approach involves building an\nensemble that not only outperforms the input classifiers and policy-aligns\nthem, but also acts as a debiasing regularizer. We introduce two\nthreshold-agnostic metrics to assess the counterfactual fairness of a model,\nand demonstrate how combining these metrics with Fair Data Reweighting (FDW)\nhelps mitigate biases. We create an expanded Open AI dataset, and a new\ntemplated LLM-generated dataset based on user-prompts, both of which are\ncounterfactually balanced across identity groups and cover four key areas of\nsafety; we will work towards publicly releasing these datasets. Our results\nshow that our approach improves counterfactual fairness with minimal impact on\nmodel performance.\n","authors":["Olivia Sturman","Aparna Joshi","Bhaktipriya Radharapu","Piyush Kumar","Renee Shelby"],"pdf_url":"https://arxiv.org/pdf/2409.13705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16597v1","updated":"2024-10-22T00:47:54Z","published":"2024-10-22T00:47:54Z","title":"Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for\n  Improved Coverage and Efficiency","summary":"  Knowledge graphs (KGs) generated by large language models (LLMs) are becoming\nincreasingly valuable for Retrieval-Augmented Generation (RAG) applications\nthat require knowledge-intensive reasoning. However, existing KG extraction\nmethods predominantly rely on prompt-based approaches, which are inefficient\nfor processing large-scale corpora. These approaches often suffer from\ninformation loss, particularly with long documents, due to the lack of\nspecialized design for KG construction. Additionally, there is a gap in\nevaluation datasets and methodologies for ontology-free KG construction. To\novercome these limitations, we propose SynthKG, a multi-step, document-level\nontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM\non the synthesized document-KG pairs, we streamline the multi-step process into\na single-step KG generation approach called Distill-SynthKG, substantially\nreducing the number of LLM inference calls. Furthermore, we re-purpose existing\nquestion-answering datasets to establish KG evaluation datasets and introduce\nnew evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a\nnovel graph-based retrieval framework for RAG. Experimental results demonstrate\nthat Distill-SynthKG not only surpasses all baseline models in KG quality --\nincluding models up to eight times larger -- but also consistently excels in\nretrieval and question-answering tasks. Our proposed graph retrieval framework\nalso outperforms all KG-retrieval methods across multiple benchmark datasets.\nWe release the SynthKG dataset and Distill-SynthKG model publicly to support\nfurther research and development.\n","authors":["Prafulla Kumar Choubey","Xin Su","Man Luo","Xiangyu Peng","Caiming Xiong","Tiep Le","Shachar Rosenman","Vasudev Lal","Phil Mui","Ricky Ho","Phillip Howard","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2410.16597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11020v3","updated":"2024-10-22T00:42:39Z","published":"2024-10-14T19:16:56Z","title":"Improving the Language Understanding Capabilities of Large Language\n  Models Using Reinforcement Learning","summary":"  Large language models (LLMs), built on decoder-only transformers, excel in\nnatural language generation and adapt to diverse tasks using zero-shot and\nfew-shot prompting. However, these prompting methods often struggle on natural\nlanguage understanding (NLU) tasks, where encoder-only models like BERT-base\noutperform LLMs on benchmarks like GLUE and SuperGLUE. This paper explores two\napproaches-supervised fine-tuning (SFT) and proximal policy optimization\n(PPO)-to enhance LLMs' NLU abilities. To reduce the cost of full-model\nfine-tuning, we integrate low-rank adaptation (LoRA) layers, limiting updates\nto these layers during both SFT and PPO. In SFT, task-specific prompts are\nconcatenated with input queries and ground-truth labels, optimizing with\nnext-token prediction. Despite this, LLMs still underperform compared to models\nlike BERT-base on several NLU tasks. To close this gap, we apply PPO, a\nreinforcement learning technique that treats each token generation as an action\nand uses a reward function based on alignment with ground-truth answers. PPO\nthen updates the model to maximize these rewards, aligning outputs with correct\nlabels. Our experiments with LLAMA2-7B show that PPO improves performance, with\na 6.3-point gain over SFT on GLUE. PPO exceeds zero-shot by 38.7 points and\nfew-shot by 26.1 points on GLUE, while surpassing these by 28.8 and 28.5 points\non SuperGLUE. Additionally, PPO outperforms BERT-large by 2.7 points on GLUE\nand 9.3 points on SuperGLUE. The improvements are consistent across models like\nQwen2.5-7B and MPT-7B, highlighting PPO's robustness in enhancing LLMs' NLU\ncapabilities.\n","authors":["Bokai Hu","Sai Ashish Somayajula","Xin Pan","Zihan Huang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2410.11020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16592v1","updated":"2024-10-22T00:30:08Z","published":"2024-10-22T00:30:08Z","title":"ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding","summary":"  The rise of social media and short-form video (SFV) has facilitated a\nbreeding ground for misinformation. With the emergence of large language\nmodels, significant research has gone into curbing this misinformation problem\nwith automatic false claim detection for text. Unfortunately, the automatic\ndetection of misinformation in SFV is a more complex problem that remains\nlargely unstudied. While text samples are monomodal (only containing words),\nSFVs comprise three different modalities: words, visuals, and non-linguistic\naudio. In this work, we introduce Video Masked Autoencoders for Misinformation\nGuarding (ViMGuard), the first deep-learning architecture capable of\nfact-checking an SFV through analysis of all three of its constituent\nmodalities. ViMGuard leverages a dual-component system. First, Video and Audio\nMasked Autoencoders analyze the visual and non-linguistic audio elements of a\nvideo to discern its intention; specifically whether it intends to make an\ninformative claim. If it is deemed that the SFV has informative intent, it is\npassed through our second component: a Retrieval Augmented Generation system\nthat validates the factual accuracy of spoken words. In evaluation, ViMGuard\noutperformed three cutting-edge fact-checkers, thus setting a new standard for\nSFV fact-checking and marking a significant stride toward trustworthy news on\nsocial platforms. To promote further testing and iteration, VimGuard was\ndeployed into a Chrome extension and all code was open-sourced on GitHub.\n","authors":["Andrew Kan","Christopher Kan","Zaid Nabulsi"],"pdf_url":"https://arxiv.org/pdf/2410.16592v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.05841v2","updated":"2024-10-22T00:16:21Z","published":"2024-07-08T11:38:49Z","title":"An Empirical Comparison of Vocabulary Expansion and Initialization\n  Approaches for Language Models","summary":"  Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods. We release our code publicly\n(https://github.com/AI4Bharat/VocabAdaptation_LLM/tree/CW2V).\n","authors":["Nandini Mundra","Aditya Nanda Kishore","Raj Dabre","Ratish Puduppully","Anoop Kunchukuttan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.05841v2.pdf","comment":"CONLL 2024 (EMNLP 2024)"},{"id":"http://arxiv.org/abs/2410.16589v1","updated":"2024-10-22T00:14:36Z","published":"2024-10-22T00:14:36Z","title":"Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis\n  with Large Language Models","summary":"  Sentiment analysis has become increasingly important for assessing public\nopinion and informing decision-making. Large language models (LLMs) have\nrevolutionized this field by capturing nuanced language patterns. However,\nadapting LLMs to domain-specific sentiment analysis tasks remains challenging\ndue to computational constraints and the need for optimal fine-tuning. To\naddress these challenges, we propose a novel Dynamic Adaptive Rank Space\nExploration (DARSE) framework for efficient and effective sentiment analysis\nusing LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the\noptimal rank range, a fine-grained exploration algorithm to refine rank\nselection, and a dynamic rank allocation method to determine the optimal rank\ncombination for each LLM layer. Extensive experiments demonstrate that DARSE\nsignificantly improves sentiment analysis accuracy, achieving a 15.1%\nimprovement in MSE and a 4.3% improvement in accuracy compared to previous\nwork. Our framework strikes a balance between computational efficiency and\nmodel performance, making it a promising approach for sentiment analysis with\nLLMs.\n","authors":["Hongcheng Ding","Fuzhen Hu","Xuanze Zhao","Zixiao Jiang","Shamsul Nahar Abdullah","Deshinta Arrova Dewi"],"pdf_url":"https://arxiv.org/pdf/2410.16589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11100v2","updated":"2024-10-22T00:02:28Z","published":"2024-05-17T21:27:32Z","title":"Are Large Language Models Moral Hypocrites? A Study Based on Moral\n  Foundations","summary":"  Large language models (LLMs) have taken centre stage in debates on Artificial\nIntelligence. Yet there remains a gap in how to assess LLMs' conformity to\nimportant human values. In this paper, we investigate whether state-of-the-art\nLLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid\nresults) are moral hypocrites. We employ two research instruments based on the\nMoral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which\ninvestigates which values are considered morally relevant in abstract moral\njudgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate\nmoral cognition in concrete scenarios related to each moral foundation. We\ncharacterise conflicts in values between these different abstractions of moral\nevaluation as hypocrisy. We found that both models displayed reasonable\nconsistency within each instrument compared to humans, but they displayed\ncontradictory and hypocritical behaviour when we compared the abstract values\npresent in the MFQ to the evaluation of concrete moral violations of the MFV.\n","authors":["José Luiz Nunes","Guilherme F. C. F. Almeida","Marcelo de Araujo","Simone D. J. Barbosa"],"pdf_url":"https://arxiv.org/pdf/2405.11100v2.pdf","comment":"Final version available at:\n  https://ojs.aaai.org/index.php/AIES/article/view/31704 13 pages, 4 figures, 2\n  tables"},{"id":"http://arxiv.org/abs/2410.17477v1","updated":"2024-10-22T23:24:15Z","published":"2024-10-22T23:24:15Z","title":"Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination","summary":"  The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to \\textit{hallucinate} false or misleading information, limiting\ntheir reliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.\n","authors":["Jerry Huang","Prasanna Parthasarathi","Mehdi Rezagholizadeh","Boxing Chen","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2410.17477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11402v2","updated":"2024-10-22T23:13:34Z","published":"2024-09-17T17:59:06Z","title":"NVLM: Open Frontier-Class Multimodal LLMs","summary":"  We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B\nand will open-source the training code for the community soon.\n","authors":["Wenliang Dai","Nayeon Lee","Boxin Wang","Zhuolin Yang","Zihan Liu","Jon Barker","Tuomas Rintamaki","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2409.11402v2.pdf","comment":"Fixed the typos. For more information, please visit our project page\n  at: https://research.nvidia.com/labs/adlr/NVLM-1"},{"id":"http://arxiv.org/abs/2410.17462v1","updated":"2024-10-22T22:43:14Z","published":"2024-10-22T22:43:14Z","title":"Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain\n  Annotation","summary":"  Time series data is ubiquitous across various domains, including\nmanufacturing, finance, and healthcare. High-quality annotations are essential\nfor effectively understanding time series and facilitating downstream tasks;\nhowever, obtaining such annotations is challenging, particularly in\nmission-critical domains. In this paper, we propose TESSA, a multi-agent system\ndesigned to automatically generate both general and domain-specific annotations\nfor time series data. TESSA introduces two agents: a general annotation agent\nand a domain-specific annotation agent. The general agent captures common\npatterns and knowledge across multiple source domains, leveraging both\ntime-series-wise and text-wise features to generate general annotations.\nMeanwhile, the domain-specific agent utilizes limited annotations from the\ntarget domain to learn domain-specific terminology and generate targeted\nannotations. Extensive experiments on multiple synthetic and real-world\ndatasets demonstrate that TESSA effectively generates high-quality annotations,\noutperforming existing methods.\n","authors":["Minhua Lin","Zhengzhang Chen","Yanchi Liu","Xujiang Zhao","Zongyu Wu","Junxiang Wang","Xiang Zhang","Suhang Wang","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.17462v1.pdf","comment":"23 pages, 9 figures, 24 tables"},{"id":"http://arxiv.org/abs/2410.14872v2","updated":"2024-10-22T22:18:14Z","published":"2024-10-18T21:38:21Z","title":"How to Evaluate Reward Models for RLHF","summary":"  We introduce a new benchmark for reward models that quantifies their ability\nto produce strong language models through RLHF (Reinforcement Learning from\nHuman Feedback). The gold-standard approach is to run a full RLHF training\npipeline and directly probe downstream LLM performance. However, this process\nis prohibitively expensive. To address this, we build a predictive model of\ndownstream LLM performance by evaluating the reward model on proxy tasks. These\nproxy tasks consist of a large-scale human preference and a verifiable\ncorrectness preference dataset, in which we measure 12 metrics across 12\ndomains. To investigate which reward model metrics are most correlated to\ngold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a\nlarge-scale crowdsourced human preference platform to view real reward model\ndownstream performance as ground truth. Ultimately, we compile our data and\nfindings into Preference Proxy Evaluations (PPE), the first reward model\nbenchmark explicitly linked to post-RLHF real-world human preference\nperformance, which we open-source for public use and further development. Our\ncode and evaluations can be found at https://github.com/lmarena/PPE .\n","authors":["Evan Frick","Tianle Li","Connor Chen","Wei-Lin Chiang","Anastasios N. Angelopoulos","Jiantao Jiao","Banghua Zhu","Joseph E. Gonzalez","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2410.14872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13639v2","updated":"2024-10-22T22:05:16Z","published":"2024-10-17T15:09:03Z","title":"A Comparative Study on Reasoning Patterns of OpenAI's o1 Model","summary":"  Enabling Large Language Models (LLMs) to handle a wider range of complex\ntasks (e.g., coding, math) has drawn great attention from many researchers. As\nLLMs continue to evolve, merely increasing the number of model parameters\nyields diminishing performance improvements and heavy computational costs.\nRecently, OpenAI's o1 model has shown that inference strategies (i.e.,\nTest-time Compute methods) can also significantly enhance the reasoning\ncapabilities of LLMs. However, the mechanisms behind these methods are still\nunexplored. In our work, to investigate the reasoning patterns of o1, we\ncompare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent\nWorkflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general\nreasoning benchmarks in three domains (i.e., math, coding, commonsense\nreasoning). Specifically, first, our experiments show that the o1 model has\nachieved the best performance on most datasets. Second, as for the methods of\nsearching diverse responses (e.g., BoN), we find the reward models' capability\nand the search space both limit the upper boundary of these methods. Third, as\nfor the methods that break the problem into many sub-problems, the Agent\nWorkflow has achieved better performance than Step-wise BoN due to the\ndomain-specific system prompt for planning better reasoning processes. Fourth,\nit is worth mentioning that we have summarized six reasoning patterns of o1,\nand provided a detailed analysis on several reasoning benchmarks.\n","authors":["Siwei Wu","Zhongyuan Peng","Xinrun Du","Tuney Zheng","Minghao Liu","Jialong Wu","Jiachen Ma","Yizhi Li","Jian Yang","Wangchunshu Zhou","Qunshu Lin","Junbo Zhao","Zhaoxiang Zhang","Wenhao Huang","Ge Zhang","Chenghua Lin","J. H. Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17450v1","updated":"2024-10-22T21:55:54Z","published":"2024-10-22T21:55:54Z","title":"Interação entre robôs humanoides: desenvolvendo a\n  colaboração e comunicação autônoma","summary":"  This study investigates the interaction between humanoid robots NAO and\nPepper, emphasizing their potential applications in educational settings. NAO,\nwidely used in education, and Pepper, designed for social interactions, of er\nnew opportunities for autonomous communication and collaboration. Through a\nseries of programmed interactions, the robots demonstrated their ability to\ncommunicate and coordinate actions autonomously, highlighting their potential\nas tools for enhancing learning environments. The research also explores the\nintegration of emerging technologies, such as artificial intelligence, into\nthese systems, allowing robots to learn from each other and adapt their\nbehavior. The findings suggest that NAO and Pepper can significantly contribute\nto both technical learning and the development of social and emotional skills\nin students, of ering innovative pedagogical approaches through the use of\nhumanoid robotics.\n","authors":["Moraes Pablo","Peters Christopher","Rodríguez Mónica","Sodre Hiago","Mazondo Ahilen","Sandin Vincent","Barcelona Sebastian","Moraes William","Fernández Santiago","Assunção Nathalie","de Vargas Bruna","Dörnbach Tobias","Kelbouscas André","Grando Ricardo"],"pdf_url":"https://arxiv.org/pdf/2410.17450v1.pdf","comment":"in Portuguese language"},{"id":"http://arxiv.org/abs/2410.17448v1","updated":"2024-10-22T21:50:52Z","published":"2024-10-22T21:50:52Z","title":"In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models","summary":"  Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We also demonstrate how strategic prompting improves the\nmodel's performance and how the natural language interface simplifies\nintegrating theory with data. Although this approach does not outperform\nestablished SR programs where target equations are more complex, LLMs can\nnonetheless iterate toward improved solutions while following instructions and\nincorporating scientific context in natural language.\n","authors":["Samiha Sharlin","Tyler R. Josephson"],"pdf_url":"https://arxiv.org/pdf/2410.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17439v1","updated":"2024-10-22T21:30:58Z","published":"2024-10-22T21:30:58Z","title":"Evaluating AI-Generated Essays with GRE Analytical Writing Assessment","summary":"  The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing GPT-4o received an average score of 4.67, falling between\n\"generally thoughtful, well-developed analysis of the issue and conveys meaning\nclearly\" and \"presents a competent analysis of the issue and conveys meaning\nwith acceptable clarity\" according to the GRE scoring guideline. We also\nevaluated the detection accuracy of these essays, with detectors trained on\nessays generated by the same and different LLMs.\n","authors":["Yang Zhong","Jiangang Hao","Michael Fauss","Chen Li","Yuan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17439v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17423v1","updated":"2024-10-22T20:52:51Z","published":"2024-10-22T20:52:51Z","title":"Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis","summary":"  The current surge in Artificial Intelligence (AI) interest, reflected in\nheightened media coverage since 2009, has sparked significant debate on AI's\nimplications for privacy, social justice, workers' rights, and democracy. The\nmedia plays a crucial role in shaping public perception and acceptance of AI\ntechnologies. However, research into how AI appears in media has primarily\nfocused on anglophone contexts, leaving a gap in understanding how AI is\nrepresented globally. This study addresses this gap by analyzing 3,560 news\narticles from Brazilian media published between July 1, 2023, and February 29,\n2024, from 13 popular online news outlets. Using Computational Grounded Theory\n(CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and\nNamed-Entity Recognition to investigate the main topics in AI coverage and the\nentities represented. The findings reveal that Brazilian news coverage of AI is\ndominated by topics related to applications in the workplace and product\nlaunches, with limited space for societal concerns, which mostly focus on\ndeepfakes and electoral integrity. The analysis also highlights a significant\npresence of industry-related entities, indicating a strong influence of\ncorporate agendas in the country's news. This study underscores the need for a\nmore critical and nuanced discussion of AI's societal impacts in Brazilian\nmedia.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2410.17423v1.pdf","comment":"18 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.12843v2","updated":"2024-10-22T20:50:56Z","published":"2024-07-04T15:10:51Z","title":"NutriBench: A Dataset for Evaluating Large Language Models in\n  Carbohydrate Estimation from Meal Descriptions","summary":"  Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html\n","authors":["Andong Hua","Mehak Preet Dhaliwal","Ryan Burke","Yao Qin"],"pdf_url":"https://arxiv.org/pdf/2407.12843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17413v1","updated":"2024-10-22T20:39:21Z","published":"2024-10-22T20:39:21Z","title":"Scalable Influence and Fact Tracing for Large Language Model Pretraining","summary":"  Training data attribution (TDA) methods aim to attribute model outputs back\nto specific training examples, and the application of these methods to large\nlanguage model (LLM) outputs could significantly advance model transparency and\ndata curation. However, it has been challenging to date to apply these methods\nto the full scale of LLM pretraining. In this paper, we refine existing\ngradient-based methods to work effectively at scale, allowing us to retrieve\ninfluential examples for an 8B-parameter language model from a pretraining\ncorpus of over 160B tokens with no need for subsampling or pre-filtering. Our\nmethod combines several techniques, including optimizer state correction, a\ntask-specific Hessian approximation, and normalized encodings, which we find to\nbe critical for performance at scale. In quantitative evaluations on a fact\ntracing task, our method performs best at identifying examples that influence\nmodel predictions, but classical, model-agnostic retrieval methods such as BM25\nstill perform better at finding passages which explicitly contain relevant\nfacts. These results demonstrate a misalignment between factual attribution and\ncausal influence. With increasing model size and training tokens, we find that\ninfluence more closely aligns with attribution. Finally, we examine different\ntypes of examples identified as influential by our method, finding that while\nmany directly entail a particular fact, others support the same output by\nreinforcing priors on relation types, common entities, and names.\n","authors":["Tyler A. Chang","Dheeraj Rajagopal","Tolga Bolukbasi","Lucas Dixon","Ian Tenney"],"pdf_url":"https://arxiv.org/pdf/2410.17413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17401v1","updated":"2024-10-22T20:18:26Z","published":"2024-10-22T20:18:26Z","title":"AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents","summary":"  Vision Language Models (VLMs) have revolutionized the creation of generalist\nweb agents, empowering them to autonomously complete diverse tasks on\nreal-world websites, thereby boosting human efficiency and productivity.\nHowever, despite their remarkable capabilities, the safety and security of\nthese agents against malicious attacks remain critically underexplored, raising\nsignificant concerns about their safe deployment. To uncover and exploit such\nvulnerabilities in web agents, we provide AdvWeb, a novel black-box attack\nframework designed against web agents. AdvWeb trains an adversarial prompter\nmodel that generates and injects adversarial prompts into web pages, misleading\nweb agents into executing targeted adversarial actions such as inappropriate\nstock purchases or incorrect bank transactions, actions that could lead to\nsevere real-world consequences. With only black-box access to the web agent, we\ntrain and optimize the adversarial prompter model using DPO, leveraging both\nsuccessful and failed attack strings against the target agent. Unlike prior\napproaches, our adversarial string injection maintains stealth and control: (1)\nthe appearance of the website remains unchanged before and after the attack,\nmaking it nearly impossible for users to detect tampering, and (2) attackers\ncan modify specific substrings within the generated adversarial string to\nseamlessly change the attack objective (e.g., purchasing stocks from a\ndifferent company), enhancing attack flexibility and efficiency. We conduct\nextensive evaluations, demonstrating that AdvWeb achieves high success rates in\nattacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings\nexpose critical vulnerabilities in current LLM/VLM-based agents, emphasizing\nthe urgent need for developing more reliable web agents and effective defenses.\nOur code and data are available at https://ai-secure.github.io/AdvWeb/ .\n","authors":["Chejian Xu","Mintong Kang","Jiawei Zhang","Zeyi Liao","Lingbo Mo","Mengqi Yuan","Huan Sun","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.17401v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2405.06626v2","updated":"2024-10-22T20:05:32Z","published":"2024-05-10T17:40:02Z","title":"Characterizing the Accuracy -- Efficiency Trade-off of Low-rank\n  Decomposition in Language Models","summary":"  Recent large language models (LLMs) employ billions of parameters to enable\nbroad problem-solving capabilities. Such language models also tend to be\nmemory-bound because of the dominance of matrix-vector and matrix-matrix\nmultiplications with low arithmetic intensity. Therefore, optimizing the memory\nfootprint and traffic is an important optimization direction for LLMs today.\nModel compression methods such as quantization and parameter pruning have been\nactively explored to achieve memory footprint and traffic optimization.\nHowever, the accuracy-efficiency trade-off of rank pruning (i.e., low-rank\ndecomposition) for LLMs is not well-understood yet. Therefore, in this work, we\ncharacterize the accuracy-efficiency trade-off of a low-rank decomposition\nmethod, specifically Tucker decomposition, on recent language models, including\nan open-source LLM, Llama 2. We formalize the low-rank decomposition design\nspace and show that the decomposition design space is enormous (e.g.,\nO($2^{39}$) for Llama2-7B). To navigate such a vast design space, we formulate\nit and perform thorough case studies of accuracy-efficiency trade-offs using\nsix widely used LLM benchmarks on BERT and Llama 2 models. Our results show\nthat we can achieve a 9\\% model size reduction with minimal accuracy drops,\nwhich range from 4\\%p (\\%p refers to \"percentage point,\" which refers to the\nabsolute difference between two percentage numbers; 74\\% -> 78\\% = 4\\%p\nincrease) to 10\\%p, depending on the difficulty of the benchmark, without any\nretraining to recover accuracy after decomposition. The results show that\nlow-rank decomposition can be a promising direction for LLM-based applications\nthat require real-time service at scale (e.g., AI agent and real-time coding\nassistant), where the latency is as important as the model accuracy.\n","authors":["Chakshu Moar","Faraz Tahmasebi","Michael Pellauer","Hyoukjun Kwon"],"pdf_url":"https://arxiv.org/pdf/2405.06626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12856v4","updated":"2024-10-22T19:53:58Z","published":"2024-05-21T15:13:12Z","title":"LLM Processes: Numerical Predictive Distributions Conditioned on Natural\n  Language","summary":"  Machine learning practitioners often face significant challenges in formally\nintegrating their prior knowledge and beliefs into predictive models, limiting\nthe potential for nuanced and context-aware analyses. Moreover, the expertise\nneeded to integrate this prior knowledge into probabilistic modeling typically\nlimits the application of these models to specialists. Our goal is to build a\nregression model that can process numerical data and make probabilistic\npredictions at arbitrary locations, guided by natural language text which\ndescribes a user's prior knowledge. Large Language Models (LLMs) provide a\nuseful starting point for designing such a tool since they 1) provide an\ninterface where users can incorporate expert insights in natural language and\n2) provide an opportunity for leveraging latent problem-relevant knowledge\nencoded in LLMs that users may not have themselves. We start by exploring\nstrategies for eliciting explicit, coherent numerical predictive distributions\nfrom LLMs. We examine these joint predictive distributions, which we call LLM\nProcesses, over arbitrarily-many quantities in settings such as forecasting,\nmulti-dimensional regression, black-box optimization, and image modeling. We\ninvestigate the practical details of prompting to elicit coherent predictive\ndistributions, and demonstrate their effectiveness at regression. Finally, we\ndemonstrate the ability to usefully incorporate text into numerical\npredictions, improving predictive performance and giving quantitative structure\nthat reflects qualitative descriptions. This lets us begin to explore the rich,\ngrounded hypothesis space that LLMs implicitly encode.\n","authors":["James Requeima","John Bronskill","Dami Choi","Richard E. Turner","David Duvenaud"],"pdf_url":"https://arxiv.org/pdf/2405.12856v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17385v1","updated":"2024-10-22T19:39:15Z","published":"2024-10-22T19:39:15Z","title":"Do Vision-Language Models Represent Space and How? Evaluating Spatial\n  Frame of Reference Under Ambiguities","summary":"  Spatial expressions in situated communication can be ambiguous, as their\nmeanings vary depending on the frames of reference (FoR) adopted by speakers\nand listeners. While spatial language understanding and reasoning by\nvision-language models (VLMs) have gained increasing attention, potential\nambiguities in these models are still under-explored. To address this issue, we\npresent the COnsistent Multilingual Frame Of Reference Test (COMFORT), an\nevaluation protocol to systematically assess the spatial reasoning capabilities\nof VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing\nsome alignment with English conventions in resolving ambiguities, our\nexperiments reveal significant shortcomings of VLMs: notably, the models (1)\nexhibit poor robustness and consistency, (2) lack the flexibility to\naccommodate multiple FoRs, and (3) fail to adhere to language-specific or\nculture-specific conventions in cross-lingual tests, as English tends to\ndominate other languages. With a growing effort to align vision-language models\nwith human cognitive intuitions, we call for more attention to the ambiguous\nnature and cross-cultural diversity of spatial reasoning.\n","authors":["Zheyuan Zhang","Fengyuan Hu","Jayjun Lee","Freda Shi","Parisa Kordjamshidi","Joyce Chai","Ziqiao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.17385v1.pdf","comment":"Accepted to Pluralistic Alignment @ NeurIPS 2024 | Project page:\n  https://spatial-comfort.github.io/"},{"id":"http://arxiv.org/abs/2410.17375v1","updated":"2024-10-22T19:15:35Z","published":"2024-10-22T19:15:35Z","title":"AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM\n  Acceleration","summary":"  Large language models typically generate tokens autoregressively, using each\ntoken as input for the next. Recent work on Speculative Decoding has sought to\naccelerate this process by employing a smaller, faster draft model to more\nquickly generate candidate tokens. These candidates are then verified in\nparallel by the larger (original) verify model, resulting in overall speedup\ncompared to using the larger model by itself in an autoregressive fashion. In\nthis work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding),\na system that further accelerates generation by decoupling the draft and verify\nphases into a continuous, asynchronous approach. Unlike conventional\nspeculative decoding, where only one model (draft or verify) performs token\ngeneration at a time, AMUSD enables both models to perform predictions\nindependently on separate devices (e.g., GPUs). We evaluate our approach over\nmultiple datasets and show that AMUSD achieves an average 29% improvement over\nspeculative decoding and up to 1.96$\\times$ speedup over conventional\nautoregressive decoding, while achieving identical output quality. Our system\nis open-source and available at https://github.com/BradMcDanel/AMUSD/.\n","authors":["Bradley McDanel"],"pdf_url":"https://arxiv.org/pdf/2410.17375v1.pdf","comment":"4 pages, 5 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2405.11724v2","updated":"2024-10-22T19:07:08Z","published":"2024-05-20T01:57:34Z","title":"Token-wise Influential Training Data Retrieval for Large Language Models","summary":"  Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.\n","authors":["Huawei Lin","Jikai Long","Zhaozhuo Xu","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.11724v2.pdf","comment":"Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution"},{"id":"http://arxiv.org/abs/2402.13459v2","updated":"2024-10-22T19:01:35Z","published":"2024-02-21T01:30:03Z","title":"Learning to Poison Large Language Models During Instruction Tuning","summary":"  The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where\nadversaries insert backdoor triggers into training data to manipulate outputs\nfor malicious purposes. This work further identifies additional security risks\nin LLMs by designing a new data poisoning attack tailored to exploit the\ninstruction tuning process. We propose a novel gradient-guided backdoor trigger\nlearning (GBTL) algorithm to identify adversarial triggers efficiently,\nensuring an evasion of detection by conventional defenses while maintaining\ncontent integrity. Through experimental validation across various tasks,\nincluding sentiment analysis, domain generation, and question answering, our\npoisoning strategy demonstrates a high success rate in compromising various\nLLMs' outputs. We further propose two defense strategies against data poisoning\nattacks, including in-context learning (ICL) and continuous learning (CL),\nwhich effectively rectify the behavior of LLMs and significantly reduce the\ndecline in performance. Our work highlights the significant security risks\npresent during the instruction tuning of LLMs and emphasizes the necessity of\nsafeguarding LLMs against data poisoning attacks.\n","authors":["Yao Qiang","Xiangyu Zhou","Saleh Zare Zade","Mohammad Amin Roshani","Prashant Khanduri","Douglas Zytko","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.13459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15226v2","updated":"2024-10-22T18:54:23Z","published":"2024-10-19T22:14:07Z","title":"On the Diversity of Synthetic Data and its Impact on Training Large\n  Language Models","summary":"  The rise of Large Language Models (LLMs) has accentuated the need for\ndiverse, high-quality pre-training data. Synthetic data emerges as a viable\nsolution to the challenges of data scarcity and inaccessibility. While previous\nliterature has focused predominantly on the quality and quantity of real data,\nour work enables the measurement of diversity in synthetic data and explores\nits impact on LLM performance. We study the downstream effects of synthetic\ndata diversity during both the pre-training and fine-tuning stages by\nintroducing a new diversity metric, \\textit{LLM cluster-agent}, designed to\nevaluate the diversity of synthetic datasets. Through a series of controlled\nexperiments with models of 350M and 1.4B parameters, we demonstrate that the\nproposed cluster-based LLM scoring of diversity correlates positively with both\npre-training and supervised fine-tuning performance. Our findings also reveal\nthat synthetic data diversity in pre-training affects supervised fine-tuning\nmore significantly than pre-training itself, even for smaller models. We hope\nthis study advances our understanding of the optimal use of synthetic data in\nLLM training and opens new avenues for efficient data generation processes.\n","authors":["Hao Chen","Abdul Waheed","Xiang Li","Yidong Wang","Jindong Wang","Bhiksha Raj","Marah I. Abdin"],"pdf_url":"https://arxiv.org/pdf/2410.15226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17542v3","updated":"2024-10-22T18:51:01Z","published":"2024-06-25T13:29:14Z","title":"CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization","summary":"  Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses greedy coordinate descent to minimize\nthe layer-wise reconstruction loss to achieve high-quality quantized weights.\nOur algorithm is easy to implement and scales efficiently to models with\nhundreds of billions of parameters. We perform extensive evaluation on Gemma,\nand PaLM2 model families, and demonstrate that CDQuant consistently outperforms\nGPTQ in 2-4 bit weight quantization. Moreover, CDQuant improves the performance\nof state-of-the-art PTQ techniques such as QuIP and FrameQuant when used as a\nreplacement for their GPTQ component, resulting in further gains in quality.\n","authors":["Pranav Ajit Nair","Arun Sai Suggala"],"pdf_url":"https://arxiv.org/pdf/2406.17542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17355v1","updated":"2024-10-22T18:47:46Z","published":"2024-10-22T18:47:46Z","title":"All Entities are Not Created Equal: Examining the Long Tail for\n  Fine-Grained Entity Typing","summary":"  Pre-trained language models (PLMs) are trained on large amounts of data,\nwhich helps capture world knowledge alongside linguistic competence. Due to\nthis, they are extensively used for ultra-fine entity typing tasks, where they\nprovide the entity knowledge held in its parameter space. Given that PLMs learn\nfrom co-occurrence patterns, they likely contain more knowledge or less\nknowledge about entities depending on their how frequent they are in the\npre-training data. In this work, we probe PLMs to elicit encoded entity\nprobabilities and demonstrate that they highly correlate with their frequency\nin large-scale internet data. Then, we demonstrate that entity-typing\napproaches that rely on PLMs struggle with entities at the long tail on the\ndistribution. Our findings suggests that we need to go beyond PLMs to produce\nsolutions that perform well for rare, new or infrequent entities.\n","authors":["Advait Deshmukh","Ashwin Umadi","Dananjay Srinivas","Maria Leonor Pacheco"],"pdf_url":"https://arxiv.org/pdf/2410.17355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17678v5","updated":"2024-10-22T18:26:51Z","published":"2024-07-25T00:27:07Z","title":"S2-Attention: Hardware-Aware Context Sharding Among Attention Heads","summary":"  Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.\n","authors":["Xihui Lin","Yunan Zhang","Suyu Ge","Liliang Ren","Barun Patra","Vishrav Chaudhary","Hao Peng","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.17678v5.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.02024v3","updated":"2024-10-22T18:22:11Z","published":"2024-10-02T20:45:51Z","title":"FLAG: Financial Long Document Classification via AMR-based GNN","summary":"  The advent of large language models (LLMs) has initiated much research into\ntheir various financial applications. However, in applying LLMs on long\ndocuments, semantic relations are not explicitly incorporated, and a full or\narbitrarily sparse attention operation is employed. In recent years, progress\nhas been made in Abstract Meaning Representation (AMR), which is a graph-based\nrepresentation of text to preserve its semantic relations. Since AMR can\nrepresent semantic relationships at a deeper level, it can be beneficially\nutilized by graph neural networks (GNNs) for constructing effective\ndocument-level graph representations built upon LLM embeddings to predict\ntarget metrics in the financial domain. We propose FLAG: Financial Long\ndocument classification via AMR-based GNN, an AMR graph based framework to\ngenerate document-level embeddings for long financial document classification.\nWe construct document-level graphs from sentence-level AMR graphs, endow them\nwith specialized LLM word embeddings in the financial domain, apply a deep\nlearning mechanism that utilizes a GNN, and examine the efficacy of our\nAMR-based approach in predicting labeled target data from long financial\ndocuments. Extensive experiments are conducted on a dataset of quarterly\nearnings calls transcripts of companies in various sectors of the economy, as\nwell as on a corpus of more recent earnings calls of companies in the S&P 1500\nComposite Index. We find that our AMR-based approach outperforms fine-tuning\nLLMs directly on text in predicting stock price movement trends at different\ntime horizons in both datasets. Our work also outperforms previous work\nutilizing document graphs and GNNs for text classification.\n","authors":["Bolun \"Namir\" Xia","Aparna Gupta","Mohammed J. Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.02024v3.pdf","comment":"8 pages, 3 figures, to be published in CIFEr Conference 2024 as\n  \"Semantic Graph Learning for Trend Prediction from Long Financial Documents\""},{"id":"http://arxiv.org/abs/2410.17337v1","updated":"2024-10-22T18:11:43Z","published":"2024-10-22T18:11:43Z","title":"Captions Speak Louder than Images (CASLIE): Generalizing Foundation\n  Models for E-commerce from High-quality Multimodal Instruction Data","summary":"  Leveraging multimodal data to drive breakthroughs in e-commerce applications\nthrough Multimodal Foundation Models (MFMs) is gaining increasing attention\nfrom the research community. However, there are significant challenges that\nhinder the optimal use of multimodal e-commerce data by foundation models: (1)\nthe scarcity of large-scale, high-quality multimodal benchmark datasets; and\n(2) the lack of effective multimodal information integration methods. To\naddress these challenges, in this paper, we introduce MMECInstruct, the\nfirst-ever, large-scale, and high-quality multimodal instruction dataset for\ne-commerce. We also develop CASLIE, a simple, lightweight, yet effective\nframework for integrating multimodal information for e-commerce. Leveraging\nMMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted\nas CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models\nsubstantially outperform 5 categories of advanced baseline models in the\nin-domain evaluation. Moreover, CASLIE models show strong generalizability to\nout-of-domain settings. MMECInstruct and CASLIE models are publicly accessible\nthrough https://ninglab.github.io/CASLIE/.\n","authors":["Xinyi Ling","Bo Peng","Hanwen Du","Zhihui Zhu","Xia Ning"],"pdf_url":"https://arxiv.org/pdf/2410.17337v1.pdf","comment":"Xinyi Ling and Bo Peng contributed equally to this paper"},{"id":"http://arxiv.org/abs/2410.17333v1","updated":"2024-10-22T18:08:25Z","published":"2024-10-22T18:08:25Z","title":"Are Large Language Models Ready for Travel Planning?","summary":"  While large language models (LLMs) show promise in hospitality and tourism,\ntheir ability to provide unbiased service across demographic groups remains\nunclear. This paper explores gender and ethnic biases when LLMs are utilized as\ntravel planning assistants. To investigate this issue, we apply machine\nlearning techniques to analyze travel suggestions generated from three\nopen-source LLMs. Our findings reveal that the performance of race and gender\nclassifiers substantially exceeds random chance, indicating differences in how\nLLMs engage with varied subgroups. Specifically, outputs align with cultural\nexpectations tied to certain races and genders. To minimize the effect of these\nstereotypes, we used a stop-word classification strategy, which decreased\nidentifiable differences, with no disrespectful terms found. However,\nhallucinations related to African American and gender minority groups were\nnoted. In conclusion, while LLMs can generate travel plans seemingly free from\nbias, it remains essential to verify the accuracy and appropriateness of their\nrecommendations.\n","authors":["Ruiping Ren","Xing Yao","Shu Cole","Haining Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17309v1","updated":"2024-10-22T18:00:00Z","published":"2024-10-22T18:00:00Z","title":"Literature Meets Data: A Synergistic Approach to Hypothesis Generation","summary":"  AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.\n","authors":["Haokun Liu","Yangqiaoyu Zhou","Mingxuan Li","Chenfei Yuan","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17309v1.pdf","comment":"30 pages, 7 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation"},{"id":"http://arxiv.org/abs/2311.18567v2","updated":"2024-10-22T17:21:21Z","published":"2023-11-30T13:58:13Z","title":"The Causal Influence of Grammatical Gender on Distributional Semantics","summary":"  How much meaning influences gender assignment across languages is an active\narea of research in linguistics and cognitive science. We can view current\napproaches as aiming to determine where gender assignment falls on a spectrum,\nfrom being fully arbitrarily determined to being largely semantically\ndetermined. For the latter case, there is a formulation of the neo-Whorfian\nhypothesis, which claims that even inanimate noun gender influences how people\nconceive of and talk about objects (using the choice of adjective used to\nmodify inanimate nouns as a proxy for meaning). We offer a novel, causal\ngraphical model that jointly represents the interactions between a noun's\ngrammatical gender, its meaning, and adjective choice. In accordance with past\nresults, we find a significant relationship between the gender of nouns and the\nadjectives that modify them. However, when we control for the meaning of the\nnoun, the relationship between grammatical gender and adjective choice is near\nzero and insignificant.\n","authors":["Karolina Stańczak","Kevin Du","Adina Williams","Isabelle Augenstein","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2311.18567v2.pdf","comment":null}]},"2024-10-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.18077v1","updated":"2024-10-23T17:58:49Z","published":"2024-10-23T17:58:49Z","title":"ALTA: Compiler-Based Analysis of Transformers","summary":"  We propose a new programming language called ALTA and a compiler that can map\nALTA programs to Transformer weights. ALTA is inspired by RASP, a language\nproposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler\nfrom RASP programs to Transformer weights. ALTA complements and extends this\nprior work, offering the ability to express loops and to compile programs to\nUniversal Transformers, among other advantages. ALTA allows us to\nconstructively show how Transformers can represent length-invariant algorithms\nfor computing parity and addition, as well as a solution to the SCAN benchmark\nof compositional generalization tasks, without requiring intermediate\nscratchpad decoding steps. We also propose tools to analyze cases where the\nexpressibility of an algorithm is established, but end-to-end training on a\ngiven training set fails to induce behavior consistent with the desired\nalgorithm. To this end, we explore training from ALTA execution traces as a\nmore fine-grained supervision signal. This enables additional experiments and\ntheoretical analyses relating the learnability of various algorithms to data\navailability and modeling decisions, such as positional encodings. We make the\nALTA framework -- language specification, symbolic interpreter, and weight\ncompiler -- available to the community to enable further applications and\ninsights.\n","authors":["Peter Shaw","James Cohan","Jacob Eisenstein","Kenton Lee","Jonathan Berant","Kristina Toutanova"],"pdf_url":"https://arxiv.org/pdf/2410.18077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18071v1","updated":"2024-10-23T17:54:43Z","published":"2024-10-23T17:54:43Z","title":"TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts","summary":"  Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.\n","authors":["Yuxuan Xie","Tianhua Li","Wenqi Shao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15240v2","updated":"2024-10-23T17:47:58Z","published":"2024-09-23T17:38:41Z","title":"MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue\n  Generation","summary":"  Long-term memory is important for chatbots and dialogue systems (DS) to\ncreate consistent and human-like conversations, evidenced by numerous developed\nmemory-augmented DS (MADS). To evaluate the effectiveness of such MADS,\nexisting commonly used evaluation metrics, like retrieval accuracy and\nperplexity (PPL), mainly focus on query-oriented factualness and language\nquality assessment. However, these metrics often lack practical value.\nMoreover, the evaluation dimensions are insufficient for human-like assessment\nin DS. Regarding memory-recalling paradigms, current evaluation schemes only\nconsider passive memory retrieval while ignoring diverse memory recall with\nrich triggering factors, e.g., emotions and surroundings, which can be\nessential in emotional support scenarios. To bridge the gap, we construct a\nnovel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering various\nmemory-recalling paradigms based on cognitive science and psychology theories.\nThe benchmark assesses two tasks separately: memory retrieval and memory\nrecognition with the incorporation of both passive and proactive memory recall\ndata. We introduce new scoring criteria to the evaluation, including memory\ninjection, emotion support (ES) proficiency, and intimacy, to comprehensively\nassess generated responses. Results from cutting-edge embedding models and\nlarge language models on this benchmark indicate the potential for further\nadvancement. Extensive testing further reveals correlations between memory\ninjection, ES proficiency, and intimacy.\n","authors":["Junqing He","Liang Zhu","Rui Wang","Xi Wang","Reza Haffari","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.15240v2.pdf","comment":"Submitted to NAACL 2025"},{"id":"http://arxiv.org/abs/2404.19442v2","updated":"2024-10-23T17:46:13Z","published":"2024-04-30T10:45:40Z","title":"Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs","summary":"  Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples.\n","authors":["David Ifeoluwa Adelani","A. Seza Doğruöz","Iyanuoluwa Shode","Anuoluwapo Aremu"],"pdf_url":"https://arxiv.org/pdf/2404.19442v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2407.15762v2","updated":"2024-10-23T17:42:39Z","published":"2024-07-22T16:13:38Z","title":"Conditional Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning","summary":"  Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning.\n","authors":["Kaiwen Wang","Rahul Kidambi","Ryan Sullivan","Alekh Agarwal","Christoph Dann","Andrea Michi","Marco Gelmi","Yunxuan Li","Raghav Gupta","Avinava Dubey","Alexandre Ramé","Johan Ferret","Geoffrey Cideron","Le Hou","Hongkun Yu","Amr Ahmed","Aranyak Mehta","Léonard Hussenot","Olivier Bachem","Edouard Leurent"],"pdf_url":"https://arxiv.org/pdf/2407.15762v2.pdf","comment":"40 pages. Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.18057v1","updated":"2024-10-23T17:30:50Z","published":"2024-10-23T17:30:50Z","title":"CLEAR: Character Unlearning in Textual and Visual Modalities","summary":"  Machine Unlearning (MU) is critical for enhancing privacy and security in\ndeep learning models, particularly in large multimodal language models (MLLMs),\nby removing specific private or hazardous information. While MU has made\nsignificant progress in textual and visual modalities, multimodal unlearning\n(MMU) remains significantly underexplored, partially due to the absence of a\nsuitable open-source benchmark. To address this, we introduce CLEAR, a new\nbenchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious\nindividuals and 3,700 images linked with corresponding question-answer pairs,\nenabling a thorough evaluation across modalities. We assess 10 MU methods,\nadapting them for MMU, and highlight new challenges specific to multimodal\nforgetting. We also demonstrate that simple $\\ell_1$ regularization on LoRA\nweights significantly mitigates catastrophic forgetting, preserving model\nperformance on retained data. The dataset is available at\nhttps://huggingface.co/datasets/therem/CLEAR\n","authors":["Alexey Dontsov","Dmitrii Korzh","Alexey Zhavoronkin","Boris Mikheev","Denis Bobkov","Aibek Alanov","Oleg Y. Rogov","Ivan Oseledets","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2410.18057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18050v1","updated":"2024-10-23T17:24:58Z","published":"2024-10-23T17:24:58Z","title":"LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering","summary":"  Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.\n","authors":["Qingfei Zhao","Ruobing Wang","Yukuo Cen","Daren Zha","Shicheng Tan","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2410.18050v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.18040v1","updated":"2024-10-23T17:07:32Z","published":"2024-10-23T17:07:32Z","title":"Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases","summary":"  Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts.\n","authors":["Anna Glazkova","Dmitry Morozov","Timur Garipov"],"pdf_url":"https://arxiv.org/pdf/2410.18040v1.pdf","comment":"The 12th International Conference on Analysis of Images, Social\n  Networks and Texts (AIST'2024)"},{"id":"http://arxiv.org/abs/2410.18035v1","updated":"2024-10-23T17:04:40Z","published":"2024-10-23T17:04:40Z","title":"MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language\n  Models Fine-tuning","summary":"  Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are\nhighly effective parameter-efficient fine-tuning (PEFT) methods. However, they\nintroduce significant latency in multi-tenant settings due to the LoRA modules\nand MOE routers added to multiple linear modules in the Transformer layer. To\naddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel\nand efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods\nby considering each LoRA module as an expert and employing a prompt-aware\nrouting mechanism. This mechanism calculates expert routing results once before\ngenerating the first new token and reuses these results for subsequent tokens,\nreducing latency. Extensive experiments and analysis on commonsense reasoning\ntasks, math reasoning tasks, and widely used LLM evaluation benchmarks\ndemonstrate that MiLoRA consistently outperforms strong PEFT baselines with\ncomparable tunable parameter budgets. Additionally, MiLoRA significantly\nreduces latency in multi-tenant settings compared to previous LoRA-based\nmethods.\n","authors":["Jingfan Zhang","Yi Zhao","Dan Chen","Xing Tian","Huanran Zheng","Wei Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.18035v1.pdf","comment":"Accepted by EMNLP 2024 Findings. arXiv admin note: substantial text\n  overlap with arXiv:2405.18203"},{"id":"http://arxiv.org/abs/2410.18032v1","updated":"2024-10-23T17:02:59Z","published":"2024-10-23T17:02:59Z","title":"GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration","summary":"  Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.\n","authors":["Xin Li","Qizhi Chu","Yubin Chen","Yang Liu","Yaoqi Liu","Zekai Yu","Weize Chen","Chen Qian","Chuan Shi","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18027v1","updated":"2024-10-23T17:00:13Z","published":"2024-10-23T17:00:13Z","title":"Cross-lingual Transfer of Reward Models in Multilingual Alignment","summary":"  Reinforcement learning with human feedback (RLHF) is shown to largely benefit\nfrom precise reward models (RMs). However, recent studies in reward modeling\nschemes are skewed towards English, limiting the applicability of RLHF in\nmultilingual alignments. In this work, we investigate the cross-lingual\ntransfer of RMs trained in diverse languages, primarily from English. Our\nexperimental results demonstrate the strong cross-lingual transfer of English\nRMs, exceeding target language RMs by 3~4% average increase in Multilingual\nRewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through\nthe representation shifts. Finally, we perform multilingual alignment to\nexemplify how cross-lingual transfer in RM propagates to enhanced multilingual\ninstruction-following capability, along with extensive analyses on\noff-the-shelf RMs. We release the code, model, and data.\n","authors":["Jiwoo Hong","Noah Lee","Rodrigo Martínez-Castaño","César Rodríguez","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2410.18027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11757v4","updated":"2024-10-23T16:41:45Z","published":"2024-06-17T17:16:45Z","title":"STAR: SocioTechnical Approach to Red Teaming Language Models","summary":"  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n","authors":["Laura Weidinger","John Mellor","Bernat Guillen Pegueroles","Nahema Marchal","Ravin Kumar","Kristian Lum","Canfer Akbulut","Mark Diaz","Stevie Bergman","Mikel Rodriguez","Verena Rieser","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2406.11757v4.pdf","comment":"8 pages, 5 figures, 5 pages appendix. * denotes equal contribution"},{"id":"http://arxiv.org/abs/2409.17270v2","updated":"2024-10-23T16:27:20Z","published":"2024-09-25T18:35:45Z","title":"Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.\n","authors":["Debargha Ganguly","Srinivasan Iyengar","Vipin Chaudhary","Shivkumar Kalyanaraman"],"pdf_url":"https://arxiv.org/pdf/2409.17270v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) System 2 Reasoning At Scale Workshop"},{"id":"http://arxiv.org/abs/2407.10992v2","updated":"2024-10-23T16:19:06Z","published":"2024-06-24T09:29:14Z","title":"AlleNoise: large-scale text classification benchmark dataset with\n  real-world label noise","summary":"  Label noise remains a challenge for training robust classification models.\nMost methods for mitigating label noise have been benchmarked using primarily\ndatasets with synthetic noise. While the need for datasets with realistic noise\ndistribution has partially been addressed by web-scraped benchmarks such as\nWebVision and Clothing1M, those benchmarks are restricted to the computer\nvision domain. With the growing importance of Transformer-based models, it is\ncrucial to establish text classification benchmarks for learning with noisy\nlabels. In this paper, we present AlleNoise, a new curated text classification\nbenchmark dataset with real-world instance-dependent label noise, containing\nover 500,000 examples across approximately 5,600 classes, complemented with a\nmeaningful, hierarchical taxonomy of categories. The noise distribution comes\nfrom actual users of a major e-commerce marketplace, so it realistically\nreflects the semantics of human mistakes. In addition to the noisy labels, we\nprovide human-verified clean labels, which help to get a deeper insight into\nthe noise distribution, unlike web-scraped datasets typically used in the\nfield. We demonstrate that a representative selection of established methods\nfor learning with noisy labels is inadequate to handle such real-world noise.\nIn addition, we show evidence that these algorithms do not alleviate excessive\nmemorization. As such, with AlleNoise, we set the bar high for the development\nof label noise methods that can handle real-world label noise in text\nclassification tasks. The code and dataset are available for download at\nhttps://github.com/allegro/AlleNoise.\n","authors":["Alicja Rączkowska","Aleksandra Osowska-Kurczab","Jacek Szczerbiński","Kalina Jasinska-Kobus","Klaudia Nazarko"],"pdf_url":"https://arxiv.org/pdf/2407.10992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15720v4","updated":"2024-10-23T16:12:39Z","published":"2024-04-24T08:13:02Z","title":"Annotator-Centric Active Learning for Subjective NLP Tasks","summary":"  Active Learning (AL) addresses the high costs of collecting human annotations\nby strategically annotating the most informative samples. However, for\nsubjective NLP tasks, incorporating a wide range of perspectives in the\nannotation process is crucial to capture the variability in human judgments. We\nintroduce Annotator-Centric Active Learning (ACAL), which incorporates an\nannotator selection strategy following data sampling. Our objective is\ntwo-fold: 1) to efficiently approximate the full diversity of human judgments,\nand 2) to assess model performance using annotator-centric metrics, which value\nminority and majority perspectives equally. We experiment with multiple\nannotator selection strategies across seven subjective NLP tasks, employing\nboth traditional and novel, human-centered evaluation metrics. Our findings\nindicate that ACAL improves data efficiency and excels in annotator-centric\nperformance evaluations. However, its success depends on the availability of a\nsufficiently large and diverse pool of annotators to sample from.\n","authors":["Michiel van der Meer","Neele Falk","Pradeep K. Murukannaiah","Enrico Liscio"],"pdf_url":"https://arxiv.org/pdf/2404.15720v4.pdf","comment":"Accepted at EMNLP2024"},{"id":"http://arxiv.org/abs/2410.14979v2","updated":"2024-10-23T15:43:28Z","published":"2024-10-19T05:01:56Z","title":"Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration","summary":"  Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.\n","authors":["Wei Xie","Shuoyoucheng Ma","Zhenhua Wang","Enze Wang","Baosheng Wang","Jinshu Su"],"pdf_url":"https://arxiv.org/pdf/2410.14979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17973v1","updated":"2024-10-23T15:37:08Z","published":"2024-10-23T15:37:08Z","title":"Together We Can: Multilingual Automatic Post-Editing for Low-Resource\n  Languages","summary":"  This exploratory study investigates the potential of multilingual Automatic\nPost-Editing (APE) systems to enhance the quality of machine translations for\nlow-resource Indo-Aryan languages. Focusing on two closely related language\npairs, English-Marathi and English-Hindi, we exploit the linguistic\nsimilarities to develop a robust multilingual APE model. To facilitate\ncross-linguistic transfer, we generate synthetic Hindi-Marathi and\nMarathi-Hindi APE triplets. Additionally, we incorporate a Quality Estimation\n(QE)-APE multi-task learning framework. While the experimental results\nunderline the complementary nature of APE and QE, we also observe that QE-APE\nmultitask learning facilitates effective domain adaptation. Our experiments\ndemonstrate that the multilingual APE models outperform their corresponding\nEnglish-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TER\npoints, respectively, with further notable improvements over the multilingual\nAPE model observed through multi-task learning ($+1.29$ and $+1.44$ TER\npoints), data augmentation ($+0.53$ and $+0.45$ TER points) and domain\nadaptation ($+0.35$ and $+0.45$ TER points). We release the synthetic data,\ncode, and models accrued during this study publicly at\nhttps://github.com/cfiltnlp/Multilingual-APE.\n","authors":["Sourabh Deoghare","Diptesh Kanojia","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2410.17973v1.pdf","comment":"Accepted at Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17972v1","updated":"2024-10-23T15:37:02Z","published":"2024-10-23T15:37:02Z","title":"Dependency Graph Parsing as Sequence Labeling","summary":"  Various linearizations have been proposed to cast syntactic dependency\nparsing as sequence labeling. However, these approaches do not support more\ncomplex graph-based representations, such as semantic dependencies or enhanced\nuniversal dependencies, as they cannot handle reentrancy or cycles. By\nextending them, we define a range of unbounded and bounded linearizations that\ncan be used to cast graph parsing as a tagging task, enlarging the toolbox of\nproblems that can be solved under this paradigm. Experimental results on\nsemantic dependency and enhanced UD parsing show that with a good choice of\nencoding, sequence-labeling dependency graph parsers combine high efficiency\nwith accuracies close to the state of the art, in spite of their simplicity.\n","authors":["Ana Ezquerro","David Vilares","Carlos Gómez-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2410.17972v1.pdf","comment":"Accepted at EMNLP-2024"},{"id":"http://arxiv.org/abs/2410.17963v1","updated":"2024-10-23T15:30:37Z","published":"2024-10-23T15:30:37Z","title":"A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024","summary":"  The eRisk laboratory aims to address issues related to early risk detection\non the Web. In this year's edition, three tasks were proposed, where Task 2 was\nabout early detection of signs of anorexia. Early risk detection is a problem\nwhere precision and speed are two crucial objectives. Our research group solved\nTask 2 by defining a CPI+DMC approach, addressing both objectives\nindependently, and a time-aware approach, where precision and speed are\nconsidered a combined single-objective. We implemented the last approach by\nexplicitly integrating time during the learning process, considering the\nERDE{\\theta} metric as the training objective. It also allowed us to\nincorporate temporal metrics to validate and select the optimal models. We\nachieved outstanding results for the ERDE50 metric and ranking-based metrics,\ndemonstrating consistency in solving ERD problems.\n","authors":["Horacio Thompson","Marcelo Errecalde"],"pdf_url":"https://arxiv.org/pdf/2410.17963v1.pdf","comment":"In Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble,\n  France"},{"id":"http://arxiv.org/abs/2410.17960v1","updated":"2024-10-23T15:28:53Z","published":"2024-10-23T15:28:53Z","title":"Zeitenwenden: Detecting changes in the German political discourse","summary":"  From a monarchy to a democracy, to a dictatorship and back to a democracy --\nthe German political landscape has been constantly changing ever since the\nfirst German national state was formed in 1871. After World War II, the Federal\nRepublic of Germany was formed in 1949. Since then every plenary session of the\nGerman Bundestag was logged and even has been digitized over the course of the\nlast few years. We analyze these texts using a time series variant of the topic\nmodel LDA to investigate which events had a lasting effect on the political\ndiscourse and how the political topics changed over time. This allows us to\ndetect changes in word frequency (and thus key discussion points) in political\ndiscourse.\n","authors":["Kai-Robin Lange","Jonas Rieger","Niklas Benner","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2410.17960v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2201.12091v5","updated":"2024-10-23T15:28:38Z","published":"2022-01-28T13:00:17Z","title":"Linear Adversarial Concept Erasure","summary":"  Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem. We formulate the problem of identifying and erasing a linear subspace\nthat corresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear maximin\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, \\method, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod is highly expressive, effectively mitigating bias in deep nonlinear\nclassifiers while maintaining tractability and interpretability.\n","authors":["Shauli Ravfogel","Michael Twiton","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12091v5.pdf","comment":"Accepted in ICML 2022; a revised version"},{"id":"http://arxiv.org/abs/2410.17954v1","updated":"2024-10-23T15:24:54Z","published":"2024-10-23T15:24:54Z","title":"ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference","summary":"  Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.\n","authors":["Xin He","Shunkang Zhang","Yuxin Wang","Haiyan Yin","Zihao Zeng","Shaohuai Shi","Zhenheng Tang","Xiaowen Chu","Ivor Tsang","Ong Yew Soon"],"pdf_url":"https://arxiv.org/pdf/2410.17954v1.pdf","comment":"Mixture-of-Experts, Inference, Offloading"},{"id":"http://arxiv.org/abs/2410.17952v1","updated":"2024-10-23T15:24:16Z","published":"2024-10-23T15:24:16Z","title":"SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains","summary":"  Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.\n","authors":["Ran Xu","Hui Liu","Sreyashi Nag","Zhenwei Dai","Yaochen Xie","Xianfeng Tang","Chen Luo","Yang Li","Joyce C. Ho","Carl Yang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2410.17952v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2406.12295v2","updated":"2024-10-23T15:23:00Z","published":"2024-06-18T05:59:28Z","title":"Fast and Slow Generating: An Empirical Study on Large and Small Language\n  Models Collaborative Decoding","summary":"  Large Language Models (LLMs) exhibit impressive capabilities across various\napplications but encounter substantial challenges such as high inference\nlatency, considerable training costs, and the generation of hallucinations.\nCollaborative decoding between large and small language models (SLMs) presents\na promising strategy to mitigate these issues through methods including\nspeculative decoding, contrastive decoding, and emulator or proxy fine-tuning.\nHowever, the specifics of such collaborations, particularly from a unified\nperspective, remain largely unexplored. Inspired by dual-process cognitive\ntheory, we propose a unified framework in this paper, termed Fast and Slow\nGenerating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)\nare categorized as System 2 (slow and deliberate), while independent SLMs are\ndesignated as System 1 (fast and intuitive). We provide a comprehensive\nanalysis of these collaborative methodologies, elucidating their common\nproperties and shedding light on the differential knowledge capabilities of\nSystem 2 versus System 1 through the FS-GEN framework. Our findings indicate\nthat only a small proportion of collaborative interactions (approximately less\nthan 20\\% in most instances) are necessary across various methods. These\ninteractions between System 1 and System 2 conform to a scaling law related to\nthe parameter ratios, enabling predictable collaboration. Furthermore, we\nexplore the specific conditions under which collaboration proves most\neffective, particularly from an uncertainty perspective, offering novel\ninsights that may guide future optimization efforts. Our research underscores\nthat the fundamental distinction between System 1 and System 2 lies in the\nuncertainty of next token predictions, where interventions by System 2 are\ncrucial to support System 1. Code for Reproduction:\nhttps://github.com/TsinghuaC3I/FS-GEN\n","authors":["Kaiyan Zhang","Jianyu Wang","Ning Ding","Biqing Qi","Ermo Hua","Xingtai Lv","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.12295v2.pdf","comment":"update figures and results on Pythia Series"},{"id":"http://arxiv.org/abs/2410.12018v2","updated":"2024-10-23T15:21:54Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v2.pdf","comment":"ACCV 2024 Oral"},{"id":"http://arxiv.org/abs/2402.04957v3","updated":"2024-10-23T15:08:57Z","published":"2024-02-07T15:40:22Z","title":"Reconfidencing LLMs from the Grouping Loss Perspective","summary":"  Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses.\n","authors":["Lihu Chen","Alexandre Perez-Lebel","Fabian M. Suchanek","Gaël Varoquaux"],"pdf_url":"https://arxiv.org/pdf/2402.04957v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.01622v4","updated":"2024-10-23T15:02:57Z","published":"2024-02-02T18:39:51Z","title":"TravelPlanner: A Benchmark for Real-World Planning with Language Agents","summary":"  Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.\n","authors":["Jian Xie","Kai Zhang","Jiangjie Chen","Tinghui Zhu","Renze Lou","Yuandong Tian","Yanghua Xiao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2402.01622v4.pdf","comment":"ICML 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2311.05876v3","updated":"2024-10-23T14:48:20Z","published":"2023-11-10T05:24:04Z","title":"Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications","summary":"  Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.\n","authors":["Zhangyin Feng","Weitao Ma","Weijiang Yu","Lei Huang","Haotian Wang","Qianglong Chen","Weihua Peng","Xiaocheng Feng","Bing Qin","Ting liu"],"pdf_url":"https://arxiv.org/pdf/2311.05876v3.pdf","comment":"Work in progress; 22 pages. This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2408.01119v2","updated":"2024-10-23T14:37:50Z","published":"2024-08-02T09:00:03Z","title":"Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer","summary":"  Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.\n","authors":["Robert Belanec","Simon Ostermann","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2408.01119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00279v3","updated":"2024-10-23T14:33:44Z","published":"2023-07-01T09:18:24Z","title":"Let Me Teach You: Pedagogical Foundations of Feedback for Language\n  Models","summary":"  Natural Language Feedback (NLF) is an increasingly popular mechanism for\naligning Large Language Models (LLMs) to human preferences. Despite the\ndiversity of the information it can convey, NLF methods are often hand-designed\nand arbitrary, with little systematic grounding. At the same time, research in\nlearning sciences has long established several effective feedback models. In\nthis opinion piece, we compile ideas from pedagogy to introduce FELT, a\nfeedback framework for LLMs that outlines various characteristics of the\nfeedback space, and a feedback content taxonomy based on these variables,\nproviding a general mapping of the feedback space. In addition to streamlining\nNLF designs, FELT also brings out new, unexplored directions for research in\nNLF. We make our taxonomy available to the community, providing guides and\nexamples for mapping our categorizations to future research.\n","authors":["Beatriz Borges","Niket Tandon","Tanja Käser","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2307.00279v3.pdf","comment":"EMNLP 2024; 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.17901v1","updated":"2024-10-23T14:18:25Z","published":"2024-10-23T14:18:25Z","title":"ELAICHI: Enhancing Low-resource TTS by Addressing Infrequent and\n  Low-frequency Character Bigrams","summary":"  Recent advancements in Text-to-Speech (TTS) technology have led to\nnatural-sounding speech for English, primarily due to the availability of\nlarge-scale, high-quality web data. However, many other languages lack access\nto such resources, relying instead on limited studio-quality data. This\nscarcity results in synthesized speech that often suffers from intelligibility\nissues, particularly with low-frequency character bigrams. In this paper, we\npropose three solutions to address this challenge. First, we leverage\nhigh-quality data from linguistically or geographically related languages to\nimprove TTS for the target language. Second, we utilize low-quality Automatic\nSpeech Recognition (ASR) data recorded in non-studio environments, which is\nrefined using denoising and speech enhancement models. Third, we apply\nknowledge distillation from large-scale models using synthetic data to generate\nmore robust outputs. Our experiments with Hindi demonstrate significant\nreductions in intelligibility issues, as validated by human evaluators. We\npropose this methodology as a viable alternative for languages with limited\naccess to high-quality data, enabling them to collectively benefit from shared\nresources.\n","authors":["Srija Anand","Praveen Srinivasa Varadhan","Mehak Singal","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2410.17901v1.pdf","comment":"11 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2410.17897v1","updated":"2024-10-23T14:15:07Z","published":"2024-10-23T14:15:07Z","title":"Value Residual Learning For Alleviating Attention Concentration In\n  Transformers","summary":"  Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.\n","authors":["Zhanchao Zhou","Tianyi Wu","Zhiyun Jiang","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2410.17897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15592v2","updated":"2024-10-23T14:08:10Z","published":"2024-10-21T02:21:56Z","title":"CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein\n  Representation and Origin Evaluation","summary":"  Protein structures are important for understanding their functions and\ninteractions. Currently, many protein structure prediction methods are\nenriching the structure database. Discriminating the origin of structures is\ncrucial for distinguishing between experimentally resolved and computationally\npredicted structures, evaluating the reliability of prediction methods, and\nguiding downstream biological studies. Building on works in structure\nprediction, We developed a structure-sensitive supervised deep learning model,\nCrystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent\nand discriminate the origin of protein structures. CPE-Pro learns the\nstructural information of proteins and captures inter-structural differences to\nachieve accurate traceability on four data classes, and is expected to be\nextended to more. Simultaneously, we utilized Foldseek to encode protein\nstructures into \"structure-sequences\" and trained a protein Structural Sequence\nLanguage Model, SSLM. Preliminary experiments demonstrated that, compared to\nlarge-scale protein language models pre-trained on vast amounts of amino acid\nsequences, the \"structure-sequence\" enables the language model to learn more\ninformative protein features, enhancing and optimizing structural\nrepresentations. We have provided the code, model weights, and all related\nmaterials on https://github.com/GouWenrui/CPE-Pro-main.git.\n","authors":["Wenrui Gou","Wenhui Ge","Yang Tan","Mingchen Li","Guisheng Fan","Huiqun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.15592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17891v1","updated":"2024-10-23T14:04:22Z","published":"2024-10-23T14:04:22Z","title":"Scaling Diffusion Language Models via Adaptation from Autoregressive\n  Models","summary":"  Diffusion Language Models (DLMs) have emerged as a promising new paradigm for\ntext generative modeling, potentially addressing limitations of autoregressive\n(AR) models. However, current DLMs have been studied at a smaller scale\ncompared to their AR counterparts and lack fair comparison on language modeling\nbenchmarks. Additionally, training diffusion models from scratch at scale\nremains challenging. Given the prevalence of open-source AR language models, we\npropose adapting these models to build text diffusion models. We demonstrate\nconnections between AR and diffusion modeling objectives and introduce a simple\ncontinual pre-training approach for training diffusion models. Through\nsystematic evaluation on language modeling, reasoning, and commonsense\nbenchmarks, we show that we can convert AR models ranging from 127M to 7B\nparameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA,\nusing less than 200B tokens for training. Our experimental results reveal that\nthese models outperform earlier DLMs and are competitive with their AR\ncounterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters)\ncapable of generating fluent text, performing in-context learning, filling in\nthe middle without prompt re-ordering, and following instructions\n\\url{https://github.com/HKUNLP/DiffuLLaMA}.\n","authors":["Shansan Gong","Shivam Agarwal","Yizhe Zhang","Jiacheng Ye","Lin Zheng","Mukai Li","Chenxin An","Peilin Zhao","Wei Bi","Jiawei Han","Hao Peng","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17891v1.pdf","comment":"25 pages. Code: https://github.com/HKUNLP/DiffuLLaMA"},{"id":"http://arxiv.org/abs/2406.14703v2","updated":"2024-10-23T14:01:14Z","published":"2024-06-20T19:50:56Z","title":"Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics","summary":"  Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.\n","authors":["Seungbeen Lee","Seungwon Lim","Seungju Han","Giyeong Oh","Hyungjoo Chae","Jiwan Chung","Minju Kim","Beong-woo Kwak","Yeonsoo Lee","Dongha Lee","Jinyoung Yeo","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.14703v2.pdf","comment":"Preprint; Under review"},{"id":"http://arxiv.org/abs/2410.17886v1","updated":"2024-10-23T14:00:48Z","published":"2024-10-23T14:00:48Z","title":"SpeakGer: A meta-data enriched speech corpus of German state and federal\n  parliaments","summary":"  The application of natural language processing on political texts as well as\nspeeches has become increasingly relevant in political sciences due to the\nability to analyze large text corpora which cannot be read by a single person.\nBut such text corpora often lack critical meta information, detailing for\ninstance the party, age or constituency of the speaker, that can be used to\nprovide an analysis tailored to more fine-grained research questions. To enable\nresearchers to answer such questions with quantitative approaches such as\nnatural language processing, we provide the SpeakGer data set, consisting of\nGerman parliament debates from all 16 federal states of Germany as well as the\nGerman Bundestag from 1947-2023, split into a total of 10,806,105 speeches.\nThis data set includes rich meta data in form of information on both reactions\nfrom the audience towards the speech as well as information about the speaker's\nparty, their age, their constituency and their party's political alignment,\nwhich enables a deeper analysis. We further provide three exploratory analyses,\ndetailing topic shares of different parties throughout time, a descriptive\nanalysis of the development of the age of an average speaker as well as a\nsentiment analysis of speeches of different parties with regards to the\nCOVID-19 pandemic.\n","authors":["Kai-Robin Lange","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2410.17886v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.07799v2","updated":"2024-10-23T14:00:40Z","published":"2024-07-10T16:16:02Z","title":"Attribute or Abstain: Large Language Models as Long Document Assistants","summary":"  LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiments with different approaches to attribution on 5 LLMs\nof different sizes.\n  We find that citation, i.e. response generation and evidence extraction in\none step, performs best for large and fine-tuned models, while additional\nretrieval can help for small, prompted models. We investigate whether the \"Lost\nin the Middle'' phenomenon exists for attribution, but do not find this. We\nalso find that evidence quality can predict response quality on datasets with\nsimple responses, but not so for complex responses, as models struggle with\nproviding evidence for complex claims.\n","authors":["Jan Buchmann","Xiao Liu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.07799v2.pdf","comment":"Accepted at EMNLP 2024. Code and data:\n  https://github.com/UKPLab/arxiv2024-attribute-or-abstain"},{"id":"http://arxiv.org/abs/2407.12819v2","updated":"2024-10-23T14:00:36Z","published":"2024-07-01T10:33:46Z","title":"I've Got 99 Problems But FLOPS Ain't One","summary":"  Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.\n","authors":["Alexandru M. Gherghescu","Vlad-Andrei Bădoiu","Alexandru Agache","Mihai-Valentin Dumitru","Iuliu Vasilescu","Radu Mantu","Costin Raiciu"],"pdf_url":"https://arxiv.org/pdf/2407.12819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17875v1","updated":"2024-10-23T13:47:05Z","published":"2024-10-23T13:47:05Z","title":"Understanding Layer Significance in LLM Alignment","summary":"  Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss.\n","authors":["Guangyuan Shi","Zexin Lu","Xiaoyu Dong","Wenlong Zhang","Xuanyu Zhang","Yujie Feng","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2410.17875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15055v2","updated":"2024-10-23T13:20:15Z","published":"2024-02-23T02:15:47Z","title":"Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions","summary":"  Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.\n","authors":["Clement Neo","Shay B. Cohen","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2402.15055v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.14774v2","updated":"2024-10-23T13:01:14Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data. Code is available at:\nhttps://github.com/lionel-w2/FAP.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15956v2","updated":"2024-10-23T13:00:27Z","published":"2024-10-21T12:34:17Z","title":"Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs","summary":"  Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.\n","authors":["Yanzhu Guo","Simone Conia","Zelin Zhou","Min Li","Saloni Potdar","Henry Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.15956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16845v2","updated":"2024-10-23T12:53:00Z","published":"2024-06-24T17:49:28Z","title":"RaTEScore: A Metric for Radiology Report Generation","summary":"  This paper introduces a novel, entity-aware metric, termed as Radiological\nReport (Text) Evaluation (RaTEScore), to assess the quality of medical reports\ngenerated by AI models. RaTEScore emphasizes crucial medical entities such as\ndiagnostic outcomes and anatomical details, and is robust against complex\nmedical synonyms and sensitive to negation expressions. Technically, we\ndeveloped a comprehensive medical NER dataset, RaTE-NER, and trained an NER\nmodel specifically for this purpose. This model enables the decomposition of\ncomplex radiological reports into constituent medical entities. The metric\nitself is derived by comparing the similarity of entity embeddings, obtained\nfrom a language model, based on their types and relevance to clinical\nsignificance. Our evaluations demonstrate that RaTEScore aligns more closely\nwith human preference than existing metrics, validated both on established\npublic benchmarks and our newly proposed RaTE-Eval benchmark.\n","authors":["Weike Zhao","Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2406.16845v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06022v2","updated":"2024-10-23T12:49:32Z","published":"2024-10-08T13:23:58Z","title":"Can Language Models Induce Grammatical Knowledge from Indirect Evidence?","summary":"  What kinds of and how much data is necessary for language models to induce\ngrammatical knowledge to judge sentence acceptability? Recent language models\nstill have much room for improvement in their data efficiency compared to\nhumans. This paper investigates whether language models efficiently use\nindirect data (indirect evidence), from which they infer sentence\nacceptability. In contrast, humans use indirect evidence efficiently, which is\nconsidered one of the inductive biases contributing to efficient language\nacquisition. To explore this question, we introduce the Wug InDirect Evidence\nTest (WIDET), a dataset consisting of training instances inserted into the\npre-training data and evaluation instances. We inject synthetic instances with\nnewly coined wug words into pretraining data and explore the model's behavior\non evaluation data that assesses grammatical acceptability regarding those\nwords. We prepare the injected instances by varying their levels of\nindirectness and quantity. Our experiments surprisingly show that language\nmodels do not induce grammatical knowledge even after repeated exposure to\ninstances with the same structure but differing only in lexical items from\nevaluation instances in certain language phenomena. Our findings suggest a\npotential direction for future research: developing models that use latent\nindirect evidence to induce grammatical knowledge.\n","authors":["Miyu Oba","Yohei Oseki","Akiyo Fukatsu","Akari Haga","Hiroki Ouchi","Taro Watanabe","Saku Sugawara"],"pdf_url":"https://arxiv.org/pdf/2410.06022v2.pdf","comment":"This paper is accepted at EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.17820v1","updated":"2024-10-23T12:26:10Z","published":"2024-10-23T12:26:10Z","title":"Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination","summary":"  Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. While using even a smaller model as the discriminator, scaling\nthe generator leads to notable improvements in ToT performance, whereas scaling\nthe discriminator with a fixed generator yields only marginal gains. Our\nresults show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT.\n","authors":["Qiqi Chen","Xinpeng Wang","Philipp Mondorf","Michael A. Hedderich","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.17820v1.pdf","comment":"Code: github.com/mainlp/tot-eval"},{"id":"http://arxiv.org/abs/2410.17799v1","updated":"2024-10-23T11:58:58Z","published":"2024-10-23T11:58:58Z","title":"OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation","summary":"  Full-duplex spoken dialogue systems significantly advance over traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex communication\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text-based large language model (LLM) backbone into a speech-text\ndialogue LLM, capable of generating text and speech in real time, without\nmodifying the architecture of the backbone LLM. The training process comprises\nthree stages: modality alignment, half-duplex dialogue learning, and\nfull-duplex dialogue learning. Throughout all training stages, we standardize\nthe data using a flattening operation, which allows us to unify the training\nmethods and the model architecture across different modalities and tasks. Our\napproach offers a straightforward modeling technique and a promising research\ndirection for developing efficient and natural end-to-end full-duplex spoken\ndialogue systems. Audio samples of dialogues generated by OmniFlatten can be\nfound at this web site (https://omniflatten.github.io/).\n","authors":["Qinglin Zhang","Luyao Cheng","Chong Deng","Qian Chen","Wen Wang","Siqi Zheng","Jiaqing Liu","Hai Yu","Chaohong Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17799v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.05804v4","updated":"2024-10-23T11:36:57Z","published":"2024-06-09T14:42:55Z","title":"A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning","summary":"  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work.\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2406.05804v4.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.17783v1","updated":"2024-10-23T11:32:46Z","published":"2024-10-23T11:32:46Z","title":"Leveraging the Domain Adaptation of Retrieval Augmented Generation\n  Models for Question Answering and Reducing Hallucination","summary":"  While ongoing advancements in Large Language Models have demonstrated\nremarkable success across various NLP tasks, Retrieval Augmented Generation\nModel stands out to be highly effective on downstream applications like\nQuestion Answering. Recently, RAG-end2end model further optimized the\narchitecture and achieved notable performance improvements on domain\nadaptation. However, the effectiveness of these RAG-based architectures remains\nrelatively unexplored when fine-tuned on specialized domains such as customer\nservice for building a reliable conversational AI system. Furthermore, a\ncritical challenge persists in reducing the occurrence of hallucinations while\nmaintaining high domain-specific accuracy. In this paper, we investigated the\nperformance of diverse RAG and RAG-like architectures through domain adaptation\nand evaluated their ability to generate accurate and relevant response grounded\nin the contextual knowledge base. To facilitate the evaluation of the models,\nwe constructed a novel dataset HotelConvQA, sourced from wide range of\nhotel-related conversations and fine-tuned all the models on our domain\nspecific dataset. We also addressed a critical research gap on determining the\nimpact of domain adaptation on reducing hallucinations across different RAG\narchitectures, an aspect that was not properly measured in prior work. Our\nevaluation shows positive results in all metrics by employing domain\nadaptation, demonstrating strong performance on QA tasks and providing insights\ninto their efficacy in reducing hallucinations. Our findings clearly indicate\nthat domain adaptation not only enhances the models' performance on QA tasks\nbut also significantly reduces hallucination across all evaluated RAG\narchitectures.\n","authors":["Salman Rakin","Md. A. R. Shibly","Zahin M. Hossain","Zeeshan Khan","Md. Mostofa Akbar"],"pdf_url":"https://arxiv.org/pdf/2410.17783v1.pdf","comment":"Initial Version fine-tuned on HotelConvQA"},{"id":"http://arxiv.org/abs/2404.19232v7","updated":"2024-10-23T11:19:02Z","published":"2024-04-30T03:29:30Z","title":"GRAMMAR: Grounded and Modular Methodology for Assessment of\n  Closed-Domain Retrieval-Augmented Language Model","summary":"  Retrieval-Augmented Generation (RAG) systems are widely used across various\nindustries for querying closed-domain and in-house knowledge bases. However,\nevaluating these systems presents significant challenges due to the private\nnature of closed-domain data and a scarcity of queries with verifiable ground\ntruths. Moreover, there is a lack of analytical methods to diagnose problematic\nmodules and identify types of failure, such as those caused by knowledge\ndeficits or issues with robustness. To address these challenges, we introduce\nGRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation\nframework comprising a grounded data generation process and an evaluation\nprotocol that effectively pinpoints defective modules. Our validation\nexperiments reveal that GRAMMAR provides a reliable approach for identifying\nvulnerable modules and supports hypothesis testing for textual form\nvulnerabilities. An open-source tool accompanying this framework is available\nin our GitHub repository (see https://github.com/xinzhel/grammar), allowing for\neasy reproduction of our results and enabling reliable and modular evaluation\nin closed-domain settings.\n","authors":["Xinzhe Li","Ming Liu","Shang Gao"],"pdf_url":"https://arxiv.org/pdf/2404.19232v7.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.16144v2","updated":"2024-10-23T11:17:42Z","published":"2024-10-21T16:14:57Z","title":"1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs","summary":"  Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet.\n","authors":["Jinheng Wang","Hansong Zhou","Ting Song","Shaoguang Mao","Shuming Ma","Hongyu Wang","Yan Xia","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.16144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17759v1","updated":"2024-10-23T10:50:40Z","published":"2024-10-23T10:50:40Z","title":"Latent Structures of Intertextuality in French Fiction","summary":"  Intertextuality is a key concept in literary theory that challenges\ntraditional notions of text, signification or authorship. It views texts as\npart of a vast intertextual network that is constantly evolving and being\nreconfigured. This paper argues that the field of computational literary\nstudies is the ideal place to conduct a study of intertextuality since we have\nnow the ability to systematically compare texts with each others. Specifically,\nwe present a work on a corpus of more than 12.000 French fictions from the\n18th, 19th and early 20th century. We focus on evaluating the underlying roles\nof two literary notions, sub-genres and the literary canon in the framing of\ntextuality. The article attempts to operationalize intertextuality using\nstate-of-the-art contextual language models to encode novels and capture\nfeatures that go beyond simple lexical or thematic approaches. Previous\nresearch (Hughes, 2012) supports the existence of a literary \"style of a time\",\nand our findings further reinforce this concept. Our findings also suggest that\nboth subgenres and canonicity play a significant role in shaping textual\nsimilarities within French fiction. These discoveries point to the importance\nof considering genre and canon as dynamic forces that influence the evolution\nand intertextual connections of literary works within specific historical\ncontexts.\n","authors":["Jean Barré"],"pdf_url":"https://arxiv.org/pdf/2410.17759v1.pdf","comment":"13 pages, 6 figures. Computational Humanities Research Conference\n  2024"},{"id":"http://arxiv.org/abs/2410.17739v1","updated":"2024-10-23T10:12:35Z","published":"2024-10-23T10:12:35Z","title":"Local Contrastive Editing of Gender Stereotypes","summary":"  Stereotypical bias encoded in language models (LMs) poses a threat to safe\nlanguage technology, yet our understanding of how bias manifests in the\nparameters of LMs remains incomplete. We introduce local contrastive editing\nthat enables the localization and editing of a subset of weights in a target\nmodel in relation to a reference model. We deploy this approach to identify and\nmodify subsets of weights that are associated with gender stereotypes in LMs.\nThrough a series of experiments, we demonstrate that local contrastive editing\ncan precisely localize and control a small subset (< 0.5%) of weights that\nencode gender bias. Our work (i) advances our understanding of how\nstereotypical biases can manifest in the parameter space of LMs and (ii) opens\nup new avenues for developing parameter-efficient strategies for controlling\nmodel properties in a contrastive manner.\n","authors":["Marlene Lutz","Rochelle Choenni","Markus Strohmaier","Anne Lauscher"],"pdf_url":"https://arxiv.org/pdf/2410.17739v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17736v1","updated":"2024-10-23T10:11:40Z","published":"2024-10-23T10:11:40Z","title":"MojoBench: Language Modeling and Benchmarks for Mojo","summary":"  The recently introduced Mojo programming language (PL) by Modular, has\nreceived significant attention in the scientific community due to its claimed\nsignificant speed boost over Python. Despite advancements in code Large\nLanguage Models (LLMs) across various PLs, Mojo remains unexplored in this\ncontext. To address this gap, we introduce MojoBench, the first framework for\nMojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset\ndesigned for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM\npretrained and finetuned for Mojo code generation, which supports instructions\nin 5 natural languages (NLs). Our results show that Mojo-Coder achieves a\n30-35% performance improvement over leading models like GPT-4o and\nClaude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with\nunderrepresented and unseen PLs, offering potential strategies for enhancing\nmodel adaptability. MojoBench contributes to our understanding of LLM\ncapabilities and limitations in emerging programming paradigms fostering more\nrobust code generation systems.\n","authors":["Nishat Raihan","Joanna C. S. Santos","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2410.17736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14622v2","updated":"2024-10-23T10:06:10Z","published":"2024-02-22T15:10:45Z","title":"From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access","summary":"  This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates.\n","authors":["Mahsa Shamsabadi","Jennifer D'Souza"],"pdf_url":"https://arxiv.org/pdf/2402.14622v2.pdf","comment":"8 pages, 3 figures | Accepted for publication as a poster paper at\n  the International Semantic Web Conference (ISWC 2024)"},{"id":"http://arxiv.org/abs/2410.17728v1","updated":"2024-10-23T10:00:23Z","published":"2024-10-23T10:00:23Z","title":"Dialectal and Low Resource Machine Translation for Aromanian","summary":"  We present a neural machine translation system that can translate between\nRomanian, English, and Aromanian (an endangered Eastern Romance language); the\nfirst of its kind. BLEU scores range from 17 to 32 depending on the direction\nand genre of the text. Alongside, we release the biggest known\nAromanian-Romanian bilingual corpus, consisting of 79k cleaned sentence pairs.\nAdditional tools such as an agnostic sentence embedder (used for both text\nmining and automatic evaluation) and a diacritics converter are also presented.\nWe publicly release our findings and models. Finally, we describe the\ndeployment of our quantized model at https://arotranslate.com.\n","authors":["Alexandru-Iulius Jerpelea","Alina-Ştefania Rădoi","Sergiu Nisioi"],"pdf_url":"https://arxiv.org/pdf/2410.17728v1.pdf","comment":"16 pages, 3 figures, 6 tables, submitted to COLING 2025"},{"id":"http://arxiv.org/abs/2406.14282v3","updated":"2024-10-23T09:42:59Z","published":"2024-06-20T13:07:38Z","title":"Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs","summary":"  Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.\n","authors":["Junjie Wang","Mingyang Chen","Binbin Hu","Dan Yang","Ziqi Liu","Yue Shen","Peng Wei","Zhiqiang Zhang","Jinjie Gu","Jun Zhou","Jeff Z. Pan","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14282v3.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.17714v1","updated":"2024-10-23T09:40:15Z","published":"2024-10-23T09:40:15Z","title":"CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient\n  Semantic Steering in Large Language Models","summary":"  Despite their impressive capabilities, large language models (LLMs) often\nlack interpretability and can generate toxic content. While using LLMs as\nfoundation models and applying semantic steering methods are widely practiced,\nwe believe that efficient methods should be based on a thorough understanding\nof LLM behavior. To this end, we propose using eye movement measures to\ninterpret LLM behavior across layers. We find that LLMs exhibit patterns\nsimilar to human gaze across layers and different layers function differently.\nInspired by these findings, we introduce a heuristic steering layer selection\nand apply it to layer intervention methods via fine-tuning and inference. Using\nlanguage toxification and detoxification as test beds, we demonstrate that our\nproposed CogSteer methods achieve better results in terms of toxicity scores\nwhile efficiently saving 97% of the computational resources and 60% of the\ntraining time. Our model-agnostic approach can be adopted into various LLMs,\ncontributing to their interpretability and promoting trustworthiness for safe\ndeployment.\n","authors":["Xintong Wang","Jingheng Pan","Longqin Jiang","Liang Ding","Xingshan Li","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.17714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17711v1","updated":"2024-10-23T09:36:21Z","published":"2024-10-23T09:36:21Z","title":"Beware of Calibration Data for Pruning Large Language Models","summary":"  As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL).\n","authors":["Yixin Ji","Yang Xiang","Juntao Li","Qingrong Xia","Ping Li","Xinyu Duan","Zhefeng Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.17711v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.17694v1","updated":"2024-10-23T09:14:57Z","published":"2024-10-23T09:14:57Z","title":"An Adaptive Framework for Generating Systematic Explanatory Answer in\n  Online Q&A Platforms","summary":"  Question Answering (QA) systems face challenges in handling complex questions\nthat require multi-domain knowledge synthesis. The naive RAG models, although\neffective in information retrieval, struggle with complex questions that\nrequire comprehensive and in-depth answers. The pioneering task is defined as\nexplanatory answer generation, which entails handling identified challenges\nsuch as the requirement for comprehensive information and logical coherence\nwithin the generated context. To address these issues, we refer to systematic\nthinking theory and propose SynthRAG, an innovative framework designed to\nenhance QA performance. SynthRAG improves on conventional models by employing\nadaptive outlines for dynamic content structuring, generating systematic\ninformation to ensure detailed coverage, and producing customized answers\ntailored to specific user inquiries. This structured approach guarantees\nlogical coherence and thorough integration of information, yielding responses\nthat are both insightful and methodically organized. Empirical evaluations\nunderscore SynthRAG's effectiveness, demonstrating its superiority in handling\ncomplex questions, overcoming the limitations of naive RAG models, and\nsignificantly improving answer quality and depth. Furthermore, an online\ndeployment on the Zhihu platform revealed that SynthRAG's answers achieved\nnotable user engagement, with each response averaging 5.73 upvotes and\nsurpassing the performance of 79.8% of human contributors, highlighting the\npractical relevance and impact of the proposed framework. Our code is available\nat https://github.com/czy1999/SynthRAG .\n","authors":["Ziyang Chen","Xiaobin Wang","Yong Jiang","Jinzhi Liao","Pengjun Xie","Fei Huang","Xiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17694v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17676v1","updated":"2024-10-23T08:49:51Z","published":"2024-10-23T08:49:51Z","title":"Towards a Similarity-adjusted Surprisal Theory","summary":"  Surprisal theory posits that the cognitive effort required to comprehend a\nword is determined by its contextual predictability, quantified as surprisal.\nTraditionally, surprisal theory treats words as distinct entities, overlooking\nany potential similarity between them. Giulianelli et al. (2023) address this\nlimitation by introducing information value, a measure of predictability\ndesigned to account for similarities between communicative units. Our work\nleverages Ricotta and Szeidl's (2006) diversity index to extend surprisal into\na metric that we term similarity-adjusted surprisal, exposing a mathematical\nrelationship between surprisal and information value. Similarity-adjusted\nsurprisal aligns with information value when considering graded similarities\nand reduces to standard surprisal when words are treated as distinct.\nExperimental results with reading time data indicate that similarity-adjusted\nsurprisal adds predictive power beyond standard surprisal for certain datasets,\nsuggesting it serves as a complementary measure of comprehension effort.\n","authors":["Clara Meister","Mario Giulianelli","Tiago Pimentel"],"pdf_url":"https://arxiv.org/pdf/2410.17676v1.pdf","comment":"EMNLP 2024 main conference proceedings"},{"id":"http://arxiv.org/abs/2410.17670v1","updated":"2024-10-23T08:42:36Z","published":"2024-10-23T08:42:36Z","title":"Quantifying the Risks of Tool-assisted Rephrasing to Linguistic\n  Diversity","summary":"  Writing assistants and large language models see widespread use in the\ncreation of text content. While their effectiveness for individual users has\nbeen evaluated in the literature, little is known about their proclivity to\nchange language or reduce its richness when adopted by a large user base. In\nthis paper, we take a first step towards quantifying this risk by measuring the\nsemantic and vocabulary change enacted by the use of rephrasing tools on a\nmulti-domain corpus of human-generated text.\n","authors":["Mengying Wang","Andreas Spitz"],"pdf_url":"https://arxiv.org/pdf/2410.17670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15993v4","updated":"2024-10-23T08:33:54Z","published":"2024-04-24T17:10:35Z","title":"Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach","summary":"  In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box.\n","authors":["Linyu Liu","Yu Pan","Xiaocheng Li","Guanting Chen"],"pdf_url":"https://arxiv.org/pdf/2404.15993v4.pdf","comment":"29 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.10216v2","updated":"2024-10-23T08:22:44Z","published":"2024-06-14T17:49:59Z","title":"Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs","summary":"  Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm.\n","authors":["Rui Yang","Ruomeng Ding","Yong Lin","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10216v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17657v1","updated":"2024-10-23T08:19:18Z","published":"2024-10-23T08:19:18Z","title":"ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents","summary":"  Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks.\n","authors":["Yusheng Liao","Shuyang Jiang","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17657v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2410.17635v1","updated":"2024-10-23T07:53:29Z","published":"2024-10-23T07:53:29Z","title":"Markov Chain of Thought for Efficient Mathematical Reasoning","summary":"  Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.\n","authors":["Wen Yang","Kai Fan","Minpeng Liao"],"pdf_url":"https://arxiv.org/pdf/2410.17635v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.17632v1","updated":"2024-10-23T07:48:51Z","published":"2024-10-23T07:48:51Z","title":"LMLPA: Language Model Linguistic Personality Assessment","summary":"  Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing.\n","authors":["Jingyao Zheng","Xian Wang","Simo Hosio","Xiaoxian Xu","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03622v3","updated":"2024-10-23T07:20:26Z","published":"2024-04-04T17:45:08Z","title":"Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning\n  in Large Language Models","summary":"  Large language models (LLMs) have exhibited impressive performance in\nlanguage comprehension and various reasoning tasks. However, their abilities in\nspatial reasoning, a crucial aspect of human cognition, remain relatively\nunexplored. Human possess a remarkable ability to create mental images of\nunseen objects and actions through a process known as the Mind's Eye, enabling\nthe imagination of the unseen world. Inspired by this cognitive capacity, we\npropose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial\nreasoning of LLMs by visualizing their reasoning traces, thereby guiding\nsubsequent reasoning steps. We employed VoT for multi-hop spatial reasoning\ntasks, including natural language navigation, visual navigation, and visual\ntiling in 2D grid worlds. Experimental results demonstrated that VoT\nsignificantly enhances the spatial reasoning abilities of LLMs. Notably, VoT\noutperformed existing multimodal large language models (MLLMs) in these tasks.\nWhile VoT works surprisingly well on LLMs, the ability to generate mental\nimages to facilitate spatial reasoning resembles the mind's eye process,\nsuggesting its potential viability in MLLMs. Please find the dataset and codes\nat https://microsoft.github.io/visualization-of-thought\n","authors":["Wenshan Wu","Shaoguang Mao","Yadong Zhang","Yan Xia","Li Dong","Lei Cui","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2404.03622v3.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2305.12987v3","updated":"2024-10-23T07:13:49Z","published":"2023-05-22T12:47:48Z","title":"GPT-SW3: An Autoregressive Language Model for the Nordic Languages","summary":"  This paper details the process of developing the first native large\ngenerative language model for the Nordic languages, GPT-SW3. We cover all parts\nof the development process, from data collection and processing, training\nconfiguration and instruction finetuning, to evaluation and considerations for\nrelease strategies. We hope that this paper can serve as a guide and reference\nfor other researchers that undertake the development of large generative models\nfor smaller languages.\n","authors":["Ariel Ekgren","Amaru Cuba Gyllensten","Felix Stollenwerk","Joey Öhman","Tim Isbister","Evangelia Gogoulou","Fredrik Carlsson","Alice Heiman","Judit Casademont","Magnus Sahlgren"],"pdf_url":"https://arxiv.org/pdf/2305.12987v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17195v2","updated":"2024-10-23T07:02:09Z","published":"2024-10-22T17:13:38Z","title":"Non-myopic Generation of Language Model for Reasoning and Planning","summary":"  Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.\n","authors":["Chang Ma","Haiteng Zhao","Junlei Zhang","Junxian He","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.17195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17600v1","updated":"2024-10-23T06:54:03Z","published":"2024-10-23T06:54:03Z","title":"Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective","summary":"  Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion.\n","authors":["Rui Yang","Boming Yang","Aosong Feng","Sixun Ouyang","Moritz Blum","Tianwei She","Yuang Jiang","Freddy Lecue","Jinghui Lu","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2410.17600v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.10794"},{"id":"http://arxiv.org/abs/2410.17599v1","updated":"2024-10-23T06:52:09Z","published":"2024-10-23T06:52:09Z","title":"Cross-model Control: Improving Multiple Large Language Models in\n  One-time Training","summary":"  The number of large language models (LLMs) with varying parameter scales and\nvocabularies is increasing. While they deliver powerful performance, they also\nface a set of common optimization needs to meet specific requirements or\nstandards, such as instruction following or avoiding the output of sensitive\ninformation from the real world. However, how to reuse the fine-tuning outcomes\nof one model to other models to reduce training costs remains a challenge. To\nbridge this gap, we introduce Cross-model Control (CMC), a method that improves\nmultiple LLMs in one-time training with a portable tiny language model.\nSpecifically, we have observed that the logit shift before and after\nfine-tuning is remarkably similar across different models. Based on this\ninsight, we incorporate a tiny language model with a minimal number of\nparameters. By training alongside a frozen template LLM, the tiny model gains\nthe capability to alter the logits output by the LLMs. To make this tiny\nlanguage model applicable to models with different vocabularies, we propose a\nnovel token mapping strategy named PM-MinED. We have conducted extensive\nexperiments on instruction tuning and unlearning tasks, demonstrating the\neffectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC.\n","authors":["Jiayi Wu","Hao Sun","Hengyi Cai","Lixin Su","Shuaiqiang Wang","Dawei Yin","Xiang Li","Ming Gao"],"pdf_url":"https://arxiv.org/pdf/2410.17599v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.12617v2","updated":"2024-10-23T06:28:19Z","published":"2024-02-20T00:51:05Z","title":"Generative AI Security: Challenges and Countermeasures","summary":"  Generative AI's expanding footprint across numerous industries has led to\nboth excitement and increased scrutiny. This paper delves into the unique\nsecurity challenges posed by Generative AI, and outlines potential research\ndirections for managing these risks.\n","authors":["Banghua Zhu","Norman Mu","Jiantao Jiao","David Wagner"],"pdf_url":"https://arxiv.org/pdf/2402.12617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15573v2","updated":"2024-10-23T06:21:09Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v2.pdf","comment":"Resources: https://github.com/mzhaojp22/openmu"},{"id":"http://arxiv.org/abs/2410.17578v1","updated":"2024-10-23T06:04:55Z","published":"2024-10-23T06:04:55Z","title":"MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and\n  Reward Models","summary":"  Large language models (LLMs) are commonly used as evaluators in tasks (e.g.,\nreward modeling, LLM-as-a-judge), where they act as proxies for human\npreferences or judgments. This leads to the need for meta-evaluation:\nevaluating the credibility of LLMs as evaluators. However, existing benchmarks\nprimarily focus on English, offering limited insight into LLMs' effectiveness\nas evaluators in non-English contexts. To address this, we introduce MM-Eval, a\nmultilingual meta-evaluation benchmark that covers 18 languages across six\ncategories. MM-Eval evaluates various dimensions, including language-specific\nchallenges like linguistics and language hallucinations. Evaluation results\nshow that both proprietary and open-source language models have considerable\nroom for improvement. Further analysis reveals a tendency for these models to\nassign middle-ground scores to low-resource languages. We publicly release our\nbenchmark and code.\n","authors":["Guijin Son","Dongkeun Yoon","Juyoung Suk","Javier Aula-Blasco","Mano Aslan","Vu Trong Kim","Shayekh Bin Islam","Jaume Prats-Cristià","Lucía Tormo-Bañuelos","Seungone Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17578v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.17566v1","updated":"2024-10-23T05:19:51Z","published":"2024-10-23T05:19:51Z","title":"Differentially Private Learning Needs Better Model Initialization and\n  Self-Distillation","summary":"  Differentially private SGD (DPSGD) enables privacy-preserving training of\nlanguage models, but often reduces utility, diversity, and linguistic quality.\nWe introduce DPRefine, a three-phase method that initializes a model using data\nsynthesis from a small pre-trained LM with rigorous filtering, applies DP\nfinetuning on private data, and performs self-distillation to refine outputs.\nThis approach significantly outperforms vanilla DPSGD, with AlpacaEval\npreferring DPRefine's generations in 78.4% of cases across all datasets. Our\nanalysis reveals that DPRefine reduces linguistic errors in generated text by\n84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.\nIt also reduces inconsistencies of non-private models, such as hallucinated\ndetails and misattributed quotes. We find that small models like GPT-2 can be\neffective for initialization and distillation, highlighting their potential in\nenabling scalable and efficient deployment of privacy-preserving language.\n","authors":["Ivoline C. Ngong","Joseph P. Near","Niloofar Mireshghallah"],"pdf_url":"https://arxiv.org/pdf/2410.17566v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2402.14146v2","updated":"2024-10-23T04:39:25Z","published":"2024-02-21T22:02:37Z","title":"Reinforcement Learning with Dynamic Multi-Reward Weighting for\n  Multi-Style Controllable Generation","summary":"  Textual style expresses a diverse set of information, including interpersonal\ndynamics (e.g., formality) and the author's emotions or attitudes (e.g.,\ndisgust). An open question is how language models can be explicitly controlled\nso that they weave together target styles when generating text: for example, to\nproduce text that is both negative and non-toxic. One approach to such\ncontrolled generation is multi-objective reinforcement learning (RL), but how\nbest to combine multiple objectives in a reward function is an open question.\nIn this paper, we investigate various formulations of multi-style rewards,\nincluding calibrated outputs from discriminators and dynamic weighting by\ndiscriminator gradient magnitudes. We find that our proposed dynamic weighting\noutperforms static weighting approaches with respect to style control while\nmaintaining linguistic quality, and we explore its effectiveness in 2- and\n3-style control.\n","authors":["Karin de Langis","Ryan Koo","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2402.14146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17552v1","updated":"2024-10-23T04:34:49Z","published":"2024-10-23T04:34:49Z","title":"ESpeW: Robust Copyright Protection for LLM-based EaaS via\n  Embedding-Specific Watermark","summary":"  Embeddings as a Service (EaaS) is emerging as a crucial role in AI\napplications. Unfortunately, EaaS is vulnerable to model extraction attacks,\nhighlighting the urgent need for copyright protection.Although some preliminary\nworks propose applying embedding watermarks to protect EaaS, recent research\nreveals that these watermarks can be easily removed. Hence, it is crucial to\ninject robust watermarks resistant to watermark removal attacks.Existing\nwatermarking methods typically inject a target embedding into embeddings\nthrough linear interpolation when the text contains triggers. However, this\nmechanism results in each watermarked embedding having the same component,\nwhich makes the watermark easy to identify and eliminate.Motivated by this, in\nthis paper, we propose a novel embedding-specific watermarking (ESpeW)\nmechanism to offer robust copyright protection for EaaS. Our approach involves\ninjecting unique, yet readily identifiable watermarks into each embedding.\nWatermarks inserted by ESpeW are designed to maintain a significant distance\nfrom one another and to avoid sharing common components, thus making it\nsignificantly more challenging to remove the watermarks.Extensive experiments\non four popular datasets demonstrate that ESpeW can even watermark successfully\nagainst a highly aggressive removal strategy without sacrificing the quality of\nembeddings.\n","authors":["Zongqi Wang","Baoyuan Wu","Jingyuan Deng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2410.17552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17546v1","updated":"2024-10-23T03:53:46Z","published":"2024-10-23T03:53:46Z","title":"ProtoLens: Advancing Prototype Learning for Fine-Grained\n  Interpretability in Text Classification","summary":"  Deep neural networks have achieved remarkable performance in various\ntext-based tasks but often lack interpretability, making them less suitable for\napplications where transparency is critical. To address this, we propose\nProtoLens, a novel prototype-based model that provides fine-grained,\nsub-sentence level interpretability for text classification. ProtoLens uses a\nPrototype-aware Span Extraction module to identify relevant text spans\nassociated with learned prototypes and a Prototype Alignment mechanism to\nensure prototypes are semantically meaningful throughout training. By aligning\nthe prototype embeddings with human-understandable examples, ProtoLens provides\ninterpretable predictions while maintaining competitive accuracy. Extensive\nexperiments demonstrate that ProtoLens outperforms both prototype-based and\nnon-interpretable baselines on multiple text classification benchmarks. Code\nand data are available at\n\\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.\n","authors":["Bowen Wei","Ziwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16638v2","updated":"2024-10-23T03:41:49Z","published":"2024-10-22T02:27:57Z","title":"LLMScan: Causal Scan for LLM Misbehavior Detection","summary":"  Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.\n","authors":["Mengdi Zhang","Kai Kiat Goh","Peixin Zhang","Jun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.16638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17532v1","updated":"2024-10-23T03:19:15Z","published":"2024-10-23T03:19:15Z","title":"Responsible Multilingual Large Language Models: A Survey of Development,\n  Applications, and Societal Impact","summary":"  Multilingual Large Language Models (MLLMs) represent a pivotal advancement in\ndemocratizing artificial intelligence across linguistic boundaries. While\ntheoretical foundations are well-established, practical implementation\nguidelines remain scattered. This work bridges this gap by providing a\ncomprehensive end-to-end framework for developing and deploying MLLMs in\nproduction environments. We make three distinctive contributions: First, we\npresent an actionable pipeline from data pre-processing through deployment,\nintegrating insights from academic research and industrial applications.\nSecond, using Llama2 as a case study, we provide detailed optimization\nstrategies for enhancing multilingual capabilities, including curriculum\nlearning approaches for balancing high-resource and low-resource languages,\ntokenization strategies, and effective sampling methods. Third, we offer an\ninterdisciplinary analysis that considers technical, linguistic, and cultural\nperspectives in MLLM development. Our findings reveal critical challenges in\nsupporting linguistic diversity, with 88.38% of world languages categorized as\nlow-resource, affecting over a billion speakers. We examine practical solutions\nthrough real-world applications in customer service, search engines, and\nmachine translation. By synthesizing theoretical frameworks with\nproduction-ready implementation strategies, this survey provides essential\nguidance for practitioners and researchers working to develop more inclusive\nand effective multilingual AI systems.\n","authors":["Junhua Liu","Bin Fu"],"pdf_url":"https://arxiv.org/pdf/2410.17532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17529v1","updated":"2024-10-23T03:14:07Z","published":"2024-10-23T03:14:07Z","title":"Navigate Complex Physical Worlds via Geometrically Constrained LLM","summary":"  This study investigates the potential of Large Language Models (LLMs) for\nreconstructing and constructing the physical world solely based on textual\nknowledge. It explores the impact of model performance on spatial understanding\nabilities. To enhance the comprehension of geometric and spatial relationships\nin the complex physical world, the study introduces a set of geometric\nconventions and develops a workflow based on multi-layer graphs and multi-agent\nsystem frameworks. It examines how LLMs achieve multi-step and multi-objective\ngeometric inference in a spatial environment using multi-layer graphs under\nunified geometric conventions. Additionally, the study employs a genetic\nalgorithm, inspired by large-scale model knowledge, to solve geometric\nconstraint problems. In summary, this work innovatively explores the\nfeasibility of using text-based LLMs as physical world builders and designs a\nworkflow to enhance their capabilities.\n","authors":["Yongqiang Huang","Wentao Ye","Liyao Li","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14687v2","updated":"2024-10-23T03:05:37Z","published":"2024-10-03T14:17:43Z","title":"BrainTransformers: SNN-LLM","summary":"  This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.\n","authors":["Zhengzheng Tang","Eva Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.14687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11303v2","updated":"2024-10-23T03:00:41Z","published":"2024-10-15T05:54:17Z","title":"TSDS: Data Selection for Task-Specific Model Finetuning","summary":"  Finetuning foundation models for specific tasks is an emerging paradigm in\nmodern machine learning. The efficacy of task-specific finetuning largely\ndepends on the selection of appropriate training data. We present TSDS\n(Task-Specific Data Selection), a framework to select data for task-specific\nmodel finetuning, guided by a small but representative set of examples from the\ntarget task. To do so, we formulate data selection for task-specific finetuning\nas an optimization problem with a distribution alignment loss based on optimal\ntransport to capture the discrepancy between the selected data and the target\ndistribution. In addition, we add a regularizer to encourage the diversity of\nthe selected data and incorporate kernel density estimation into the\nregularizer to reduce the negative effects of near-duplicates among the\ncandidate data. We connect our optimization problem to nearest neighbor search\nand design efficient algorithms to compute the optimal solution based on\napproximate nearest neighbor search techniques. We evaluate our method on data\nselection for both continued pretraining and instruction tuning of language\nmodels. We show that instruction tuning using data selected by our method with\na 1% selection ratio often outperforms using the full dataset and beats the\nbaseline selection methods by 1.5 points in F1 score on average.\n","authors":["Zifan Liu","Amin Karbasi","Theodoros Rekatsinas"],"pdf_url":"https://arxiv.org/pdf/2410.11303v2.pdf","comment":"31 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.18925v3","updated":"2024-10-23T02:57:31Z","published":"2024-06-27T06:32:56Z","title":"Selective Vision is the Challenge for Visual Reasoning: A Benchmark for\n  Visual Argument Understanding","summary":"  Visual arguments, often used in advertising or social causes, rely on images\nto persuade viewers to do or believe something. Understanding these arguments\nrequires selective vision: only specific visual stimuli within an image are\nrelevant to the argument, and relevance can only be understood within the\ncontext of a broader argumentative structure. While visual arguments are\nreadily appreciated by human audiences, we ask: are today's AI capable of\nsimilar understanding? We present VisArgs, a dataset of 1,611 images annotated\nwith 5,112 visual premises (with regions), 5,574 commonsense premises, and\nreasoning trees connecting them into structured arguments. We propose three\ntasks for evaluating visual argument understanding: premise localization,\npremise identification, and conclusion deduction. Experiments show that 1)\nmachines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy,\nwhile humans reached 98.0%. Models also performed 19.5% worse when\ndistinguishing between irrelevant objects within the image compared to external\nobjects. 2) Providing relevant visual premises improved model performance\nsignificantly.\n","authors":["Jiwan Chung","Sungjae Lee","Minseo Kim","Seungju Han","Ashkan Yousefpour","Jack Hessel","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18925v3.pdf","comment":"12 pages, 6 figures. Accepted as main paper in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.17520v1","updated":"2024-10-23T02:51:43Z","published":"2024-10-23T02:51:43Z","title":"MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control","summary":"  Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications. To\nclearly evaluate safety apart from general capabilities, we design separate\ntasks measuring safety and tasks evaluating helpfulness. The safety tasks\nchallenge agents with managing potential risks prevalent in daily life and\ninclude tests to evaluate robustness against indirect prompt injections. Our\nexperiments demonstrate that while baseline agents, based on state-of-the-art\nLLMs, perform well in executing helpful tasks, they show poor performance in\nsafety tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/.\n","authors":["Juyong Lee","Dongyoon Hahm","June Suk Choi","W. Bradley Knox","Kimin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.17520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17519v1","updated":"2024-10-23T02:51:33Z","published":"2024-10-23T02:51:33Z","title":"Large Language Models Still Exhibit Bias in Long Text","summary":"  Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks.\n","authors":["Wonje Jeung","Dongjae Jeon","Ashkan Yousefpour","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2410.17519v1.pdf","comment":"22 page, 38 figures, Neurips (SoLaR Workshop)"},{"id":"http://arxiv.org/abs/2410.13334v2","updated":"2024-10-23T02:15:52Z","published":"2024-10-17T08:46:09Z","title":"Do LLMs Have Political Correctness? Analyzing Ethical Biases and\n  Jailbreak Vulnerabilities in AI Systems","summary":"  Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content. To address\nthese issues, many LLM developers have implemented various safety measures to\nalign these models. This alignment involves several techniques, including data\nfiltering during pre-training, supervised fine-tuning, reinforcement learning\nfrom human feedback, and red-teaming exercises. These methods often introduce\ndeliberate and intentional biases similar to Political Correctness (PC) to\nensure the ethical behavior of LLMs. In this paper, we delve into the\nintentional biases injected into LLMs for safety purposes and examine methods\nto circumvent these safety alignment techniques. Notably, these intentional\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20% between non-binary and cisgender keywords and by 16% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of PCJailbreak, highlighting the inherent risks posed by\nthese safety-induced biases. Additionally, we propose an efficient defense\nmethod PCDefense, which prevents jailbreak attempts by injecting defense\nprompts prior to generation. PCDefense stands as an appealing alternative to\nGuard Models, such as Llama-Guard, that require additional inference cost after\ntext generation. Our findings emphasize the urgent need for LLM developers to\nadopt a more responsible approach when designing and implementing safety\nmeasures.\n","authors":["Isack Lee","Haebin Seong"],"pdf_url":"https://arxiv.org/pdf/2410.13334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16451v2","updated":"2024-10-23T01:56:15Z","published":"2024-10-21T19:25:31Z","title":"Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between\n  Ghana and the U.S","summary":"  Recent work has highlighted the culturally-contingent nature of commonsense\nknowledge. We introduce AMAMMER${\\epsilon}$, a test set of 525 multiple-choice\nquestions designed to evaluate the commonsense knowledge of English LLMs,\nrelative to the cultural contexts of Ghana and the United States. To create\nAMAMMER${\\epsilon}$, we select a set of multiple-choice questions (MCQs) from\nexisting commonsense datasets and rewrite them in a multi-stage process\ninvolving surveys of Ghanaian and U.S. participants. In three rounds of\nsurveys, participants from both pools are solicited to (1) write correct and\nincorrect answer choices, (2) rate individual answer choices on a 5-point\nLikert scale, and (3) select the best answer choice from the newly-constructed\nMCQ items, in a final validation step. By engaging participants at multiple\nstages, our procedure ensures that participant perspectives are incorporated\nboth in the creation and validation of test items, resulting in high levels of\nagreement within each pool. We evaluate several off-the-shelf English LLMs on\nAMAMMER${\\epsilon}$. Uniformly, models prefer answers choices that align with\nthe preferences of U.S. annotators over Ghanaian annotators. Additionally, when\ntest items specify a cultural context (Ghana or the U.S.), models exhibit some\nability to adapt, but performance is consistently better in U.S. contexts than\nGhanaian. As large resources are devoted to the advancement of English LLMs,\nour findings underscore the need for culturally adaptable models and\nevaluations to meet the needs of diverse English-speaking populations around\nthe world.\n","authors":["Christabel Acquaye","Haozhe An","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2410.16451v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2404.03881v4","updated":"2024-10-23T01:39:31Z","published":"2024-04-05T04:04:23Z","title":"A Bi-consolidating Model for Joint Relational Triple Extraction","summary":"  Current methods to extract relational triples directly make a prediction\nbased on a possible entity pair in a raw sentence without depending on entity\nrecognition. The task suffers from a serious semantic overlapping problem, in\nwhich several relation triples may share one or two entities in a sentence. In\nthis paper, based on a two-dimensional sentence representation, a\nbi-consolidating model is proposed to address this problem by simultaneously\nreinforcing the local and global semantic features relevant to a relation\ntriple. This model consists of a local consolidation component and a global\nconsolidation component. The first component uses a pixel difference\nconvolution to enhance semantic information of a possible triple representation\nfrom adjacent regions and mitigate noise in neighbouring neighbours. The second\ncomponent strengthens the triple representation based a channel attention and a\nspatial attention, which has the advantage to learn remote semantic\ndependencies in a sentence. They are helpful to improve the performance of both\nentity identification and relation type classification in relation triple\nextraction. After evaluated on several publish datasets, the bi-consolidating\nmodel achieves competitive performance. Analytical experiments demonstrate the\neffectiveness of our model for relational triple extraction and give motivation\nfor other natural language processing tasks.\n","authors":["Xiaocheng Luo","Yanping Chen","Ruixue Tang","Caiwei Yang","Ruizhang Huang","Yongbin Qin"],"pdf_url":"https://arxiv.org/pdf/2404.03881v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17498v1","updated":"2024-10-23T01:38:10Z","published":"2024-10-23T01:38:10Z","title":"Mechanisms of Symbol Processing for In-Context Learning in Transformer\n  Networks","summary":"  Large Language Models (LLMs) have demonstrated impressive abilities in symbol\nprocessing through in-context learning (ICL). This success flies in the face of\ndecades of predictions that artificial neural networks cannot master abstract\nsymbol manipulation. We seek to understand the mechanisms that can enable\nrobust symbol processing in transformer networks, illuminating both the\nunanticipated success, and the significant limitations, of transformers in\nsymbol processing. Borrowing insights from symbolic AI on the power of\nProduction System architectures, we develop a high-level language, PSL, that\nallows us to write symbolic programs to do complex, abstract symbol processing,\nand create compilers that precisely implement PSL programs in transformer\nnetworks which are, by construction, 100% mechanistically interpretable. We\ndemonstrate that PSL is Turing Universal, so the work can inform the\nunderstanding of transformer ICL in general. The type of transformer\narchitecture that we compile from PSL programs suggests a number of paths for\nenhancing transformers' capabilities at symbol processing. (Note: The first\nsection of the paper gives an extended synopsis of the entire paper.)\n","authors":["Paul Smolensky","Roland Fernandez","Zhenghao Herbert Zhou","Mattia Opper","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2410.17498v1.pdf","comment":"101 pages (including 30 pages of Appendices), 18 figures"},{"id":"http://arxiv.org/abs/2410.17492v1","updated":"2024-10-23T01:14:54Z","published":"2024-10-23T01:14:54Z","title":"BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers","summary":"  Attacking fairness is crucial because compromised models can introduce biased\noutcomes, undermining trust and amplifying inequalities in sensitive\napplications like hiring, healthcare, and law enforcement. This highlights the\nurgent need to understand how fairness mechanisms can be exploited and to\ndevelop defenses that ensure both fairness and robustness. We introduce\nBadFair, a novel backdoored fairness attack methodology. BadFair stealthily\ncrafts a model that operates with accuracy and fairness under regular\nconditions but, when activated by certain triggers, discriminates and produces\nincorrect results for specific groups. This type of attack is particularly\nstealthy and dangerous, as it circumvents existing fairness detection methods,\nmaintaining an appearance of fairness in normal use. Our findings reveal that\nBadFair achieves a more than 85% attack success rate in attacks aimed at target\ngroups on average while only incurring a minimal accuracy loss. Moreover, it\nconsistently exhibits a significant discrimination score, distinguishing\nbetween pre-defined target and non-target attacked groups across various\ndatasets and models.\n","authors":["Jiaqi Xue","Qian Lou","Mengxin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.17492v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.10601v2","updated":"2024-10-23T00:38:14Z","published":"2024-02-16T11:37:05Z","title":"When \"Competency\" in Reasoning Opens the Door to Vulnerability:\n  Jailbreaking LLMs via Novel Complex Ciphers","summary":"  Recent advancements in the safety of Large Language Models (LLMs) have\nprimarily focused on mitigating attacks crafted in natural language or in\ncommon encryption techniques like Base64. However, new models which often\npossess better reasoning capabilities, open the door to new attack vectors that\nwere previously non-existent in older models. This seems counter-intuitive at\nfirst glance, but these advanced models can decipher more complex cryptic\nqueries that previous models could not, making them susceptible to attacks\nusing such prompts. To exploit this vulnerability, we propose Attacks using\nCustom Encryptions (ACE), a novel method to jailbreak LLMs by leveraging custom\nencryption schemes. We evaluate the effectiveness of ACE on four\nstate-of-the-art LLMs, achieving Attack Success Rates (ASR) of up to 66% on\nclose-source models and 88% on open-source models. Building upon this, we\nintroduce Layered Attacks using Custom Encryptions (LACE), which employs\nmultiple layers of encryption through our custom ciphers to further enhance the\nASR. Our findings demonstrate that LACE significantly enhances the ability to\njailbreak LLMs, increasing the ASR of GPT-4o from 40% to 78%, a 38%\nimprovement. Our results highlight that the advanced capabilities of LLMs\nintroduce unforeseen vulnerabilities to complex attacks. Specifically complex\nand layered ciphers increase the chance of jailbreaking.\n","authors":["Divij Handa","Zehua Zhang","Amir Saeidi","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2402.10601v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17485v1","updated":"2024-10-23T00:36:06Z","published":"2024-10-23T00:36:06Z","title":"VoiceTextBlender: Augmenting Large Language Models with Speech\n  Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning","summary":"  Recent studies have augmented large language models (LLMs) with speech\ncapabilities, leading to the development of speech language models (SpeechLMs).\nEarlier SpeechLMs focused on single-turn speech-based question answering (QA),\nwhere user input comprised a speech context and a text question. More recent\nstudies have extended this to multi-turn conversations, though they often\nrequire complex, multi-stage supervised fine-tuning (SFT) with diverse data.\nAnother critical challenge with SpeechLMs is catastrophic forgetting-where\nmodels optimized for speech tasks suffer significant degradation in text-only\nperformance. To mitigate these issues, we propose a novel single-stage joint\nspeech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone.\nOur joint SFT combines text-only SFT data with three types of speech-related\ndata: speech recognition and translation, speech-based QA, and mixed-modal SFT.\nCompared to previous SpeechLMs with 7B or 13B parameters, our 3B model\ndemonstrates superior performance across various speech benchmarks while\npreserving the original capabilities on text-only tasks. Furthermore, our model\nshows emergent abilities of effectively handling previously unseen prompts and\ntasks, including multi-turn, mixed-modal inputs.\n","authors":["Yifan Peng","Krishna C. Puvvada","Zhehuai Chen","Piotr Zelasko","He Huang","Kunal Dhawan","Ke Hu","Shinji Watanabe","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2410.17485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17484v1","updated":"2024-10-23T00:31:17Z","published":"2024-10-23T00:31:17Z","title":"Which Client is Reliable?: A Reliable and Personalized Prompt-based\n  Federated Learning for Medical Image Question Answering","summary":"  Conventional medical artificial intelligence (AI) models face barriers in\nclinical application and ethical issues owing to their inability to handle the\nprivacy-sensitive characteristics of medical data. We present a novel\npersonalized federated learning (pFL) method for medical visual question\nanswering (VQA) models, addressing privacy reliability challenges in the\nmedical domain. Our method introduces learnable prompts into a Transformer\narchitecture to efficiently train it on diverse medical datasets without\nmassive computational costs. Then we introduce a reliable client VQA model that\nincorporates Dempster-Shafer evidence theory to quantify uncertainty in\npredictions, enhancing the model's reliability. Furthermore, we propose a novel\ninter-client communication mechanism that uses maximum likelihood estimation to\nbalance accuracy and uncertainty, fostering efficient integration of insights\nacross clients.\n","authors":["He Zhu","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2410.17484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17482v1","updated":"2024-10-23T00:05:57Z","published":"2024-10-23T00:05:57Z","title":"Is artificial intelligence still intelligence? LLMs generalize to novel\n  adjective-noun pairs, but don't mimic the full human distribution","summary":"  Inferences from adjective-noun combinations like \"Is artificial intelligence\nstill intelligence?\" provide a good test bed for LLMs' understanding of meaning\nand compositional generalization capability, since there are many combinations\nwhich are novel to both humans and LLMs but nevertheless elicit convergent\nhuman judgments. We study a range of LLMs and find that the largest models we\ntested are able to draw human-like inferences when the inference is determined\nby context and can generalize to unseen adjective-noun combinations. We also\npropose three methods to evaluate LLMs on these inferences out of context,\nwhere there is a distribution of human-like answers rather than a single\ncorrect answer. We find that LLMs show a human-like distribution on at most\n75\\% of our dataset, which is promising but still leaves room for improvement.\n","authors":["Hayley Ross","Kathryn Davidson","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2410.17482v1.pdf","comment":"9 pages (23 pages with appendix). Accepted to GenBench 2024"}]}}