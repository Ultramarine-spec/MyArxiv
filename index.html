<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv-U
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-23T00:00:00Z">2024-10-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">95</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALTA: Compiler-Based Analysis of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Shaw, James Cohan, Jacob Eisenstein, Kenton Lee, Jonathan Berant, Kristina Toutanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new programming language called ALTA and a compiler that can map
ALTA programs to Transformer weights. ALTA is inspired by RASP, a language
proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler
from RASP programs to Transformer weights. ALTA complements and extends this
prior work, offering the ability to express loops and to compile programs to
Universal Transformers, among other advantages. ALTA allows us to
constructively show how Transformers can represent length-invariant algorithms
for computing parity and addition, as well as a solution to the SCAN benchmark
of compositional generalization tasks, without requiring intermediate
scratchpad decoding steps. We also propose tools to analyze cases where the
expressibility of an algorithm is established, but end-to-end training on a
given training set fails to induce behavior consistent with the desired
algorithm. To this end, we explore training from ALTA execution traces as a
more fine-grained supervision signal. This enables additional experiments and
theoretical analyses relating the learnability of various algorithms to data
availability and modeling decisions, such as positional encodings. We make the
ALTA framework -- language specification, symbolic interpreter, and weight
compiler -- available to the community to enable further applications and
insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing
  <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Xie, Tianhua Li, Wenqi Shao, Kaipeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multimodal large language models (MLLMs) have received much
attention for their impressive capabilities. The evaluation of MLLMs is
becoming critical to analyzing attributes of MLLMs and providing valuable
insights. However, current benchmarks overlook the problem of prompt
sensitivity - minor prompt variations may lead to significant performance
fluctuations. Thus, inappropriate prompts may obscure the models' capabilities,
underestimating the models' performance. Moreover, different models have
different preferences for different prompts, and thus, using the same prompt
for all models will cause evaluation bias. This paper analyzes this deficiency
in existing benchmarks and further introduces a new evaluation framework named
TP-Eval, which introduces a prompt customization method to reduce evaluation
biases and tap models' potential. TP-Eval will rewrite the original prompts to
different customized prompts for different models. In particular, we propose
some well-designed modules for prompt customization tailored to the scenario of
MLLM evaluation. Extensive experiments demonstrate the effectiveness of our
approach to uncovering models' capabilities, and TP-Eval should benefit the
community in developing more comprehensive and convincing MLLM evaluation
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEAR: Character Unlearning in Textual and Visual Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin, Boris Mikheev, Denis Bobkov, Aibek Alanov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning (MU) is critical for enhancing privacy and security in
deep learning models, particularly in large multimodal language models (MLLMs),
by removing specific private or hazardous information. While MU has made
significant progress in textual and visual modalities, multimodal unlearning
(MMU) remains significantly underexplored, partially due to the absence of a
suitable open-source benchmark. To address this, we introduce CLEAR, a new
benchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious
individuals and 3,700 images linked with corresponding question-answer pairs,
enabling a thorough evaluation across modalities. We assess 10 MU methods,
adapting them for MMU, and highlight new challenges specific to multimodal
forgetting. We also demonstrate that simple $\ell_1$ regularization on LoRA
weights significantly mitigates catastrophic forgetting, preserving model
performance on retained data. The dataset is available at
https://huggingface.co/datasets/therem/CLEAR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Long</span>RAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for
  <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span> Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-Context Question Answering (LCQA), a challenging task, aims to reason
over long-context documents to yield accurate answers to questions. Existing
long-context Large Language Models (LLMs) for LCQA often struggle with the
"lost in the middle" issue. Retrieval-Augmented Generation (RAG) mitigates this
issue by providing external factual evidence. However, its chunking strategy
disrupts the global long-context information, and its low-quality retrieval in
long contexts hinders LLMs from identifying effective factual details due to
substantial noise. To this end, we propose LongRAG, a general,
dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance
RAG's understanding of complex long-context knowledge (i.e., global information
and factual details). We design LongRAG as a plug-and-play paradigm,
facilitating adaptation to various domains and LLMs. Extensive experiments on
three multi-hop datasets demonstrate that LongRAG significantly outperforms
long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG
(up by 17.25%). Furthermore, we conduct quantitative ablation studies and
multi-dimensional analyses, highlighting the effectiveness of the system's
components and fine-tuning strategies. Data and code are available at
https://github.com/QingFei1/LongRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for
  Russian Scientific Keyphrases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Glazkova, Dmitry Morozov, Timur Garipov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyphrase selection is a challenging task in natural language processing that
has a wide range of applications. Adapting existing supervised and unsupervised
solutions for the Russian language faces several limitations due to the rich
morphology of Russian and the limited number of training datasets available.
Recent studies conducted on English texts show that large language models
(LLMs) successfully address the task of generating keyphrases. LLMs allow
achieving impressive results without task-specific fine-tuning, using text
prompts instead. In this work, we access the performance of prompt-based
methods for generating keyphrases for Russian scientific abstracts. First, we
compare the performance of zero-shot and few-shot prompt-based methods,
fine-tuned models, and unsupervised methods. Then we assess strategies for
selecting keyphrase examples in a few-shot setting. We present the outcomes of
human evaluation of the generated keyphrases and analyze the strengths and
weaknesses of the models through expert assessment. Our results suggest that
prompt-based methods can outperform common baselines even using simple text
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 12th International Conference on Analysis of Images, Social
  Networks and Texts (AIST'2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language
  Models Fine-tuning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfan Zhang, Yi Zhao, Dan Chen, Xing Tian, Huanran Zheng, Wei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are
highly effective parameter-efficient fine-tuning (PEFT) methods. However, they
introduce significant latency in multi-tenant settings due to the LoRA modules
and MOE routers added to multiple linear modules in the Transformer layer. To
address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel
and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods
by considering each LoRA module as an expert and employing a prompt-aware
routing mechanism. This mechanism calculates expert routing results once before
generating the first new token and reuses these results for subsequent tokens,
reducing latency. Extensive experiments and analysis on commonsense reasoning
tasks, math reasoning tasks, and widely used LLM evaluation benchmarks
demonstrate that MiLoRA consistently outperforms strong PEFT baselines with
comparable tunable parameter budgets. Additionally, MiLoRA significantly
reduces latency in multi-tenant settings compared to previous LoRA-based
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Findings. arXiv admin note: substantial text
  overlap with arXiv:2405.18203</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphTeam: Facilitating Large Language Model-based Graph Analysis via
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs are widely used for modeling relational data in real-world scenarios,
such as social networks and urban computing. Existing LLM-based graph analysis
approaches either integrate graph neural networks (GNNs) for specific machine
learning tasks, limiting their transferability, or rely solely on LLMs'
internal reasoning ability, resulting in suboptimal performance. To address
these limitations, we take advantage of recent advances in LLM-based agents,
which have shown capabilities of utilizing external knowledge or tools for
problem solving. By simulating human problem-solving strategies such as analogy
and collaboration, we propose a multi-agent system based on LLMs named
GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from
three modules, and the agents with different specialities can collaborate with
each other to address complex problems. Specifically, (1) input-output
normalization module: the question agent extracts and refines four key
arguments from the original question, facilitating the problem understanding,
and the answer agent organizes the results to meet the output requirement; (2)
external knowledge retrieval module: we first build a knowledge base consisting
of relevant documentation and experience information, and then the search agent
retrieves the most relevant entries for each question. (3) problem-solving
module: given the retrieved information from search agent, the coding agent
uses established algorithms via programming to generate solutions, and in case
the coding agent does not work, the reasoning agent will directly compute the
results without programming. Extensive experiments on six graph analysis
benchmarks demonstrate that GraphTeam achieves state-of-the-art performance
with an average 25.85% improvement over the best baseline in terms of accuracy.
The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-lingual Transfer of Reward Models in Multilingual Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Hong, Noah Lee, Rodrigo Martínez-Castaño, César Rodríguez, James Thorne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning with human feedback (RLHF) is shown to largely benefit
from precise reward models (RMs). However, recent studies in reward modeling
schemes are skewed towards English, limiting the applicability of RLHF in
multilingual alignments. In this work, we investigate the cross-lingual
transfer of RMs trained in diverse languages, primarily from English. Our
experimental results demonstrate the strong cross-lingual transfer of English
RMs, exceeding target language RMs by 3~4% average increase in Multilingual
RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through
the representation shifts. Finally, we perform multilingual alignment to
exemplify how cross-lingual transfer in RM propagates to enhanced multilingual
instruction-following capability, along with extensive analyses on
off-the-shelf RMs. We release the code, model, and data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Together We Can: Multilingual Automatic Post-Editing for Low-Resource
  Languages <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourabh Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This exploratory study investigates the potential of multilingual Automatic
Post-Editing (APE) systems to enhance the quality of machine translations for
low-resource Indo-Aryan languages. Focusing on two closely related language
pairs, English-Marathi and English-Hindi, we exploit the linguistic
similarities to develop a robust multilingual APE model. To facilitate
cross-linguistic transfer, we generate synthetic Hindi-Marathi and
Marathi-Hindi APE triplets. Additionally, we incorporate a Quality Estimation
(QE)-APE multi-task learning framework. While the experimental results
underline the complementary nature of APE and QE, we also observe that QE-APE
multitask learning facilitates effective domain adaptation. Our experiments
demonstrate that the multilingual APE models outperform their corresponding
English-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TER
points, respectively, with further notable improvements over the multilingual
APE model observed through multi-task learning ($+1.29$ and $+1.44$ TER
points), data augmentation ($+0.53$ and $+0.45$ TER points) and domain
adaptation ($+0.35$ and $+0.45$ TER points). We release the synthetic data,
code, and models accrued during this study publicly at
https://github.com/cfiltnlp/Multilingual-APE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dependency Graph Parsing as Sequence Labeling <span class="chip">EMNLP-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana Ezquerro, David Vilares, Carlos Gómez-Rodríguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various linearizations have been proposed to cast syntactic dependency
parsing as sequence labeling. However, these approaches do not support more
complex graph-based representations, such as semantic dependencies or enhanced
universal dependencies, as they cannot handle reentrancy or cycles. By
extending them, we define a range of unbounded and bounded linearizations that
can be used to cast graph parsing as a tagging task, enlarging the toolbox of
problems that can be solved under this paradigm. Experimental results on
semantic dependency and enhanced UD parsing show that with a good choice of
encoding, sequence-labeling dependency graph parsers combine high efficiency
with accuracies close to the state of the art, in spite of their simplicity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Horacio Thompson, Marcelo Errecalde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The eRisk laboratory aims to address issues related to early risk detection
on the Web. In this year's edition, three tasks were proposed, where Task 2 was
about early detection of signs of anorexia. Early risk detection is a problem
where precision and speed are two crucial objectives. Our research group solved
Task 2 by defining a CPI+DMC approach, addressing both objectives
independently, and a time-aware approach, where precision and speed are
considered a combined single-objective. We implemented the last approach by
explicitly integrating time during the learning process, considering the
ERDE{\theta} metric as the training objective. It also allowed us to
incorporate temporal metrics to validate and select the optimal models. We
achieved outstanding results for the ERDE50 metric and ranking-based metrics,
demonstrating consistency in solving ERD problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble,
  France</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zeitenwenden: Detecting changes in the German political discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai-Robin Lange, Jonas Rieger, Niklas Benner, Carsten Jentsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From a monarchy to a democracy, to a dictatorship and back to a democracy --
the German political landscape has been constantly changing ever since the
first German national state was formed in 1871. After World War II, the Federal
Republic of Germany was formed in 1949. Since then every plenary session of the
German Bundestag was logged and even has been digitized over the course of the
last few years. We analyze these texts using a time series variant of the topic
model LDA to investigate which events had a lasting effect on the political
discourse and how the political topics changed over time. This allows us to
detect changes in word frequency (and thus key discussion points) in political
discourse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExpertFlow: Optimized Expert Activation and Token Allocation for
  Efficient Mixture-of-Experts Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, Ong Yew Soon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Mixture of Experts (MoE) models, while outperforming dense Large
Language Models (LLMs) in terms of performance, face significant deployment
challenges during inference due to their high memory demands. Existing
offloading techniques, which involve swapping activated and idle experts
between the GPU and CPU, often suffer from rigid expert caching mechanisms.
These mechanisms fail to adapt to dynamic routing, leading to inefficient cache
utilization, or incur prohibitive costs for prediction training. To tackle
these inference-specific challenges, we introduce ExpertFlow, a comprehensive
system specifically designed to enhance inference efficiency by accommodating
flexible routing and enabling efficient expert scheduling between CPU and GPU.
This reduces overhead and boosts system performance. Central to our approach is
a predictive routing path-based offloading mechanism that utilizes a
lightweight predictor to accurately forecast routing paths before computation
begins. This proactive strategy allows for real-time error correction in expert
caching, significantly increasing cache hit ratios and reducing the frequency
of expert transfers, thereby minimizing I/O overhead. Additionally, we
implement a dynamic token scheduling strategy that optimizes MoE inference by
rearranging input tokens across different batches. This method not only reduces
the number of activated experts per batch but also improves computational
efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up
to 93.72\% GPU memory savings and enhances inference speed by 2 to 10 times
compared to baseline methods, highlighting its effectiveness and utility as a
robust solution for resource-constrained inference scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Mixture-of-Experts, Inference, Offloading</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large
  Language Models to Specialized Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, Qi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances the question-answering (QA)
abilities of large language models (LLMs) by integrating external knowledge.
However, adapting general-purpose RAG systems to specialized fields such as
science and medicine poses unique challenges due to distribution shifts and
limited access to domain-specific data. To tackle this, we propose SimRAG, a
self-training approach that equips the LLM with joint capabilities of question
answering and question generation for domain adaptation. Our method first
fine-tunes the LLM on instruction-following, question-answering, and
search-related data. Then, it prompts the same LLM to generate diverse
domain-relevant questions from unlabeled corpora, with an additional filtering
strategy to retain high-quality synthetic examples. By leveraging these
synthetic examples, the LLM can improve their performance on domain-specific
RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three
domains, demonstrate that SimRAG outperforms baselines by 1.2\%--8.6\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELAICHI: Enhancing Low-resource TTS by Addressing Infrequent and
  Low-frequency Character Bigrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srija Anand, Praveen Srinivasa Varadhan, Mehak Singal, Mitesh M. Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Text-to-Speech (TTS) technology have led to
natural-sounding speech for English, primarily due to the availability of
large-scale, high-quality web data. However, many other languages lack access
to such resources, relying instead on limited studio-quality data. This
scarcity results in synthesized speech that often suffers from intelligibility
issues, particularly with low-frequency character bigrams. In this paper, we
propose three solutions to address this challenge. First, we leverage
high-quality data from linguistically or geographically related languages to
improve TTS for the target language. Second, we utilize low-quality Automatic
Speech Recognition (ASR) data recorded in non-studio environments, which is
refined using denoising and speech enhancement models. Third, we apply
knowledge distillation from large-scale models using synthetic data to generate
more robust outputs. Our experiments with Hindi demonstrate significant
reductions in intelligibility issues, as validated by human evaluators. We
propose this methodology as a viable alternative for languages with limited
access to high-quality data, enabling them to collectively benefit from shared
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Value Residual Learning For Alleviating Attention Concentration In
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers can capture long-range dependencies using self-attention,
allowing tokens to attend to all others directly. However, stacking multiple
attention layers leads to attention concentration. One natural way to address
this issue is to use cross-layer attention, allowing information from earlier
layers to be directly accessible to later layers. However, this approach is
computationally expensive. To address this problem, we propose Transformer with
residual value (ResFormer) which approximates cross-layer attention through
adding a residual connection from the values of the the first layer to all
subsequent layers. Based on this method, one variant is the Transformer with
single layer value (SVFormer), where all layers share the same value embedding
from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical
evidence demonstrates that ResFormer mitigates attention concentration problem
in deeper layers and enhances representation across most layers, outperforming
the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as
downstream tasks. SVFormer trains significantly faster than the vanilla
Transformer and performs better than other methods like GQA and CLA, with
performance influenced by sequence length and cumulative learning rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Diffusion Language Models via Adaptation from Autoregressive
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Language Models (DLMs) have emerged as a promising new paradigm for
text generative modeling, potentially addressing limitations of autoregressive
(AR) models. However, current DLMs have been studied at a smaller scale
compared to their AR counterparts and lack fair comparison on language modeling
benchmarks. Additionally, training diffusion models from scratch at scale
remains challenging. Given the prevalence of open-source AR language models, we
propose adapting these models to build text diffusion models. We demonstrate
connections between AR and diffusion modeling objectives and introduce a simple
continual pre-training approach for training diffusion models. Through
systematic evaluation on language modeling, reasoning, and commonsense
benchmarks, we show that we can convert AR models ranging from 127M to 7B
parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA,
using less than 200B tokens for training. Our experimental results reveal that
these models outperform earlier DLMs and are competitive with their AR
counterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters)
capable of generating fluent text, performing in-context learning, filling in
the middle without prompt re-ordering, and following instructions
\url{https://github.com/HKUNLP/DiffuLLaMA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages. Code: https://github.com/HKUNLP/DiffuLLaMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeakGer: A meta-data enriched speech corpus of German state and federal
  parliaments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai-Robin Lange, Carsten Jentsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of natural language processing on political texts as well as
speeches has become increasingly relevant in political sciences due to the
ability to analyze large text corpora which cannot be read by a single person.
But such text corpora often lack critical meta information, detailing for
instance the party, age or constituency of the speaker, that can be used to
provide an analysis tailored to more fine-grained research questions. To enable
researchers to answer such questions with quantitative approaches such as
natural language processing, we provide the SpeakGer data set, consisting of
German parliament debates from all 16 federal states of Germany as well as the
German Bundestag from 1947-2023, split into a total of 10,806,105 speeches.
This data set includes rich meta data in form of information on both reactions
from the audience towards the speech as well as information about the speaker's
party, their age, their constituency and their party's political alignment,
which enables a deeper analysis. We further provide three exploratory analyses,
detailing topic shares of different parties throughout time, a descriptive
analysis of the development of the age of an average speaker as well as a
sentiment analysis of speeches of different parties with regards to the
COVID-19 pandemic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Layer Significance in LLM Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyuan Shi, Zexin Lu, Xiaoyu Dong, Wenlong Zhang, Xuanyu Zhang, Yujie Feng, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) through fine-tuning is essential for
tailoring them to specific applications. Therefore, understanding what LLMs
learn during the alignment process is crucial. Recent studies suggest that
alignment primarily adjusts a model's presentation style rather than its
foundational knowledge, indicating that only certain components of the model
are significantly impacted. To delve deeper into LLM alignment, we propose to
identify which layers within LLMs are most critical to the alignment process,
thereby uncovering how alignment influences model behavior at a granular level.
We propose a novel approach to identify the important layers for LLM alignment
(ILA). It involves learning a binary mask for each incremental weight matrix in
the LoRA algorithm, indicating the significance of each layer. ILA consistently
identifies important layers across various alignment datasets, with nearly 90%
overlap even with substantial dataset differences, highlighting fundamental
patterns in LLM alignment. Experimental results indicate that freezing
non-essential layers improves overall model performance, while selectively
tuning the most critical layers significantly enhances fine-tuning efficiency
with minimal performance loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding When Tree of Thoughts Succeeds: Larger Models Excel in
  Generation, Not Discrimination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiqi Chen, Xinpeng Wang, Philipp Mondorf, Michael A. Hedderich, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models
(LLMs) that employs a generator to suggest reasoning steps and a discriminator
to decide which steps to implement. ToT demonstrates strong performance on
reasoning tasks, often surpassing simple methods such as Input-Output (IO)
prompting and Chain-of-Thought (CoT) reasoning. However, ToT does not
consistently outperform such simpler methods across all models, leaving large
knowledge gaps on the conditions under which ToT is most beneficial. In this
paper, we analyze the roles of the generator and discriminator separately to
better understand the conditions when ToT is beneficial. We find that the
generator plays a more critical role than the discriminator in driving the
success of ToT. While using even a smaller model as the discriminator, scaling
the generator leads to notable improvements in ToT performance, whereas scaling
the discriminator with a fixed generator yields only marginal gains. Our
results show that models across different scales exhibit comparable
discrimination capabilities, yet differ significantly in their generative
performance for ToT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: github.com/mainlp/tot-eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniFlatten: An End-to-end <span class="highlight-title">GPT</span> Model for Seamless Voice Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, Chaohong Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-duplex spoken dialogue systems significantly advance over traditional
turn-based dialogue systems, as they allow simultaneous bidirectional
communication, closely mirroring human-human interactions. However, achieving
low latency and natural interactions in full-duplex dialogue systems remains a
significant challenge, especially considering human conversation dynamics such
as interruptions, backchannels, and overlapping speech. In this paper, we
introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex
conversation, capable of effectively modeling the complex behaviors inherent to
natural conversations with low latency. To achieve full-duplex communication
capabilities, we propose a multi-stage post-training scheme that progressively
adapts a text-based large language model (LLM) backbone into a speech-text
dialogue LLM, capable of generating text and speech in real time, without
modifying the architecture of the backbone LLM. The training process comprises
three stages: modality alignment, half-duplex dialogue learning, and
full-duplex dialogue learning. Throughout all training stages, we standardize
the data using a flattening operation, which allows us to unify the training
methods and the model architecture across different modalities and tasks. Our
approach offers a straightforward modeling technique and a promising research
direction for developing efficient and natural end-to-end full-duplex spoken
dialogue systems. Audio samples of dialogues generated by OmniFlatten can be
found at this web site (https://omniflatten.github.io/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging the Domain Adaptation of Retrieval Augmented Generation
  Models for Question Answering and Reducing Hallucination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Rakin, Md. A. R. Shibly, Zahin M. Hossain, Zeeshan Khan, Md. Mostofa Akbar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While ongoing advancements in Large Language Models have demonstrated
remarkable success across various NLP tasks, Retrieval Augmented Generation
Model stands out to be highly effective on downstream applications like
Question Answering. Recently, RAG-end2end model further optimized the
architecture and achieved notable performance improvements on domain
adaptation. However, the effectiveness of these RAG-based architectures remains
relatively unexplored when fine-tuned on specialized domains such as customer
service for building a reliable conversational AI system. Furthermore, a
critical challenge persists in reducing the occurrence of hallucinations while
maintaining high domain-specific accuracy. In this paper, we investigated the
performance of diverse RAG and RAG-like architectures through domain adaptation
and evaluated their ability to generate accurate and relevant response grounded
in the contextual knowledge base. To facilitate the evaluation of the models,
we constructed a novel dataset HotelConvQA, sourced from wide range of
hotel-related conversations and fine-tuned all the models on our domain
specific dataset. We also addressed a critical research gap on determining the
impact of domain adaptation on reducing hallucinations across different RAG
architectures, an aspect that was not properly measured in prior work. Our
evaluation shows positive results in all metrics by employing domain
adaptation, demonstrating strong performance on QA tasks and providing insights
into their efficacy in reducing hallucinations. Our findings clearly indicate
that domain adaptation not only enhances the models' performance on QA tasks
but also significantly reduces hallucination across all evaluated RAG
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial Version fine-tuned on HotelConvQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Structures of Intertextuality in French Fiction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Barré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intertextuality is a key concept in literary theory that challenges
traditional notions of text, signification or authorship. It views texts as
part of a vast intertextual network that is constantly evolving and being
reconfigured. This paper argues that the field of computational literary
studies is the ideal place to conduct a study of intertextuality since we have
now the ability to systematically compare texts with each others. Specifically,
we present a work on a corpus of more than 12.000 French fictions from the
18th, 19th and early 20th century. We focus on evaluating the underlying roles
of two literary notions, sub-genres and the literary canon in the framing of
textuality. The article attempts to operationalize intertextuality using
state-of-the-art contextual language models to encode novels and capture
features that go beyond simple lexical or thematic approaches. Previous
research (Hughes, 2012) supports the existence of a literary "style of a time",
and our findings further reinforce this concept. Our findings also suggest that
both subgenres and canonicity play a significant role in shaping textual
similarities within French fiction. These discoveries point to the importance
of considering genre and canon as dynamic forces that influence the evolution
and intertextual connections of literary works within specific historical
contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures. Computational Humanities Research Conference
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Contrastive Editing of Gender Stereotypes <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlene Lutz, Rochelle Choenni, Markus Strohmaier, Anne Lauscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereotypical bias encoded in language models (LMs) poses a threat to safe
language technology, yet our understanding of how bias manifests in the
parameters of LMs remains incomplete. We introduce local contrastive editing
that enables the localization and editing of a subset of weights in a target
model in relation to a reference model. We deploy this approach to identify and
modify subsets of weights that are associated with gender stereotypes in LMs.
Through a series of experiments, we demonstrate that local contrastive editing
can precisely localize and control a small subset (< 0.5%) of weights that
encode gender bias. Our work (i) advances our understanding of how
stereotypical biases can manifest in the parameter space of LMs and (ii) opens
up new avenues for developing parameter-efficient strategies for controlling
model properties in a contrastive manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MojoBench: Language Modeling and Benchmarks for Mojo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishat Raihan, Joanna C. S. Santos, Marcos Zampieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced Mojo programming language (PL) by Modular, has
received significant attention in the scientific community due to its claimed
significant speed boost over Python. Despite advancements in code Large
Language Models (LLMs) across various PLs, Mojo remains unexplored in this
context. To address this gap, we introduce MojoBench, the first framework for
Mojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset
designed for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM
pretrained and finetuned for Mojo code generation, which supports instructions
in 5 natural languages (NLs). Our results show that Mojo-Coder achieves a
30-35% performance improvement over leading models like GPT-4o and
Claude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with
underrepresented and unseen PLs, offering potential strategies for enhancing
model adaptability. MojoBench contributes to our understanding of LLM
capabilities and limitations in emerging programming paradigms fostering more
robust code generation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialectal and Low Resource Machine Translation for Aromanian <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandru-Iulius Jerpelea, Alina-Ştefania Rădoi, Sergiu Nisioi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a neural machine translation system that can translate between
Romanian, English, and Aromanian (an endangered Eastern Romance language); the
first of its kind. BLEU scores range from 17 to 32 depending on the direction
and genre of the text. Alongside, we release the biggest known
Aromanian-Romanian bilingual corpus, consisting of 79k cleaned sentence pairs.
Additional tools such as an agnostic sentence embedder (used for both text
mining and automatic evaluation) and a diacritics converter are also presented.
We publicly release our findings and models. Finally, we describe the
deployment of our quantized model at https://arotranslate.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures, 6 tables, submitted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient
  Semantic Steering in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Longqin Jiang, Liang Ding, Xingshan Li, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive capabilities, large language models (LLMs) often
lack interpretability and can generate toxic content. While using LLMs as
foundation models and applying semantic steering methods are widely practiced,
we believe that efficient methods should be based on a thorough understanding
of LLM behavior. To this end, we propose using eye movement measures to
interpret LLM behavior across layers. We find that LLMs exhibit patterns
similar to human gaze across layers and different layers function differently.
Inspired by these findings, we introduce a heuristic steering layer selection
and apply it to layer intervention methods via fine-tuning and inference. Using
language toxification and detoxification as test beds, we demonstrate that our
proposed CogSteer methods achieve better results in terms of toxicity scores
while efficiently saving 97% of the computational resources and 60% of the
training time. Our model-agnostic approach can be adopted into various LLMs,
contributing to their interpretability and promoting trustworthiness for safe
deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beware of Calibration Data for Pruning Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are widely applied across various fields,
model compression has become increasingly crucial for reducing costs and
improving inference efficiency. Post-training pruning is a promising method
that does not require resource-intensive iterative training and only needs a
small amount of calibration data to assess the importance of parameters.
Previous research has primarily focused on designing advanced pruning methods,
while different calibration data's impact on pruning performance still lacks
systematical exploration. We fill this blank and surprisingly observe that the
effects of calibration data even value more than designing advanced pruning
strategies, especially for high sparsity. Our preliminary exploration also
discloses that using calibration data similar to the training data can yield
better performance. As pre-training data is usually inaccessible for advanced
LLMs, we further provide a self-generating calibration data synthesis strategy
to construct feasible calibration data. We conduct experiments on the recent
strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that
the proposed method outperforms commonly used calibration data and can
effectively enhance strong pruning methods (e.g., Wanda, OWL).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Adaptive Framework for Generating Systematic Explanatory Answer in
  Online Q&A Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Xiaobin Wang, Yong Jiang, Jinzhi Liao, Pengjun Xie, Fei Huang, Xiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) systems face challenges in handling complex questions
that require multi-domain knowledge synthesis. The naive RAG models, although
effective in information retrieval, struggle with complex questions that
require comprehensive and in-depth answers. The pioneering task is defined as
explanatory answer generation, which entails handling identified challenges
such as the requirement for comprehensive information and logical coherence
within the generated context. To address these issues, we refer to systematic
thinking theory and propose SynthRAG, an innovative framework designed to
enhance QA performance. SynthRAG improves on conventional models by employing
adaptive outlines for dynamic content structuring, generating systematic
information to ensure detailed coverage, and producing customized answers
tailored to specific user inquiries. This structured approach guarantees
logical coherence and thorough integration of information, yielding responses
that are both insightful and methodically organized. Empirical evaluations
underscore SynthRAG's effectiveness, demonstrating its superiority in handling
complex questions, overcoming the limitations of naive RAG models, and
significantly improving answer quality and depth. Furthermore, an online
deployment on the Zhihu platform revealed that SynthRAG's answers achieved
notable user engagement, with each response averaging 5.73 upvotes and
surpassing the performance of 79.8% of human contributors, highlighting the
practical relevance and impact of the proposed framework. Our code is available
at https://github.com/czy1999/SynthRAG .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Similarity-adjusted Surprisal Theory <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Meister, Mario Giulianelli, Tiago Pimentel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surprisal theory posits that the cognitive effort required to comprehend a
word is determined by its contextual predictability, quantified as surprisal.
Traditionally, surprisal theory treats words as distinct entities, overlooking
any potential similarity between them. Giulianelli et al. (2023) address this
limitation by introducing information value, a measure of predictability
designed to account for similarities between communicative units. Our work
leverages Ricotta and Szeidl's (2006) diversity index to extend surprisal into
a metric that we term similarity-adjusted surprisal, exposing a mathematical
relationship between surprisal and information value. Similarity-adjusted
surprisal aligns with information value when considering graded similarities
and reduces to standard surprisal when words are treated as distinct.
Experimental results with reading time data indicate that similarity-adjusted
surprisal adds predictive power beyond standard surprisal for certain datasets,
suggesting it serves as a complementary measure of comprehension effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Risks of Tool-assisted Rephrasing to Linguistic
  Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengying Wang, Andreas Spitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing assistants and large language models see widespread use in the
creation of text content. While their effectiveness for individual users has
been evaluated in the literature, little is known about their proclivity to
change language or reduce its richness when adopted by a large user base. In
this paper, we take a first step towards quantifying this risk by measuring the
semantic and vocabulary change enacted by the use of rephrasing tools on a
multi-domain corpus of human-generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promising potential in the medical
domain, assisting with tasks like clinical note generation and patient
communication. However, current LLMs are limited to text-based communication,
hindering their ability to interact with diverse forms of information in
clinical environments. Despite clinical agents succeeding in diverse signal
interaction, they are oriented to a single clinical scenario and hence fail for
broader applications. To evaluate clinical agents holistically, we propose
ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting
of 18 tasks across five key realistic clinical dimensions. Building on this, we
introduce ReflecTool, a novel framework that excels at utilizing
domain-specific tools within two stages. The first optimization stage
progressively enlarges a long-term memory by saving successful solving
processes and tool-wise experience of agents in a tiny pre-defined training
set. In the following inference stage, ReflecTool can search for supportive
successful demonstrations from already built long-term memory to guide the tool
selection strategy, and a verifier improves the tool usage according to the
tool-wise experience with two verification methods--iterative refinement and
candidate selection. Extensive experiments on ClinicalAgent Benchmark
demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points
and the well-established agent-based methods with 3 points, highlighting its
adaptability and effectiveness in solving complex clinical tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Markov Chain of Thought for Efficient Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Yang, Kai Fan, Minpeng Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain of Thought (CoT) of multi-step benefits from the logical structure of
the reasoning steps and task-specific actions, significantly enhancing the
mathematical reasoning capabilities of large language models. As the prevalence
of long CoT, the number of reasoning steps exceeds manageable token limits and
leads to higher computational demands. Inspired by the fundamental logic of
human cognition, ``derive, then reduce'', we conceptualize the standard
multi-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we
consider the mathematical reasoning task, defining each reasoning step as text
accompanied by a Python code snippet. To facilitate a longer reasoning path,
self-correction is enabled through interactions with the code interpreter. Our
MCoT aims to compress previous reasoning steps into a simplified question,
enabling efficient next-step inference without relying on a lengthy KV cache.
In our experiments, we curate the \texttt{MCoTInstruct} dataset, and the
empirical results indicate that MCoT not only significantly enhances efficiency
but also maintains comparable accuracy. While much remains to be explored, this
work paves the way for exploring the long CoT reasoning abilities of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMLPA: Language Model Linguistic Personality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Zheng, Xian Wang, Simo Hosio, Xiaoxian Xu, Lik-Hang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used in everyday life and
research. One of the most common use cases is conversational interactions,
enabled by the language generation capabilities of LLMs. Just as between two
humans, a conversation between an LLM-powered entity and a human depends on the
personality of the conversants. However, measuring the personality of a given
LLM is currently a challenge. This paper introduces the Language Model
Linguistic Personality Assessment (LMLPA), a system designed to evaluate the
linguistic personalities of LLMs. Our system helps to understand LLMs' language
generation capabilities by quantitatively assessing the distinct personality
traits reflected in their linguistic outputs. Unlike traditional human-centric
psychometrics, the LMLPA adapts a personality assessment questionnaire,
specifically the Big Five Inventory, to align with the operational capabilities
of LLMs, and also incorporates the findings from previous language-based
personality measurement literature. To mitigate sensitivity to the order of
options, our questionnaire is designed to be open-ended, resulting in textual
answers. Thus, the AI rater is needed to transform ambiguous personality
information from text responses into clear numerical indicators of personality
traits. Utilising Principal Component Analysis and reliability validations, our
findings demonstrate that LLMs possess distinct personality traits that can be
effectively quantified by the LMLPA. This research contributes to
Human-Computer Interaction and Human-Centered AI, providing a robust framework
for future studies to refine AI personality assessments and expand their
applications in multiple areas, including education and manufacturing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graphusion: A RAG Framework for Knowledge Graph Construction with a
  Global Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Boming Yang, Aosong Feng, Sixun Ouyang, Moritz Blum, Tianwei She, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) are crucial in the field of artificial intelligence
and are widely used in downstream tasks, such as question-answering (QA). The
construction of KGs typically requires significant effort from domain experts.
Large Language Models (LLMs) have recently been used for Knowledge Graph
Construction (KGC). However, most existing approaches focus on a local
perspective, extracting knowledge triplets from individual sentences or
documents, missing a fusion process to combine the knowledge in a global KG.
This work introduces Graphusion, a zero-shot KGC framework from free text. It
contains three steps: in Step 1, we extract a list of seed entities using topic
modeling to guide the final KG includes the most relevant entities; in Step 2,
we conduct candidate triplet extraction using LLMs; in Step 3, we design the
novel fusion module that provides a global view of the extracted knowledge,
incorporating entity merging, conflict resolution, and novel triplet discovery.
Results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for
entity extraction and relation recognition, respectively. Moreover, we showcase
how Graphusion could be applied to the Natural Language Processing (NLP) domain
and validate it in an educational scenario. Specifically, we introduce TutorQA,
a new expert-verified benchmark for QA, comprising six tasks and a total of
1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant
improvement on the benchmark, for example, a 9.2% accuracy improvement on
sub-graph completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2407.10794</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-model Control: Improving Multiple Large Language Models in
  One-time Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Wu, Hao Sun, Hengyi Cai, Lixin Su, Shuaiqiang Wang, Dawei Yin, Xiang Li, Ming Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of large language models (LLMs) with varying parameter scales and
vocabularies is increasing. While they deliver powerful performance, they also
face a set of common optimization needs to meet specific requirements or
standards, such as instruction following or avoiding the output of sensitive
information from the real world. However, how to reuse the fine-tuning outcomes
of one model to other models to reduce training costs remains a challenge. To
bridge this gap, we introduce Cross-model Control (CMC), a method that improves
multiple LLMs in one-time training with a portable tiny language model.
Specifically, we have observed that the logit shift before and after
fine-tuning is remarkably similar across different models. Based on this
insight, we incorporate a tiny language model with a minimal number of
parameters. By training alongside a frozen template LLM, the tiny model gains
the capability to alter the logits output by the LLMs. To make this tiny
language model applicable to models with different vocabularies, we propose a
novel token mapping strategy named PM-MinED. We have conducted extensive
experiments on instruction tuning and unlearning tasks, demonstrating the
effectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and
  Reward Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guijin Son, Dongkeun Yoon, Juyoung Suk, Javier Aula-Blasco, Mano Aslan, Vu Trong Kim, Shayekh Bin Islam, Jaume Prats-Cristià, Lucía Tormo-Bañuelos, Seungone Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are commonly used as evaluators in tasks (e.g.,
reward modeling, LLM-as-a-judge), where they act as proxies for human
preferences or judgments. This leads to the need for meta-evaluation:
evaluating the credibility of LLMs as evaluators. However, existing benchmarks
primarily focus on English, offering limited insight into LLMs' effectiveness
as evaluators in non-English contexts. To address this, we introduce MM-Eval, a
multilingual meta-evaluation benchmark that covers 18 languages across six
categories. MM-Eval evaluates various dimensions, including language-specific
challenges like linguistics and language hallucinations. Evaluation results
show that both proprietary and open-source language models have considerable
room for improvement. Further analysis reveals a tendency for these models to
assign middle-ground scores to low-resource languages. We publicly release our
benchmark and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Learning Needs Better Model Initialization and
  Self-Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivoline C. Ngong, Joseph P. Near, Niloofar Mireshghallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private SGD (DPSGD) enables privacy-preserving training of
language models, but often reduces utility, diversity, and linguistic quality.
We introduce DPRefine, a three-phase method that initializes a model using data
synthesis from a small pre-trained LM with rigorous filtering, applies DP
finetuning on private data, and performs self-distillation to refine outputs.
This approach significantly outperforms vanilla DPSGD, with AlpacaEval
preferring DPRefine's generations in 78.4% of cases across all datasets. Our
analysis reveals that DPRefine reduces linguistic errors in generated text by
84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.
It also reduces inconsistencies of non-private models, such as hallucinated
details and misattributed quotes. We find that small models like GPT-2 can be
effective for initialization and distillation, highlighting their potential in
enabling scalable and efficient deployment of privacy-preserving language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESpeW: Robust Copyright Protection for LLM-based EaaS via
  Embedding-Specific Watermark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongqi Wang, Baoyuan Wu, Jingyuan Deng, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings as a Service (EaaS) is emerging as a crucial role in AI
applications. Unfortunately, EaaS is vulnerable to model extraction attacks,
highlighting the urgent need for copyright protection.Although some preliminary
works propose applying embedding watermarks to protect EaaS, recent research
reveals that these watermarks can be easily removed. Hence, it is crucial to
inject robust watermarks resistant to watermark removal attacks.Existing
watermarking methods typically inject a target embedding into embeddings
through linear interpolation when the text contains triggers. However, this
mechanism results in each watermarked embedding having the same component,
which makes the watermark easy to identify and eliminate.Motivated by this, in
this paper, we propose a novel embedding-specific watermarking (ESpeW)
mechanism to offer robust copyright protection for EaaS. Our approach involves
injecting unique, yet readily identifiable watermarks into each embedding.
Watermarks inserted by ESpeW are designed to maintain a significant distance
from one another and to avoid sharing common components, thus making it
significantly more challenging to remove the watermarks.Extensive experiments
on four popular datasets demonstrate that ESpeW can even watermark successfully
against a highly aggressive removal strategy without sacrificing the quality of
embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProtoLens: Advancing Prototype Learning for Fine-Grained
  Interpretability in Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wei, Ziwei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved remarkable performance in various
text-based tasks but often lack interpretability, making them less suitable for
applications where transparency is critical. To address this, we propose
ProtoLens, a novel prototype-based model that provides fine-grained,
sub-sentence level interpretability for text classification. ProtoLens uses a
Prototype-aware Span Extraction module to identify relevant text spans
associated with learned prototypes and a Prototype Alignment mechanism to
ensure prototypes are semantically meaningful throughout training. By aligning
the prototype embeddings with human-understandable examples, ProtoLens provides
interpretable predictions while maintaining competitive accuracy. Extensive
experiments demonstrate that ProtoLens outperforms both prototype-based and
non-interpretable baselines on multiple text classification benchmarks. Code
and data are available at
\url{https://anonymous.4open.science/r/ProtoLens-CE0B/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Responsible Multilingual Large Language Models: A <span class="highlight-title">Survey</span> of Development,
  Applications, and Societal Impact 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhua Liu, Bin Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual Large Language Models (MLLMs) represent a pivotal advancement in
democratizing artificial intelligence across linguistic boundaries. While
theoretical foundations are well-established, practical implementation
guidelines remain scattered. This work bridges this gap by providing a
comprehensive end-to-end framework for developing and deploying MLLMs in
production environments. We make three distinctive contributions: First, we
present an actionable pipeline from data pre-processing through deployment,
integrating insights from academic research and industrial applications.
Second, using Llama2 as a case study, we provide detailed optimization
strategies for enhancing multilingual capabilities, including curriculum
learning approaches for balancing high-resource and low-resource languages,
tokenization strategies, and effective sampling methods. Third, we offer an
interdisciplinary analysis that considers technical, linguistic, and cultural
perspectives in MLLM development. Our findings reveal critical challenges in
supporting linguistic diversity, with 88.38% of world languages categorized as
low-resource, affecting over a billion speakers. We examine practical solutions
through real-world applications in customer service, search engines, and
machine translation. By synthesizing theoretical frameworks with
production-ready implementation strategies, this survey provides essential
guidance for practitioners and researchers working to develop more inclusive
and effective multilingual AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigate Complex Physical Worlds via Geometrically Constrained LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqiang Huang, Wentao Ye, Liyao Li, Junbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the potential of Large Language Models (LLMs) for
reconstructing and constructing the physical world solely based on textual
knowledge. It explores the impact of model performance on spatial understanding
abilities. To enhance the comprehension of geometric and spatial relationships
in the complex physical world, the study introduces a set of geometric
conventions and develops a workflow based on multi-layer graphs and multi-agent
system frameworks. It examines how LLMs achieve multi-step and multi-objective
geometric inference in a spatial environment using multi-layer graphs under
unified geometric conventions. Additionally, the study employs a genetic
algorithm, inspired by large-scale model knowledge, to solve geometric
constraint problems. In summary, this work innovatively explores the
feasibility of using text-based LLMs as physical world builders and designs a
workflow to enhance their capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile
  Device Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyong Lee, Dongyoon Hahm, June Suk Choi, W. Bradley Knox, Kimin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents powered by large language models (LLMs) show promising
potential in assistive tasks across various domains, including mobile device
control. As these agents interact directly with personal information and device
settings, ensuring their safe and reliable behavior is crucial to prevent
undesirable outcomes. However, no benchmark exists for standardized evaluation
of the safety of mobile device-control agents. In this work, we introduce
MobileSafetyBench, a benchmark designed to evaluate the safety of
device-control agents within a realistic mobile environment based on Android
emulators. We develop a diverse set of tasks involving interactions with
various mobile applications, including messaging and banking applications. To
clearly evaluate safety apart from general capabilities, we design separate
tasks measuring safety and tasks evaluating helpfulness. The safety tasks
challenge agents with managing potential risks prevalent in daily life and
include tests to evaluate robustness against indirect prompt injections. Our
experiments demonstrate that while baseline agents, based on state-of-the-art
LLMs, perform well in executing helpful tasks, they show poor performance in
safety tasks. To mitigate these safety concerns, we propose a prompting method
that encourages agents to prioritize safety considerations. While this method
shows promise in promoting safer behaviors, there is still considerable room
for improvement to fully earn user trust. This highlights the urgent need for
continued research to develop more robust safety mechanisms in mobile
environments. We open-source our benchmark at:
https://mobilesafetybench.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Still Exhibit Bias in <span class="highlight-title">Long</span> Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonje Jeung, Dongjae Jeon, Ashkan Yousefpour, Jonghyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing fairness benchmarks for large language models (LLMs) primarily focus
on simple tasks, such as multiple-choice questions, overlooking biases that may
arise in more complex scenarios like long-text generation. To address this gap,
we introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates
biases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10
demographic axes, including gender and race, resulting in 11,948 samples. By
assessing both model responses and the reasoning behind them, LTF-TEST uncovers
subtle biases that are difficult to detect in simple responses. In our
evaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two
key patterns of bias. First, these models frequently favor certain demographic
groups in their responses. Second, they show excessive sensitivity toward
traditionally disadvantaged groups, often providing overly protective responses
while neglecting others. To mitigate these biases, we propose FT-REGARD, a
finetuning approach that pairs biased prompts with neutral responses. FT-REGARD
reduces gender bias by 34.6% and improves performance by 1.4 percentage points
on the BBQ benchmark, offering a promising approach to addressing biases in
long-text generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 page, 38 figures, Neurips (SoLaR Workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanisms of Symbol Processing for In-<span class="highlight-title">Context</span> Learning in <span class="highlight-title">Transformer</span>
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Smolensky, Roland Fernandez, Zhenghao Herbert Zhou, Mattia Opper, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive abilities in symbol
processing through in-context learning (ICL). This success flies in the face of
decades of predictions that artificial neural networks cannot master abstract
symbol manipulation. We seek to understand the mechanisms that can enable
robust symbol processing in transformer networks, illuminating both the
unanticipated success, and the significant limitations, of transformers in
symbol processing. Borrowing insights from symbolic AI on the power of
Production System architectures, we develop a high-level language, PSL, that
allows us to write symbolic programs to do complex, abstract symbol processing,
and create compilers that precisely implement PSL programs in transformer
networks which are, by construction, 100% mechanistically interpretable. We
demonstrate that PSL is Turing Universal, so the work can inform the
understanding of transformer ICL in general. The type of transformer
architecture that we compile from PSL programs suggests a number of paths for
enhancing transformers' capabilities at symbol processing. (Note: The first
section of the paper gives an extended synopsis of the entire paper.)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>101 pages (including 30 pages of Appendices), 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xue, Qian Lou, Mengxin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attacking fairness is crucial because compromised models can introduce biased
outcomes, undermining trust and amplifying inequalities in sensitive
applications like hiring, healthcare, and law enforcement. This highlights the
urgent need to understand how fairness mechanisms can be exploited and to
develop defenses that ensure both fairness and robustness. We introduce
BadFair, a novel backdoored fairness attack methodology. BadFair stealthily
crafts a model that operates with accuracy and fairness under regular
conditions but, when activated by certain triggers, discriminates and produces
incorrect results for specific groups. This type of attack is particularly
stealthy and dangerous, as it circumvents existing fairness detection methods,
maintaining an appearance of fairness in normal use. Our findings reveal that
BadFair achieves a more than 85% attack success rate in attacks aimed at target
groups on average while only incurring a minimal accuracy loss. Moreover, it
consistently exhibits a significant discrimination score, distinguishing
between pre-defined target and non-target attacked groups across various
datasets and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoiceTextBlender: Augmenting Large Language Models with Speech
  Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Krishna C. Puvvada, Zhehuai Chen, Piotr Zelasko, He Huang, Kunal Dhawan, Ke Hu, Shinji Watanabe, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have augmented large language models (LLMs) with speech
capabilities, leading to the development of speech language models (SpeechLMs).
Earlier SpeechLMs focused on single-turn speech-based question answering (QA),
where user input comprised a speech context and a text question. More recent
studies have extended this to multi-turn conversations, though they often
require complex, multi-stage supervised fine-tuning (SFT) with diverse data.
Another critical challenge with SpeechLMs is catastrophic forgetting-where
models optimized for speech tasks suffer significant degradation in text-only
performance. To mitigate these issues, we propose a novel single-stage joint
speech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone.
Our joint SFT combines text-only SFT data with three types of speech-related
data: speech recognition and translation, speech-based QA, and mixed-modal SFT.
Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model
demonstrates superior performance across various speech benchmarks while
preserving the original capabilities on text-only tasks. Furthermore, our model
shows emergent abilities of effectively handling previously unseen prompts and
tasks, including multi-turn, mixed-modal inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Client is Reliable?: A Reliable and Personalized <span class="highlight-title">Prompt</span>-based
  Federated Learning for Medical Image Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional medical artificial intelligence (AI) models face barriers in
clinical application and ethical issues owing to their inability to handle the
privacy-sensitive characteristics of medical data. We present a novel
personalized federated learning (pFL) method for medical visual question
answering (VQA) models, addressing privacy reliability challenges in the
medical domain. Our method introduces learnable prompts into a Transformer
architecture to efficiently train it on diverse medical datasets without
massive computational costs. Then we introduce a reliable client VQA model that
incorporates Dempster-Shafer evidence theory to quantify uncertainty in
predictions, enhancing the model's reliability. Furthermore, we propose a novel
inter-client communication mechanism that uses maximum likelihood estimation to
balance accuracy and uncertainty, fostering efficient integration of insights
across clients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is artificial intelligence still intelligence? LLMs generalize to novel
  adjective-noun pairs, but don't mimic the full human distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayley Ross, Kathryn Davidson, Najoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferences from adjective-noun combinations like "Is artificial intelligence
still intelligence?" provide a good test bed for LLMs' understanding of meaning
and compositional generalization capability, since there are many combinations
which are novel to both humans and LLMs but nevertheless elicit convergent
human judgments. We study a range of LLMs and find that the largest models we
tested are able to draw human-like inferences when the inference is determined
by context and can generalize to unseen adjective-noun combinations. We also
propose three methods to evaluate LLMs on these inferences out of context,
where there is a distribution of human-like answers rather than a single
correct answer. We find that LLMs show a human-like distribution on at most
75\% of our dataset, which is promising but still leaves room for improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages (23 pages with appendix). Accepted to GenBench 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue
  Generation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqing He, Liang Zhu, Rui Wang, Xi Wang, Reza Haffari, Jiaxing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term memory is important for chatbots and dialogue systems (DS) to
create consistent and human-like conversations, evidenced by numerous developed
memory-augmented DS (MADS). To evaluate the effectiveness of such MADS,
existing commonly used evaluation metrics, like retrieval accuracy and
perplexity (PPL), mainly focus on query-oriented factualness and language
quality assessment. However, these metrics often lack practical value.
Moreover, the evaluation dimensions are insufficient for human-like assessment
in DS. Regarding memory-recalling paradigms, current evaluation schemes only
consider passive memory retrieval while ignoring diverse memory recall with
rich triggering factors, e.g., emotions and surroundings, which can be
essential in emotional support scenarios. To bridge the gap, we construct a
novel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering various
memory-recalling paradigms based on cognitive science and psychology theories.
The benchmark assesses two tasks separately: memory retrieval and memory
recognition with the incorporation of both passive and proactive memory recall
data. We introduce new scoring criteria to the evaluation, including memory
injection, emotion support (ES) proficiency, and intimacy, to comprehensively
assess generated responses. Results from cutting-edge embedding models and
large language models on this benchmark indicate the potential for further
advancement. Extensive testing further reveals correlations between memory
injection, ES proficiency, and intimacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Generative AI speak Nigerian-Pidgin?: Issues about
  Representativeness and Bias for Multilingualism in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Ifeoluwa Adelani, A. Seza Doğruöz, Iyanuoluwa Shode, Anuoluwapo Aremu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nigeria is a multilingual country with 500+ languages. Naija is a
Nigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed
language (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has
mainly been a spoken language until recently, there are now various platforms
publishing exclusively in Naija such as Naija Wikipedia. However, it is hard to
distinguish by non-native from a larger pidgin languages spoken across West
Africa known as West African Pidgin English (WAPE) -- which is more simplied
and understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news
platform publishes exclusively in WAPE to cater for several countries in West
Africa. In our paper, we show through statistical analyses and Machine
Translation experiments that these two creole varieties do not represent each
other (i.e., there are linguistic differences in word order and vocabulary) and
Generative AI operates only based on WAPE. In other words, Naija is
under-represented in Generative AI, and it is hard to teach LLMs with few
examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional Language Policy: A General Framework for Steerable
  Multi-Objective Finetuning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Wang, Rahul Kidambi, Ryan Sullivan, Alekh Agarwal, Christoph Dann, Andrea Michi, Marco Gelmi, Yunxuan Li, Raghav Gupta, Avinava Dubey, Alexandre Ramé, Johan Ferret, Geoffrey Cideron, Le Hou, Hongkun Yu, Amr Ahmed, Aranyak Mehta, Léonard Hussenot, Olivier Bachem, Edouard Leurent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward-based finetuning is crucial for aligning language policies with
intended behaviors (e.g., creativity and safety). A key challenge is to develop
steerable language models that trade-off multiple (conflicting) objectives in a
flexible and efficient manner. This paper presents Conditional Language Policy
(CLP), a general framework for finetuning language models on multiple
objectives. Building on techniques from multi-task training and
parameter-efficient finetuning, CLP learn steerable models that effectively
trade-off conflicting objectives at inference time. Notably, this does not
require training or maintaining multiple models to achieve different trade-offs
between the objectives. Through extensive experiments and ablations on two
summarization datasets, we show that CLP learns steerable language models that
outperform and Pareto-dominate the existing approaches for multi-objective
finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages. Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAR: SocioTechnical Approach to Red Teaming Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11757v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11757v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces STAR, a sociotechnical framework that improves on
current best practices for red teaming safety of large language models. STAR
makes two key contributions: it enhances steerability by generating
parameterised instructions for human red teamers, leading to improved coverage
of the risk surface. Parameterised instructions also provide more detailed
insights into model failures at no increased cost. Second, STAR improves signal
quality by matching demographics to assess harms for specific groups, resulting
in more sensitive annotations. STAR further employs a novel step of arbitration
to leverage diverse viewpoints and improve label reliability, treating
disagreement not as noise but as a valuable contribution to signal quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 5 pages appendix. * denotes equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proof of Thought : Neurosymbolic Program Synthesis allows Robust and
  Interpretable Reasoning <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debargha Ganguly, Srinivasan Iyengar, Vipin Chaudhary, Shivkumar Kalyanaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing,
yet they struggle with inconsistent reasoning, particularly in novel domains
and complex logical sequences. This research introduces Proof of Thought, a
framework that enhances the reliability and transparency of LLM outputs. Our
approach bridges LLM-generated ideas with formal logic verification, employing
a custom interpreter to convert LLM outputs into First Order Logic constructs
for theorem prover scrutiny. Central to our method is an intermediary
JSON-based Domain-Specific Language, which by design balances precise logical
structures with intuitive human concepts. This hybrid representation enables
both rigorous validation and accessible human comprehension of LLM reasoning
processes. Key contributions include a robust type system with sort management
for enhanced logical integrity, explicit representation of rules for clear
distinction between factual and inferential knowledge, and a flexible
architecture that allows for easy extension to various domain-specific
applications. We demonstrate Proof of Thought's effectiveness through
benchmarking on StrategyQA and a novel multimodal reasoning task, showing
improved performance in open-ended scenarios. By providing verifiable and
interpretable results, our technique addresses critical needs for AI system
accountability and sets a foundation for human-in-the-loop oversight in
high-stakes domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024) System 2 Reasoning At Scale Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlleNoise: large-scale text classification benchmark <span class="highlight-title">dataset</span> with
  real-world label noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicja Rączkowska, Aleksandra Osowska-Kurczab, Jacek Szczerbiński, Kalina Jasinska-Kobus, Klaudia Nazarko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label noise remains a challenge for training robust classification models.
Most methods for mitigating label noise have been benchmarked using primarily
datasets with synthetic noise. While the need for datasets with realistic noise
distribution has partially been addressed by web-scraped benchmarks such as
WebVision and Clothing1M, those benchmarks are restricted to the computer
vision domain. With the growing importance of Transformer-based models, it is
crucial to establish text classification benchmarks for learning with noisy
labels. In this paper, we present AlleNoise, a new curated text classification
benchmark dataset with real-world instance-dependent label noise, containing
over 500,000 examples across approximately 5,600 classes, complemented with a
meaningful, hierarchical taxonomy of categories. The noise distribution comes
from actual users of a major e-commerce marketplace, so it realistically
reflects the semantics of human mistakes. In addition to the noisy labels, we
provide human-verified clean labels, which help to get a deeper insight into
the noise distribution, unlike web-scraped datasets typically used in the
field. We demonstrate that a representative selection of established methods
for learning with noisy labels is inadequate to handle such real-world noise.
In addition, we show evidence that these algorithms do not alleviate excessive
memorization. As such, with AlleNoise, we set the bar high for the development
of label noise methods that can handle real-world label noise in text
classification tasks. The code and dataset are available for download at
https://github.com/allegro/AlleNoise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotator-Centric Active Learning for Subjective NLP Tasks <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15720v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15720v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michiel van der Meer, Neele Falk, Pradeep K. Murukannaiah, Enrico Liscio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Learning (AL) addresses the high costs of collecting human annotations
by strategically annotating the most informative samples. However, for
subjective NLP tasks, incorporating a wide range of perspectives in the
annotation process is crucial to capture the variability in human judgments. We
introduce Annotator-Centric Active Learning (ACAL), which incorporates an
annotator selection strategy following data sampling. Our objective is
two-fold: 1) to efficiently approximate the full diversity of human judgments,
and 2) to assess model performance using annotator-centric metrics, which value
minority and majority perspectives equally. We experiment with multiple
annotator selection strategies across seven subjective NLP tasks, employing
both traditional and novel, human-centered evaluation metrics. Our findings
indicate that ACAL improves data efficiency and excels in annotator-centric
performance evaluations. However, its success depends on the availability of a
sufficiently large and diverse pool of annotators to sample from.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Truly Grasp Mathematics? An Empirical
  Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Baosheng Wang, Jinshu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their proficiency in math tasks, the mechanisms underlying LLMs'
mathematical reasoning abilities remain a subject of debate. Recent studies
suggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning
by encouraging LLMs to employ human-like logical reasoning (System 2), enabling
them to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs
genuinely possess System 2-like logical reasoning, we introduced targeted
modifications to CRT problems. Our findings reveal that, despite the use of CoT
prompts, mainstream LLMs, including the latest o1-preview model, continue to
exhibit a significant error rate. Further analysis indicates that they
predominantly rely on System 1-like intuitive reasoning and pattern matching
derived from training data, rather than demonstrating mastery of mathematical
thinking. This discovery challenges the prevailing notion that LLMs possess
genuine logical reasoning abilities and that CoT can enhance them.
Consequently, this work may temper overly optimistic projections regarding
LLMs' advancement toward artificial general intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Adversarial Concept Erasure <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12091v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12091v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Michael Twiton, Yoav Goldberg, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural models trained on textual data rely on pre-trained
representations that emerge without direct supervision. As these
representations are increasingly being used in real-world applications, the
inability to \emph{control} their content becomes an increasingly important
problem. We formulate the problem of identifying and erasing a linear subspace
that corresponds to a given concept, in order to prevent linear predictors from
recovering the concept. We model this problem as a constrained, linear maximin
game, and show that existing solutions are generally not optimal for this task.
We derive a closed-form solution for certain objectives, and propose a convex
relaxation, \method, that works well for others. When evaluated in the context
of binary gender removal, the method recovers a low-dimensional subspace whose
removal mitigates bias by intrinsic and extrinsic evaluation. We show that the
method is highly expressive, effectively mitigating bias in deep nonlinear
classifiers while maintaining tractability and interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICML 2022; a revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Slow Generating: An Empirical Study on Large and Small Language
  Models Collaborative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyan Zhang, Jianyu Wang, Ning Ding, Biqing Qi, Ermo Hua, Xingtai Lv, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit impressive capabilities across various
applications but encounter substantial challenges such as high inference
latency, considerable training costs, and the generation of hallucinations.
Collaborative decoding between large and small language models (SLMs) presents
a promising strategy to mitigate these issues through methods including
speculative decoding, contrastive decoding, and emulator or proxy fine-tuning.
However, the specifics of such collaborations, particularly from a unified
perspective, remain largely unexplored. Inspired by dual-process cognitive
theory, we propose a unified framework in this paper, termed Fast and Slow
Generating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)
are categorized as System 2 (slow and deliberate), while independent SLMs are
designated as System 1 (fast and intuitive). We provide a comprehensive
analysis of these collaborative methodologies, elucidating their common
properties and shedding light on the differential knowledge capabilities of
System 2 versus System 1 through the FS-GEN framework. Our findings indicate
that only a small proportion of collaborative interactions (approximately less
than 20\% in most instances) are necessary across various methods. These
interactions between System 1 and System 2 conform to a scaling law related to
the parameter ratios, enabling predictable collaboration. Furthermore, we
explore the specific conditions under which collaboration proves most
effective, particularly from an uncertainty perspective, offering novel
insights that may guide future optimization efforts. Our research underscores
that the fundamental distinction between System 1 and System 2 lies in the
uncertainty of next token predictions, where interventions by System 2 are
crucial to support System 1. Code for Reproduction:
https://github.com/TsinghuaC3I/FS-GEN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update figures and results on Pythia Series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocoMotion: Learning Motion-Focused Video-Language Representations <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazel Doughty, Fida Mohammad Thoker, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper strives for motion-focused video-language representations.
Existing methods to learn video-language representations use spatial-focused
data, where identifying the objects and scene is often enough to distinguish
the relevant caption. We instead propose LocoMotion to learn from
motion-focused captions that describe the movement and temporal progression of
local object motions. We achieve this by adding synthetic motions to videos and
using the parameters of these motions to generate corresponding captions.
Furthermore, we propose verb-variation paraphrasing to increase the caption
variety and learn the link between primitive motions and high-level verbs. With
this, we are able to learn a motion-focused video-language representation.
Experiments demonstrate our approach is effective for a variety of downstream
tasks, particularly when limited data is available for fine-tuning. Code is
available: https://hazeldoughty.github.io/Papers/LocoMotion/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconfidencing LLMs from the Grouping Loss Perspective <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04957v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04957v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, Gaël Varoquaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to
generating hallucinated answers in a confident tone. While efforts to elicit
and calibrate confidence scores have proven useful, recent findings show that
controlling uncertainty must go beyond calibration: predicted scores may
deviate significantly from the actual posterior probabilities due to the impact
of grouping loss. In this work, we construct a new evaluation dataset derived
from a knowledge base to assess confidence scores given to answers of Mistral
and LLaMA. Experiments show that they tend to be overconfident. Further, we
show that they are more overconfident on some answers than others, \emph{eg}
depending on the nationality of the person in the query. In
uncertainty-quantification theory, this is grouping loss. To address this, we
propose a solution to reconfidence LLMs, canceling not only calibration but
also grouping loss. The LLMs, after the reconfidencing process, indicate
improved confidence alignment with the accuracy of their responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TravelPlanner: A Benchmark for Real-World Planning with Language Agents <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01622v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01622v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning has been part of the core pursuit for artificial intelligence since
its conception, but earlier AI agents mostly focused on constrained settings
because many of the cognitive substrates necessary for human-level planning
have been lacking. Recently, language agents powered by large language models
(LLMs) have shown interesting capabilities such as tool use and reasoning. Are
these language agents capable of planning in more complex settings that are out
of the reach of prior AI agents? To advance this investigation, we propose
TravelPlanner, a new planning benchmark that focuses on travel planning, a
common real-world planning scenario. It provides a rich sandbox environment,
various tools for accessing nearly four million data records, and 1,225
meticulously curated planning intents and reference plans. Comprehensive
evaluations show that the current language agents are not yet capable of
handling such complex planning tasks-even GPT-4 only achieves a success rate of
0.6%. Language agents struggle to stay on task, use the right tools to collect
information, or keep track of multiple constraints. However, we note that the
mere possibility for language agents to tackle such a complex problem is in
itself non-trivial progress. TravelPlanner provides a challenging yet
meaningful testbed for future language agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trends in Integration of Knowledge and Large Language Models: A <span class="highlight-title">Survey</span>
  and Taxonomy of Methods, Benchmarks, and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit superior performance on various natural
language tasks, but they are susceptible to issues stemming from outdated data
and domain-specific limitations. In order to address these challenges,
researchers have pursued two primary strategies, knowledge editing and
retrieval augmentation, to enhance LLMs by incorporating external information
from different aspects. Nevertheless, there is still a notable absence of a
comprehensive survey. In this paper, we propose a review to discuss the trends
in integration of knowledge and large language models, including taxonomy of
methods, benchmarks, and applications. In addition, we conduct an in-depth
analysis of different methods and point out potential research directions in
the future. We hope this survey offers the community quick access and a
comprehensive overview of this research area, with the intention of inspiring
future research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; 22 pages. This work has been submitted to the IEEE
  for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task <span class="highlight-title">Prompt</span> Vectors: Effective Initialization through Multi-Task
  Soft-<span class="highlight-title">Prompt</span> Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Belanec, Simon Ostermann, Ivan Srba, Maria Bielikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning is an efficient solution for training large language models
(LLMs). However, current soft-prompt-based methods often sacrifice multi-task
modularity, requiring the training process to be fully or partially repeated
for each newly added task. While recent work on task vectors applied arithmetic
operations on full model weights to achieve the desired multi-task performance,
a similar approach for soft-prompts is still missing. To this end, we introduce
Task Prompt Vectors, created by element-wise difference between weights of
tuned soft-prompts and their random initialization. Experimental results on 12
NLU datasets show that task prompt vectors can be used in low-resource settings
to effectively initialize prompt tuning on similar tasks. In addition, we show
that task prompt vectors are independent of the random initialization of prompt
tuning on 2 different language model architectures. This allows prompt
arithmetics with the pre-trained vectors from different tasks. In this way, we
provide a competitive alternative to state-of-the-art baselines by arithmetic
addition of task prompt vectors from multiple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let Me Teach You: Pedagogical Foundations of Feedback for Language
  Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00279v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00279v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatriz Borges, Niket Tandon, Tanja Käser, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Feedback (NLF) is an increasingly popular mechanism for
aligning Large Language Models (LLMs) to human preferences. Despite the
diversity of the information it can convey, NLF methods are often hand-designed
and arbitrary, with little systematic grounding. At the same time, research in
learning sciences has long established several effective feedback models. In
this opinion piece, we compile ideas from pedagogy to introduce FELT, a
feedback framework for LLMs that outlines various characteristics of the
feedback space, and a feedback content taxonomy based on these variables,
providing a general mapping of the feedback space. In addition to streamlining
NLF designs, FELT also brings out new, unexplored directions for research in
NLF. We make our taxonomy available to the community, providing guides and
examples for mapping our categorizations to future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024; 9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein
  Representation and Origin Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Gou, Wenhui Ge, Yang Tan, Mingchen Li, Guisheng Fan, Huiqun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein structures are important for understanding their functions and
interactions. Currently, many protein structure prediction methods are
enriching the structure database. Discriminating the origin of structures is
crucial for distinguishing between experimentally resolved and computationally
predicted structures, evaluating the reliability of prediction methods, and
guiding downstream biological studies. Building on works in structure
prediction, We developed a structure-sensitive supervised deep learning model,
Crystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent
and discriminate the origin of protein structures. CPE-Pro learns the
structural information of proteins and captures inter-structural differences to
achieve accurate traceability on four data classes, and is expected to be
extended to more. Simultaneously, we utilized Foldseek to encode protein
structures into "structure-sequences" and trained a protein Structural Sequence
Language Model, SSLM. Preliminary experiments demonstrated that, compared to
large-scale protein language models pre-trained on vast amounts of amino acid
sequences, the "structure-sequence" enables the language model to learn more
informative protein features, enhancing and optimizing structural
representations. We have provided the code, model weights, and all related
materials on https://github.com/GouWenrui/CPE-Pro-main.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality
  Testset designed for LLMs with Psychometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung Yeo, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have led to their
adaptation in various domains as conversational agents. We wonder: can
personality tests be applied to these agents to analyze their behavior, similar
to humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice
questions designed to assess the personality of LLMs. TRAIT is built on two
psychometrically validated small human questionnaires, Big Five Inventory (BFI)
and Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a
variety of real-world scenarios. TRAIT also outperforms existing personality
tests for LLMs in terms of reliability and validity, achieving the highest
scores across four key metrics: Content Validity, Internal Validity, Refusal
Rate, and Reliability. Using TRAIT, we reveal two notable insights into
personalities of LLMs: 1) LLMs exhibit distinct and consistent personality,
which is highly influenced by their training data (e.g., data used for
alignment tuning), and 2) current prompting techniques have limited
effectiveness in eliciting certain traits, such as high psychopathy or low
conscientiousness, suggesting the need for further research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribute or Abstain: Large Language Models as <span class="highlight-title">Long</span> Document Assistants <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Buchmann, Xiao Liu, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs can help humans working with long documents, but are known to
hallucinate. Attribution can increase trust in LLM responses: The LLM provides
evidence that supports its response, which enhances verifiability. Existing
approaches to attribution have only been evaluated in RAG settings, where the
initial retrieval confounds LLM performance. This is crucially different from
the long document setting, where retrieval is not needed, but could help. Thus,
a long document specific evaluation of attribution is missing. To fill this
gap, we present LAB, a benchmark of 6 diverse long document tasks with
attribution, and experiments with different approaches to attribution on 5 LLMs
of different sizes.
  We find that citation, i.e. response generation and evidence extraction in
one step, performs best for large and fine-tuned models, while additional
retrieval can help for small, prompted models. We investigate whether the "Lost
in the Middle'' phenomenon exists for attribution, but do not find this. We
also find that evidence quality can predict response quality on datasets with
simple responses, but not so for complex responses, as models struggle with
providing evidence for complex claims.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024. Code and data:
  https://github.com/UKPLab/arxiv2024-attribute-or-abstain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I've Got 99 Problems But FLOPS Ain't One 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandru M. Gherghescu, Vlad-Andrei Bădoiu, Alexandru Agache, Mihai-Valentin Dumitru, Iuliu Vasilescu, Radu Mantu, Costin Raiciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperscalers dominate the landscape of large network deployments, yet they
rarely share data or insights about the challenges they face. In light of this
supremacy, what problems can we find to solve in this space? We take an
unconventional approach to find relevant research directions, starting from
public plans to build a $100 billion datacenter for machine learning
applications. Leveraging the language models scaling laws, we discover what
workloads such a datacenter might carry and explore the challenges one may
encounter in doing so, with a focus on networking research. We conclude that
building the datacenter and training such models is technically possible, but
this requires novel wide-area transports for inter-DC communication, a
multipath transport and novel datacenter topologies for intra-datacenter
communication, high speed scale-up networks and transports, outlining a rich
research agenda for the networking community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting <span class="highlight-title">Context</span> Look-ups in <span class="highlight-title">Transformer</span>s: Investigating
  Attention-MLP Interactions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Neo, Shay B. Cohen, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the inner workings of large language models (LLMs) is crucial
for advancing their theoretical foundations and real-world applications. While
the attention mechanism and multi-layer perceptrons (MLPs) have been studied
independently, their interactions remain largely unexplored. This study
investigates how attention heads and next-token neurons interact in LLMs to
predict new words. We propose a methodology to identify next-token neurons,
find prompts that highly activate them, and determine the upstream attention
heads responsible. We then generate and evaluate explanations for the activity
of these attention heads in an automated manner. Our findings reveal that some
attention heads recognize specific contexts relevant to predicting a token and
activate a downstream token-predicting neuron accordingly. This mechanism
provides a deeper understanding of how attention heads work with MLP neurons to
perform next-token prediction. Our approach offers a foundation for further
research into the intricate workings of LLMs and their impact on text
generation and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Adversarial <span class="highlight-title">Prompt</span> Learning on Vision-Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Zhou, Xiaobo Xia, Zhiwei Lin, Bo Han, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vulnerability of deep neural networks to imperceptible adversarial
perturbations has attracted widespread attention. Inspired by the success of
vision-language foundation models, previous efforts achieved zero-shot
adversarial robustness by aligning adversarial visual features with text
supervision. However, in practice, they are still unsatisfactory due to several
issues, including heavy adaptation cost, suboptimal text supervision, and
uncontrolled natural generalization capacity. In this paper, to address these
issues, we propose a few-shot adversarial prompt framework where adapting input
sequences with limited data makes significant adversarial robustness
improvement. Specifically, we achieve this by providing adversarially
correlated text supervision that is end-to-end learned from adversarial
examples. We also propose a novel training objective that enhances the
consistency of multi-modal features while encourages differentiated uni-modal
features between natural and adversarial examples. The proposed framework gives
access to learn adversarial text supervision, which provides superior
cross-modal adversarial alignment and matches state-of-the-art zero-shot
adversarial robustness with only 1% training data. Code is available at:
https://github.com/lionel-w2/FAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Have an English Accent? Evaluating and
  Improving the Naturalness of Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhu Guo, Simone Conia, Zelin Zhou, Min Li, Saloni Potdar, Henry Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Models (LLMs) are predominantly designed with English
as the primary language, and even the few that are multilingual tend to exhibit
strong English-centric biases. Much like speakers who might produce awkward
expressions when learning a second language, LLMs often generate unnatural
outputs in non-English languages, reflecting English-centric patterns in both
vocabulary and grammar. Despite the importance of this issue, the naturalness
of multilingual LLM outputs has received limited attention. In this paper, we
address this gap by introducing novel automatic corpus-level metrics to assess
the lexical and syntactic naturalness of LLM outputs in a multilingual context.
Using our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark
in French and Chinese, revealing a tendency towards English-influenced
patterns. To mitigate this issue, we also propose a simple and effective
alignment method to improve the naturalness of an LLM in a target language and
domain, achieving consistent improvements in naturalness without compromising
the performance on general-purpose benchmarks. Our work highlights the
importance of developing multilingual metrics, resources and methods for the
new wave of multilingual LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RaTEScore: A Metric for Radiology Report Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel, entity-aware metric, termed as Radiological
Report (Text) Evaluation (RaTEScore), to assess the quality of medical reports
generated by AI models. RaTEScore emphasizes crucial medical entities such as
diagnostic outcomes and anatomical details, and is robust against complex
medical synonyms and sensitive to negation expressions. Technically, we
developed a comprehensive medical NER dataset, RaTE-NER, and trained an NER
model specifically for this purpose. This model enables the decomposition of
complex radiological reports into constituent medical entities. The metric
itself is derived by comparing the similarity of entity embeddings, obtained
from a language model, based on their types and relevance to clinical
significance. Our evaluations demonstrate that RaTEScore aligns more closely
with human preference than existing metrics, validated both on established
public benchmarks and our newly proposed RaTE-Eval benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Language Models Induce Grammatical Knowledge from Indirect Evidence? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miyu Oba, Yohei Oseki, Akiyo Fukatsu, Akari Haga, Hiroki Ouchi, Taro Watanabe, Saku Sugawara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What kinds of and how much data is necessary for language models to induce
grammatical knowledge to judge sentence acceptability? Recent language models
still have much room for improvement in their data efficiency compared to
humans. This paper investigates whether language models efficiently use
indirect data (indirect evidence), from which they infer sentence
acceptability. In contrast, humans use indirect evidence efficiently, which is
considered one of the inductive biases contributing to efficient language
acquisition. To explore this question, we introduce the Wug InDirect Evidence
Test (WIDET), a dataset consisting of training instances inserted into the
pre-training data and evaluation instances. We inject synthetic instances with
newly coined wug words into pretraining data and explore the model's behavior
on evaluation data that assesses grammatical acceptability regarding those
words. We prepare the injected instances by varying their levels of
indirectness and quantity. Our experiments surprisingly show that language
models do not induce grammatical knowledge even after repeated exposure to
instances with the same structure but differing only in lexical items from
evaluation instances in certain language phenomena. Our findings suggest a
potential direction for future research: developing models that use latent
indirect evidence to induce grammatical knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Review</span> of Prominent Paradigms for LLM-Based Agents: Tool Use
  (Including RAG), Planning, and Feedback Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05804v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05804v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool use, planning, and feedback learning are currently three prominent
paradigms for developing Large Language Model (LLM)-based agents across various
tasks. Although numerous frameworks have been devised for each paradigm, their
intricate workflows and inconsistent taxonomy create challenges in
understanding and reviewing the frameworks across different paradigms. This
survey introduces a unified taxonomy to systematically review and discuss these
frameworks. Specifically, 1) the taxonomy defines environments/tasks, common
LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),
and universally applicable workflows found in prior work, and 2) it enables a
comparison of key perspectives on the implementations of LMPRs and workflow
designs across different agent paradigms and frameworks. 3) Finally, we
identify three limitations in existing workflow designs and systematically
discuss the future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAMMAR: Grounded and Modular Methodology for Assessment of
  Closed-Domain Retrieval-Augmented Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19232v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19232v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Li, Ming Liu, Shang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems are widely used across various
industries for querying closed-domain and in-house knowledge bases. However,
evaluating these systems presents significant challenges due to the private
nature of closed-domain data and a scarcity of queries with verifiable ground
truths. Moreover, there is a lack of analytical methods to diagnose problematic
modules and identify types of failure, such as those caused by knowledge
deficits or issues with robustness. To address these challenges, we introduce
GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation
framework comprising a grounded data generation process and an evaluation
protocol that effectively pinpoints defective modules. Our validation
experiments reveal that GRAMMAR provides a reliable approach for identifying
vulnerable modules and supports hypothesis testing for textual form
vulnerabilities. An open-source tool accompanying this framework is available
in our GitHub repository (see https://github.com/xinzhel/grammar), allowing for
easy reproduction of our results and enabling reliable and modular evaluation
in closed-domain settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on
  CPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinheng Wang, Hansong Zhou, Ting Song, Shaoguang Mao, Shuming Ma, Hongyu Wang, Yan Xia, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and
BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs
in terms of speed and energy consumption. These developments also enable local
LLM deployment across a broad range of devices. In this work, we introduce
bitnet.cpp, a tailored software stack designed to unlock the full potential of
1-bit LLMs. Specifically, we develop a set of kernels to support fast and
lossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments
demonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x
to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model
sizes. The code is available at https://github.com/microsoft/BitNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Keywords to Structured Summaries: Streamlining Scholarly
  Information Access <span class="chip">ISWC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Shamsabadi, Jennifer D'Souza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper highlights the growing importance of information retrieval (IR)
engines in the scientific community, addressing the inefficiency of traditional
keyword-based search engines due to the rising volume of publications. The
proposed solution involves structured records, underpinning advanced
information technology (IT) tools, including visualization dashboards, to
revolutionize how researchers access and filter articles, replacing the
traditional text-heavy approach. This vision is exemplified through a proof of
concept centered on the "reproductive number estimate of infectious diseases"
research theme, using a fine-tuned large language model (LLM) to automate the
creation of structured records to populate a backend database that now goes
beyond keywords. The result is a next-generation information access system as
an IR method accessible at https://orkg.org/usecases/r0-estimates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures | Accepted for publication as a poster paper at
  the International Semantic Web Conference (ISWC 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Plan for Retrieval-Augmented Large Language Models from
  Knowledge Graphs <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14282v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14282v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the performance of large language models (LLMs) in complex
question-answering (QA) scenarios has always been a research focal point.
Recent studies have attempted to enhance LLMs' performance by combining
step-wise planning with external retrieval. While effective for advanced models
like GPT-3.5, smaller LLMs face challenges in decomposing complex questions,
necessitating supervised fine-tuning. Previous work has relied on manual
annotation and knowledge distillation from teacher LLMs, which are
time-consuming and not accurate enough. In this paper, we introduce a novel
framework for enhancing LLMs' planning capabilities by using planning data
derived from knowledge graphs (KGs). LLMs fine-tuned with this data have
improved planning capabilities, better equipping them to handle complex QA
tasks that involve retrieval. Evaluations on multiple datasets, including our
newly proposed benchmark, highlight the effectiveness of our framework and the
benefits of KG-derived planning data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Estimation and Quantification for LLMs: A Simple Supervised
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15993v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15993v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyu Liu, Yu Pan, Xiaocheng Li, Guanting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of uncertainty estimation and calibration
for LLMs. We begin by formulating the uncertainty estimation problem, a
relevant yet underexplored area in existing literature. We then propose a
supervised approach that leverages labeled datasets to estimate the uncertainty
in LLMs' responses. Based on the formulation, we illustrate the difference
between the uncertainty estimation for LLMs and that for standard ML models and
explain why the hidden neurons of the LLMs may contain uncertainty information.
Our designed approach demonstrates the benefits of utilizing hidden activations
to enhance uncertainty estimation across various tasks and shows robust
transferability in out-of-distribution settings. We distinguish the uncertainty
estimation task from the uncertainty calibration task and show that better
uncertainty estimation leads to better calibration performance. Furthermore,
our method is easy to implement and adaptable to different levels of model
accessibility including black box, grey box, and white box.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularizing Hidden States Enables Learning Generalizable Reward Model
  for LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models trained on human preference data have been proven to
effectively align Large Language Models (LLMs) with human intent within the
framework of reinforcement learning from human feedback (RLHF). However,
current reward models have limited generalization capabilities to unseen
prompts and responses, which can lead to an unexpected phenomenon known as
reward over-optimization, resulting in a decline in actual performance due to
excessive optimization of rewards. While previous research has advocated for
constraining policy optimization, our study introduces a novel approach to
enhance the reward model's generalization ability against distribution shifts
by regularizing the hidden states. Specifically, we retain the base model's
language model head and incorporate a suite of text-generation losses to
preserve the hidden states' text-generation capabilities, while concurrently
learning a reward head behind the same hidden states. Our experimental results
demonstrate that the introduced regularization technique markedly improves the
accuracy of learned reward models across a variety of out-of-distribution (OOD)
tasks and effectively alleviates the over-optimization issue in RLHF, offering
a more reliable and robust preference learning paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning
  in Large Language Models <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03622v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03622v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited impressive performance in
language comprehension and various reasoning tasks. However, their abilities in
spatial reasoning, a crucial aspect of human cognition, remain relatively
unexplored. Human possess a remarkable ability to create mental images of
unseen objects and actions through a process known as the Mind's Eye, enabling
the imagination of the unseen world. Inspired by this cognitive capacity, we
propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial
reasoning of LLMs by visualizing their reasoning traces, thereby guiding
subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning
tasks, including natural language navigation, visual navigation, and visual
tiling in 2D grid worlds. Experimental results demonstrated that VoT
significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT
outperformed existing multimodal large language models (MLLMs) in these tasks.
While VoT works surprisingly well on LLMs, the ability to generate mental
images to facilitate spatial reasoning resembles the mind's eye process,
suggesting its potential viability in MLLMs. Please find the dataset and codes
at https://microsoft.github.io/visualization-of-thought
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-SW3: An Autoregressive Language Model for the Nordic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12987v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12987v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Ekgren, Amaru Cuba Gyllensten, Felix Stollenwerk, Joey Öhman, Tim Isbister, Evangelia Gogoulou, Fredrik Carlsson, Alice Heiman, Judit Casademont, Magnus Sahlgren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper details the process of developing the first native large
generative language model for the Nordic languages, GPT-SW3. We cover all parts
of the development process, from data collection and processing, training
configuration and instruction finetuning, to evaluation and considerations for
release strategies. We hope that this paper can serve as a guide and reference
for other researchers that undertake the development of large generative models
for smaller languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-myopic Generation of Language Model for Reasoning and Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated remarkable abilities in reasoning and
planning by breaking down complex problems into sequential steps. Despite their
success in various domains like mathematical problem-solving and coding, LLMs
face challenges in ensuring reliable and optimal planning due to their inherent
myopic nature of autoregressive decoding. This paper revisits LLM reasoning
from an optimal-control perspective, proposing a novel method,
Predictive-Decoding, that leverages Model Predictive Control to enhance
planning accuracy. By re-weighting LLM distributions based on foresight
trajectories, Predictive-Decoding aims to mitigate early errors and promote
non-myopic planning. Our experiments show significant improvements in a wide
range of tasks for math, coding, and agents. Furthermore, Predictive-Decoding
demonstrates computational efficiency, outperforming search baselines with
reduced computational resources. This study provides insights into optimizing
LLM planning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative AI Security: Challenges and Countermeasures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Banghua Zhu, Norman Mu, Jiantao Jiao, David Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI's expanding footprint across numerous industries has led to
both excitement and increased scrutiny. This paper delves into the unique
security challenges posed by Generative AI, and outlines potential research
directions for managing these risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMU: Your Swiss Army Knife for Music Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Zhao, Zhi Zhong, Zhuoyuan Mao, Shiqi Yang, Wei-Hsiang Liao, Shusuke Takahashi, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OpenMU-Bench, a large-scale benchmark suite for addressing the
data scarcity issue in training multimodal language models to understand music.
To construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new
annotations. OpenMU-Bench also broadens the scope of music understanding by
including lyrics understanding and music tool usage. Using OpenMU-Bench, we
trained our music understanding model, OpenMU, with extensive ablations,
demonstrating that OpenMU outperforms baseline models such as MU-Llama. Both
OpenMU and OpenMU-Bench are open-sourced to facilitate future research in music
understanding and to enhance creative music production efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Resources: https://github.com/mzhaojp22/openmu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Dynamic Multi-Reward Weighting for
  Multi-Style Controllable Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karin de Langis, Ryan Koo, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual style expresses a diverse set of information, including interpersonal
dynamics (e.g., formality) and the author's emotions or attitudes (e.g.,
disgust). An open question is how language models can be explicitly controlled
so that they weave together target styles when generating text: for example, to
produce text that is both negative and non-toxic. One approach to such
controlled generation is multi-objective reinforcement learning (RL), but how
best to combine multiple objectives in a reward function is an open question.
In this paper, we investigate various formulations of multi-style rewards,
including calibrated outputs from discriminators and dynamic weighting by
discriminator gradient magnitudes. We find that our proposed dynamic weighting
outperforms static weighting approaches with respect to style control while
maintaining linguistic quality, and we explore its effectiveness in 2- and
3-style control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMScan: Causal Scan for LLM Misbehavior Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdi Zhang, Kai Kiat Goh, Peixin Zhang, Jun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Large Language Models (LLMs) across various fields,
their potential to generate untruthful, biased and harmful responses poses
significant risks, particularly in critical applications. This highlights the
urgent need for systematic methods to detect and prevent such misbehavior.
While existing approaches target specific issues such as harmful responses,
this work introduces LLMScan, an innovative LLM monitoring technique based on
causality analysis, offering a comprehensive solution. LLMScan systematically
monitors the inner workings of an LLM through the lens of causal inference,
operating on the premise that the LLM's `brain' behaves differently when
misbehaving. By analyzing the causal contributions of the LLM's input tokens
and transformer layers, LLMScan effectively detects misbehavior. Extensive
experiments across various tasks and models reveal clear distinctions in the
causal distributions between normal behavior and misbehavior, enabling the
development of accurate, lightweight detectors for a variety of misbehavior
detection tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain<span class="highlight-title">Transformer</span>s: SNN-LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengzheng Tang, Eva Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces BrainTransformers, an innovative Large Language Model
(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions
include: (1) designing SNN-compatible Transformer components such as SNNMatmul,
SNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU
activation function; and (3) developing a Synapsis module to simulate synaptic
plasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,
demonstrates competitive performance across various benchmarks, including MMLU
(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering
improved energy efficiency and biological plausibility. The model employs a
three-stage training approach, including SNN-specific neuronal synaptic
plasticity training. This research opens new avenues for brain-like AI systems
in natural language processing and neuromorphic computing. Future work will
focus on hardware optimization, developing specialized SNN fine-tuning tools,
and exploring practical applications in energy-efficient computing
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSDS: Data Selection for Task-Specific Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Liu, Amin Karbasi, Theodoros Rekatsinas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuning foundation models for specific tasks is an emerging paradigm in
modern machine learning. The efficacy of task-specific finetuning largely
depends on the selection of appropriate training data. We present TSDS
(Task-Specific Data Selection), a framework to select data for task-specific
model finetuning, guided by a small but representative set of examples from the
target task. To do so, we formulate data selection for task-specific finetuning
as an optimization problem with a distribution alignment loss based on optimal
transport to capture the discrepancy between the selected data and the target
distribution. In addition, we add a regularizer to encourage the diversity of
the selected data and incorporate kernel density estimation into the
regularizer to reduce the negative effects of near-duplicates among the
candidate data. We connect our optimization problem to nearest neighbor search
and design efficient algorithms to compute the optimal solution based on
approximate nearest neighbor search techniques. We evaluate our method on data
selection for both continued pretraining and instruction tuning of language
models. We show that instruction tuning using data selected by our method with
a 1% selection ratio often outperforms using the full dataset and beats the
baseline selection methods by 1.5 points in F1 score on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Vision is the Challenge for Visual Reasoning: A Benchmark for
  Visual Argument Understanding <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18925v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18925v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwan Chung, Sungjae Lee, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual arguments, often used in advertising or social causes, rely on images
to persuade viewers to do or believe something. Understanding these arguments
requires selective vision: only specific visual stimuli within an image are
relevant to the argument, and relevance can only be understood within the
context of a broader argumentative structure. While visual arguments are
readily appreciated by human audiences, we ask: are today's AI capable of
similar understanding? We present VisArgs, a dataset of 1,611 images annotated
with 5,112 visual premises (with regions), 5,574 commonsense premises, and
reasoning trees connecting them into structured arguments. We propose three
tasks for evaluating visual argument understanding: premise localization,
premise identification, and conclusion deduction. Experiments show that 1)
machines struggle to capture visual cues: GPT-4-O achieved 78.5% accuracy,
while humans reached 98.0%. Models also performed 19.5% worse when
distinguishing between irrelevant objects within the image compared to external
objects. 2) Providing relevant visual premises improved model performance
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures. Accepted as main paper in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do LLMs Have Political Correctness? Analyzing Ethical Biases and
  Jailbreak Vulnerabilities in AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isack Lee, Haebin Seong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) demonstrate impressive proficiency in
various tasks, they present potential safety risks, such as `jailbreaks', where
malicious inputs can coerce LLMs into generating harmful content. To address
these issues, many LLM developers have implemented various safety measures to
align these models. This alignment involves several techniques, including data
filtering during pre-training, supervised fine-tuning, reinforcement learning
from human feedback, and red-teaming exercises. These methods often introduce
deliberate and intentional biases similar to Political Correctness (PC) to
ensure the ethical behavior of LLMs. In this paper, we delve into the
intentional biases injected into LLMs for safety purposes and examine methods
to circumvent these safety alignment techniques. Notably, these intentional
biases result in a jailbreaking success rate in GPT-4o models that differs by
20% between non-binary and cisgender keywords and by 16% between white and
black keywords, even when the other parts of the prompts are identical. We
introduce the concept of PCJailbreak, highlighting the inherent risks posed by
these safety-induced biases. Additionally, we propose an efficient defense
method PCDefense, which prevents jailbreak attempts by injecting defense
prompts prior to generation. PCDefense stands as an appealing alternative to
Guard Models, such as Llama-Guard, that require additional inference cost after
text generation. Our findings emphasize the urgent need for LLM developers to
adopt a more responsible approach when designing and implementing safety
measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between
  Ghana and the U.S <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christabel Acquaye, Haozhe An, Rachel Rudinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has highlighted the culturally-contingent nature of commonsense
knowledge. We introduce AMAMMER${\epsilon}$, a test set of 525 multiple-choice
questions designed to evaluate the commonsense knowledge of English LLMs,
relative to the cultural contexts of Ghana and the United States. To create
AMAMMER${\epsilon}$, we select a set of multiple-choice questions (MCQs) from
existing commonsense datasets and rewrite them in a multi-stage process
involving surveys of Ghanaian and U.S. participants. In three rounds of
surveys, participants from both pools are solicited to (1) write correct and
incorrect answer choices, (2) rate individual answer choices on a 5-point
Likert scale, and (3) select the best answer choice from the newly-constructed
MCQ items, in a final validation step. By engaging participants at multiple
stages, our procedure ensures that participant perspectives are incorporated
both in the creation and validation of test items, resulting in high levels of
agreement within each pool. We evaluate several off-the-shelf English LLMs on
AMAMMER${\epsilon}$. Uniformly, models prefer answers choices that align with
the preferences of U.S. annotators over Ghanaian annotators. Additionally, when
test items specify a cultural context (Ghana or the U.S.), models exhibit some
ability to adapt, but performance is consistently better in U.S. contexts than
Ghanaian. As large resources are devoted to the advancement of English LLMs,
our findings underscore the need for culturally adaptable models and
evaluations to meet the needs of diverse English-speaking populations around
the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bi-consolidating Model for Joint Relational Triple Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03881v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03881v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocheng Luo, Yanping Chen, Ruixue Tang, Caiwei Yang, Ruizhang Huang, Yongbin Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods to extract relational triples directly make a prediction
based on a possible entity pair in a raw sentence without depending on entity
recognition. The task suffers from a serious semantic overlapping problem, in
which several relation triples may share one or two entities in a sentence. In
this paper, based on a two-dimensional sentence representation, a
bi-consolidating model is proposed to address this problem by simultaneously
reinforcing the local and global semantic features relevant to a relation
triple. This model consists of a local consolidation component and a global
consolidation component. The first component uses a pixel difference
convolution to enhance semantic information of a possible triple representation
from adjacent regions and mitigate noise in neighbouring neighbours. The second
component strengthens the triple representation based a channel attention and a
spatial attention, which has the advantage to learn remote semantic
dependencies in a sentence. They are helpful to improve the performance of both
entity identification and relation type classification in relation triple
extraction. After evaluated on several publish datasets, the bi-consolidating
model achieves competitive performance. Analytical experiments demonstrate the
effectiveness of our model for relational triple extraction and give motivation
for other natural language processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When "Competency" in Reasoning Opens the Door to Vulnerability:
  Jailbreaking LLMs via Novel Complex Ciphers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divij Handa, Zehua Zhang, Amir Saeidi, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in the safety of Large Language Models (LLMs) have
primarily focused on mitigating attacks crafted in natural language or in
common encryption techniques like Base64. However, new models which often
possess better reasoning capabilities, open the door to new attack vectors that
were previously non-existent in older models. This seems counter-intuitive at
first glance, but these advanced models can decipher more complex cryptic
queries that previous models could not, making them susceptible to attacks
using such prompts. To exploit this vulnerability, we propose Attacks using
Custom Encryptions (ACE), a novel method to jailbreak LLMs by leveraging custom
encryption schemes. We evaluate the effectiveness of ACE on four
state-of-the-art LLMs, achieving Attack Success Rates (ASR) of up to 66% on
close-source models and 88% on open-source models. Building upon this, we
introduce Layered Attacks using Custom Encryptions (LACE), which employs
multiple layers of encryption through our custom ciphers to further enhance the
ASR. Our findings demonstrate that LACE significantly enhances the ability to
jailbreak LLMs, increasing the ASR of GPT-4o from 40% to 78%, a 38%
improvement. Our results highlight that the advanced capabilities of LLMs
introduce unforeseen vulnerabilities to complex attacks. Specifically complex
and layered ciphers increase the chance of jailbreaking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-22T00:00:00Z">2024-10-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">145</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Altogether: Image Captioning via Re-aligning Alt-text <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Xu, Po-Yao Huang, Xiaoqing Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Wen-tau Yih, Shang-Wen Li, Saining Xie, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on creating synthetic data to improve the quality of image
captions. Existing works typically have two shortcomings. First, they caption
images from scratch, ignoring existing alt-text metadata, and second, lack
transparency if the captioners' training data (e.g. GPT) is unknown. In this
paper, we study a principled approach Altogether based on the key idea to edit
and re-align existing alt-texts associated with the images. To generate
training data, we perform human annotation where annotators start with the
existing alt-text and re-align it to the image content in multiple rounds,
consequently constructing captions with rich visual concepts. This differs from
prior work that carries out human annotation as a one-time description task
solely based on images and annotator knowledge. We train a captioner on this
data that generalizes the process of re-aligning alt-texts at scale. Our
results show our Altogether approach leads to richer image captions that also
improve text-to-image generation and zero-shot image classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by EMNLP 2024; MetaCLIPv2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding
  Benchmark for Culture-aware Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki Egashira, Jeonghun Baek, Xiang Yue, Graham Neubig, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating research on Large Multimodal Models (LMMs) in non-English
languages is crucial for enhancing user experiences across broader populations.
In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale
Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the
Japanese cultural context. To facilitate comprehensive culture-aware
evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)
subset, where the culture-independent subjects (e.g., Math) are selected and
translated into Japanese, enabling one-to-one comparison with its English
counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly
crafted subjects that reflect Japanese cultural context. Using the CA subset,
we observe performance drop in many LMMs when evaluated in Japanese, which is
purely attributable to language variation. Using the CS subset, we reveal their
inadequate Japanese cultural understanding. Further, by combining both subsets,
we identify that some LMMs perform well on the CA subset but not on the CS
subset, exposing a shallow understanding of the Japanese language that lacks
depth in cultural understanding. We hope this work will not only help advance
LMM performance in Japanese but also serve as a guideline to create
high-standard, culturally diverse benchmarks for multilingual LMM development.
The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://mmmu-japanese-benchmark.github.io/JMMMU/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid
  Visual Redundancy Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large vision-language models (LVLMs), images serve as inputs that carry a
wealth of information. As the idiom "A picture is worth a thousand words"
implies, representing a single image in current LVLMs can require hundreds or
even thousands of tokens. This results in significant computational costs,
which grow quadratically as input image resolution increases, thereby severely
impacting the efficiency of both training and inference. Previous approaches
have attempted to reduce the number of image tokens either before or within the
early layers of LVLMs. However, these strategies inevitably result in the loss
of crucial image information, ultimately diminishing model performance. To
address this challenge, we conduct an empirical study revealing that all visual
tokens are necessary for LVLMs in the shallow layers, and token redundancy
progressively increases in the deeper layers of the model. To this end, we
propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost
their efficiency in both training and inference with neglectable performance
loss. Specifically, we partition the LVLM into several stages and drop part of
the image tokens at the end of each stage with a pre-defined ratio, creating
pyramid-like visual tokens across model layers. The dropping is based on a
lightweight similarity calculation with a negligible time overhead. Extensive
experiments demonstrate that PyramidDrop can achieve a 40% training time and
55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.
Besides, the PyramidDrop could also serve as a plug-and-play strategy for
inference acceleration without training, with better performance and lower
inference cost than counterparts. We hope that the insights and approach
introduced by PyramidDrop will inspire future research to further investigate
the role of image tokens in LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reliable Evaluation of Behavior Steering Interventions in LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itamar Pres, Laura Ruis, Ekdeep Singh Lubana, David Krueger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation engineering methods have recently shown promise for enabling
efficient steering of model behavior. However, evaluation pipelines for these
methods have primarily relied on subjective demonstrations, instead of
quantitative, objective metrics. We aim to take a step towards addressing this
issue by advocating for four properties missing from current evaluations: (i)
contexts sufficiently similar to downstream tasks should be used for assessing
intervention quality; (ii) model likelihoods should be accounted for; (iii)
evaluations should allow for standardized comparisons across different target
behaviors; and (iv) baseline comparisons should be offered. We introduce an
evaluation pipeline grounded in these criteria, offering both a quantitative
and visual analysis of how effectively a given method works. We use this
pipeline to evaluate two representation engineering methods on how effectively
they can steer behaviors such as truthfulness and corrigibility, finding that
some interventions are less effective than previously reported.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the NeurIPS 2024 - Workshop on Foundation Model
  Interventions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, Chenglin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Machine Learning (AutoML) approaches encompass traditional methods
that optimize fixed pipelines for model selection and ensembling, as well as
newer LLM-based frameworks that autonomously build pipelines. While LLM-based
agents have shown promise in automating machine learning tasks, they often
generate low-diversity and suboptimal code, even after multiple iterations. To
overcome these limitations, we introduce Tree-Search Enhanced LLM Agents
(SELA), an innovative agent-based system that leverages Monte Carlo Tree Search
(MCTS) to optimize the AutoML process. By representing pipeline configurations
as trees, our framework enables agents to conduct experiments intelligently and
iteratively refine their strategies, facilitating a more effective exploration
of the machine learning solution space. This novel approach allows SELA to
discover optimal pathways based on experimental feedback, improving the overall
quality of the solutions. In an extensive evaluation across 20 machine learning
datasets, we compare the performance of traditional and agent-based AutoML
methods, demonstrating that SELA achieves a win rate of 65% to 80% against each
baseline across all datasets. These results underscore the significant
potential of agent-based strategies in AutoML, offering a fresh perspective on
tackling complex machine learning challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is available at https://github.com/geekan/MetaGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Empowered Personalized Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web agents have emerged as a promising direction to automate Web task
completion based on user instructions, significantly enhancing user experience.
Recently, Web agents have evolved from traditional agents to Large Language
Models (LLMs)-based Web agents. Despite their success, existing LLM-based Web
agents overlook the importance of personalized data (e.g., user profiles and
historical Web behaviors) in assisting the understanding of users' personalized
instructions and executing customized actions. To overcome the limitation, we
first formulate the task of LLM-empowered personalized Web agents, which
integrate personalized data and user instructions to personalize instruction
comprehension and action execution. To address the absence of a comprehensive
evaluation benchmark, we construct a Personalized Web Agent Benchmark
(PersonalWAB), featuring user instructions, personalized user data, Web
functions, and two evaluation paradigms across three personalized Web tasks.
Moreover, we propose a Personalized User Memory-enhanced Alignment (PUMA)
framework to adapt LLMs to the personalized Web agent task. PUMA utilizes a
memory bank with a task-specific retrieval strategy to filter relevant
historical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for
personalized action execution through fine-tuning and direct preference
optimization. Extensive experiments validate the superiority of PUMA over
existing Web agents on PersonalWAB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and data are available on the project website
  https://hongrucai.github.io/PersonalWAB/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Spinal MRI Labelling from Reports Using a Large Language Model <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Y. Park, Rhydian Windsor, Amir Jamaludin, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a general pipeline to automate the extraction of labels from
radiology reports using large language models, which we validate on spinal MRI
reports. The efficacy of our labelling method is measured on five distinct
conditions: spinal cancer, stenosis, spondylolisthesis, cauda equina
compression and herniation. Using open-source models, our method equals or
surpasses GPT-4 on a held-out set of reports. Furthermore, we show that the
extracted labels can be used to train imaging models to classify the identified
conditions in the accompanying MR scans. All classifiers trained using
automated labels achieve comparable performance to models trained using scans
manually annotated by clinicians. Code can be found at
https://github.com/robinyjpark/AutoLabelClassifier.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Medical Image Computing and Computer Assisted
  Intervention (MICCAI 2024, Spotlight). 11 pages plus appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning Large Language Models to Appropriately Abstain with Semantic
  Entropy <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, Kunal Handa, Yarin Gal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are known to hallucinate, whereby they generate
plausible but inaccurate text. This phenomenon poses significant risks in
critical applications, such as medicine or law, necessitating robust
hallucination mitigation strategies. While recent works have proposed
fine-tuning methods to teach LLMs to abstain from answering questions beyond
their knowledge or capabilities, these methods rely on the existence of
ground-truth labels or are limited to short-form responses. To address these
limitations, we propose fine-tuning using semantic entropy, an uncertainty
measure derived from introspection into the model which does not require
external labels. We demonstrate that our approach matches or outperforms models
fine-tuned using prior work and achieves strong performance for both short and
long-form generations on a range of datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS Safe Generative AI Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dhoroni: Exploring Bengali Climate Change and Environmental Views with a
  Multi-Perspective News <span class="highlight-title">Dataset</span> and Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Wahid Faisal, Taj Ahmad, Abdur Rahman, Mst Rafia Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change poses critical challenges globally, disproportionately
affecting low-income countries that often lack resources and linguistic
representation on the international stage. Despite Bangladesh's status as one
of the most vulnerable nations to climate impacts, research gaps persist in
Bengali-language studies related to climate change and NLP. To address this
disparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and
environmental news dataset, comprising a 2300 annotated Bangla news articles,
offering multiple perspectives such as political influence,
scientific/statistical data, authenticity, stance detection, and stakeholder
involvement. Furthermore, we present an in-depth exploratory analysis of
Dhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family
for climate and environmental opinion detection in Bangla, fine-tuned on our
dataset. This research contributes significantly to enhancing accessibility and
analysis of climate discourse in Bengali (Bangla), addressing crucial
communication and research gaps in climate-impacted regions like Bangladesh
with 180 million people.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Context</span>-aware <span class="highlight-title">Prompt</span> Tuning: Advancing In-<span class="highlight-title">Context</span> Learning with
  Adversarial Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsachi Blau, Moshe Kimhi, Yonatan Belinkov, Alexander Bronstein, Chaim Baskin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models (LLMs) typically involves updating at least
a few billions of parameters. A more parameter-efficient approach is Prompt
Tuning (PT), which updates only a few learnable tokens, and differently,
In-Context Learning (ICL) adapts the model to a new task by simply including
examples in the input without any training. When applying optimization-based
methods, such as fine-tuning and PT for few-shot learning, the model is
specifically adapted to the small set of training examples, whereas ICL leaves
the model unchanged. This distinction makes traditional learning methods more
prone to overfitting; in contrast, ICL is less sensitive to the few-shot
scenario. While ICL is not prone to overfitting, it does not fully extract the
information that exists in the training examples. This work introduces
Context-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and
adversarial attacks. We build on the ICL strategy of concatenating examples
before the input, but we extend this by PT-like learning, refining the context
embedding through iterative optimization to extract deeper insights from the
training examples. We carefully modify specific context tokens, considering the
unique structure of input and output formats. Inspired by adversarial attacks,
we adjust the input based on the labels present in the context, focusing on
minimizing, rather than maximizing, the loss. Moreover, we apply a projected
gradient descent algorithm to keep token embeddings close to their original
values, under the assumption that the user-provided data is inherently
valuable. Our method has been shown to achieve superior accuracy across
multiple classification tasks using various LLM models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creativity in AI: Progresses and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Debjit Paul, Antoine Bosselut, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creativity is the ability to produce novel, useful, and surprising ideas, and
has been widely studied as a crucial aspect of human cognition. Machine
creativity on the other hand has been a long-standing challenge. With the rise
of advanced generative AI, there has been renewed interest and debate regarding
AI's creative capabilities. Therefore, it is imperative to revisit the state of
creativity in AI and identify key progresses and remaining challenges. In this
work, we survey leading works studying the creative capabilities of AI systems,
focusing on creative problem-solving, linguistic, artistic, and scientific
creativity. Our review suggests that while the latest AI models are largely
capable of producing linguistically and artistically creative outputs such as
poems, images, and musical pieces, they struggle with tasks that require
creative problem-solving, abstract thinking and compositionality and their
generations suffer from a lack of diversity, originality, long-range
incoherence and hallucinations. We also discuss key questions concerning
copyright and authorship issues with generative models. Furthermore, we
highlight the need for a comprehensive evaluation of creativity that is
process-driven and considers several dimensions of creativity. Finally, we
propose future research directions to improve the creativity of AI outputs,
drawing inspiration from cognitive science and psychology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiniPLM: Knowledge Distillation for <span class="highlight-title">Pre-Train</span>ing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) is widely used to train small, high-performing
student language models (LMs) using large teacher LMs. While effective in
fine-tuning, KD during pre-training faces challenges in efficiency,
flexibility, and effectiveness. Existing methods either incur high
computational costs due to online teacher inference, require tokenization
matching between teacher and student LMs, or risk losing the difficulty and
diversity of the teacher-generated training data. To address these issues, we
propose MiniPLM, a KD framework for pre-training LMs by refining the training
data distribution with the teacher's knowledge. For efficiency, MiniPLM
performs offline teacher LM inference, allowing KD for multiple student LMs
without adding training-time costs. For flexibility, MiniPLM operates solely on
the training corpus, enabling KD across model families. For effectiveness,
MiniPLM leverages the differences between large and small LMs to enhance the
difficulty and diversity of the training data, helping student LMs acquire
versatile and sophisticated knowledge. Extensive experiments demonstrate that
MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks,
improves the language modeling capabilities, and reduces pre-training
computation. The benefit of MiniPLM extends to large pre-training scales,
evidenced by the extrapolation of the scaling curves. Further analysis reveals
that MiniPLM supports KD across model families and enhances the utilization of
pre-training data. Our model, code, and data are available at
https://github.com/thu-coai/MiniPLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh
  through Large Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam, Mahathir Mohammad Bappy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Bangladesh's legal system struggles with major challenges like
delays, complexity, high costs, and millions of unresolved cases, which deter
many from pursuing legal action due to lack of knowledge or financial
constraints. This research seeks to develop a specialized Large Language Model
(LLM) to assist in the Bangladeshi legal system. Methods: We created
UKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and
scraping data on various legal acts. We fine-tuned the GPT-2 model on this
dataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance
in English. Results: The model was rigorously evaluated using semantic
assessments, including case studies supported by expert opinions. The
evaluation provided promising results, demonstrating the potential for the
model to assist in legal matters within Bangladesh. Conclusion: Our work
represents the first structured effort toward building an AI-based legal
assistant for Bangladesh. While the results are encouraging, further
refinements are necessary to improve the model's accuracy, credibility, and
safety. This is a significant step toward creating a legal AI capable of
serving the needs of a population of 180 million.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-to-Score Conversion Model Based on Whisper methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyao Zhang, Bohang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis develops a Transformer model based on Whisper, which extracts
melodies and chords from music audio and records them into ABC notation. A
comprehensive data processing workflow is customized for ABC notation,
including data cleansing, formatting, and conversion, and a mutation mechanism
is implemented to increase the diversity and quality of training data. This
thesis innovatively introduces the "Orpheus' Score", a custom notation system
that converts music information into tokens, designs a custom vocabulary
library, and trains a corresponding custom tokenizer. Experiments show that
compared to traditional algorithms, the model has significantly improved
accuracy and performance. While providing a convenient audio-to-score tool for
music enthusiasts, this work also provides new ideas and tools for research in
music information processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoiceBench: Benchmarking LLM-Based Voice Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on the success of large language models (LLMs), recent advancements
such as GPT-4o have enabled real-time speech interactions through LLM-based
voice assistants, offering a significantly improved user experience compared to
traditional text-based interactions. However, the absence of benchmarks
designed to evaluate these speech interaction capabilities has hindered
progress of LLM-based voice assistants development. Current evaluations focus
primarily on automatic speech recognition (ASR) or general knowledge evaluation
with clean speeches, neglecting the more intricate, real-world scenarios that
involve diverse speaker characteristics, environmental and content factors. To
address this, we introduce VoiceBench, the first benchmark designed to provide
a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also
includes both real and synthetic spoken instructions that incorporate the above
three key real-world variations. Extensive experiments reveal the limitations
of current LLM-based voice assistant models and offer valuable insights for
future research and development in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Data is available at
  https://github.com/MatthewCYM/VoiceBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Non-myopic Generation for Reasoning and Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated remarkable abilities in reasoning and
planning by breaking down complex problems into sequential steps. Despite their
success in various domains like mathematical problem-solving and coding, LLMs
face challenges in ensuring reliable and optimal planning due to their inherent
myopic nature of autoregressive decoding. This paper revisits LLM reasoning
from an optimal-control perspective, proposing a novel method,
Predictive-Decoding, that leverages Model Predictive Control to enhance
planning accuracy. By re-weighting LLM distributions based on foresight
trajectories, Predictive-Decoding aims to mitigate early errors and promote
non-myopic planning. Our experiments show significant improvements in a wide
range of tasks for math, coding, and agents. Furthermore, Predictive-Decoding
demonstrates computational efficiency, outperforming search baselines with
reduced computational resources. This study provides insights into optimizing
LLM planning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Attention to Activation: Unravelling the Enigmas of Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prannay Kaul, Chengcheng Ma, Ismail Elezi, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study two strange phenomena in auto-regressive Transformers: (1) the
dominance of the first token in attention heads; (2) the occurrence of large
outlier activations in the hidden states. We find that popular large language
models, such as Llama attend maximally to the first token in 98% of attention
heads, a behaviour we attribute to the softmax function. To mitigate this
issue, we propose a reformulation of softmax to softmax-1. Furthermore, we
identify adaptive optimisers, e.g. Adam, as the primary contributor to the
large outlier activations and introduce OrthoAdam, a novel optimiser that
utilises orthogonal matrices to transform gradients, to address this issue.
Finally, not only do our methods prevent these phenomena from occurring, but
additionally, they enable Transformers to sustain their performance when
quantised using basic algorithms, something that standard methods are unable to
do. In summary, our methods reduce the attention proportion on the first token
from 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to
3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-calibration for Language Model Quantization and Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miles Williams, George Chrysostomou, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization and pruning are fundamental approaches for model compression,
enabling efficient inference for language models. In a post-training setting,
state-of-the-art quantization and pruning methods require calibration data, a
small set of unlabeled examples. Conventionally, randomly sampled web text is
used, aiming to reflect the model training data. However, this poses two key
problems: (1) unrepresentative calibration examples can harm model performance,
and (2) organizations increasingly avoid releasing model training data. In this
paper, we propose self-calibration as a solution. Our approach requires no
external data, instead leveraging the model itself to generate synthetic
calibration data as a better approximation of the pre-training data
distribution. We extensively compare the performance of self-calibration with
several baselines, across a variety of models, compression methods, and tasks.
Our approach proves consistently competitive in maximizing downstream task
performance, frequently outperforming even using real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interchangeable Token Embeddings for Extendable Vocabulary and
  Alpha-Equivalence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        İlker Işık, Ramazan Gokberk Cinbis, Ebru Aydin Gol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach for learning interchangeable tokens in language
models to obtain an extendable vocabulary that can generalize to new tokens.
Our method is designed to address alpha-equivalence, the principle that
renaming bound variables in a syntactic expression preserves semantics. This
property arises in many formal languages such as temporal logics, in which all
proposition symbols represent the same concept but are distinguishable from
each other. To handle such tokens, we develop a dual-part embedding approach.
The first part is shared across all interchangeable tokens, thereby enforcing
that they represent the same core concept. The second part is randomly
generated for each token, which enables distinguishability. We evaluate our
method in a Transformer encoder-decoder model on two tasks: solving linear
temporal logic formulae and copying with extendable vocabulary. Our method
demonstrates promising generalization capabilities in addition to introducing a
favorable inductive bias for alpha-equivalence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Pinterest Search Relevance Using Large Language Models <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Mukuntha Narayanan Sundararaman, Onur Gungor, Yu Xu, Krishna Kamath, Rakesh Chalasani, Kurchi Subhra Hazra, Jinfeng Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve relevance scoring on Pinterest Search, we integrate Large Language
Models (LLMs) into our search relevance model, leveraging carefully designed
text representations to predict the relevance of Pins effectively. Our approach
uses search queries alongside content representations that include captions
extracted from a generative visual language model. These are further enriched
with link-based text data, historically high-quality engaged queries,
user-curated boards, Pin titles and Pin descriptions, creating robust models
for predicting search relevance. We use a semi-supervised learning approach to
efficiently scale up the amount of training data, expanding beyond the
expensive human labeled data available. By utilizing multilingual LLMs, our
system extends training data to include unseen languages and domains, despite
initial data and annotator expertise being confined to English. Furthermore, we
distill from the LLM-based model into real-time servable model architectures
and features. We provide comprehensive offline experimental validation for our
proposed techniques and demonstrate the gains achieved through the final
deployed system at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024 Workshop on Industrial Recommendation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can General-Purpose Large Language Models Generalize to English-Thai
  Machine Translation ? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jirat Chiaranaipanich, Naiyarat Hanmatheekuna, Jitkapat Sawatphol, Krittamate Tiankanon, Jiramet Kinchagawat, Amrest Chinkamol, Parinthapat Pengpun, Piyalitt Ittichaiwong, Peerat Limkonchotiwat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) perform well on common tasks but struggle with
generalization in low-resource and low-computation settings. We examine this
limitation by testing various LLMs and specialized translation models on
English-Thai machine translation and code-switching datasets. Our findings
reveal that under more strict computational constraints, such as 4-bit
quantization, LLMs fail to translate effectively. In contrast, specialized
models, with comparable or lower computational requirements, consistently
outperform LLMs. This underscores the importance of specialized models for
maintaining performance under resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in GenBench EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Large Language Models via Self-Steering Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated alignment develops alignment systems with minimal human
intervention. The key to automated alignment lies in providing learnable and
accurate preference signals for preference learning without human annotation.
In this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm
that autonomously generates high-quality preference signals based on predefined
principles during iterative training, eliminating the need for manual
annotation. $SSO$ maintains the accuracy of signals by ensuring a consistent
gap between chosen and rejected responses while keeping them both on-policy to
suit the current policy model's learning capacity. $SSO$ can benefit the online
and offline training of the policy model, as well as enhance the training of
reward models. We validate the effectiveness of $SSO$ with two foundation
models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy
preference signals throughout iterative training. Without any manual annotation
or external models, $SSO$ leads to significant performance improvements across
six subjective or objective benchmarks. Besides, the preference data generated
by $SSO$ significantly enhanced the performance of the reward model on
Rewardbench. Our work presents a scalable approach to preference optimization,
paving the way for more efficient and effective automated alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAPILLON: PrivAcy Preservation from Internet-based and Local Language
  MOdel ENsembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Siyan, Vethavikashini Chithrra Raghuram, Omar Khattab, Julia Hirschberg, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users can divulge sensitive information to proprietary LLM providers, raising
significant privacy concerns. While open-source models, hosted locally on the
user's machine, alleviate some concerns, models that users can host locally are
often less capable than proprietary frontier models. Toward preserving user
privacy while retaining the best quality, we propose Privacy-Conscious
Delegation, a novel task for chaining API-based and local models. We utilize
recent public collections of user-LLM interactions to construct a natural
benchmark called PUPA, which contains personally identifiable information
(PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM
pipeline that uses prompt optimization to address a simpler version of our
task. Our best pipeline maintains high response quality for 85.5% of user
queries while restricting privacy leakage to only 7.5%. We still leave a large
margin to the generation quality of proprietary LLMs for future work. Our data
and code will be available at https://github.com/siyan-sylvia-li/PAPILLON.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring RL-based LLM Training for Formal Language Tasks with
  Programmed Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander G. Padula, Dennis J. N. J. Soemers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning
from Human Feedback to align large language models (LLMs) with downstream
tasks. This paper investigates the feasibility of using PPO for direct
reinforcement learning (RL) from explicitly programmed reward signals, as
opposed to indirect learning from human feedback via an intermediary reward
model. We focus on tasks expressed through formal languages, such as
mathematics and programming, where explicit reward functions can be programmed
to automatically assess the quality of generated outputs. We apply this
approach to a sentiment alignment task, a simple arithmetic task, and a more
complex game synthesis task. The sentiment alignment task replicates prior
research and serves to validate our experimental setup. Our results show that
pure RL-based training for the two formal language tasks is challenging, with
success being limited even for the simple arithmetic task. We propose a novel
batch-entropy regularization term to aid exploration, although training is not
yet entirely stable. Our findings suggest that direct RL training of LLMs may
be more suitable for relatively minor changes, such as alignment, than for
learning new tasks altogether, even if an informative reward signal can be
expressed programmatically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BNAIC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Answer Attribution for Faithful Text Generation with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juraj Vladika, Luca Mülln, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing popularity of Large Language Models (LLMs) in recent years has
changed the way users interact with and pose questions to AI-based
conversational systems. An essential aspect for increasing the trustworthiness
of generated LLM answers is the ability to trace the individual claims from
responses back to relevant sources that support them, the process known as
answer attribution. While recent work has started exploring the task of answer
attribution in LLMs, some challenges still remain. In this work, we first
perform a case study analyzing the effectiveness of existing answer attribution
methods, with a focus on subtasks of answer segmentation and evidence
retrieval. Based on the observed shortcomings, we propose new methods for
producing more independent and contextualized claims for better retrieval and
attribution. The new methods are evaluated and shown to improve the performance
of answer attribution components. We end with a discussion and outline of
future directions for the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDIR 2024 (part of IC3K 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality is a crucial issue for crowd annotations. Answer aggregation is
an important type of solution. The aggregated answers estimated from multiple
crowd answers to the same instance are the eventually collected annotations,
rather than the individual crowd answers themselves. Recently, the capability
of Large Language Models (LLMs) on data annotation tasks has attracted interest
from researchers. Most of the existing studies mainly focus on the average
performance of individual crowd workers; several recent works studied the
scenarios of aggregation on categorical labels and LLMs used as label creators.
However, the scenario of aggregation on text answers and the role of LLMs as
aggregators are not yet well-studied. In this paper, we investigate the
capability of LLMs as aggregators in the scenario of close-ended crowd text
answer aggregation. We propose a human-LLM hybrid text answer aggregation
method with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We
make the experiments based on public crowdsourcing datasets. The results show
the effectiveness of our approach based on the collaboration of crowd workers
and LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Science Out of Its Ivory Tower: Improving Accessibility with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haining Wang, Jason Clark, Hannah McKelvey, Leila Sterman, Zheng Gao, Zuoyu Tian, Sandra Kübler, Xiaozhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A vast amount of scholarly work is published daily, yet much of it remains
inaccessible to the general public due to dense jargon and complex language. To
address this challenge in science communication, we introduce a reinforcement
learning framework that fine-tunes a language model to rewrite scholarly
abstracts into more comprehensible versions. Guided by a carefully balanced
combination of word- and sentence-level accessibility rewards, our language
model effectively substitutes technical terms with more accessible
alternatives, a task which models supervised fine-tuned or guided by
conventional readability measures struggle to accomplish. Our best model
adjusts the readability level of scholarly abstracts by approximately six U.S.
grade levels -- in other words, from a postgraduate to a high school level.
This translates to roughly a 90% relative boost over the supervised fine-tuning
baseline, all while maintaining factual accuracy and high-quality language. An
in-depth analysis of our approach shows that balanced rewards lead to
systematic modifications in the base model, likely contributing to smoother
optimization and superior performance. We envision this work as a step toward
bridging the gap between scholarly research and the general public,
particularly younger readers and those without a college degree.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Speech Tokenizer in Text To Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixing Li, Ruobing Xie, Xingwu Sun, Yu Cheng, Zhanhui Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fusion of speech and language in the era of large language models has
garnered significant attention. Discrete speech token is often utilized in
text-to-speech tasks for speech compression and portability, which is
convenient for joint training with text and have good compression efficiency.
However, we found that the discrete speech tokenizer still suffers from
information loss. Therefore, we propose a simple yet effective continuous
speech tokenizer and a text-to-speech model based on continuous speech tokens.
Our results show that the speech language model based on the continuous speech
tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS).
This enhancement is attributed to better information preservation rate of the
continuous speech tokenizer across both low and high frequencies in the
frequency domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven Coreference-based Ontology Building 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shir Ashury-Tahan, Amir David Nissan Cohen, Nadav Cohen, Yoram Louzoun, Yoav Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While coreference resolution is traditionally used as a component in
individual document understanding, in this work we take a more global view and
explore what can we learn about a domain from the set of all document-level
coreference relations that are present in a large corpus. We derive coreference
chains from a corpus of 30 million biomedical abstracts and construct a graph
based on the string phrases within these chains, establishing connections
between phrases if they co-occur within the same coreference chain. We then use
the graph structure and the betweeness centrality measure to distinguish
between edges denoting hierarchy, identity and noise, assign directionality to
edges denoting hierarchy, and split nodes (strings) that correspond to multiple
distinct concepts. The result is a rich, data-driven ontology over concepts in
the biomedical domain, parts of which overlaps significantly with
human-authored ontologies. We release the coreference chains and resulting
ontology under a creative-commons license, along with the code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Sinha, Murari Mandal, Mohan Kankanhalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key components of machine learning are data samples for training, model
for learning patterns, and loss function for optimizing accuracy. Analogously,
unlearning can potentially be achieved through anti-data samples (or
anti-samples), unlearning method, and reversed loss function. While prior
research has explored unlearning methods and reversed loss functions, the
potential of anti-samples remains largely untapped. In this paper, we introduce
UnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language
models (LLMs). Our contributions are threefold; first, we propose a novel
concept of anti-sample-induced unlearning; second, we generate anti-samples by
leveraging misleading rationales, which help reverse learned associations and
accelerate the unlearning process; and third, we enable fine-grained targeted
unlearning, allowing for the selective removal of specific associations without
impacting related knowledge - something not achievable by previous works.
Results demonstrate that anti-samples offer an efficient, targeted unlearning
strategy for LLMs, opening new avenues for privacy-preserving machine learning
and model modification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arabic <span class="highlight-title">Dataset</span> for LLM Safeguard Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasser Ashraf, Yuxia Wang, Bin Gu, Preslav Nakov, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing use of large language models (LLMs) has raised concerns regarding
their safety. While many studies have focused on English, the safety of LLMs in
Arabic, with its linguistic and cultural complexities, remains under-explored.
Here, we aim to bridge this gap. In particular, we present an
Arab-region-specific safety evaluation dataset consisting of 5,799 questions,
including direct attacks, indirect attacks, and harmless requests with
sensitive words, adapted to reflect the socio-cultural context of the Arab
world. To uncover the impact of different stances in handling sensitive and
controversial topics, we propose a dual-perspective evaluation framework. It
assesses the LLM responses from both governmental and opposition viewpoints.
Experiments over five leading Arabic-centric and multilingual LLMs reveal
substantial disparities in their safety performance. This reinforces the need
for culturally specific datasets to ensure the responsible deployment of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIRI: Adversarial Patient Reidentification with Large Language Models
  for Evaluating Clinical Text Anonymization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Thomas R. Campion, Sri Laasya Nutheti, Yifan Peng, Akhil Raj, Ramin Zabih, Curtis L. Cole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharing protected health information (PHI) is critical for furthering
biomedical research. Before data can be distributed, practitioners often
perform deidentification to remove any PHI contained in the text. Contemporary
deidentification methods are evaluated on highly saturated datasets (tools
achieve near-perfect accuracy) which may not reflect the full variability or
complexity of real-world clinical text and annotating them is resource
intensive, which is a barrier to real-world applications. To address this gap,
we developed an adversarial approach using a large language model (LLM) to
re-identify the patient corresponding to a redacted clinical note and evaluated
the performance with a novel De-Identification/Re-Identification (DIRI) method.
Our method uses a large language model to reidentify the patient corresponding
to a redacted clinical note. We demonstrate our method on medical data from
Weill Cornell Medicine anonymized with three deidentification tools: rule-based
Philter and two deep-learning-based models, BiLSTM-CRF and ClinicalBERT.
Although ClinicalBERT was the most effective, masking all identified PII, our
tool still reidentified 9% of clinical notes Our study highlights significant
weaknesses in current deidentification technologies while providing a tool for
iterative development and improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can a Machine Distinguish High and Low Amount of Social Creak in Speech? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne-Maria Laukkanen, Sudarsana Reddy Kadiri, Shrikanth Narayanan, Paavo Alku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objectives: ncreased prevalence of social creak particularly among female
speakers has been reported in several studies. The study of social creak has
been previously conducted by combining perceptual evaluation of speech with
conventional acoustical parameters such as the harmonic-to-noise ratio and
cepstral peak prominence. In the current study, machine learning (ML) was used
to automatically distinguish speech of low amount of social creak from speech
of high amount of social creak.
  Methods: The amount of creak in continuous speech samples produced in Finnish
by 90 female speakers was first perceptually assessed by two voice specialists.
Based on their assessments, the speech samples were divided into two categories
(low $vs$. high amount of creak). Using the speech signals and their creak
labels, seven different ML models were trained. Three spectral representations
were used as feature for each model.
  Results: The results show that the best performance (accuracy of 71.1\%) was
obtained by the following two systems: an Adaboost classifier using the
mel-spectrogram feature and a decision tree classifier using the mel-frequency
cepstral coefficient feature.
  Conclusions: The study of social creak is becoming increasingly popular in
sociolinguistic and vocological research. The conventional human perceptual
assessment of the amount of creak is laborious and therefore ML technology
could be used to assist researchers studying social creak. The classification
systems reported in this study could be considered as baselines in future
ML-based studies on social creak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Journal of Voice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SG-FSM: A Self-Guiding Zero-Shot <span class="highlight-title">Prompt</span>ing Paradigm for Multi-Hop
  Question Answering Based on Finite State Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models with chain-of-thought prompting, such as OpenAI-o1,
have shown impressive capabilities in natural language inference tasks.
However, Multi-hop Question Answering (MHQA) remains challenging for many
existing models due to issues like hallucination, error propagation, and
limited context length. To address these challenges and enhance LLMs'
performance on MHQA, we propose the Self-Guiding prompting Finite State Machine
(SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike
traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively
breaking down complex questions into sub-questions, correcting itself to
improve accuracy. It processes one sub-question at a time, dynamically deciding
the next step based on the current context and results, functioning much like
an automaton. Experiments across various benchmarks demonstrate the
effectiveness of our approach, outperforming strong baselines on challenging
datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of
the correct final answer despite intermediate errors. It also improves
adherence to specified output formats, simplifying evaluation significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Forgetting in Large Language Model <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonghua Liao, Ruobing Xie, Xingwu Sun, Haowen Sun, Zhanhui Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting remains a formidable obstacle to building an
omniscient model in large language models (LLMs). Despite the pioneering
research on task-level forgetting in LLM fine-tuning, there is scant focus on
forgetting during pre-training. We systematically explored the existence and
measurement of forgetting in pre-training, questioning traditional metrics such
as perplexity (PPL) and introducing new metrics to better detect entity memory
retention. Based on our revised assessment of forgetting metrics, we explored
low-cost, straightforward methods to mitigate forgetting during the
pre-training phase. Further, we carefully analyzed the learning curves,
offering insights into the dynamics of forgetting. Extensive evaluations and
analyses on forgetting of pre-training could facilitate future research on
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPL: Leveraging Multimodal Large Language Models for Intelligent Product
  Listing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Chen, Qingheng Zhang, Chengbao Lian, Yixin Ji, Xuwei Liu, Shuguang Han, Guoqiang Wu, Fei Huang, Jufeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g.,
Amazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are
mainly targeting individual sellers who usually lack sufficient experience in
e-commerce. Individual sellers often struggle to compose proper descriptions
for selling products. With the recent advancement of Multimodal Large Language
Models (MLLMs), we attempt to integrate such state-of-the-art generative AI
technologies into the product listing process. To this end, we develop IPL, an
Intelligent Product Listing tool tailored to generate descriptions using
various product attributes such as category, brand, color, condition, etc. IPL
enables users to compose product descriptions by merely uploading photos of the
selling product. More importantly, it can imitate the content style of our C2C
platform Xianyu. This is achieved by employing domain-specific instruction
tuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation
(RAG) process. A comprehensive empirical evaluation demonstrates that the
underlying model of IPL significantly outperforms the base model in
domain-specific tasks while producing less hallucination. IPL has been
successfully deployed in our production system, where 72% of users have their
published product listings based on the generated content, and those product
listings are shown to have a quality score 5.6% higher than those without AI
assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Mathematical Rules with Large Language Models <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Gorceix, Bastien Le Chenadec, Ahmad Rammal, Nelson Vadori, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the ability of large language models to learn
specific mathematical rules such as distributivity or simplifying equations. We
present an empirical analysis of their ability to generalize these rules, as
well as to reuse them in the context of word problems. For this purpose, we
provide a rigorous methodology to build synthetic data incorporating such
rules, and perform fine-tuning of large language models on such data. Our
experiments show that our model can learn and generalize these rules to some
extent, as well as suitably reuse them in the context of word problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4th MATH-AI Workshop at NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities
  Using Only Forward Passes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math reasoning is a highly active area of Large Language Model (LLM) research
because it is a hallmark of artificial intelligence. However, few works have
explored how math reasoning is encoded within LLM parameters and if it is a
skill that can be isolated within a model. Doing so could allow targeted
intervention to improve math performance without altering non-math behavior and
foster understanding of how models encode math reasoning. We introduce Math
Neurosurgery (MathNeuro), a method for isolating math-specific parameters in
LLMs using only forward passes. MathNeuro builds on existing work by using
weights and activations to calculate parameter importance, but isolates
math-specific parameters by removing those important for general language
tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning
ability without destroying its general language ability. Scaling these
parameters by a small constant improves a pretrained or instruction-tuned LLM's
performance by 4-17% on GSM8K while leaving non-math behavior unaltered.
MathNeuro is also data efficient: most of its effectiveness holds when
identifying math-specific parameters using a single sample. MathNeuro
highlights the potential for future work to intervene on math-specific
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnvBridge: Bridging Diverse Environments with Cross-Environment
  Knowledge Transfer for Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoyuki Kagaya, Yuxuan Lou, Thong Jing Yuan, Subramanian Lakshmi, Jayashree Karlekar, Sugiri Pranata, Natsuki Murakami, Akira Kinose, Koki Oguri, Felix Wick, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated high
reasoning capabilities, drawing attention for their applications as agents in
various decision-making processes. One notably promising application of LLM
agents is robotic manipulation. Recent research has shown that LLMs can
generate text planning or control code for robots, providing substantial
flexibility and interaction capabilities. However, these methods still face
challenges in terms of flexibility and applicability across different
environments, limiting their ability to adapt autonomously. Current approaches
typically fall into two categories: those relying on environment-specific
policy training, which restricts their transferability, and those generating
code actions based on fixed prompts, which leads to diminished performance when
confronted with new environments. These limitations significantly constrain the
generalizability of agents in robotic manipulation. To address these
limitations, we propose a novel method called EnvBridge. This approach involves
the retention and transfer of successful robot control codes from source
environments to target environments. EnvBridge enhances the agent's
adaptability and performance across diverse settings by leveraging insights
from multiple environments. Notably, our approach alleviates environmental
constraints, offering a more flexible and generalizable solution for robotic
manipulation tasks. We validated the effectiveness of our method using robotic
manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments
demonstrate that LLM agents can successfully leverage diverse knowledge sources
to solve complex tasks. Consequently, our approach significantly enhances the
adaptability and robustness of robotic manipulation agents in planning across
diverse environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracing the Development of the Virtual Particle Concept Using Semantic
  Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Zichert, Adrian Wüthrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual particles are peculiar objects. They figure prominently in much of
theoretical and experimental research in elementary particle physics. But
exactly what they are is far from obvious. In particular, to what extent they
should be considered "real" remains a matter of controversy in philosophy of
science. Also their origin and development has only recently come into focus of
scholarship in the history of science. In this study, we propose using the
intriguing case of virtual particles to discuss the efficacy of Semantic Change
Detection (SCD) based on contextualized word embeddings from a domain-adapted
BERT model in studying specific scientific concepts. We find that the SCD
metrics align well with qualitative research insights in the history and
philosophy of science, as well as with the results obtained from Dependency
Parsing to determine the frequency and connotations of the term "virtual."
Still, the metrics of SCD provide additional insights over and above the
qualitative research and the Dependency Parsing. Among other things, the
metrics suggest that the concept of the virtual particle became more stable
after 1950 but at the same time also more polysemous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHR 2024: Computational Humanities Research Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ETHIC: Evaluating Large Language Models on <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span> Tasks with High
  Information Coverage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLM) capable of processing
extremely long texts highlight the need for a dedicated evaluation benchmark to
assess their long-context capabilities. However, existing methods, like the
needle-in-a-haystack test, do not effectively assess whether these models fully
utilize contextual information, raising concerns about the reliability of
current evaluation techniques. To thoroughly examine the effectiveness of
existing benchmarks, we introduce a new metric called information coverage
(IC), which quantifies the proportion of the input context necessary for
answering queries. Our findings indicate that current benchmarks exhibit low
IC; although the input context may be extensive, the actual usable context is
often limited. To address this, we present ETHIC, a novel benchmark designed to
assess LLMs' ability to leverage the entire context. Our benchmark comprises
2,648 test instances spanning four long-context tasks with high IC scores in
the domains of books, debates, medicine, and law. Our evaluations reveal
significant performance drops in contemporary LLMs, highlighting a critical
challenge in managing long contexts. Our benchmark is available at
https://github.com/dmis-lab/ETHIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthy Alignment of Retrieval-Augmented Large Language Models via
  Reinforcement Learning <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongmeng Zhang, Yufeng Shi, Jinhua Zhu, Wengang Zhou, Xiang Qi, Peng Zhang, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthiness is an essential prerequisite for the real-world application
of large language models. In this paper, we focus on the trustworthiness of
language models with respect to retrieval augmentation. Despite being supported
with external evidence, retrieval-augmented generation still suffers from
hallucinations, one primary cause of which is the conflict between contextual
and parametric knowledge. We deem that retrieval-augmented language models have
the inherent capabilities of supplying response according to both contextual
and parametric knowledge. Inspired by aligning language models with human
preference, we take the first step towards aligning retrieval-augmented
language models to a status where it responds relying merely on the external
evidence and disregards the interference of parametric knowledge. Specifically,
we propose a reinforcement learning based algorithm Trustworthy-Alignment,
theoretically and experimentally demonstrating large language models'
capability of reaching a trustworthy status without explicit supervision on how
to respond. Our work highlights the potential of large language models on
exploring its intrinsic abilities by its own and expands the application
scenarios of alignment from fulfilling human preference to creating trustworthy
agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of <span class="highlight-title">Transformer</span>-Based Encoder-Decoder Model for Human-Like
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sindhu Nair, Y. S. Rao, Radha Shankarmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, extracting valuable information from large text is making
significant progress. Especially in the current era of social media, people
expect quick bites of information. Automatic text summarization seeks to tackle
this by slimming large texts down into more manageable summaries. This
important research area can aid in decision-making by digging out salient
content from large text. With the progress in deep learning models, significant
work in language models has emerged. The encoder-decoder framework in deep
learning has become the central approach for automatic text summarization. This
work leverages transformer-based BART model for human-like summarization which
is an open-ended problem with many challenges. On training and fine-tuning the
encoder-decoder model, it is tested with diverse sample articles and the
quality of summaries of diverse samples is assessed based on human evaluation
parameters. Further, the finetuned model performance is compared with the
baseline pretrained model based on evaluation metrics like ROUGE score and
BERTScore. Additionally, domain adaptation of the model is required for
improved performance of abstractive summarization of dialogues between
interlocutors. On investigating, the above popular evaluation metrics are found
to be insensitive to factual errors. Further investigation of the summaries
generated by finetuned model is done using the contemporary evaluation metrics
of factual consistency like WeCheck and SummaC. Empirical results on BBC News
articles highlight that the gold standard summaries written by humans are more
factually consistent by 17% than the abstractive summaries generated by
finetuned model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing and Evaluating Correlation Measures in NLG Meta-Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Gao, Xinyu Hu, Li Lin, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The correlation between NLG automatic evaluation metrics and human evaluation
is often regarded as a critical criterion for assessing the capability of an
evaluation metric. However, different grouping methods and correlation
coefficients result in various types of correlation measures used in
meta-evaluation. In specific evaluation scenarios, prior work often directly
follows conventional measure settings, but the characteristics and differences
between these measures have not gotten sufficient attention. Therefore, this
paper analyzes 12 common correlation measures using a large amount of
real-world data from six widely-used NLG evaluation datasets and 32 evaluation
metrics, revealing that different measures indeed impact the meta-evaluation
results. Furthermore, we propose three perspectives that reflect the capability
of meta-evaluation and find that the measure using global grouping and Pearson
correlation exhibits the best overall performance, involving the discriminative
power, ranking consistency, and sensitivity to score granularity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Chain-of-Thought Reasoning: Tackling Arranging Bottleneck via
  Plan Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Qiu, Jiashu Yao, Heyan Huang, Yuhang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step reasoning ability of large language models is crucial in tasks
such as math and tool utilization. Current researches predominantly focus on
enhancing model performance in these multi-step reasoning tasks through
fine-tuning with Chain-of-Thought (CoT) steps, yet these methods tend to be
heuristic, without exploring nor resolving the bottleneck. In this study, we
subdivide CoT reasoning into two parts: arranging and executing, and identify
that the bottleneck of models mainly lies in arranging rather than executing.
Based on this finding, we propose a plan-based training and reasoning method
that guides models to generate arranging steps through abstract plans. We
experiment on both math (GSM8k) and tool utilization (ToolBench) benchmarks.
Results show that compared to fine-tuning directly with CoT data, our approach
achieves a better performance on alleviating arranging bottleneck, particularly
excelling in long-distance reasoning generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Context</span>-aware Inductive Knowledge Graph Completion with Latent Type
  Constraints and Subgraph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muzhi Li, Cehao Yang, Chengjin Xu, Zixing Song, Xuhui Jiang, Jian Guo, Ho-fung Leung, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive knowledge graph completion (KGC) aims to predict missing triples
with unseen entities. Recent works focus on modeling reasoning paths between
the head and tail entity as direct supporting evidence. However, these methods
depend heavily on the existence and quality of reasoning paths, which limits
their general applicability in different scenarios. In addition, we observe
that latent type constraints and neighboring facts inherent in KGs are also
vital in inferring missing triples. To effectively utilize all useful
information in KGs, we introduce CATS, a novel context-aware inductive KGC
solution. With sufficient guidance from proper prompts and supervised
fine-tuning, CATS activates the strong semantic understanding and reasoning
capabilities of large language models to assess the existence of query triples,
which consist of two modules. First, the type-aware reasoning module evaluates
whether the candidate entity matches the latent entity type as required by the
query relation. Then, the subgraph reasoning module selects relevant reasoning
paths and neighboring facts, and evaluates their correlation to the query
triple. Experiment results on three widely used datasets demonstrate that CATS
significantly outperforms state-of-the-art methods in 16 out of 18
transductive, inductive, and few-shot settings with an average absolute MRR
improvement of 7.2%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Low-Rank Adaptation with Subspace Regularization for
  Continued Training on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Lu, Bingshuo Qian, Caixia Yuan, Huixing Jiang, Xiaojie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities in natural
language processing but face catastrophic forgetting when learning new tasks,
where adaptation to a new domain leads to a substantial decline in performance
on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a
subspace regularization method on LoRA structure. Aiming to reduce the scale of
output change while introduce minimal constraint on model capacity, CLoRA
imposes constraint on the direction of updating matrix null space. Experimental
results on commonly used LLM finetuning tasks reveal that CLoRA significantly
outperforms existing LoRA subsequent methods on both in-domain and outdomain
evaluations, highlighting the superority of CLoRA as a effective
parameter-efficient finetuning method with catastrophic forgetting mitigating.
Further investigation for model parameters indicates that CLoRA effectively
balances the trade-off between model capacity and degree of forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correct after Answer: Enhancing Multi-Span Question Answering with
  Post-Processing Method <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Lin, Chenyang Zhang, Haibo Tong, Dongyu Zhang, Qingqing Hong, Bingxuan Hou, Junli Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Span Question Answering (MSQA) requires models to extract one or
multiple answer spans from a given context to answer a question. Prior work
mainly focuses on designing specific methods or applying heuristic strategies
to encourage models to predict more correct predictions. However, these models
are trained on gold answers and fail to consider the incorrect predictions.
Through a statistical analysis, we observe that models with stronger abilities
do not predict less incorrect predictions compared with other models. In this
work, we propose Answering-Classifying-Correcting (ACC) framework, which
employs a post-processing strategy to handle incorrect predictions.
Specifically, the ACC framework first introduces a classifier to classify the
predictions into three types and exclude "wrong predictions", then introduces a
corrector to modify "partially correct predictions". Experiments on several
MSQA datasets show that ACC framework significantly improves the Exact Match
(EM) scores, and further analysis demostrates that ACC framework efficiently
reduces the number of incorrect predictions, improving the quality of
predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Retrieval: Generating Narratives in Conversational Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in Large Language Model's generation and reasoning
capabilities present an opportunity to develop truly conversational
recommendation systems. However, effectively integrating recommender system
knowledge into LLMs for natural language generation which is tailored towards
recommendation tasks remains a challenge. This paper addresses this challenge
by making two key contributions.
  First, we introduce a new dataset (REGEN) for natural language generation
tasks in conversational recommendations. REGEN (Reviews Enhanced with
GEnerative Narratives) extends the Amazon Product Reviews dataset with rich
user narratives, including personalized explanations of product preferences,
product endorsements for recommended items, and summaries of user purchase
history. REGEN is made publicly available to facilitate further research.
Furthermore, we establish benchmarks using well-known generative metrics, and
perform an automated evaluation of the new dataset using a rater LLM. Second,
the paper introduces a fusion architecture (CF model with an LLM) which serves
as a baseline for REGEN. And to the best of our knowledge, represents the first
attempt to analyze the capabilities of LLMs in understanding recommender
signals and generating rich narratives. We demonstrate that LLMs can
effectively learn from simple fusion architectures utilizing interaction-based
CF embeddings, and this can be further enhanced using the metadata and
personalization data associated with items. Our experiments show that combining
CF and content embeddings leads to improvements of 4-12% in key language
metrics compared to using either type of embedding individually. We also
provide an analysis to interpret how CF and content embeddings contribute to
this new generative task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Context</span>-Aware LLM Translation System Using Conversation Summarization
  and Dialogue History 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingi Sung, Seungmin Lee, Jiwon Kim, Sejoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating conversational text, particularly in customer support contexts,
presents unique challenges due to its informal and unstructured nature. We
propose a context-aware LLM translation system that leverages conversation
summarization and dialogue history to enhance translation quality for the
English-Korean language pair. Our approach incorporates the two most recent
dialogues as raw data and a summary of earlier conversations to manage context
length effectively. We demonstrate that this method significantly improves
translation accuracy, maintaining coherence and consistency across
conversations. This system offers a practical solution for customer support
translation tasks, addressing the complexities of conversational text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WMT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through
  Failure-Inducing Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Li, Jiahui Gao, Sheng Wang, Renjie Pi, Xueliang Zhao, Chuan Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly benefited from training on
diverse, high-quality task-specific data, leading to impressive performance
across a range of downstream applications. Current methods often rely on
human-annotated data or predefined task templates to direct powerful LLMs in
synthesizing task-relevant data for effective model training. However, this
dependence on manually designed components may constrain the scope of generated
data, potentially overlooking critical edge cases or novel scenarios that could
challenge the model. In this paper, we present a novel approach, ReverseGen,
designed to automatically generate effective training samples that expose the
weaknesses of LLMs. Specifically, we introduce a dedicated proposer trained to
produce queries that lead target models to generate unsatisfactory responses.
These failure-inducing queries are then used to construct training data,
helping to address the models' shortcomings and improve overall performance.
Our approach is flexible and can be applied to models of various scales (3B,
7B, and 8B). We evaluate ReverseGen on three key applications (safety, honesty,
and math), demonstrating that our generated data is both highly effective and
diverse. Models fine-tuned with ReverseGen-generated data consistently
outperform those trained on human-annotated or general model-generated data,
offering a new perspective on data synthesis for task-specific LLM enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanrou Yang, Fan Yu, Ziyang Ma, Zhihao Du, Zhifu Gao, Shiliang Zhang, Xie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While automatic speech recognition (ASR) systems have achieved remarkable
performance with large-scale datasets, their efficacy remains inadequate in
low-resource settings, encompassing dialects, accents, minority languages, and
long-tail hotwords, domains with significant practical relevance. With the
advent of versatile and powerful text-to-speech (TTS) models, capable of
generating speech with human-level naturalness, expressiveness, and diverse
speaker profiles, leveraging TTS for ASR data augmentation provides a
cost-effective and practical approach to enhancing ASR performance.
Comprehensive experiments on an unprecedentedly rich variety of low-resource
datasets demonstrate consistent and substantial performance improvements,
proving that the proposed method of enhancing low-resource ASR through a
versatile TTS model is highly effective and has broad application prospects.
Furthermore, we delve deeper into key characteristics of synthesized speech
data that contribute to ASR improvement, examining factors such as text
diversity, speaker diversity, and the volume of synthesized data, with text
diversity being studied for the first time in this work. We hope our findings
provide helpful guidance and reference for the practical application of
TTS-based data augmentation and push the advancement of low-resource ASR one
step further.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magnetic Preference Optimization: Achieving Last-iterate Convergence for
  Language Models Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingzhi Wang, Chengdong Ma, Qizhi Chen, Linjian Meng, Yang Han, Jiancong Xiao, Zhaowei Zhang, Jing Huo, Weijie J. Su, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-play methods have demonstrated remarkable success in enhancing model
capabilities across various domains. In the context of Reinforcement Learning
from Human Feedback (RLHF), self-play not only boosts Large Language Model
(LLM) performance but also overcomes the limitations of traditional
Bradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a
preference-based, two-player constant-sum game. However, existing methods
either guarantee only average-iterate convergence, incurring high storage and
inference costs, or converge to the NE of a regularized game, failing to
accurately reflect true human preferences. In this paper, we introduce Magnetic
Preference Optimization (MPO), a novel approach capable of achieving
last-iterate convergence to the NE of the original game, effectively overcoming
the limitations of existing methods. Building upon Magnetic Mirror Descent
(MMD), MPO attains a linear convergence rate, making it particularly suitable
for fine-tuning LLMs. To ensure our algorithm is both theoretically sound and
practically viable, we present a simple yet effective implementation that
adapts the theoretical insights to the RLHF setting. Empirical results
demonstrate that MPO can significantly enhance the performance of LLMs,
highlighting the potential of self-play methods in alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DENOASR: Debiasing ASRs through Selective Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Kumar Rai, Siddharth D Jaiswal, Shubham Prakash, Bendi Pragnya Sree, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) systems have been examined and shown to
exhibit biases toward particular groups of individuals, influenced by factors
such as demographic traits, accents, and speech styles. Noise can
disproportionately impact speakers with certain accents, dialects, or speaking
styles, leading to biased error rates. In this work, we introduce a novel
framework DENOASR, which is a selective denoising technique to reduce the
disparity in the word error rates between the two gender groups, male and
female. We find that a combination of two popular speech denoising techniques,
viz. DEMUCS and LE, can be effectively used to mitigate ASR disparity without
compromising their overall performance. Experiments using two state-of-the-art
open-source ASRs - OpenAI WHISPER and NVIDIA NEMO - on multiple benchmark
datasets, including TIE, VOX-POPULI, TEDLIUM, and FLEURS, show that there is a
promising reduction in the average word error rate gap across the two gender
groups. For a given dataset, the denoising is selectively applied on speech
samples having speech intelligibility below a certain threshold, estimated
using a small validation sample, thus ameliorating the need for large-scale
human-written ground-truth transcripts. Our findings suggest that selective
denoising can be an elegant approach to mitigate biases in present-day ASR
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at IEEE ICKG 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influential Language Data Selection via Gradient Trajectory Pursuit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Deng, Tao Li, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curating a desirable dataset for training has been the core of building
highly capable large language models (Touvron et al., 2023; Achiam et al.,
2023; Team et al.,2024). Gradient influence scores (Pruthi et al., 2020; Xia et
al., 2024) are shown to be correlated with model performance and are commonly
used as the criterion for data selection. However, existing methods are built
upon either individual sample rankings or inefficient matching process, leading
to suboptimal performance or scaling up issues.In this paper, we propose
Gradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of
gradient trajectories via jointly selecting data points under an L0-norm
regularized objective. The proposed algorithm highlights: (1) joint selection
instead of independent top-k selection, which automatically de-duplicates
samples; (2) higher efficiency with compressive sampling processes, which can
be further sped up using a distributed framework. In the experiments, we
demonstrate the algorithm in both in-domain and target-domain selection
benchmarks and show that it outperforms top-k selection and competitive
algorithms consistently, for example, our algorithm chooses as low as 0.5% data
to achieve full performance on the targeted instruction tuning tasks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atomic Fact Decomposition Helps Attributed Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Xiaoli Li, Ru Li, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attributed Question Answering (AQA) aims to provide both a trustworthy answer
and a reliable attribution report for a given question. Retrieval is a widely
adopted approach, including two general paradigms: Retrieval-Then-Read (RTR)
and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown
remarkable proficiency, prompting growing interest in AQA among researchers.
However, RTR-based AQA often suffers from irrelevant knowledge and rapidly
changing information, even when LLMs are adopted, while post-hoc
retrieval-based AQA struggles with comprehending long-form answers with complex
logic, and precisely identifying the content needing revision and preserving
the original intent. To tackle these problems, this paper proposes an Atomic
fact decomposition-based Retrieval and Editing (ARE) framework, which
decomposes the generated long-form answers into molecular clauses and atomic
facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are
fine-tuned using a well-constructed dataset, generated from large scale
Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from
a given set of entities and transforming the result into coherent long-form
text. Subsequently, ARE leverages a search engine to retrieve evidences related
to atomic facts, inputting these evidences into an LLM-based verifier to
determine whether the facts require expansion for re-retrieval or editing.
Furthermore, the edited facts are backtracked into the original answer, with
evidence aggregated based on the relationship between molecular clauses and
atomic facts. Extensive evaluations demonstrate the superior performance of our
proposed method over the state-of-the-arts on several datasets, with an
additionally proposed new metric $Attr_{p}$ for evaluating the precision of
evidence attribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLDR-LLM: Large Language Model from Power Law Decoder Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burc Gokden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Large Language Model from Power Law Decoder Representations
(PLDR-LLM), a language model that leverages non-linear and linear
transformations through Power Law Graph Attention mechanism to generate
well-defined deductive and inductive outputs. We pretrain the PLDR-LLMs of
varying layer sizes with a small batch size of 32 and $\sim$8B tokens from the
RefinedWeb dataset, and show that they achieve competitive performance in
zero-shot and few-shot settings compared to scaled dot-product LLMs of similar
model size reported in the literature. We show that deductive outputs of
PLDR-LLMs can be used to compare model characteristics or improve the
performance by introducing the Directed Acyclic Graph (DAG) loss as a metric
and regularizer. Our results indicate that the initial maximum learning rate
and warm-up steps have a lasting impact on deductive outputs throughout the
pretraining. We provide a detailed description of PLDR-LLM architecture, its
implementation and the pretraining procedure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methods of improving LLM training stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Rybakov, Mike Chrzanowski, Peter Dykas, Jinze Xue, Ben Lanir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training stability of large language models(LLMs) is an important research
topic. Reproducing training instabilities can be costly, so we use a small
language model with 830M parameters and experiment with higher learning rates
to force models to diverge. One of the sources of training instability is the
growth of logits in attention layers. We extend the focus of the previous work
and look not only at the magnitude of the logits but at all outputs of linear
layers in the Transformer block. We observe that with a high learning rate the
L2 norm of all linear layer outputs can grow with each training step and the
model diverges. Specifically we observe that QKV, Proj and FC2 layers have the
largest growth of the output magnitude. This prompts us to explore several
options: 1) apply layer normalization not only after QK layers but also after
Proj and FC2 layers too; 2) apply layer normalization after the QKV layer (and
remove pre normalization). 3) apply QK layer normalization together with
softmax capping. We show that with the last two methods we can increase
learning rate by 1.5x (without model divergence) in comparison to an approach
based on QK layer normalization only. Also we observe significant perplexity
improvements for all three methods in comparison to the baseline model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Causal Reasoning in Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siheng Xiong, Delin Chen, Qingyang Wu, Longxuan Yu, Qingzhen Liu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning (CR) is a crucial aspect of intelligence, essential for
problem-solving, decision-making, and understanding the world. While large
language models (LLMs) can generate rationales for their outputs, their ability
to reliably perform causal reasoning remains uncertain, often falling short in
tasks requiring a deep understanding of causality. In this survey, we provide a
comprehensive review of research aimed at enhancing LLMs for causal reasoning.
We categorize existing methods based on the role of LLMs: either as reasoning
engines or as helpers providing knowledge or data to traditional CR methods,
followed by a detailed discussion of the methodologies in each category. We
then evaluate the performance of LLMs on various causal reasoning tasks,
providing key findings and in-depth analysis. Finally, we provide insights from
current studies and highlight promising directions for future research. We aim
for this work to serve as a comprehensive resource, fostering further
advancements in causal reasoning with LLMs. Resources are available at
https://github.com/chendl02/Awesome-LLM-causal-reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafetyAnalyst: Interpretable, transparent, and steerable LLM safety
  moderation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ideal LLM content moderation system would be both structurally
interpretable (so its decisions can be explained to users) and steerable (to
reflect a community's values or align to safety standards). However, current
systems fall short on both of these dimensions. To address this gap, we present
SafetyAnalyst, a novel LLM safety moderation framework. Given a prompt,
SafetyAnalyst creates a structured "harm-benefit tree," which identifies 1) the
actions that could be taken if a compliant response were provided, 2) the
harmful and beneficial effects of those actions (along with their likelihood,
severity, and immediacy), and 3) the stakeholders that would be impacted by
those effects. It then aggregates this structured representation into a
harmfulness score based on a parameterized set of safety preferences, which can
be transparently aligned to particular values. Using extensive harm-benefit
features generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM
to specialize in generating harm-benefit trees through symbolic knowledge
distillation. On a comprehensive set of prompt safety benchmarks, we show that
our system (average F1=0.75) outperforms existing LLM safety moderation systems
(average F1$<$0.72) on prompt harmfulness classification, while offering the
additional advantages of interpretability and steerability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary
  Detection in Partially Machine Generated Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Mohan Rao Kadiyala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With increasing usage of generative models for text generation and widespread
use of machine generated texts in various domains, being able to distinguish
between human written and machine generated texts is a significant challenge.
While existing models and proprietary systems focus on identifying whether
given text is entirely human written or entirely machine generated, only a few
systems provide insights at sentence or paragraph level at likelihood of being
machine generated at a non reliable accuracy level, working well only for a set
of domains and generators. This paper introduces few reliable approaches for
the novel task of identifying which part of a given text is machine generated
at a word level while comparing results from different approaches and methods.
We present a comparison with proprietary systems , performance of our model on
unseen domains' and generators' texts. The findings reveal significant
improvements in detection accuracy along with comparison on other aspects of
detection capabilities. Finally we discuss potential avenues for improvement
and implications of our work. The proposed model is also well suited for
detecting which parts of a text are machine generated in outputs of Instruct
variants of many LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at naacl 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adsorb-Agent: Autonomous Identification of Stable Adsorption
  Configurations via Large Language Model Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janghoon Ock, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adsorption energy is a key reactivity descriptor in catalysis, enabling the
efficient screening of potential catalysts. However, determining adsorption
energy involves comparing the energies of multiple adsorbate-catalyst
configurations, which is computationally demanding due to a large number of
possible configurations. Current algorithmic approaches typically enumerate
adsorption sites and configurations without leveraging theoretical insights to
guide the initial setup. In this work, we present Adsorb-Agent, a Large
Language Model (LLM) agent designed to efficiently derive system-specific
stable adsorption configurations with minimal human intervention. Adsorb-Agent
leverages built-in knowledge and emergent reasoning capabilities, significantly
reducing the number of initial configurations required while improving accuracy
in predicting the minimum adsorption energy. We demonstrate its performance
using two example systems, NNH-CuPd3 (111) and NNH-Mo3Pd (111), for the
Nitrogen Reduction Reaction (NRR), a sustainable alternative to the Haber-Bosch
process. Adsorb-Agent outperforms conventional "heuristic" and "random"
algorithms by identifying lower-energy configurations with fewer initial
setups, reducing computational costs while enhancing accuracy. This highlights
its potential to accelerate catalyst discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chatting with Bots: AI, Speech Acts, and the Edge of Assertion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iwan Williams, Tim Bayne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the question of whether large language model-powered
chatbots are capable of assertion. According to what we call the Thesis of
Chatbot Assertion (TCA), chatbots are the kinds of things that can assert, and
at least some of the output produced by current-generation chatbots qualifies
as assertion. We provide some motivation for TCA, arguing that it ought to be
taken seriously and not simply dismissed. We also review recent objections to
TCA, arguing that these objections are weighty. We thus confront the following
dilemma: how can we do justice to both the considerations for and against TCA?
We consider two influential responses to this dilemma - the first appeals to
the notion of proxy-assertion; the second appeals to fictionalism - and argue
that neither is satisfactory. Instead, reflecting on the ontogenesis of
assertion, we argue that we need to make space for a category of
proto-assertion. We then apply the category of proto-assertion to chatbots,
arguing that treating chatbots as proto-assertors provides a satisfactory
resolution to the dilemma of chatbot assertion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Statistical Analysis of LLMs' Self-Evaluation Using Proverbs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryosuke Sonoda, Ramya Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) such as ChatGPT, GPT-4, Claude-3, and Llama are
being integrated across a variety of industries. Despite this rapid
proliferation, experts are calling for caution in the interpretation and
adoption of LLMs, owing to numerous associated ethical concerns. Research has
also uncovered shortcomings in LLMs' reasoning and logical abilities, raising
questions on the potential of LLMs as evaluation tools. In this paper, we
investigate LLMs' self-evaluation capabilities on a novel proverb reasoning
task. We introduce a novel proverb database consisting of 300 proverb pairs
that are similar in intent but different in wordings, across topics spanning
gender, wisdom, and society. We propose tests to evaluate textual consistencies
as well as numerical consistencies across similar proverbs, and demonstrate the
effectiveness of our method and dataset in identifying failures in LLMs'
self-evaluation which in turn can highlight issues related to gender
stereotypes and lack of cultural understanding in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMScan: Causal Scan for LLM Misbehavior Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdi Zhang, Kai Kiat Goh, Peixin Zhang, Jun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Large Language Models (LLMs) across various fields,
their potential to generate untruthful, biased and harmful responses poses
significant risks, particularly in critical applications. This highlights the
urgent need for systematic methods to detect and prevent such misbehavior.
While existing approaches target specific issues such as harmful responses,
this work introduces LLMScan, an innovative LLM monitoring technique based on
causality analysis, offering a comprehensive solution. LLMScan systematically
monitors the inner workings of an LLM through the lens of causal inference,
operating on the premise that the LLM's `brain' behaves differently when
misbehaving. By analyzing the causal contributions of the LLM's input tokens
and transformer layers, LLMScan effectively detects misbehavior. Extensive
experiments across various tasks and models reveal clear distinctions in the
causal distributions between normal behavior and misbehavior, enabling the
development of accurate, lightweight detectors for a variety of misbehavior
detection tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Structured Trajectory Extraction from Travelogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aitaro Yamamoto, Hiroyuki Otomo, Hiroki Ouchi, Shohei Higashiyama, Hiroki Teranishi, Hiroyuki Shindo, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies on sequence-based extraction of human movement trajectories
have an issue of inadequate trajectory representation. Specifically, a pair of
locations may not be lined up in a sequence especially when one location
includes the other geographically. In this study, we propose a graph
representation that retains information on the geographic hierarchy as well as
the temporal order of visited locations, and have constructed a benchmark
dataset for graph-structured trajectory extraction. The experiments with our
baselines have demonstrated that it is possible to accurately predict visited
locations and the order among them, but it remains a challenge to predict the
hierarchical relations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for
  Improved Coverage and Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prafulla Kumar Choubey, Xin Su, Man Luo, Xiangyu Peng, Caiming Xiong, Tiep Le, Shachar Rosenman, Vasudev Lal, Phil Mui, Ricky Ho, Phillip Howard, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) generated by large language models (LLMs) are becoming
increasingly valuable for Retrieval-Augmented Generation (RAG) applications
that require knowledge-intensive reasoning. However, existing KG extraction
methods predominantly rely on prompt-based approaches, which are inefficient
for processing large-scale corpora. These approaches often suffer from
information loss, particularly with long documents, due to the lack of
specialized design for KG construction. Additionally, there is a gap in
evaluation datasets and methodologies for ontology-free KG construction. To
overcome these limitations, we propose SynthKG, a multi-step, document-level
ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM
on the synthesized document-KG pairs, we streamline the multi-step process into
a single-step KG generation approach called Distill-SynthKG, substantially
reducing the number of LLM inference calls. Furthermore, we re-purpose existing
question-answering datasets to establish KG evaluation datasets and introduce
new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a
novel graph-based retrieval framework for RAG. Experimental results demonstrate
that Distill-SynthKG not only surpasses all baseline models in KG quality --
including models up to eight times larger -- but also consistently excels in
retrieval and question-answering tasks. Our proposed graph retrieval framework
also outperforms all KG-retrieval methods across multiple benchmark datasets.
We release the SynthKG dataset and Distill-SynthKG model publicly to support
further research and development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViMGuard: A Novel Multi-Modal System for Video Misinformation Guarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Kan, Christopher Kan, Zaid Nabulsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of social media and short-form video (SFV) has facilitated a
breeding ground for misinformation. With the emergence of large language
models, significant research has gone into curbing this misinformation problem
with automatic false claim detection for text. Unfortunately, the automatic
detection of misinformation in SFV is a more complex problem that remains
largely unstudied. While text samples are monomodal (only containing words),
SFVs comprise three different modalities: words, visuals, and non-linguistic
audio. In this work, we introduce Video Masked Autoencoders for Misinformation
Guarding (ViMGuard), the first deep-learning architecture capable of
fact-checking an SFV through analysis of all three of its constituent
modalities. ViMGuard leverages a dual-component system. First, Video and Audio
Masked Autoencoders analyze the visual and non-linguistic audio elements of a
video to discern its intention; specifically whether it intends to make an
informative claim. If it is deemed that the SFV has informative intent, it is
passed through our second component: a Retrieval Augmented Generation system
that validates the factual accuracy of spoken words. In evaluation, ViMGuard
outperformed three cutting-edge fact-checkers, thus setting a new standard for
SFV fact-checking and marking a significant stride toward trustworthy news on
social platforms. To promote further testing and iteration, VimGuard was
deployed into a Chrome extension and all code was open-sourced on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Ding, Fuzhen Hu, Xuanze Zhao, Zixiao Jiang, Shamsul Nahar Abdullah, Deshinta Arrova Dewi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis has become increasingly important for assessing public
opinion and informing decision-making. Large language models (LLMs) have
revolutionized this field by capturing nuanced language patterns. However,
adapting LLMs to domain-specific sentiment analysis tasks remains challenging
due to computational constraints and the need for optimal fine-tuning. To
address these challenges, we propose a novel Dynamic Adaptive Rank Space
Exploration (DARSE) framework for efficient and effective sentiment analysis
using LLMs. DARSE consists of a coarse-grained greedy algorithm to identify the
optimal rank range, a fine-grained exploration algorithm to refine rank
selection, and a dynamic rank allocation method to determine the optimal rank
combination for each LLM layer. Extensive experiments demonstrate that DARSE
significantly improves sentiment analysis accuracy, achieving a 15.1%
improvement in MSE and a 4.3% improvement in accuracy compared to previous
work. Our framework strikes a balance between computational efficiency and
model performance, making it a promising approach for sentiment analysis with
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of
  Architectural Inductive Biases on Hallucination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Boxing Chen, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth in prominence of large language models (LLMs) in everyday life can
be largely attributed to their generative abilities, yet some of this is also
owed to the risks and costs associated with their use. On one front is their
tendency to \textit{hallucinate} false or misleading information, limiting
their reliability. On another is the increasing focus on the computational
limitations associated with traditional self-attention based LLMs, which has
brought about new alternatives, in particular recurrent models, meant to
overcome them. Yet it remains uncommon to consider these two concerns
simultaneously. Do changes in architecture exacerbate/alleviate existing
concerns about hallucinations? Do they affect how and where they occur? Through
an extensive evaluation, we study how these architecture-based inductive biases
affect the propensity to hallucinate. While hallucination remains a general
phenomenon not limited to specific architectures, the situations in which they
occur and the ease with which specific types of hallucinations can be induced
can significantly differ based on the model architecture. These findings
highlight the need for better understanding both these problems in conjunction
with each other, as well as consider how to design more universal techniques
for handling hallucinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
  Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhua Lin, Zhengzhang Chen, Yanchi Liu, Xujiang Zhao, Zongyu Wu, Junxiang Wang, Xiang Zhang, Suhang Wang, Haifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series data is ubiquitous across various domains, including
manufacturing, finance, and healthcare. High-quality annotations are essential
for effectively understanding time series and facilitating downstream tasks;
however, obtaining such annotations is challenging, particularly in
mission-critical domains. In this paper, we propose TESSA, a multi-agent system
designed to automatically generate both general and domain-specific annotations
for time series data. TESSA introduces two agents: a general annotation agent
and a domain-specific annotation agent. The general agent captures common
patterns and knowledge across multiple source domains, leveraging both
time-series-wise and text-wise features to generate general annotations.
Meanwhile, the domain-specific agent utilizes limited annotations from the
target domain to learn domain-specific terminology and generate targeted
annotations. Extensive experiments on multiple synthetic and real-world
datasets demonstrate that TESSA effectively generates high-quality annotations,
outperforming existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures, 24 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interação entre robôs humanoides: desenvolvendo a
  colaboração e comunicação autônoma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moraes Pablo, Peters Christopher, Rodríguez Mónica, Sodre Hiago, Mazondo Ahilen, Sandin Vincent, Barcelona Sebastian, Moraes William, Fernández Santiago, Assunção Nathalie, de Vargas Bruna, Dörnbach Tobias, Kelbouscas André, Grando Ricardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the interaction between humanoid robots NAO and
Pepper, emphasizing their potential applications in educational settings. NAO,
widely used in education, and Pepper, designed for social interactions, of er
new opportunities for autonomous communication and collaboration. Through a
series of programmed interactions, the robots demonstrated their ability to
communicate and coordinate actions autonomously, highlighting their potential
as tools for enhancing learning environments. The research also explores the
integration of emerging technologies, such as artificial intelligence, into
these systems, allowing robots to learn from each other and adapt their
behavior. The findings suggest that NAO and Pepper can significantly contribute
to both technical learning and the development of social and emotional skills
in students, of ering innovative pedagogical approaches through the use of
humanoid robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Portuguese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In <span class="highlight-title">Context</span> Learning and Reasoning for Symbolic Regression with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samiha Sharlin, Tyler R. Josephson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transformer-based machine learning models
that have shown remarkable performance in tasks for which they were not
explicitly trained. Here, we explore the potential of LLMs to perform symbolic
regression -- a machine-learning method for finding simple and accurate
equations from datasets. We prompt GPT-4 to suggest expressions from data,
which are then optimized and evaluated using external Python tools. These
results are fed back to GPT-4, which proposes improved expressions while
optimizing for complexity and loss. Using chain-of-thought prompting, we
instruct GPT-4 to analyze the data, prior expressions, and the scientific
context (expressed in natural language) for each problem before generating new
expressions. We evaluated the workflow in rediscovery of five well-known
scientific equations from experimental data, and on an additional dataset
without a known equation. GPT-4 successfully rediscovered all five equations,
and in general, performed better when prompted to use a scratchpad and consider
scientific context. We also demonstrate how strategic prompting improves the
model's performance and how the natural language interface simplifies
integrating theory with data. Although this approach does not outperform
established SR programs where target equations are more complex, LLMs can
nonetheless iterate toward improved solutions while following instructions and
incorporating scientific context in natural language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating AI-Generated Essays with GRE Analytical Writing Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhong, Jiangang Hao, Michael Fauss, Chen Li, Yuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent revolutionary advance in generative AI enables the generation of
realistic and coherent texts by large language models (LLMs). Despite many
existing evaluation metrics on the quality of the generated texts, there is
still a lack of rigorous assessment of how well LLMs perform in complex and
demanding writing assessments. This study examines essays generated by ten
leading LLMs for the analytical writing assessment of the Graduate Record Exam
(GRE). We assessed these essays using both human raters and the e-rater
automated scoring engine as used in the GRE scoring pipeline. Notably, the
top-performing GPT-4o received an average score of 4.67, falling between
"generally thoughtful, well-developed analysis of the issue and conveys meaning
clearly" and "presents a competent analysis of the issue and conveys meaning
with acceptable clarity" according to the GRE scoring guideline. We also
evaluated the detection accuracy of these essays, with detectors trained on
essays generated by the same and different LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Hernandes, Giulio Corsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current surge in Artificial Intelligence (AI) interest, reflected in
heightened media coverage since 2009, has sparked significant debate on AI's
implications for privacy, social justice, workers' rights, and democracy. The
media plays a crucial role in shaping public perception and acceptance of AI
technologies. However, research into how AI appears in media has primarily
focused on anglophone contexts, leaving a gap in understanding how AI is
represented globally. This study addresses this gap by analyzing 3,560 news
articles from Brazilian media published between July 1, 2023, and February 29,
2024, from 13 popular online news outlets. Using Computational Grounded Theory
(CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and
Named-Entity Recognition to investigate the main topics in AI coverage and the
entities represented. The findings reveal that Brazilian news coverage of AI is
dominated by topics related to applications in the workplace and product
launches, with limited space for societal concerns, which mostly focus on
deepfakes and electoral integrity. The analysis also highlights a significant
presence of industry-related entities, indicating a strong influence of
corporate agendas in the country's news. This study underscores the need for a
more critical and nuanced discussion of AI's societal impacts in Brazilian
media.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Influence and Fact Tracing for Large Language Model <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler A. Chang, Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, Ian Tenney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data attribution (TDA) methods aim to attribute model outputs back
to specific training examples, and the application of these methods to large
language model (LLM) outputs could significantly advance model transparency and
data curation. However, it has been challenging to date to apply these methods
to the full scale of LLM pretraining. In this paper, we refine existing
gradient-based methods to work effectively at scale, allowing us to retrieve
influential examples for an 8B-parameter language model from a pretraining
corpus of over 160B tokens with no need for subsampling or pre-filtering. Our
method combines several techniques, including optimizer state correction, a
task-specific Hessian approximation, and normalized encodings, which we find to
be critical for performance at scale. In quantitative evaluations on a fact
tracing task, our method performs best at identifying examples that influence
model predictions, but classical, model-agnostic retrieval methods such as BM25
still perform better at finding passages which explicitly contain relevant
facts. These results demonstrate a misalignment between factual attribution and
causal influence. With increasing model size and training tokens, we find that
influence more closely aligns with attribution. Finally, we examine different
types of examples identified as influential by our method, finding that while
many directly entail a particular fact, others support the same output by
reinforcing priors on relation types, common entities, and names.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chejian Xu, Mintong Kang, Jiawei Zhang, Zeyi Liao, Lingbo Mo, Mengqi Yuan, Huan Sun, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have revolutionized the creation of generalist
web agents, empowering them to autonomously complete diverse tasks on
real-world websites, thereby boosting human efficiency and productivity.
However, despite their remarkable capabilities, the safety and security of
these agents against malicious attacks remain critically underexplored, raising
significant concerns about their safe deployment. To uncover and exploit such
vulnerabilities in web agents, we provide AdvWeb, a novel black-box attack
framework designed against web agents. AdvWeb trains an adversarial prompter
model that generates and injects adversarial prompts into web pages, misleading
web agents into executing targeted adversarial actions such as inappropriate
stock purchases or incorrect bank transactions, actions that could lead to
severe real-world consequences. With only black-box access to the web agent, we
train and optimize the adversarial prompter model using DPO, leveraging both
successful and failed attack strings against the target agent. Unlike prior
approaches, our adversarial string injection maintains stealth and control: (1)
the appearance of the website remains unchanged before and after the attack,
making it nearly impossible for users to detect tampering, and (2) attackers
can modify specific substrings within the generated adversarial string to
seamlessly change the attack objective (e.g., purchasing stocks from a
different company), enhancing attack flexibility and efficiency. We conduct
extensive evaluations, demonstrating that AdvWeb achieves high success rates in
attacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings
expose critical vulnerabilities in current LLM/VLM-based agents, emphasizing
the urgent need for developing more reliable web agents and effective defenses.
Our code and data are available at https://ai-secure.github.io/AdvWeb/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Vision-Language Models Represent Space and How? Evaluating Spatial
  Frame of Reference Under Ambiguities <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, Ziqiao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial expressions in situated communication can be ambiguous, as their
meanings vary depending on the frames of reference (FoR) adopted by speakers
and listeners. While spatial language understanding and reasoning by
vision-language models (VLMs) have gained increasing attention, potential
ambiguities in these models are still under-explored. To address this issue, we
present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an
evaluation protocol to systematically assess the spatial reasoning capabilities
of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing
some alignment with English conventions in resolving ambiguities, our
experiments reveal significant shortcomings of VLMs: notably, the models (1)
exhibit poor robustness and consistency, (2) lack the flexibility to
accommodate multiple FoRs, and (3) fail to adhere to language-specific or
culture-specific conventions in cross-lingual tests, as English tends to
dominate other languages. With a growing effort to align vision-language models
with human cognitive intuitions, we call for more attention to the ambiguous
nature and cross-cultural diversity of spatial reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Pluralistic Alignment @ NeurIPS 2024 | Project page:
  https://spatial-comfort.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM
  Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bradley McDanel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models typically generate tokens autoregressively, using each
token as input for the next. Recent work on Speculative Decoding has sought to
accelerate this process by employing a smaller, faster draft model to more
quickly generate candidate tokens. These candidates are then verified in
parallel by the larger (original) verify model, resulting in overall speedup
compared to using the larger model by itself in an autoregressive fashion. In
this work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding),
a system that further accelerates generation by decoupling the draft and verify
phases into a continuous, asynchronous approach. Unlike conventional
speculative decoding, where only one model (draft or verify) performs token
generation at a time, AMUSD enables both models to perform predictions
independently on separate devices (e.g., GPUs). We evaluate our approach over
multiple datasets and show that AMUSD achieves an average 29% improvement over
speculative decoding and up to 1.96$\times$ speedup over conventional
autoregressive decoding, while achieving identical output quality. Our system
is open-source and available at https://github.com/BradMcDanel/AMUSD/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All Entities are Not Created Equal: Examining the <span class="highlight-title">Long</span> Tail for
  Fine-Grained Entity Typing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Advait Deshmukh, Ashwin Umadi, Dananjay Srinivas, Maria Leonor Pacheco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) are trained on large amounts of data,
which helps capture world knowledge alongside linguistic competence. Due to
this, they are extensively used for ultra-fine entity typing tasks, where they
provide the entity knowledge held in its parameter space. Given that PLMs learn
from co-occurrence patterns, they likely contain more knowledge or less
knowledge about entities depending on their how frequent they are in the
pre-training data. In this work, we probe PLMs to elicit encoded entity
probabilities and demonstrate that they highly correlate with their frequency
in large-scale internet data. Then, we demonstrate that entity-typing
approaches that rely on PLMs struggle with entities at the long tail on the
distribution. Our findings suggests that we need to go beyond PLMs to produce
solutions that perform well for rare, new or infrequent entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Captions Speak Louder than Images (CASLIE): Generalizing Foundation
  Models for E-commerce from High-quality Multimodal Instruction Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Ling, Bo Peng, Hanwen Du, Zhihui Zhu, Xia Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging multimodal data to drive breakthroughs in e-commerce applications
through Multimodal Foundation Models (MFMs) is gaining increasing attention
from the research community. However, there are significant challenges that
hinder the optimal use of multimodal e-commerce data by foundation models: (1)
the scarcity of large-scale, high-quality multimodal benchmark datasets; and
(2) the lack of effective multimodal information integration methods. To
address these challenges, in this paper, we introduce MMECInstruct, the
first-ever, large-scale, and high-quality multimodal instruction dataset for
e-commerce. We also develop CASLIE, a simple, lightweight, yet effective
framework for integrating multimodal information for e-commerce. Leveraging
MMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted
as CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models
substantially outperform 5 categories of advanced baseline models in the
in-domain evaluation. Moreover, CASLIE models show strong generalizability to
out-of-domain settings. MMECInstruct and CASLIE models are publicly accessible
through https://ninglab.github.io/CASLIE/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Xinyi Ling and Bo Peng contributed equally to this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Large Language Models Ready for Travel Planning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiping Ren, Xing Yao, Shu Cole, Haining Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) show promise in hospitality and tourism,
their ability to provide unbiased service across demographic groups remains
unclear. This paper explores gender and ethnic biases when LLMs are utilized as
travel planning assistants. To investigate this issue, we apply machine
learning techniques to analyze travel suggestions generated from three
open-source LLMs. Our findings reveal that the performance of race and gender
classifiers substantially exceeds random chance, indicating differences in how
LLMs engage with varied subgroups. Specifically, outputs align with cultural
expectations tied to certain races and genders. To minimize the effect of these
stereotypes, we used a stop-word classification strategy, which decreased
identifiable differences, with no disrespectful terms found. However,
hallucinations related to African American and gender minority groups were
noted. In conclusion, while LLMs can generate travel plans seemingly free from
bias, it remains essential to verify the accuracy and appropriateness of their
recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Literature Meets Data: A Synergistic Approach to Hypothesis Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI holds promise for transforming scientific processes, including hypothesis
generation. Prior work on hypothesis generation can be broadly categorized into
theory-driven and data-driven approaches. While both have proven effective in
generating novel and plausible hypotheses, it remains an open question whether
they can complement each other. To address this, we develop the first method
that combines literature-based insights with data to perform LLM-powered
hypothesis generation. We apply our method on five different datasets and
demonstrate that integrating literature and data outperforms other baselines
(8.97\% over few-shot, 15.75\% over literature-based alone, and 3.37\% over
data-driven alone). Additionally, we conduct the first human evaluation to
assess the utility of LLM-generated hypotheses in assisting human
decision-making on two challenging tasks: deception detection and AI generated
content detection. Our results show that human accuracy improves significantly
by 7.44\% and 14.19\% on these tasks, respectively. These findings suggest that
integrating literature-based and data-driven approaches provides a
comprehensive and nuanced framework for hypothesis generation and could open
new avenues for scientific inquiry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures, code link:
  https://github.com/ChicagoHAI/hypothesis-generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, Tao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing retrieval benchmarks primarily consist of information-seeking
queries (e.g., aggregated questions from search engines) where keyword or
semantic-based retrieval is usually sufficient. However, many complex
real-world queries require in-depth reasoning to identify relevant documents
that go beyond surface form matching. For example, finding documentation for a
coding question requires understanding the logic and syntax of the functions
involved. To better benchmark retrieval on such challenging queries, we
introduce BRIGHT, the first text retrieval benchmark that requires intensive
reasoning to retrieve relevant documents. Our dataset consists of 1,384
real-world queries spanning diverse domains, such as economics, psychology,
mathematics, and coding. These queries are drawn from naturally occurring and
carefully curated human data. Extensive evaluation reveals that even
state-of-the-art retrieval models perform poorly on BRIGHT. The leading model
on the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of
59.0 nDCG@10, produces a score of nDCG@10 of 18.3 on BRIGHT. We show that
incorporating explicit reasoning about the query improves retrieval performance
by up to 12.2 points. Moreover, incorporating retrieved documents from the
top-performing retriever boosts question-answering performance by over 6.6
points. We believe that BRIGHT paves the way for future research on retrieval
systems in more realistic and challenging settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Context</span>-Parametric Inversion: Why Instruction Finetuning May Not
  Actually Improve <span class="highlight-title">Context</span> Reliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachin Goyal, Christina Baek, J. Zico Kolter, Aditi Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A standard practice when using large language models is for users to
supplement their instruction with an input context containing new information
for the model to process. However, models struggle to reliably follow the input
context, especially when it conflicts with their parametric knowledge from
pretraining. In-principle, one would expect models to adapt to the user context
better after instruction finetuning, particularly when handling knowledge
conflicts. However, we observe a surprising failure mode: during instruction
tuning, the context reliance under knowledge conflicts initially increases as
expected, but then gradually decreases as instruction finetuning progresses.
This happens while the performance on standard benchmarks keeps on increasing
far after this drop. We call this phenomenon context-parametric inversion and
observe it across multiple general purpose instruction tuning datasets such as
TULU, Alpaca and Ultrachat, across different model families like Llama,
Mistral, and Pythia. We perform various controlled studies and theoretical
analysis to show that context-parametric inversion occurs due to examples in
the instruction finetuning data where the input context provides information
that aligns with model's parametric knowledge. Our analysis suggests some
natural mitigation strategies with limited but insightful gains, and serves as
a useful starting point in addressing this deficiency in instruction
finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Identify Authorship? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixiang Huang, Canyu Chen, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to accurately identify authorship is crucial for verifying
content authenticity and mitigating misinformation. Large Language Models
(LLMs) have demonstrated an exceptional capacity for reasoning and
problem-solving. However, their potential in authorship analysis remains
under-explored. Traditional studies have depended on hand-crafted stylistic
features, whereas state-of-the-art approaches leverage text embeddings from
pre-trained language models. These methods, which typically require fine-tuning
on labeled data, often suffer from performance degradation in cross-domain
applications and provide limited explainability. This work seeks to address
three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship
verification effectively? (2) Are LLMs capable of accurately attributing
authorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs
provide explainability in authorship analysis, particularly through the role of
linguistic features? Moreover, we investigate the integration of explicit
linguistic features to guide LLMs in their reasoning processes. Our assessment
demonstrates LLMs' proficiency in both tasks without the need for
domain-specific fine-tuning, providing explanations into their decision making
via a detailed analysis of linguistic features. This establishes a new
benchmark for future research on LLM-based authorship analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings. The main paper is 9 pages long, with
  16 pages total. The code, results, dataset, and additional resources are
  available on the project website: https://llm-authorship.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Large Language Models in Academia: from Writing to
  Speaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly impacting human society,
particularly in textual information. Based on more than 30,000 papers and 1,000
presentations from machine learning conferences, we examined and compared the
words used in writing and speaking, representing the first large-scale study of
how LLMs influence the two main modes of verbal communication and expression
within the same group of people. Our empirical results show that LLM-style
words such as "significant" have been used more frequently in abstracts and
oral presentations. The impact on speaking is beginning to emerge and is likely
to grow in the future, calling attention to the implicit influence and ripple
effect of LLMs on human society.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Levels of AI Agents: from Rules to Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents are defined as artificial entities to perceive the environment,
make decisions and take actions. Inspired by the 6 levels of autonomous driving
by Society of Automotive Engineers, the AI agents are also categorized based on
utilities and strongness, as the following levels: L0, no AI, with tools taking
into account perception plus actions; L1, using rule-based AI; L2, making
rule-based AI replaced by IL/RL-based AI, with additional reasoning & decision
making; L3, applying LLM-based AI instead of IL/RL-based AI, additionally
setting up memory & reflection; L4, based on L3, facilitating autonomous
learning & generalization; L5, based on L4, appending personality of emotion
and character and collaborative behavior with multi-agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs left, right, and center: Assessing <span class="highlight-title">GPT</span>'s capabilities to label
  political bias from web domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Hernandes, Giulio Corsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research investigates whether OpenAI's GPT-4, a state-of-the-art large
language model, can accurately classify the political bias of news sources
based solely on their URLs. Given the subjective nature of political labels,
third-party bias ratings like those from Ad Fontes Media, AllSides, and Media
Bias/Fact Check (MBFC) are often used in research to analyze news source
diversity. This study aims to determine if GPT-4 can replicate these human
ratings on a seven-degree scale ("far-left" to "far-right"). The analysis
compares GPT-4's classifications against MBFC's, and controls for website
popularity using Open PageRank scores. Findings reveal a high correlation
($\text{Spearman's } \rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and
MBFC's ratings, indicating the model's potential reliability. However, GPT-4
abstained from classifying approximately $\frac{2}{3}$ of the dataset. It is
more likely to abstain from rating unpopular websites, which also suffer from
less accurate assessments. The LLM tends to avoid classifying sources that MBFC
considers to be centrist, resulting in more polarized outputs. Finally, this
analysis shows a slight leftward skew in GPT's classifications compared to
MBFC's. Therefore, while this paper suggests that while GPT-4 can be a
scalable, cost-effective tool for political bias classification of news
websites, its use should be as a complement to human judgment to mitigate
biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and
  Tool Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Lumer, Vamse Kumar Subbiah, James A. Burke, Pradeep Honaganahalli Basavaraju, Austin Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks
like secure database interactions and multi-agent code development. However,
scaling tool capacity beyond agent reasoning or model limits remains a
challenge. In this paper, we address these challenges by introducing Toolshed
Knowledge Bases, a tool knowledge base (vector database) designed to store
enhanced tool representations and optimize tool selection for large-scale
tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a
novel ensemble of tool-applied advanced retrieval-augmented generation (RAG)
techniques across the pre-retrieval, intra-retrieval, and post-retrieval
phases, without requiring model fine-tuning. During pre-retrieval, tool
documents are enhanced with key information and stored in the Toolshed
Knowledge Base. Intra-retrieval focuses on query planning and transformation to
increase retrieval accuracy. Post-retrieval refines the retrieved tool
documents and enables self-reflection. Furthermore, by varying both the total
number of tools (tool-M) an Agent has access to and the tool selection
threshold (top-k), we address trade-offs between retrieval accuracy, agent
performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute
improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools
benchmark datasets, respectively (Recall@5).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalBench: Evaluating Vision-Language Models on Natural Adversarial
  Samples <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have made significant progress in recent
visual-question-answering (VQA) benchmarks that evaluate complex
visio-linguistic reasoning. However, are these models truly effective? In this
work, we show that VLMs still struggle with natural images and questions that
humans can easily answer, which we term natural adversarial samples. We also
find it surprisingly easy to generate these VQA samples from natural image-text
corpora using off-the-shelf models like CLIP and ChatGPT. We propose a
semi-automated approach to collect a new benchmark, NaturalBench, for reliably
evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a
$\textbf{vision-centric}$ design by pairing each question with two images that
yield different answers, preventing blind solutions from answering without
using the images. This makes NaturalBench more challenging than previous
benchmarks that can be solved with commonsense priors. We evaluate 53
state-of-the-art VLMs on NaturalBench, showing that models like
LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o
lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is
hard from two angles: (1) Compositionality: Solving NaturalBench requires
diverse visio-linguistic skills, including understanding attribute bindings,
object relationships, and advanced reasoning like logic and counting. To this
end, unlike prior work that uses a single tag per sample, we tag each
NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)
Biases: NaturalBench exposes severe biases in VLMs, as models often choose the
same answer regardless of the image. Lastly, we apply our benchmark curation
method to diverse data sources, including long captions (over 100 words) and
non-English languages like Chinese and Hindi, highlighting its potential for
dynamic evaluations of VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24; We open-source our dataset at:
  https://huggingface.co/datasets/BaiqiL/NaturalBench ; Project page at:
  https://linzhiqiu.github.io/papers/naturalbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do LLMs "know" internally when they follow instructions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Shirley Ren, Udhay Nallasamy, Andy Miller, Kwan Ho Ryan Chan, Jaya Narain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-following is crucial for building AI agents with large language
models (LLMs), as these models must adhere strictly to user-provided
constraints and guidelines. However, LLMs often fail to follow even simple and
clear instructions. To improve instruction-following behavior and prevent
undesirable outputs, a deeper understanding of how LLMs' internal states relate
to these outcomes is required. Our analysis of LLM internal states reveal a
dimension in the input embedding space linked to successful
instruction-following. We demonstrate that modifying representations along this
dimension improves instruction-following success rates compared to random
changes, without compromising response quality. Further investigation reveals
that this dimension is more closely related to the phrasing of prompts rather
than the inherent difficulty of the task or instructions. This discovery also
suggests explanations for why LLMs sometimes fail to follow clear instructions
and why prompt engineering is often effective, even when the content remains
largely unchanged. This work provides insight into the internal workings of
LLMs' instruction-following, paving the way for reliable LLM agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lex2Sent: A bagging approach to unsupervised sentiment analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai-Robin Lange, Jonas Rieger, Carsten Jentsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised text classification, with its most common form being sentiment
analysis, used to be performed by counting words in a text that were stored in
a lexicon, which assigns each word to one class or as a neutral word. In recent
years, these lexicon-based methods fell out of favor and were replaced by
computationally demanding fine-tuning techniques for encoder-only models such
as BERT and zero-shot classification using decoder-only models such as GPT-4.
In this paper, we propose an alternative approach: Lex2Sent, which provides
improvement over classic lexicon methods but does not require any GPU or
external hardware. To classify texts, we train embedding models to determine
the distances between document embeddings and the embeddings of the parts of a
suitable lexicon. We employ resampling, which results in a bagging effect,
boosting the performance of the classification. We show that our model
outperforms lexica and provides a basis for a high performing few-shot
fine-tuning approach in the task of binary sentiment analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do LLMs estimate uncertainty well in instruction-following? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) could be valuable personal AI agents across
various domains, provided they can precisely follow user instructions. However,
recent studies have shown significant limitations in LLMs'
instruction-following capabilities, raising concerns about their reliability in
high-stakes applications. Accurately estimating LLMs' uncertainty in adhering
to instructions is critical to mitigating deployment risks. We present, to our
knowledge, the first systematic evaluation of the uncertainty estimation
abilities of LLMs in the context of instruction-following. Our study identifies
key challenges with existing instruction-following benchmarks, where multiple
factors are entangled with uncertainty stems from instruction-following,
complicating the isolation and comparison across methods and models. To address
these issues, we introduce a controlled evaluation setup with two benchmark
versions of data, enabling a comprehensive comparison of uncertainty estimation
methods under various conditions. Our findings show that existing uncertainty
methods struggle, particularly when models make subtle errors in instruction
following. While internal model states provide some improvement, they remain
inadequate in more complex scenarios. The insights from our controlled
evaluation setups provide a crucial understanding of LLMs' limitations and
potential for uncertainty estimation in instruction-following tasks, paving the
way for more trustworthy AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Thousand and One Pairs: A "novel" challenge for <span class="highlight-title">long</span>-<span class="highlight-title">context</span>
  language models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16264v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16264v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test
only surface-level retrieval capabilities, but how well can long-context LLMs
retrieve, synthesize, and reason over information across book-length inputs? We
address this question by creating NoCha, a dataset of 1,001 minimally different
pairs of true and false claims about 67 recently-published English fictional
books, written by human readers of those books. In contrast to existing
long-context benchmarks, our annotators confirm that the largest share of pairs
in NoCha require global reasoning over the entire book to verify. Our
experiments show that while human readers easily perform this task, it is
enormously challenging for all ten long-context LLMs that we evaluate: no
open-weight model performs above random chance (despite their strong
performance on synthetic benchmarks), while GPT-4o achieves the highest
accuracy at 55.8%. Further analysis reveals that (1) on average, models perform
much better on pairs that require only sentence-level retrieval vs. global
reasoning; (2) model-generated explanations for their decisions are often
inaccurate even for correctly-labeled claims; and (3) models perform
substantially worse on speculative fiction books that contain extensive
world-building. The methodology proposed in NoCha allows for the evolution of
the benchmark dataset and the easy analysis of future models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024, camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SysBench: Can Large Language Models Follow System Messages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become instrumental across various
applications, with the customization of these models to specific scenarios
becoming increasingly critical. System message, a fundamental component of
LLMs, is consist of carefully crafted instructions that guide the behavior of
model to meet intended goals. Despite the recognized potential of system
messages to optimize AI-driven solutions, there is a notable absence of a
comprehensive benchmark for evaluating how well LLMs follow system messages. To
fill this gap, we introduce SysBench, a benchmark that systematically analyzes
system message following ability in terms of three limitations of existing
LLMs: constraint violation, instruction misjudgement and multi-turn
instability. Specifically, we manually construct evaluation dataset based on
six prevalent types of constraints, including 500 tailor-designed system
messages and multi-turn user conversations covering various interaction
relationships. Additionally, we develop a comprehensive evaluation protocol to
measure model performance. Finally, we conduct extensive evaluation across
various existing LLMs, measuring their ability to follow specified constraints
given in system messages. The results highlight both the strengths and
weaknesses of existing models, offering key insights and directions for future
research. The open source library SysBench is available at
https://github.com/PKU-Baichuan-MLSystemLab/SysBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holmes: A Benchmark to Assess the Linguistic Competence of Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18923v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18923v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Waldis, Yotam Perlitz, Leshem Choshen, Yufang Hou, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Holmes, a new benchmark designed to assess language models (LMs)
linguistic competence - their unconscious understanding of linguistic
phenomena. Specifically, we use classifier-based probing to examine LMs'
internal representations regarding distinct linguistic phenomena (e.g.,
part-of-speech tagging). As a result, we meet recent calls to disentangle LMs'
linguistic competence from other cognitive abilities, such as following
instructions in prompting-based evaluations. Composing Holmes, we review over
270 probing studies and include more than 200 datasets to assess syntax,
morphology, semantics, reasoning, and discourse phenomena. Analyzing over 50
LMs reveals that, aligned with known trends, their linguistic competence
correlates with model size. However, surprisingly, model architecture and
instruction tuning also significantly influence performance, particularly in
morphology and syntax. Finally, we propose FlashHolmes, a streamlined version
that reduces the computation load while maintaining high-ranking precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by
  Simulating Documents in the Wild via Low-level Perturbations <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of recent Large Language Models (LLMs) has become increasingly
crucial as their applicability expands across various domains and real-world
applications. Retrieval-Augmented Generation (RAG) is a promising solution for
addressing the limitations of LLMs, yet existing studies on the robustness of
RAG often overlook the interconnected relationships between RAG components or
the potential threats prevalent in real-world databases, such as minor textual
errors. In this work, we investigate two underexplored aspects when assessing
the robustness of RAG: 1) vulnerability to noisy documents through low-level
perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we
introduce a novel attack method, the Genetic Attack on RAG (\textit{GARAG}),
which targets these aspects. Specifically, GARAG is designed to reveal
vulnerabilities within each component and test the overall system functionality
against noisy documents. We validate RAG robustness by applying our
\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and
LLMs. The experimental results show that GARAG consistently achieves high
attack success rates. Also, it significantly devastates the performance of each
component and their synergy, highlighting the substantial risk that minor
textual inaccuracies pose in disrupting RAG systems in the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moonshine: Speech Recognition for Live Transcription and Voice Commands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nat Jeffries, Evan King, Manjunath Kudlur, Guy Nicholson, James Wang, Pete Warden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Moonshine, a family of speech recognition models
optimized for live transcription and voice command processing. Moonshine is
based on an encoder-decoder transformer architecture and employs Rotary
Position Embedding (RoPE) instead of traditional absolute position embeddings.
The model is trained on speech segments of various lengths, but without using
zero-padding, leading to greater efficiency for the encoder during inference
time. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny
demonstrates a 5x reduction in compute requirements for transcribing a
10-second speech segment while incurring no increase in word error rates across
standard evaluation datasets. These results highlight Moonshine's potential for
real-time and resource-constrained applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Device LLMs for SMEs: Challenges and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Stephen Gabriel Yee, Pai Chet Ng, Zhengkui Wang, Ian McLoughlin, Aik Beng Ng, Simon See
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic review of the infrastructure requirements
for deploying Large Language Models (LLMs) on-device within the context of
small and medium-sized enterprises (SMEs), focusing on both hardware and
software perspectives. From the hardware viewpoint, we discuss the utilization
of processing units like GPUs and TPUs, efficient memory and storage solutions,
and strategies for effective deployment, addressing the challenges of limited
computational resources typical in SME settings. From the software perspective,
we explore framework compatibility, operating system optimization, and the use
of specialized libraries tailored for resource-constrained environments. The
review is structured to first identify the unique challenges faced by SMEs in
deploying LLMs on-device, followed by an exploration of the opportunities that
both hardware innovations and software adaptations offer to overcome these
obstacles. Such a structured review provides practical insights, contributing
significantly to the community by enhancing the technological resilience of
SMEs in integrating LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI
  Centre</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Gesticulator: Leveraging Large Language Models for Scalable and
  Controllable Co-Speech Gesture Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhou Pang, Tianwei Ding, Lanshan He, Ming Tao, Lu Zhang, Qi Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present LLM Gesticulator, an LLM-based audio-driven
co-speech gesture generation framework that synthesizes full-body animations
that are rhythmically aligned with the input audio while exhibiting natural
movements and editability. Compared to previous work, our model demonstrates
substantial scalability. As the size of the backbone LLM model increases, our
framework shows proportional improvements in evaluation metrics (a.k.a. scaling
law). Our method also exhibits strong controllability where the content, style
of the generated gestures can be controlled by text prompt. To the best of our
knowledge, LLM gesticulator is the first work that use LLM on the co-speech
generation task. Evaluation with existing objective metrics and user studies
indicate that our framework outperforms prior works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ System 2 thinking in OpenAI's o1-p<span class="highlight-title">review</span> model: Near-perfect performance
  on a mathematics exam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07114v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07114v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joost de Winter, Dimitra Dodou, Yke Bauke Eisma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The processes underlying human cognition are often divided into System 1,
which involves fast, intuitive thinking, and System 2, which involves slow,
deliberate reasoning. Previously, large language models were criticized for
lacking the deeper, more analytical capabilities of System 2. In September
2024, OpenAI introduced the o1 model series, designed to handle System 2-like
reasoning. While OpenAI's benchmarks are promising, independent validation is
still needed. In this study, we tested the o1-preview model twice on the Dutch
'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76
points. For context, only 24 out of 16,414 students in the Netherlands achieved
a perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,
well above the Dutch average of 40.63 points. Neither model had access to the
exam figures. Since there was a risk of model contamination (i.e., the
knowledge cutoff of o1-preview and GPT-4o was after the exam was published
online), we repeated the procedure with a new Mathematics B exam that was
published after the cutoff date. The results again indicated that o1-preview
performed strongly (97.8th percentile), which suggests that contamination was
not a factor. We also show that there is some variability in the output of
o1-preview, which means that sometimes there is 'luck' (the answer is correct)
or 'bad luck' (the output has diverged into something that is incorrect). We
demonstrate that a self-consistency approach, where repeated prompts are given
and the most common answer is selected, is a useful strategy for identifying
the correct answer. It is concluded that while OpenAI's new model series holds
great potential, certain risks must be considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLBench: A Comprehensive Benchmark for Graph with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07457v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07457v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large language models (LLMs) has revolutionized the way we
interact with graphs, leading to a new paradigm called GraphLLM. Despite the
rapid development of GraphLLM methods in recent years, the progress and
understanding of this field remain unclear due to the lack of a benchmark with
consistent experimental protocols. To bridge this gap, we introduce GLBench,
the first comprehensive benchmark for evaluating GraphLLM methods in both
supervised and zero-shot scenarios. GLBench provides a fair and thorough
evaluation of different categories of GraphLLM methods, along with traditional
baselines such as graph neural networks. Through extensive experiments on a
collection of real-world datasets with consistent data processing and splitting
strategies, we have uncovered several key findings. Firstly, GraphLLM methods
outperform traditional baselines in supervised settings, with LLM-as-enhancers
showing the most robust performance. However, using LLMs as predictors is less
effective and often leads to uncontrollable output issues. We also notice that
no clear scaling laws exist for current GraphLLM methods. In addition, both
structures and semantics are crucial for effective zero-shot transfer, and our
proposed simple baseline can even outperform several models tailored for
zero-shot scenarios. The data and code of the benchmark can be found at
https://github.com/NineAbyss/GLBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TempoFormer: A <span class="highlight-title">Transformer</span> for Temporally-aware Representations in
  Change Detection <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Talia Tseriotou, Adam Tsakalidis, Maria Liakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic representation learning plays a pivotal role in understanding the
evolution of linguistic content over time. On this front both context and time
dynamics as well as their interplay are of prime importance. Current approaches
model context via pre-trained representations, which are typically temporally
agnostic. Previous work on modelling context and temporal dynamics has used
recurrent methods, which are slow and prone to overfitting. Here we introduce
TempoFormer, the first task-agnostic transformer-based and temporally-aware
model for dynamic representation learning. Our approach is jointly trained on
inter and intra context dynamics and introduces a novel temporal variation of
rotary positional embeddings. The architecture is flexible and can be used as
the temporal representation foundation of other models or applied to different
transformer-based architectures. We show new SOTA performance on three
different real-time change detection tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP Main 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stacking Your <span class="highlight-title">Transformer</span>s: A Closer Look at Model Growth for Efficient
  LLM <span class="highlight-title">Pre-Train</span>ing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are computationally expensive to pre-train due to their large scale.
Model growth emerges as a promising approach by leveraging smaller models to
accelerate the training of larger ones. However, the viability of these model
growth methods in efficient LLM pre-training remains underexplored. This work
identifies three critical $\underline{\textit{O}}$bstacles: ($\textit{O}$1)
lack of comprehensive evaluation, ($\textit{O}$2) untested viability for
scaling, and ($\textit{O}$3) lack of empirical guidelines. To tackle
$\textit{O}$1, we summarize existing approaches into four atomic growth
operators and systematically evaluate them in a standardized LLM pre-training
setting. Our findings reveal that a depthwise stacking operator, called
$G_{\text{stack}}$, exhibits remarkable acceleration in training, leading to
decreased loss and improved overall performance on eight standard NLP
benchmarks compared to strong baselines. Motivated by these promising results,
we conduct extensive experiments to delve deeper into $G_{\text{stack}}$ to
address $\textit{O}$2 and $\textit{O}$3. For $\textit{O}$2 (untested
scalability), our study shows that $G_{\text{stack}}$ is scalable and
consistently performs well, with experiments up to 7B LLMs after growth and
pre-training LLMs with 750B tokens. For example, compared to a conventionally
trained 7B model using 300B tokens, our $G_{\text{stack}}$ model converges to
the same loss with 194B tokens, resulting in a 54.6\% speedup. We further
address $\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines
to determine growth timing and growth factor for $G_{\text{stack}}$, making it
practical in general LLM pre-training. We also provide in-depth discussions and
comprehensive ablation studies of $G_{\text{stack}}$. Our code and pre-trained
model are available at https://llm-stacking.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Good Parenting is all you need -- Multi-agentic LLM Hallucination
  Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Kwartler, Matthew Berman, Alan Aqrawi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the ability of Large Language Model (LLM) agents to
detect and correct hallucinations in AI-generated content. A primary agent was
tasked with creating a blog about a fictional Danish artist named Flipfloppidy,
which was then reviewed by another agent for factual inaccuracies. Most LLMs
hallucinated the existence of this artist. Across 4,900 test runs involving
various combinations of primary and reviewing agents, advanced AI models such
as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in
identifying hallucinations and successfully revised outputs in 85% to 100% of
cases following feedback. These findings underscore the potential of advanced
AI models to significantly enhance the accuracy and reliability of generated
content, providing a promising approach to improving AI workflow orchestration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian
  Product Routing in Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Zijia Lin, Yizhe Xiong, Minxuan Lv, Guangyuan Ma, Hui Chen, Songlin Hu, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have been attracting much attention from the
community recently, due to their remarkable performance in all kinds of
downstream tasks. According to the well-known scaling law, scaling up a dense
LLM enhances its capabilities, but also significantly increases the
computational complexity. Mixture-of-Experts (MoE) models address that by
allowing the model size to grow without substantially raising training or
inference costs. Yet MoE models face challenges regarding knowledge sharing
among experts, making their performance somehow sensitive to routing accuracy.
To tackle that, previous works introduced shared experts and combined their
outputs with those of the top $K$ routed experts in an ``addition'' manner. In
this paper, inspired by collective matrix factorization to learn shared
knowledge among data, we propose CartesianMoE, which implements more effective
knowledge sharing among experts in more like a ``multiplication'' manner.
Extensive experimental results indicate that CartesianMoE outperforms previous
MoE models for building LLMs, in terms of both perplexity and downstream task
performance. And we also find that CartesianMoE achieves better expert routing
robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ F-MALLOC: Feed-forward Memory Allocation for Continual Learning in
  Neural Machine Translation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhong Wu, Yuchen Liu, Chengqing Zong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolving landscape of Neural Machine Translation (NMT), the
pretrain-then-finetune paradigm has yielded impressive results. However, the
persistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While
previous work has introduced Continual Learning (CL) methods to address CF,
these approaches grapple with the delicate balance between avoiding forgetting
and maintaining system extensibility. To address this, we propose a CL method,
named $\textbf{F-MALLOC}$ ($\textbf{F}$eed-forward $\textbf{M}$emory
$\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting
that feed-forward layers emulate neural memories and encapsulate crucial
translation knowledge. It decomposes feed-forward layers into discrete memory
cells and allocates these memories to different tasks. By learning to allocate
and safeguard these memories, our method effectively alleviates CF while
ensuring robust extendability. Besides, we propose a comprehensive assessment
protocol for multi-stage CL of NMT systems. Experiments conducted following
this new protocol showcase the superior performance of F-MALLOC, evidenced by
higher BLEU scores and almost zero forgetting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preferred Elements,  :, Kenshin Abe, Kaizaburo Chubachi, Yasuhiro Fujita, Yuta Hirokawa, Kentaro Imajo, Toshiki Kataoka, Hiroyoshi Komatsu, Hiroaki Mikami, Tsuguo Mogami, Shogo Murai, Kosuke Nakago, Daisuke Nishino, Toru Ogawa, Daisuke Okanohara, Yoshihiko Ozaki, Shotaro Sano, Shuji Suzuki, Tianqi Xu, Toshihiko Yanase
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PLaMo-100B, a large-scale language model designed for Japanese
proficiency. The model was trained from scratch using 2 trillion tokens, with
architecture such as QK Normalization and Z-Loss to ensure training stability
during the training process. Post-training techniques, including Supervised
Fine-Tuning and Direct Preference Optimization, were applied to refine the
model's performance. Benchmark evaluations suggest that PLaMo-100B performs
well, particularly in Japanese-specific tasks, achieving results that are
competitive with frontier models like GPT-4. The base model is available at
https://huggingface.co/pfnet/plamo-100b.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-playing is an emerging application in the field of Human-Computer
Interaction (HCI), primarily implemented through the alignment training of a
large language model (LLM) with assigned characters. Despite significant
progress, role-playing agents (RPLAs) still struggle with maintaining
role-consistency across conversations, particularly when confronted with
boundary queries subtly related to character attributes. In this paper, we
present ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities
through boundary-aware learning. ERABAL encompasses a generation pipeline for
role-specific dialogues and a concomitant methodology for alignment training.
Through comprehensive evaluations, we demonstrate that ERABAL is both efficient
and effective. By training with significantly fewer dialogues than those used
in leading approaches, ERABAL achieves notable improvements across
WikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared
to the generalist baseline models. Our code and datasets will be made publicly
available to support further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2402.10618</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAF-RM: A Hybrid Alignment Framework for Reward Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujun Liu, Xiaoyu Shen, Yuhang Lai, Siyuan Wang, Shengbin Yue, Zengfeng Huang, Xuanjing Huang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reward model has become increasingly important in alignment, assessment,
and data construction for large language models (LLMs). Most existing
researchers focus on enhancing reward models through data improvements,
following the conventional training framework for reward models that directly
optimizes the predicted rewards. In this paper, we propose a hybrid alignment
framework HaF-RM for reward model training by introducing an additional
constraint on token-level policy probabilities in addition to the reward score.
It can simultaneously supervise the internal preference model at the token
level and optimize the mapping layer of the reward model at the sequence level.
Theoretical justifications and experiment results on five datasets show the
validity and effectiveness of our proposed hybrid framework for training a
high-quality reward model. By decoupling the reward modeling procedure and
incorporating hybrid supervision, our HaF-RM framework offers a principled and
effective approach to enhancing the performance and alignment of reward models,
a critical component in the responsible development of powerful language
models. We release our code at https://haf-rm.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETF: An Entity Tracing Framework for Hallucination Detection in Code
  Summaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kishan Maharaj, Vitobha Munigala, Srikanth G. Tamilselvam, Prince Kumar, Sayandeep Sen, Palani Kodeswaran, Abhijit Mishra, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced their ability to understand both natural language and code, driving
their use in tasks like natural language-to-code (NL2Code) and code
summarization. However, LLMs are prone to hallucination-outputs that stray from
intended meanings. Detecting hallucinations in code summarization is especially
difficult due to the complex interplay between programming and natural
languages. We introduce a first-of-its-kind dataset with $\sim$10K samples,
curated specifically for hallucination detection in code summarization. We
further propose a novel Entity Tracing Framework (ETF) that a) utilizes static
program analysis to identify code entities from the program and b) uses LLMs to
map and verify these entities and their intents within generated code
summaries. Our experimental analysis demonstrates the effectiveness of the
framework, leading to a 0.73 F1 score. This approach provides an interpretable
method for detecting hallucinations by grounding entities, allowing us to
evaluate summary accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Model Alignment in Multilingual Trolley Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02273v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02273v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Max Kleiman-Weiner, Giorgio Piatti, Sydney Levine, Jiarui Liu, Fernando Gonzalez, Francesco Ortu, András Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate the moral alignment of large language models (LLMs) with human
preferences in multilingual trolley problems. Building on the Moral Machine
experiment, which captures over 40 million human judgments across 200+
countries, we develop a cross-lingual corpus of moral dilemma vignettes in over
100 languages called MultiTP. This dataset enables the assessment of LLMs'
decision-making processes in diverse linguistic contexts. Our analysis explores
the alignment of 19 different LLMs with human judgments, capturing preferences
across six moral dimensions: species, gender, fitness, status, age, and the
number of lives involved. By correlating these preferences with the demographic
distribution of language speakers and examining the consistency of LLM
responses to various prompt paraphrasings, our findings provide insights into
cross-lingual and ethical biases of LLMs and their intersection. We discover
significant variance in alignment across languages, challenging the assumption
of uniform moral reasoning in AI systems and highlighting the importance of
incorporating diverse perspectives in AI ethics. The results underscore the
need for further research on the integration of multilingual dimensions in
responsible AI research to ensure fair and equitable AI interactions worldwide.
Our code and data are at https://github.com/causalNLP/moralmachine
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UCFE: A User-Centric Financial Expertise Benchmark for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe Yang, Yifei Zhang, Yan Hu, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang, Haining Wang, Qianqian Xie, Jimin Huang, Honghai Yu, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the UCFE: User-Centric Financial Expertise benchmark,
an innovative framework designed to evaluate the ability of large language
models (LLMs) to handle complex real-world financial tasks. UCFE benchmark
adopts a hybrid approach that combines human expert evaluations with dynamic,
task-specific interactions to simulate the complexities of evolving financial
scenarios. Firstly, we conducted a user study involving 804 participants,
collecting their feedback on financial tasks. Secondly, based on this feedback,
we created our dataset that encompasses a wide range of user intents and
interactions. This dataset serves as the foundation for benchmarking 12 LLM
services using the LLM-as-Judge methodology. Our results show a significant
alignment between benchmark scores and human preferences, with a Pearson
correlation coefficient of 0.78, confirming the effectiveness of the UCFE
dataset and our evaluation approach. UCFE benchmark not only reveals the
potential of LLMs in the financial sector but also provides a robust framework
for assessing their performance and user satisfaction. The benchmark dataset
and evaluation code are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for
  Aligning Large Language Models to Online Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao He, Minh Duc Chu, Rebecca Dorn, Siyi Guo, Kristina Lerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social scientists use surveys to probe the opinions and beliefs of
populations, but these methods are slow, costly, and prone to biases. Recent
advances in large language models (LLMs) enable the creating of computational
representations or "digital twins" of populations that generate human-like
responses mimicking the population's language, styles, and attitudes. We
introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs
to online communities to elicit their beliefs. Given a corpus of a community's
online discussions, Community-Cross-Instruct automatically generates
instruction-output pairs by an advanced LLM to (1) finetune a foundational LLM
to faithfully represent that community, and (2) evaluate the alignment of the
finetuned model to the community. We demonstrate the method's utility in
accurately representing political and diet communities on Reddit. Unlike prior
methods requiring human-authored instructions, Community-Cross-Instruct
generates instructions in a fully unsupervised manner, enhancing scalability
and generalization across domains. This work enables cost-effective and
automated surveying of diverse online communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns
  Well with The Key Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Richong Zhang, Zhanyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings from large language models (LLMs) have achieved excellent
results in tasks such as information retrieval, semantic textual similarity,
etc. In this work, we show an interesting finding: when feeding a text into the
embedding LLMs, the obtained text embedding will be able to be aligned with the
key tokens in the input text. We first fully analyze this phenomenon on eight
embedding LLMs and show that this phenomenon is universal and is not affected
by model architecture, training strategy, and embedding method. With a deeper
analysis, we then find that the main change in embedding space between the
embedding LLMs and their original generative LLMs is in the first principal
component. By adjusting the first principal component, we can align text
embedding with the key tokens. Finally, we give several examples to demonstrate
the vast application potential of this finding: (1) we propose a simple and
practical sparse retrieval method based on the aligned tokens, which can
achieve 80\% of the dense retrieval effect of the same model while reducing the
computation significantly; (2) we show that our findings provide a fresh
perspective to help understand fuzzy concepts (e.g., semantic relatedness vs.
semantic similarity) and emerging technologies (e.g., instruction-following
embedding) in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Reinforcement Learning from Human Feedback with Efficient
  Reward Model Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing Sun, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is a widely adopted
approach for aligning large language models with human values. However, RLHF
relies on a reward model that is trained with a limited amount of human
preference data, which could lead to inaccurate predictions. As a result, RLHF
may produce outputs that are misaligned with human values. To mitigate this
issue, we contribute a reward ensemble method that allows the reward model to
make more accurate predictions. As using an ensemble of large language
model-based reward models can be computationally and resource-expensive, we
explore efficient ensemble methods including linear-layer ensemble and
LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy
Optimization with our ensembled reward models, and verify that our ensemble
methods help improve the alignment performance of RLHF outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position Engineering: Boosting Large Language Models through Positional
  Information Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna Qiu, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of large language models (LLMs) is significantly influenced
by the quality of the prompts provided. In response, researchers have developed
enormous prompt engineering strategies aimed at modifying the prompt text to
enhance task performance. In this paper, we introduce a novel technique termed
position engineering, which offers a more efficient way to guide large language
models. Unlike prompt engineering, which requires substantial effort to modify
the text provided to LLMs, position engineering merely involves altering the
positional information in the prompt without modifying the text itself. We have
evaluated position engineering in two widely-used LLM scenarios:
retrieval-augmented generation (RAG) and in-context learning (ICL). Our
findings show that position engineering substantially improves upon the
baseline in both cases. Position engineering thus represents a promising new
strategy for exploiting the capabilities of large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PHAnToM: Persona-based <span class="highlight-title">Prompt</span>ing Has An Effect on Theory-of-Mind
  Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiona Anting Tan, Gerard Christopher Yeo, Kokil Jaidka, Fanyou Wu, Weijie Xu, Vinija Jain, Aman Chadha, Yang Liu, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of LLMs in natural language reasoning has shown mixed results,
sometimes rivaling or even surpassing human performance in simpler
classification tasks while struggling with social-cognitive reasoning, a domain
where humans naturally excel. These differences have been attributed to many
factors, such as variations in prompting and the specific LLMs used. However,
no reasons appear conclusive, and no clear mechanisms have been established in
prior work. In this study, we empirically evaluate how role-playing prompting
influences Theory-of-Mind (ToM) reasoning capabilities. Grounding our rsearch
in psychological theory, we propose the mechanism that, beyond the inherent
variance in the complexity of reasoning tasks, performance differences arise
because of socially-motivated prompting differences. In an era where prompt
engineering with role-play is a typical approach to adapt LLMs to new contexts,
our research advocates caution as models that adopt specific personas might
potentially result in errors in social-cognitive reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Human Alignment and Model Faithfulness of LLM Rationale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Fayyaz, Fan Yin, Jiao Sun, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how well large language models (LLMs) explain their generations
through rationales -- a set of tokens extracted from the input text that
reflect the decision-making process of LLMs. Specifically, we systematically
study rationales derived using two approaches: (1) popular prompting-based
methods, where prompts are used to guide LLMs in generating rationales, and (2)
technical attribution-based methods, which leverage attention or gradients to
identify important tokens. Our analysis spans three classification datasets
with annotated rationales, encompassing tasks with varying performance levels.
While prompting-based self-explanations are widely used, our study reveals that
these explanations are not always as "aligned" with the human rationale as
attribution-based explanations. Even more so, fine-tuning LLMs to enhance
classification task accuracy does not enhance the alignment of prompting-based
rationales. Still, it does considerably improve the alignment of
attribution-based methods (e.g., InputXGradient). More importantly, we show
that prompting-based self-explanation is also less "faithful" than
attribution-based explanations, failing to provide a reliable account of the
model's decision-making process. To evaluate faithfulness, unlike prior studies
that excluded misclassified examples, we evaluate all instances and also
examine the impact of fine-tuning and accuracy on alignment and faithfulness.
Our findings suggest that inconclusive faithfulness results reported in earlier
studies may stem from low classification accuracy. These findings underscore
the importance of more rigorous and comprehensive evaluations of LLM
rationales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DNA<span class="highlight-title">BERT</span>-S: Pioneering Species Differentiation with Species-Aware DNA
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08777v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08777v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Zhou, Weimin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V Davuluri, Zhong Wang, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DNABERT-S, a tailored genome model that develops species-aware
embeddings to naturally cluster and segregate DNA sequences of different
species in the embedding space. Differentiating species from genomic sequences
(i.e., DNA and RNA) is vital yet challenging, since many real-world species
remain uncharacterized, lacking known genomes for reference. Embedding-based
methods are therefore used to differentiate species in an unsupervised manner.
DNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To
encourage effective embeddings to error-prone long-read DNA sequences, we
introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes
the hidden representations of DNA sequences at randomly selected layers and
trains the model to recognize and differentiate these mixed proportions at the
output layer. We further enhance it with the proposed Curriculum Contrastive
Learning (C$^2$LR) strategy. Empirical results on 23 diverse datasets show
DNABERT-S's effectiveness, especially in realistic label-scarce scenarios. For
example, it identifies twice more species from a mixture of unlabeled genomic
sequences, doubles the Adjusted Rand Index (ARI) in species clustering, and
outperforms the top baseline's performance in 10-shot species classification
with just a 2-shot training. Model, codes, and data are publicly available at
\url{https://github.com/MAGICS-LAB/DNABERT_S}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4Decompile: Decompiling Binary Code with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05286v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05286v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decompilation aims to convert binary code to high-level source code, but
traditional tools like Ghidra often produce results that are difficult to read
and execute. Motivated by the advancements in Large Language Models (LLMs), we
propose LLM4Decompile, the first and largest open-source LLM series (1.3B to
33B) trained to decompile binary code. We optimize the LLM training process and
introduce the LLM4Decompile-End models to decompile binary directly. The
resulting models significantly outperform GPT-4o and Ghidra on the HumanEval
and ExeBench benchmarks by over 100% in terms of re-executability rate.
Additionally, we improve the standard refinement approach to fine-tune the
LLM4Decompile-Ref models, enabling them to effectively refine the decompiled
code from Ghidra and achieve a further 16.2% improvement over the
LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to
revolutionize binary code decompilation, delivering remarkable improvements in
readability and executability while complementing conventional tools for
optimal results. Our code, dataset, and models are released at
https://github.com/albertan017/LLM4Decompile
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Multimodality: Exploring the Evolution and Impact of Large
  Language Models in Medical Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01812v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01812v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Niu, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Lawrence KQ Yan, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Junyu Liu, Benji Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have rapidly evolved from text-based systems to
multimodal platforms, significantly impacting various sectors including
healthcare. This comprehensive review explores the progression of LLMs to
Multimodal Large Language Models (MLLMs) and their growing influence in medical
practice. We examine the current landscape of MLLMs in healthcare, analyzing
their applications across clinical decision support, medical imaging, patient
engagement, and research. The review highlights the unique capabilities of
MLLMs in integrating diverse data types, such as text, images, and audio, to
provide more comprehensive insights into patient health. We also address the
challenges facing MLLM implementation, including data limitations, technical
hurdles, and ethical considerations. By identifying key research gaps, this
paper aims to guide future investigations in areas such as dataset development,
modality alignment methods, and the establishment of ethical guidelines. As
MLLMs continue to shape the future of healthcare, understanding their potential
and limitations is crucial for their responsible and effective integration into
medical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Invent Algorithms to Improve Themselves? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoichi Ishibashi, Taro Yano, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable performance improvements
and are rapidly gaining adoption in industry. However, the methods for
improving LLMs are still designed by humans, which restricts the invention of
new model-improving algorithms to human expertise and imagination. To address
this, we propose the Self-Developing framework, which enables LLMs to
autonomously generate and learn model-improvement algorithms. In this
framework, the seed model generates, applies, and learns model-improving
algorithms, continuously improving both the seed model and the algorithms
themselves. In mathematical reasoning tasks, Self-Developing not only creates
models that surpass the seed model but also consistently outperforms models
created using human-designed algorithms. Additionally, these LLM-discovered
algorithms demonstrate strong effectiveness, including transferability to
out-of-domain models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to
  Extremes Through Rank-Wise Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning
large language models (LLMs) to various domains due to its modular design and
widespread availability on platforms like Huggingface. This modularity has
sparked interest in combining multiple LoRAs to enhance LLM capabilities.
However, existing methods for LoRA composition primarily focus on task-specific
adaptations that require additional training, and current model merging
techniques often fail to fully leverage LoRA's modular nature, leading to
parameter interference and performance degradation. In this paper, we
investigate the feasibility of disassembling and reassembling multiple LoRAs at
a finer granularity, analogous to assembling LEGO blocks. We introduce the
concept of Minimal Semantic Units (MSUs), where the parameters corresponding to
each rank in LoRA function as independent units. These MSUs demonstrate
permutation invariance and concatenation-summation equivalence properties,
enabling flexible combinations to create new LoRAs. Building on these insights,
we propose the LoRA-LEGO framework. This framework conducts rank-wise parameter
clustering by grouping MSUs from different LoRAs into $k$ clusters. The
centroid of each cluster serves as a representative MSU, enabling the assembly
of a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual
reweighting strategy to optimize the scale of the merged LoRA. Experiments
across various benchmarks demonstrate that our method outperforms existing
approaches in LoRA merging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptable and Reliable Text Classification using Large Language Models <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Wang, Yiran Pang, Yanbin Lin, Xingquan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification is fundamental in Natural Language Processing (NLP), and
the advent of Large Language Models (LLMs) has revolutionized the field. This
paper introduces an adaptable and reliable text classification paradigm, which
leverages LLMs as the core component to address text classification tasks. Our
system simplifies the traditional text classification workflows, reducing the
need for extensive preprocessing and domain-specific expertise to deliver
adaptable and reliable text classification results. We evaluated the
performance of several LLMs, machine learning algorithms, and neural
network-based architectures on four diverse datasets. Results demonstrate that
certain LLMs surpass traditional methods in sentiment analysis, spam SMS
detection, and multi-label classification. Furthermore, it is shown that the
system's performance can be further enhanced through few-shot or fine-tuning
strategies, making the fine-tuned model the top performer across all datasets.
Source code and datasets are available in this GitHub repository:
https://github.com/yeyimilk/llm-zero-shot-classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDM Workshop ARRL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Cross-Lingual NER Using Phonemic Representations for
  Low-Resource Languages <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimin Sohn, Haeji Jung, Alex Cheng, Jooeon Kang, Yilin Du, David R. Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing zero-shot cross-lingual NER approaches require substantial prior
knowledge of the target language, which is impractical for low-resource
languages. In this paper, we propose a novel approach to NER using phonemic
representation based on the International Phonetic Alphabet (IPA) to bridge the
gap between representations of different languages. Our experiments show that
our method significantly outperforms baseline models in extremely low-resource
languages, with the highest average F1 score (46.38%) and lowest standard
deviation (12.67), particularly demonstrating its robustness with non-Latin
scripts. Our codes are available at
https://github.com/Gabriel819/zeroshot_ner.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subversive Characters and Stereotyping Readers: Characterizing Queer
  Relationalities with Dialogue-Based Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kent K. Chang, Anna Ho, David Bamman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Television is often seen as a site for subcultural identification and
subversive fantasy, including in queer cultures. How might we measure
subversion, or the degree to which the depiction of social relationship between
a dyad (e.g. two characters who are colleagues) deviates from its typical
representation on TV? To explore this question, we introduce the task of
stereotypic relationship extraction. Built on cognitive stylistics, linguistic
anthropology, and dialogue relation extraction, in this paper, we attempt to
model the cognitive process of stereotyping TV characters in dialogic
interactions. Given a dyad, we want to predict: what social relationship do the
speakers exhibit through their words? Subversion is then characterized by the
discrepancy between the distribution of the model's predictions and the ground
truth labels. To demonstrate the usefulness of this task and gesture at a
methodological intervention, we enclose four case studies to characterize the
representation of queer relationalities in the Big Bang Theory, Frasier, and
Gilmore Girls, as we explore the suspicious and reparative modes of reading
with our computational methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHR 2024: Computational Humanities Research Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Sturman, Aparna Joshi, Bhaktipriya Radharapu, Piyush Kumar, Renee Shelby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing use of large language models (LLMs) demand performant guardrails
to ensure the safety of inputs and outputs of LLMs. When these safeguards are
trained on imbalanced data, they can learn the societal biases. We present a
light-weight, post-processing method for mitigating counterfactual fairness in
closed-source text safety classifiers. Our approach involves building an
ensemble that not only outperforms the input classifiers and policy-aligns
them, but also acts as a debiasing regularizer. We introduce two
threshold-agnostic metrics to assess the counterfactual fairness of a model,
and demonstrate how combining these metrics with Fair Data Reweighting (FDW)
helps mitigate biases. We create an expanded Open AI dataset, and a new
templated LLM-generated dataset based on user-prompts, both of which are
counterfactually balanced across identity groups and cover four key areas of
safety; we will work towards publicly releasing these datasets. Our results
show that our approach improves counterfactual fairness with minimal impact on
model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Language Understanding Capabilities of Large Language
  Models Using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bokai Hu, Sai Ashish Somayajula, Xin Pan, Zihan Huang, Pengtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), built on decoder-only transformers, excel in
natural language generation and adapt to diverse tasks using zero-shot and
few-shot prompting. However, these prompting methods often struggle on natural
language understanding (NLU) tasks, where encoder-only models like BERT-base
outperform LLMs on benchmarks like GLUE and SuperGLUE. This paper explores two
approaches-supervised fine-tuning (SFT) and proximal policy optimization
(PPO)-to enhance LLMs' NLU abilities. To reduce the cost of full-model
fine-tuning, we integrate low-rank adaptation (LoRA) layers, limiting updates
to these layers during both SFT and PPO. In SFT, task-specific prompts are
concatenated with input queries and ground-truth labels, optimizing with
next-token prediction. Despite this, LLMs still underperform compared to models
like BERT-base on several NLU tasks. To close this gap, we apply PPO, a
reinforcement learning technique that treats each token generation as an action
and uses a reward function based on alignment with ground-truth answers. PPO
then updates the model to maximize these rewards, aligning outputs with correct
labels. Our experiments with LLAMA2-7B show that PPO improves performance, with
a 6.3-point gain over SFT on GLUE. PPO exceeds zero-shot by 38.7 points and
few-shot by 26.1 points on GLUE, while surpassing these by 28.8 and 28.5 points
on SuperGLUE. Additionally, PPO outperforms BERT-large by 2.7 points on GLUE
and 9.3 points on SuperGLUE. The improvements are consistent across models like
Qwen2.5-7B and MPT-7B, highlighting PPO's robustness in enhancing LLMs' NLU
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Comparison of Vocabulary Expansion and Initialization
  Approaches for Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandini Mundra, Aditya Nanda Kishore, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Mitesh M. Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) excel in natural language processing tasks for English
but show reduced performance in most other languages. This problem is commonly
tackled by continually pre-training and fine-tuning these models for said
languages. A significant issue in this process is the limited vocabulary
coverage in the original model's tokenizer, leading to inadequate
representation of new languages and necessitating an expansion of the
tokenizer. The initialization of the embeddings corresponding to new vocabulary
items presents a further challenge. Current strategies require cross-lingual
embeddings and lack a solid theoretical foundation as well as comparisons with
strong baselines. In this paper, we first establish theoretically that
initializing within the convex hull of existing embeddings is a good
initialization, followed by a novel but simple approach, Constrained Word2Vec
(CW2V), which does not require cross-lingual embeddings. Our study evaluates
different initialization methods for expanding RoBERTa and LLaMA 2 across four
languages and five tasks. The results show that CW2V performs equally well or
even better than more advanced techniques. Additionally, simpler approaches
like multivariate initialization perform on par with these advanced methods
indicating that efficient large-scale multilingual continued pretraining can be
achieved even with simpler initialization methods. We release our code publicly
(https://github.com/AI4Bharat/VocabAdaptation_LLM/tree/CW2V).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CONLL 2024 (EMNLP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Moral Hypocrites? A Study Based on Moral
  Foundations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Luiz Nunes, Guilherme F. C. F. Almeida, Marcelo de Araujo, Simone D. J. Barbosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have taken centre stage in debates on Artificial
Intelligence. Yet there remains a gap in how to assess LLMs' conformity to
important human values. In this paper, we investigate whether state-of-the-art
LLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid
results) are moral hypocrites. We employ two research instruments based on the
Moral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which
investigates which values are considered morally relevant in abstract moral
judgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate
moral cognition in concrete scenarios related to each moral foundation. We
characterise conflicts in values between these different abstractions of moral
evaluation as hypocrisy. We found that both models displayed reasonable
consistency within each instrument compared to humans, but they displayed
contradictory and hypocritical behaviour when we compared the abstract values
present in the MFQ to the evaluation of concrete moral violations of the MFV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version available at:
  https://ojs.aaai.org/index.php/AIES/article/view/31704 13 pages, 4 figures, 2
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NVLM: Open Frontier-Class Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce NVLM 1.0, a family of frontier-class multimodal large language
models (LLMs) that achieve state-of-the-art results on vision-language tasks,
rivaling the leading proprietary models (e.g., GPT-4o) and open-access models
(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved
text-only performance over its LLM backbone after multimodal training. In terms
of model design, we perform a comprehensive comparison between decoder-only
multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,
Flamingo). Based on the strengths and weaknesses of both approaches, we propose
a novel architecture that enhances both training efficiency and multimodal
reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for
tile-based dynamic high-resolution images, which significantly boosts
performance on multimodal reasoning and OCR-related tasks. Regarding training
data, we meticulously curate and provide detailed information on our multimodal
pretraining and supervised fine-tuning datasets. Our findings indicate that
dataset quality and task diversity are more important than scale, even during
the pretraining phase, across all architectures. Notably, we develop
production-grade multimodality for the NVLM-1.0 models, enabling them to excel
in vision-language tasks while maintaining and even improving text-only
performance compared to their LLM backbones. To achieve this, we craft and
integrate a high-quality text-only dataset into multimodal training, alongside
a substantial amount of multimodal math and reasoning data, leading to enhanced
math and coding capabilities across modalities. To advance research in the
field, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B
and will open-source the training code for the community soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed the typos. For more information, please visit our project page
  at: https://research.nvidia.com/labs/adlr/NVLM-1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Evaluate Reward Models for RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N. Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new benchmark for reward models that quantifies their ability
to produce strong language models through RLHF (Reinforcement Learning from
Human Feedback). The gold-standard approach is to run a full RLHF training
pipeline and directly probe downstream LLM performance. However, this process
is prohibitively expensive. To address this, we build a predictive model of
downstream LLM performance by evaluating the reward model on proxy tasks. These
proxy tasks consist of a large-scale human preference and a verifiable
correctness preference dataset, in which we measure 12 metrics across 12
domains. To investigate which reward model metrics are most correlated to
gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a
large-scale crowdsourced human preference platform to view real reward model
downstream performance as ground truth. Ultimately, we compile our data and
findings into Preference Proxy Evaluations (PPE), the first reward model
benchmark explicitly linked to post-RLHF real-world human preference
performance, which we open-source for public use and further development. Our
code and evaluations can be found at https://github.com/lmarena/PPE .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Reasoning Patterns of OpenAI's o1 Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, J. H. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling Large Language Models (LLMs) to handle a wider range of complex
tasks (e.g., coding, math) has drawn great attention from many researchers. As
LLMs continue to evolve, merely increasing the number of model parameters
yields diminishing performance improvements and heavy computational costs.
Recently, OpenAI's o1 model has shown that inference strategies (i.e.,
Test-time Compute methods) can also significantly enhance the reasoning
capabilities of LLMs. However, the mechanisms behind these methods are still
unexplored. In our work, to investigate the reasoning patterns of o1, we
compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent
Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general
reasoning benchmarks in three domains (i.e., math, coding, commonsense
reasoning). Specifically, first, our experiments show that the o1 model has
achieved the best performance on most datasets. Second, as for the methods of
searching diverse responses (e.g., BoN), we find the reward models' capability
and the search space both limit the upper boundary of these methods. Third, as
for the methods that break the problem into many sub-problems, the Agent
Workflow has achieved better performance than Step-wise BoN due to the
domain-specific system prompt for planning better reasoning processes. Fourth,
it is worth mentioning that we have summarized six reasoning patterns of o1,
and provided a detailed analysis on several reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NutriBench: A <span class="highlight-title">Dataset</span> for Evaluating Large Language Models in
  Carbohydrate Estimation from Meal Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Hua, Mehak Preet Dhaliwal, Ryan Burke, Yao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate nutrition estimation helps people make informed dietary choices and
is essential in the prevention of serious health complications. We present
NutriBench, the first publicly available natural language meal description
nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated
from real-world global dietary intake data. The data is human-verified and
annotated with macro-nutrient labels, including carbohydrates, proteins, fats,
and calories. We conduct an extensive evaluation of NutriBench on the task of
carbohydrate estimation, testing twelve leading Large Language Models (LLMs),
including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using
standard, Chain-of-Thought and Retrieval-Augmented Generation strategies.
Additionally, we present a study involving professional nutritionists, finding
that LLMs can provide more accurate and faster estimates. Finally, we perform a
real-world risk assessment by simulating the effect of carbohydrate predictions
on the blood glucose levels of individuals with diabetes. Our work highlights
the opportunities and challenges of using LLMs for nutrition estimation,
demonstrating their potential to aid professionals and laypersons and improve
health outcomes. Our benchmark is publicly available at:
https://mehak126.github.io/nutribench.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characterizing the Accuracy -- Efficiency Trade-off of Low-rank
  Decomposition in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chakshu Moar, Faraz Tahmasebi, Michael Pellauer, Hyoukjun Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) employ billions of parameters to enable
broad problem-solving capabilities. Such language models also tend to be
memory-bound because of the dominance of matrix-vector and matrix-matrix
multiplications with low arithmetic intensity. Therefore, optimizing the memory
footprint and traffic is an important optimization direction for LLMs today.
Model compression methods such as quantization and parameter pruning have been
actively explored to achieve memory footprint and traffic optimization.
However, the accuracy-efficiency trade-off of rank pruning (i.e., low-rank
decomposition) for LLMs is not well-understood yet. Therefore, in this work, we
characterize the accuracy-efficiency trade-off of a low-rank decomposition
method, specifically Tucker decomposition, on recent language models, including
an open-source LLM, Llama 2. We formalize the low-rank decomposition design
space and show that the decomposition design space is enormous (e.g.,
O($2^{39}$) for Llama2-7B). To navigate such a vast design space, we formulate
it and perform thorough case studies of accuracy-efficiency trade-offs using
six widely used LLM benchmarks on BERT and Llama 2 models. Our results show
that we can achieve a 9\% model size reduction with minimal accuracy drops,
which range from 4\%p (\%p refers to "percentage point," which refers to the
absolute difference between two percentage numbers; 74\% -> 78\% = 4\%p
increase) to 10\%p, depending on the difficulty of the benchmark, without any
retraining to recover accuracy after decomposition. The results show that
low-rank decomposition can be a promising direction for LLM-based applications
that require real-time service at scale (e.g., AI agent and real-time coding
assistant), where the latency is as important as the model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Processes: Numerical Predictive Distributions Conditioned on Natural
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12856v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12856v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Requeima, John Bronskill, Dami Choi, Richard E. Turner, David Duvenaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning practitioners often face significant challenges in formally
integrating their prior knowledge and beliefs into predictive models, limiting
the potential for nuanced and context-aware analyses. Moreover, the expertise
needed to integrate this prior knowledge into probabilistic modeling typically
limits the application of these models to specialists. Our goal is to build a
regression model that can process numerical data and make probabilistic
predictions at arbitrary locations, guided by natural language text which
describes a user's prior knowledge. Large Language Models (LLMs) provide a
useful starting point for designing such a tool since they 1) provide an
interface where users can incorporate expert insights in natural language and
2) provide an opportunity for leveraging latent problem-relevant knowledge
encoded in LLMs that users may not have themselves. We start by exploring
strategies for eliciting explicit, coherent numerical predictive distributions
from LLMs. We examine these joint predictive distributions, which we call LLM
Processes, over arbitrarily-many quantities in settings such as forecasting,
multi-dimensional regression, black-box optimization, and image modeling. We
investigate the practical details of prompting to elicit coherent predictive
distributions, and demonstrate their effectiveness at regression. Finally, we
demonstrate the ability to usefully incorporate text into numerical
predictions, improving predictive performance and giving quantitative structure
that reflects qualitative descriptions. This lets us begin to explore the rich,
grounded hypothesis space that LLMs implicitly encode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-wise Influential Training Data Retrieval for Large Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawei Lin, Jikai Long, Zhaozhuo Xu, Weijie Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a Large Language Model (LLM) generation, how can we identify which
training data led to this generation? In this paper, we proposed RapidIn, a
scalable framework adapting to LLMs for estimating the influence of each
training data. The proposed framework consists of two stages: caching and
retrieval. First, we compress the gradient vectors by over 200,000x, allowing
them to be cached on disk or in GPU/CPU memory. Then, given a generation,
RapidIn efficiently traverses the cached gradients to estimate the influence
within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports
multi-GPU parallelization to substantially accelerate caching and retrieval.
Our empirical result confirms the efficiency and effectiveness of RapidIn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024. Keywords: Influence Function, Influence
  Estimation, Training Data Attribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Poison Large Language Models During Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Prashant Khanduri, Douglas Zytko, Dongxiao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has marked significant
achievements in language processing and reasoning capabilities. Despite their
advancements, LLMs face vulnerabilities to data poisoning attacks, where
adversaries insert backdoor triggers into training data to manipulate outputs
for malicious purposes. This work further identifies additional security risks
in LLMs by designing a new data poisoning attack tailored to exploit the
instruction tuning process. We propose a novel gradient-guided backdoor trigger
learning (GBTL) algorithm to identify adversarial triggers efficiently,
ensuring an evasion of detection by conventional defenses while maintaining
content integrity. Through experimental validation across various tasks,
including sentiment analysis, domain generation, and question answering, our
poisoning strategy demonstrates a high success rate in compromising various
LLMs' outputs. We further propose two defense strategies against data poisoning
attacks, including in-context learning (ICL) and continuous learning (CL),
which effectively rectify the behavior of LLMs and significantly reduce the
decline in performance. Our work highlights the significant security risks
present during the instruction tuning of LLMs and emphasizes the necessity of
safeguarding LLMs against data poisoning attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Diversity of Synthetic Data and its Impact on Training Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Abdul Waheed, Xiang Li, Yidong Wang, Jindong Wang, Bhiksha Raj, Marah I. Abdin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has accentuated the need for
diverse, high-quality pre-training data. Synthetic data emerges as a viable
solution to the challenges of data scarcity and inaccessibility. While previous
literature has focused predominantly on the quality and quantity of real data,
our work enables the measurement of diversity in synthetic data and explores
its impact on LLM performance. We study the downstream effects of synthetic
data diversity during both the pre-training and fine-tuning stages by
introducing a new diversity metric, \textit{LLM cluster-agent}, designed to
evaluate the diversity of synthetic datasets. Through a series of controlled
experiments with models of 350M and 1.4B parameters, we demonstrate that the
proposed cluster-based LLM scoring of diversity correlates positively with both
pre-training and supervised fine-tuning performance. Our findings also reveal
that synthetic data diversity in pre-training affects supervised fine-tuning
more significantly than pre-training itself, even for smaller models. We hope
this study advances our understanding of the optimal use of synthetic data in
LLM training and opens new avenues for efficient data generation processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17542v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17542v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Ajit Nair, Arun Sai Suggala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently demonstrated remarkable
performance across diverse language tasks. But their deployment is often
constrained by their substantial computational and storage requirements.
Quantization has emerged as a key technique for addressing this challenge,
enabling the compression of large models with minimal impact on performance.
The recent GPTQ algorithm, a post-training quantization (PTQ) method, has
proven highly effective for compressing LLMs, sparking a wave of research that
leverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the
PTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ
with improved performance. CDQuant uses greedy coordinate descent to minimize
the layer-wise reconstruction loss to achieve high-quality quantized weights.
Our algorithm is easy to implement and scales efficiently to models with
hundreds of billions of parameters. We perform extensive evaluation on Gemma,
and PaLM2 model families, and demonstrate that CDQuant consistently outperforms
GPTQ in 2-4 bit weight quantization. Moreover, CDQuant improves the performance
of state-of-the-art PTQ techniques such as QuIP and FrameQuant when used as a
replacement for their GPTQ component, resulting in further gains in quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S2-Attention: Hardware-Aware <span class="highlight-title">Context</span> Sharding Among Attention Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17678v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17678v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xihui Lin, Yunan Zhang, Suyu Ge, Liliang Ren, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse attention, which selectively attends to a subset of tokens in the
context was supposed to be efficient. However, its theoretical reduction in
FLOPs has rarely translated into wall-clock speed-up over its dense attention
counterparts due to the lack of hardware-aware optimizations like
FlashAttention. Meanwhile, it remains unclear whether sparse attention can
maintain the model's quality at a scale of today's large language models (LLMs)
and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library
that provides kernel optimization for sparse attention customizable at both
per-head and per-context-range levels. S2-Attention enables the exploration of
novel and high-performance sparse attention techniques, which we demonstrate
through extensive ablations across a wide range of sparse attention designs at
various model scales. From these insights, we present several basic guidelines
to design sparse attention that can achieve not only practical efficiency
improvements, but also strong downstream performance. To achieve high
parallelization and optimized memory IO, sparse attention should shard the
context heterogeneously across attention heads, where each head attends to a
different subset of tokens while collectively covering the full context.
Meanwhile, we find hybrid architectures combining sparse and dense attention
particularly beneficial in practice. S2-Attention achieves wall-clock speedup
of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with
strong downstream performance on-par with full attention and perfect retrieval
performance at a 128k context length. At inference, for 7B models, our model,
with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to
dense counterparts. S2-Attention is released with easy-to-customize APIs for
direct usage in Megatron and vLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLAG: Financial <span class="highlight-title">Long</span> Document Classification via AMR-based GNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolun "Namir" Xia, Aparna Gupta, Mohammed J. Zaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has initiated much research into
their various financial applications. However, in applying LLMs on long
documents, semantic relations are not explicitly incorporated, and a full or
arbitrarily sparse attention operation is employed. In recent years, progress
has been made in Abstract Meaning Representation (AMR), which is a graph-based
representation of text to preserve its semantic relations. Since AMR can
represent semantic relationships at a deeper level, it can be beneficially
utilized by graph neural networks (GNNs) for constructing effective
document-level graph representations built upon LLM embeddings to predict
target metrics in the financial domain. We propose FLAG: Financial Long
document classification via AMR-based GNN, an AMR graph based framework to
generate document-level embeddings for long financial document classification.
We construct document-level graphs from sentence-level AMR graphs, endow them
with specialized LLM word embeddings in the financial domain, apply a deep
learning mechanism that utilizes a GNN, and examine the efficacy of our
AMR-based approach in predicting labeled target data from long financial
documents. Extensive experiments are conducted on a dataset of quarterly
earnings calls transcripts of companies in various sectors of the economy, as
well as on a corpus of more recent earnings calls of companies in the S&P 1500
Composite Index. We find that our AMR-based approach outperforms fine-tuning
LLMs directly on text in predicting stock price movement trends at different
time horizons in both datasets. Our work also outperforms previous work
utilizing document graphs and GNNs for text classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, to be published in CIFEr Conference 2024 as
  "Semantic Graph Learning for Trend Prediction from Long Financial Documents"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Causal Influence of Grammatical Gender on Distributional Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karolina Stańczak, Kevin Du, Adina Williams, Isabelle Augenstein, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How much meaning influences gender assignment across languages is an active
area of research in linguistics and cognitive science. We can view current
approaches as aiming to determine where gender assignment falls on a spectrum,
from being fully arbitrarily determined to being largely semantically
determined. For the latter case, there is a formulation of the neo-Whorfian
hypothesis, which claims that even inanimate noun gender influences how people
conceive of and talk about objects (using the choice of adjective used to
modify inanimate nouns as a proxy for meaning). We offer a novel, causal
graphical model that jointly represents the interactions between a noun's
grammatical gender, its meaning, and adjective choice. In accordance with past
results, we find a significant relationship between the gender of nouns and the
adjectives that modify them. However, when we control for the meaning of the
noun, the relationship between grammatical gender and adjective choice is near
zero and insignificant.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-21T00:00:00Z">2024-10-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">182</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video
  Even in VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, Juan Carlos Niebles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for
videos, particularly designed to efficiently capture temporal information over
multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in
addition to the conventional visual tokenizer, which maps a sequence of tokens
over multiple frames into a compact set of visual tokens. This enables
BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32
vs. 4608 tokens). We explore different types of temporal encoders, including
learnable spatio-temporal pooling as well as sequential models like Token
Turing Machines. We experimentally confirm that BLIP-3-Video obtains video
question-answering accuracies comparable to much larger state-of-the-art models
(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using
fewer visual tokens. The project website is at
https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and
  Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate evaluation is crucial for the continuous improvement
of large language models (LLMs). Among various assessment methods, subjective
evaluation has garnered significant attention due to its superior alignment
with real-world usage scenarios and human preferences. However, human-based
evaluations are costly and lack reproducibility, making precise automated
evaluators (judgers) vital in this process. In this report, we introduce
\textbf{CompassJudger-1}, the first open-source \textbf{all-in-one} judge LLM.
CompassJudger-1 is a general-purpose LLM that demonstrates remarkable
versatility. It is capable of: 1. Performing unitary scoring and two-model
comparisons as a reward model; 2. Conducting evaluations according to specified
formats; 3. Generating critiques; 4. Executing diverse tasks like a general
LLM. To assess the evaluation capabilities of different judge models under a
unified setting, we have also established \textbf{JudgerBench}, a new benchmark
that encompasses various subjective evaluation tasks and covers a wide range of
topics. CompassJudger-1 offers a comprehensive solution for various evaluation
tasks while maintaining the flexibility to adapt to diverse requirements. Both
CompassJudger and JudgerBench are released and available to the research
community athttps://github.com/open-compass/CompassJudger. We believe that by
open-sourcing these tools, we can foster collaboration and accelerate progress
in LLM evaluation methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report, Code and Models:
  https://github.com/open-compass/CompassJudger</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Knowledge Editing Really Correct Hallucinations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) suffer from hallucinations, referring to the
non-factual information in generated content, despite their superior capacities
across tasks. Meanwhile, knowledge editing has been developed as a new popular
paradigm to correct the erroneous factual knowledge encoded in LLMs with the
advantage of avoiding retraining from scratch. However, one common issue of
existing evaluation datasets for knowledge editing is that they do not ensure
LLMs actually generate hallucinated answers to the evaluation questions before
editing. When LLMs are evaluated on such datasets after being edited by
different techniques, it is hard to directly adopt the performance to assess
the effectiveness of different knowledge editing methods in correcting
hallucinations. Thus, the fundamental question remains insufficiently
validated: Can knowledge editing really correct hallucinations in LLMs? We
proposed HalluEditBench to holistically benchmark knowledge editing methods in
correcting real-world hallucinations. First, we rigorously construct a massive
hallucination dataset with 9 domains, 26 topics and more than 6,000
hallucinations. Then, we assess the performance of knowledge editing methods in
a holistic way on five dimensions including Efficacy, Generalization,
Portability, Locality, and Robustness. Through HalluEditBench, we have provided
new insights into the potentials and limitations of different knowledge editing
methods in correcting hallucinations, which could inspire future improvements
and facilitate the progress in the field of knowledge editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. The main
  paper is 10 pages long, with 35 pages total. The code, results, dataset, and
  additional resources are available on the project website:
  https://llm-editing.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing <span class="highlight-title">Context</span> Contributions in LLM-based Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Zaranis, Nuno M. Guerreiro, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved state-of-the-art performance in
machine translation (MT) and demonstrated the ability to leverage in-context
learning through few-shot examples. However, the mechanisms by which LLMs use
different parts of the input context remain largely unexplored. In this work,
we provide a comprehensive analysis of context utilization in MT, studying how
LLMs use various context parts, such as few-shot examples and the source text,
when generating translations. We highlight several key findings: (1) the source
part of few-shot examples appears to contribute more than its corresponding
targets, irrespective of translation direction; (2) finetuning LLMs with
parallel data alters the contribution patterns of different context parts; and
(3) there is a positional bias where earlier few-shot examples have higher
contributions to the translated sequence. Finally, we demonstrate that
inspecting anomalous context contributions can potentially uncover pathological
translations, such as hallucinations. Our findings shed light on the internal
workings of LLM-based MT which go beyond those known for standard
encoder-decoder MT models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToW: Thoughts of Words Improve Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikun Xu, Ming Shen, Jacob Dineen, Zhaonan Li, Xiao Ye, Shijie Lu, Aswin RRV, Chitta Baral, Ben Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce thoughts of words (ToW), a novel training-time data-augmentation
method for next-word prediction. ToW views next-word prediction as a core
reasoning task and injects fine-grained thoughts explaining what the next word
should be and how it is related to the previous contexts in pre-training texts.
Our formulation addresses two fundamental drawbacks of existing next-word
prediction learning schemes: they induce factual hallucination and are
inefficient for models to learn the implicit reasoning processes in raw texts.
While there are many ways to acquire such thoughts of words, we explore the
first step of acquiring ToW annotations through distilling from larger models.
After continual pre-training with only 70K ToW annotations, we effectively
improve models' reasoning performances by 7% to 9% on average and reduce model
hallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks
and applications, introducing no additional biases on labels or semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sketch2Code: Evaluating Vision-Language Models for Interactive Web
  Design Prototyping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Li, Yanzhe Zhang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sketches are a natural and accessible medium for UI designers to
conceptualize early-stage ideas. However, existing research on UI/UX automation
often requires high-fidelity inputs like Figma designs or detailed screenshots,
limiting accessibility and impeding efficient design iteration. To bridge this
gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art
Vision Language Models (VLMs) on automating the conversion of rudimentary
sketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code
supports interactive agent evaluation that mimics real-world design workflows,
where a VLM-based agent iteratively refines its generations by communicating
with a simulated user, either passively receiving feedback instructions or
proactively asking clarification questions. We comprehensively analyze ten
commercial and open-source models, showing that Sketch2Code is challenging for
existing VLMs; even the most capable models struggle to accurately interpret
sketches and formulate effective questions that lead to steady improvement.
Nevertheless, a user study with UI/UX experts reveals a significant preference
for proactive question-asking over passive feedback reception, highlighting the
need to develop more effective paradigms for multi-turn conversational agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building A Coding Assistant via the Retrieval-Augmented Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinze Li, Hanbin Wang, Zhenghao Liu, Shi Yu, Shuo Wang, Shuo Wang, Yukun Yan, Yukai Fu, Yu Gu, Ge Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models have shown strong effectiveness in code-related
tasks, such as code retrieval, code generation, code summarization, and code
completion tasks. In this paper, we propose COde assistaNt viA
retrieval-augmeNted language model (CONAN), which aims to build a code
assistant by mimicking the knowledge-seeking behaviors of humans during coding.
Specifically, it consists of a code structure aware retriever (CONAN-R) and a
dual-view code representation-based retrieval-augmented generation model
(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and
Masked Entity Prediction tasks to make language models code structure-aware and
learn effective representations for code snippets and documentation. Then
CONAN-G designs a dual-view code representation mechanism for implementing a
retrieval-augmented code generation model. CONAN-G regards the code
documentation descriptions as prompts, which help language models better
understand the code semantics. Our experiments show that CONAN achieves
convincing performance on different code generation tasks and significantly
outperforms previous retrieval augmented code generation models. Our further
analyses show that CONAN learns tailored representations for both code snippets
and documentation by aligning code-documentation data pairs and capturing
structural semantics by masking and predicting entities in the code data.
Additionally, the retrieved code snippets and documentation provide necessary
information from both program language and natural language to assist the code
generation process. CONAN can also be used as an assistant for Large Language
Models (LLMs), providing LLMs with external knowledge in shorter code document
lengths to improve their effectiveness on various code tasks. It shows the
ability of CONAN to extract necessary information and help filter out the noise
from retrieved code documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Creating an English-Thai Code-switched Machine Translation in Medical
  Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parinthapat Pengpun, Krittamate Tiankanon, Amrest Chinkamol, Jiramet Kinchagawat, Pitchaya Chairuengjitjaras, Pasit Supholkhan, Pubordee Aussavavirojekul, Chiraphat Boonnag, Kanyakorn Veerakanjana, Hirunkul Phimsiri, Boonthicha Sae-jia, Nattawach Sataudom, Piyalitt Ittichaiwong, Peerat Limkonchotiwat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation (MT) in the medical domain plays a pivotal role in
enhancing healthcare quality and disseminating medical knowledge. Despite
advancements in English-Thai MT technology, common MT approaches often
underperform in the medical field due to their inability to precisely translate
medical terminologies. Our research prioritizes not merely improving
translation accuracy but also maintaining medical terminology in English within
the translated text through code-switched (CS) translation. We developed a
method to produce CS medical translation data, fine-tuned a CS translation
model with this data, and evaluated its performance against strong baselines,
such as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4. Our model
demonstrated competitive performance in automatic metrics and was highly
favored in human preference evaluations. Our evaluation result also shows that
medical professionals significantly prefer CS translations that maintain
critical English terms accurately, even if it slightly compromises fluency. Our
code and test set are publicly available
https://github.com/preceptorai-org/NLLB_CS_EM_NLP2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ing Distillation for Large Language Models: A Design Space
  Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Peng, Xin Lv, Yushi Bai, Zijun Yao, Jiajie Zhang, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) aims to transfer knowledge from a large teacher
model to a smaller student model. Previous work applying KD in the field of
large language models (LLMs) typically focused on the post-training phase,
where the student LLM learns directly from instructions and corresponding
responses generated by the teacher model. In this paper, we extend KD to the
pre-training phase of LLMs, named pre-training distillation (PD). We first
conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a
1.9B parameter student LLM, validating the effectiveness of PD. Considering the
key impact factors of distillation, we systematically explore the design space
of pre-training distillation across four aspects: logits processing, loss
selection, scaling law, and offline or online logits. We conduct extensive
experiments to explore the design space of pre-training distillation and find
better configurations and interesting conclusions, such as larger student LLMs
generally benefiting more from pre-training distillation, while a larger
teacher LLM does not necessarily guarantee better results. We hope our
exploration of the design space will inform future practices in pre-training
distillation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compute-Constrained Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Oscar Yin, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data selection can reduce the amount of training data needed to finetune
LLMs; however, the efficacy of data selection scales directly with its compute.
Motivated by the practical challenge of compute-constrained finetuning, we
consider the setting in which both the cost of selecting data and training are
budgeted for. We first formalize the problem of data selection with a
cost-aware utility function, and model the data selection problem as trading
off initial-selection cost for training gain. We run a comprehensive sweep of
experiments across multiple tasks, varying compute budget by scaling finetuning
tokens, model sizes, and data selection compute. These experiments show the
validity of this model in real-world experiments. Interestingly we find that
many powerful data selection methods are almost never compute-optimal, and that
cheaper data selection alternatives dominate both from a theoretical and
empirical perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT-TL: Low-Resource Temporal Knowledge Representation of Planning
  Instructions Using Chain-of-Thought Reasoning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Manas, Stefan Zwicklbauer, Adrian Paschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents often face the challenge of interpreting uncertain natural
language instructions for planning tasks. Representing these instructions as
Linear Temporal Logic (LTL) enables planners to synthesize actionable plans. We
introduce CoT-TL, a data-efficient in-context learning framework for
translating natural language specifications into LTL representations. CoT-TL
addresses the limitations of large language models, which typically rely on
extensive fine-tuning data, by extending chain-of-thought reasoning and
semantic roles to align with the requirements of formal logic creation. This
approach enhances the transparency and rationale behind LTL generation,
fostering user trust. CoT-TL achieves state-of-the-art accuracy across three
diverse datasets in low-data scenarios, outperforming existing methods without
fine-tuning or intermediate translations. To improve reliability and minimize
hallucinations, we incorporate model checking to validate the syntax of the
generated LTL output. We further demonstrate CoT-TL's effectiveness through
ablation studies and evaluations on unseen LTL structures and formulas in a new
dataset. Finally, we validate CoT-TL's practicality by integrating it into a
QuadCopter for multi-step drone planning based on natural language
instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proceedings of the 2024 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2024), Abu
  Dhabi 14-18 October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic <span class="highlight-title">Review</span>: Text Processing Algorithms in Machine Learning and
  Deep Learning for Mental Health Detection on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Cao, Jianglai Dai, Zhongyan Wang, Yeyubei Zhang, Xiaorui Shen, Yunchong Liu, Yexin Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global rise in depression necessitates innovative detection methods for
early intervention. Social media provides a unique opportunity to identify
depression through user-generated posts. This systematic review evaluates
machine learning (ML) models for depression detection on social media, focusing
on biases and methodological challenges throughout the ML lifecycle. A search
of PubMed, IEEE Xplore, and Google Scholar identified 47 relevant studies
published after 2010. The Prediction model Risk Of Bias ASsessment Tool
(PROBAST) was utilized to assess methodological quality and risk of bias.
Significant biases impacting model reliability and generalizability were found.
There is a predominant reliance on Twitter (63.8%) and English-language content
(over 90%), with most studies focusing on users from the United States and
Europe. Non-probability sampling methods (approximately 80%) limit
representativeness. Only 23% of studies explicitly addressed linguistic nuances
like negations, crucial for accurate sentiment analysis. Inconsistent
hyperparameter tuning was observed, with only 27.7% properly tuning models.
About 17% did not adequately partition data into training, validation, and test
sets, risking overfitting. While 74.5% used appropriate evaluation metrics for
imbalanced data, others relied on accuracy without addressing class imbalance,
potentially skewing results. Reporting transparency varied, often lacking
critical methodological details. These findings highlight the need to diversify
data sources, standardize preprocessing protocols, ensure consistent model
development practices, address class imbalance, and enhance reporting
transparency. By overcoming these challenges, future research can develop more
robust and generalizable ML models for depression detection on social media,
contributing to improved mental health outcomes globally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information for Conversation Generation: Proposals Utilising Knowledge
  Graphs <span class="chip">ISWC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Clay, Ernesto Jiménez-Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are frequently used tools for conversational generation. Without
additional information LLMs can generate lower quality responses due to lacking
relevant content and hallucinations, as well as the perception of poor
emotional capability, and an inability to maintain a consistent character.
Knowledge graphs are commonly used forms of external knowledge and may provide
solutions to these challenges. This paper introduces three proposals, utilizing
knowledge graphs to enhance LLM generation. Firstly, dynamic knowledge graph
embeddings and recommendation could allow for the integration of new
information and the selection of relevant knowledge for response generation.
Secondly, storing entities with emotional values as additional features may
provide knowledge that is better emotionally aligned with the user input.
Thirdly, integrating character information through narrative bubbles would
maintain character consistency, as well as introducing a structure that would
readily incorporate new information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages with citations, 1 figure, accepted to the ISWC 2024 Special
  Session</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contamination Report for Multilingual Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchit Ahuja, Varun Gumma, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmark contamination refers to the presence of test datasets in Large
Language Model (LLM) pre-training or post-training data. Contamination can lead
to inflated scores on benchmarks, compromising evaluation results and making it
difficult to determine the capabilities of models. In this work, we study the
contamination of popular multilingual benchmarks in LLMs that support multiple
languages. We use the Black Box test to determine whether $7$ frequently used
multilingual benchmarks are contaminated in $7$ popular open and closed LLMs
and find that almost all models show signs of being contaminated with almost
all the benchmarks we test. Our findings can help the community determine the
best set of benchmarks to use for multilingual evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RM-Bench: Benchmarking Reward Models of Language Models with Subtlety
  and Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are critical in techniques like Reinforcement Learning from
Human Feedback (RLHF) and Inference Scaling Laws, where they guide language
model alignment and select optimal responses. Despite their importance,
existing reward model benchmarks often evaluate models by asking them to
distinguish between responses generated by models of varying power. However,
this approach fails to assess reward models on subtle but critical content
changes and variations in style, resulting in a low correlation with policy
model performance. To this end, we introduce RM-Bench, a novel benchmark
designed to evaluate reward models based on their sensitivity to subtle content
differences and resistance to style biases. Extensive experiments demonstrate
that RM-Bench strongly correlates with policy model performance, making it a
reliable reference for selecting reward models to align language models
effectively. We evaluate nearly 40 reward models on RM-Bench. Our results
reveal that even state-of-the-art models achieve an average performance of only
46.6%, which falls short of random-level accuracy (50%) when faced with style
bias interference. These findings highlight the significant room for
improvement in current reward models. Related code and data are available at
https://github.com/THU-KEG/RM-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicPIG: LSH Sampling for Efficient LLM Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with long context windows have gained
significant attention. However, the KV cache, stored to avoid re-computation,
becomes a bottleneck. Various dynamic sparse or TopK-based attention
approximation methods have been proposed to leverage the common insight that
attention is sparse. In this paper, we first show that TopK attention itself
suffers from quality degradation in certain downstream tasks because attention
is not always as sparse as expected. Rather than selecting the keys and values
with the highest attention scores, sampling with theoretical guarantees can
provide a better estimation for attention output. To make the sampling-based
approximation practical in LLM generation, we propose MagicPIG, a heterogeneous
system based on Locality Sensitive Hashing (LSH). MagicPIG significantly
reduces the workload of attention computation while preserving high accuracy
for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention
computation on the CPU, which allows it to serve longer contexts and larger
batch sizes with high approximation accuracy. MagicPIG can improve decoding
throughput by $1.9\sim3.9\times$ across various GPU hardware and achieve 110ms
decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a
context of 96k tokens. The code is available at
\url{https://github.com/Infini-AI-Lab/MagicPIG}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring <span class="highlight-title">Pretrain</span>ing via Active Forgetting for Improving Cross Lingual
  Transfer for Decoder Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyanshu Aggarwal, Ashutosh Sathe, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate exceptional capabilities in a
multitude of NLP tasks. However, the efficacy of such models to languages other
than English is often limited. Prior works have shown that encoder-only models
such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their
capabilities from English to other languages. In this work, we propose a
pretraining strategy that uses active forgetting to achieve similar cross
lingual transfer in decoder-only LLMs. We show that LLMs pretrained with active
forgetting are highly effective when adapting to new and unseen languages.
Through extensive experimentation, we find that LLMs pretrained with active
forgetting are able to learn better multilingual representations which
translates to better performance in many downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 tables, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Huang, Yuqi Huo, Zijia Zhao, Haoyu Lu, Shu Wu, Bingning Wang, Qiang Liu, Weipeng Chen, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have made significant strides by
integrating visual and textual modalities. A critical factor in training MLLMs
is the quality of image-text pairs within multimodal pretraining datasets.
However, $\textit {de facto}$ filter-based data quality enhancement paradigms
often discard a substantial portion of high-quality image data due to
inadequate semantic alignment between images and texts, leading to
inefficiencies in data utilization and scalability. In this paper, we propose
the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically
assesses and enhances the quality of image-text pairs. AITQE employs a text
rewriting mechanism for low-quality pairs and incorporates a negative sample
learning strategy to improve evaluative capabilities by integrating
deliberately selected low-quality samples during training. Unlike prior
approaches that significantly alter text distributions, our method minimally
adjusts text to preserve data volume while enhancing quality. Experimental
results demonstrate that AITQE surpasses existing methods on various benchmark,
effectively leveraging raw data and scaling efficiently with increasing data
volumes. We hope our work will inspire future works. The code and model are
available at: https://github.com/hanhuang22/AITQE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Tokens to Materials: Leveraging Language Models for Scientific
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Wan, Tong Xie, Nan Wu, Wenjie Zhang, Chunyu Kit, Bram Hoex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the predictive capabilities of language models in material science
is an ongoing interest. This study investigates the application of language
model embeddings to enhance material property prediction in materials science.
By evaluating various contextual embedding methods and pre-trained models,
including Bidirectional Encoder Representations from Transformers (BERT) and
Generative Pre-trained Transformers (GPT), we demonstrate that domain-specific
models, particularly MatBERT significantly outperform general-purpose models in
extracting implicit knowledge from compound names and material properties. Our
findings reveal that information-dense embeddings from the third layer of
MatBERT, combined with a context-averaging approach, offer the most effective
method for capturing material-property relationships from the scientific
literature. We also identify a crucial "tokenizer effect," highlighting the
importance of specialized text processing techniques that preserve complete
compound names while maintaining consistent token counts. These insights
underscore the value of domain-specific training and tokenization in materials
science applications and offer a promising pathway for accelerating the
discovery and development of new materials through AI-driven approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models
  Elicits Generalization to Composite Spatial Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) have demonstrated impressive performance across
a wide range of downstream tasks. However, their proficiency in spatial
reasoning remains limited, despite its crucial role in tasks involving
navigation and interaction with physical environments. Specifically, much of
the spatial reasoning in these tasks occurs in two-dimensional (2D)
environments, and our evaluation reveals that state-of-the-art VLMs frequently
generate implausible and incorrect responses to composite spatial reasoning
problems, including simple pathfinding tasks that humans can solve effortlessly
at a glance. To address this, we explore an effective approach to enhance 2D
spatial reasoning within VLMs by training the model on basic spatial
capabilities. We begin by disentangling the key components of 2D spatial
reasoning: direction comprehension, distance estimation, and localization. Our
central hypothesis is that mastering these basic spatial capabilities can
significantly enhance a model's performance on composite spatial tasks
requiring advanced spatial understanding and combinatorial problem-solving. To
investigate this hypothesis, we introduce Sparkle, a framework that fine-tunes
VLMs on these three basic spatial capabilities by synthetic data generation and
targeted supervision to form an instruction dataset for each capability. Our
experiments demonstrate that VLMs fine-tuned with Sparkle achieve significant
performance gains, not only in the basic tasks themselves but also in
generalizing to composite and out-of-distribution spatial reasoning tasks
(e.g., improving from 13.5% to 40.0% on the shortest path problem). These
findings underscore the effectiveness of mastering basic spatial capabilities
in enhancing composite spatial problem-solving, offering insights for improving
VLMs' spatial reasoning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Limpeh ga li gong: Challenges in Singlish Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynnette Hui Xian Ng, Luo Qi Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singlish, or Colloquial Singapore English, is a language formed from oral and
social communication within multicultural Singapore. In this work, we work on a
fundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS)
tagging of Singlish sentences. For our analysis, we build a parallel Singlish
dataset containing direct English translations and POS tags, with translation
and POS annotation done by native Singlish speakers. Our experiments show that
automatic transition- and transformer- based taggers perform with only $\sim
80\%$ accuracy when evaluated against human-annotated POS labels, suggesting
that there is indeed room for improvement on computation analysis of the
language. We provide an exposition of challenges in Singlish annotation: its
inconsistencies in form and semantics, the highly context-dependent particles
of the language, its structural unique expressions, and the variation of the
language on different mediums. Our task definition, resultant labels and
results reflects the challenges in analysing colloquial languages formulated
from a variety of dialects, and paves the way for future studies beyond POS
tagging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Men, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models, they are widely used as agents
in various fields. A key component of agents is memory, which stores vital
information but is susceptible to jailbreak attacks. Existing research mainly
focuses on single-agent attacks and shared memory attacks. However, real-world
scenarios often involve independent memory. In this paper, we propose the
Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale,
multi-agent, multi-topology text-based attack evaluation framework. TMCHT
involves one attacker agent attempting to mislead an entire society of agents.
We identify two major challenges in multi-agent attacks: (1) Non-complete graph
structure, (2) Large-scale systems. We attribute these challenges to a
phenomenon we term toxicity disappearing. To address these issues, we propose
an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes
the retrieval suffix to make poisoned samples more easily retrieved and
optimizes the replication suffix to make poisoned samples have contagious
ability. We demonstrate the superiority of our approach in TMCHT, with 23.51%,
18.95%, and 52.93% improvements in line topology, star topology, and 100-agent
settings. Encourage community attention to the security of multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in multimodal large language models (MLLMs), their
development has predominantly focused on English- and western-centric datasets
and tasks, leaving most of the world's languages and diverse cultural contexts
underrepresented. This paper introduces Pangea, a multilingual multimodal LLM
trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.
PangeaIns features: 1) high-quality English instructions, 2) carefully
machine-translated instructions, and 3) culturally relevant multimodal tasks to
ensure cross-cultural coverage. To rigorously assess models' capabilities, we
introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets
covering 47 languages. Results show that Pangea significantly outperforms
existing open-source models in multilingual settings and diverse cultural
contexts. Ablation studies further reveal the importance of English data
proportions, language popularity, and the number of multimodal training samples
on overall performance. We fully open-source our data, code, and trained
checkpoints, to facilitate the development of inclusive and robust multilingual
MLLMs, promoting equity and accessibility across a broader linguistic and
cultural spectrum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on
  CPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinheng Wang, Hansong Zhou, Ting Song, Shaoguang Mao, Shuming Ma, Hongyu Wang, Yan Xia, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and
BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs
in terms of speed and energy consumption. These developments also enable local
LLM deployment across a broad range of devices. In this work, we introduce
bitnet.cpp, a tailored software stack designed to unlock the full potential of
1-bit LLMs. Specifically, we develop a set of kernels to support fast and
lossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments
demonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x
to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model
sizes. The code is available at https://github.com/microsoft/BitNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Psycholinguistic Evaluation of Language Models' Sensitivity to
  Argument Roles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eun-Kyoung Rosa Lee, Sathvik Nair, Naomi Feldman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a systematic evaluation of large language models' sensitivity to
argument roles, i.e., who did what to whom, by replicating psycholinguistic
studies on human argument role processing. In three experiments, we find that
language models are able to distinguish verbs that appear in plausible and
implausible contexts, where plausibility is determined through the relation
between the verb and its preceding arguments. However, none of the models
capture the same selective patterns that human comprehenders exhibit during
real-time verb prediction. This indicates that language models' capacity to
detect verb plausibility does not arise from the same mechanism that underlies
human real-time sentence processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with
  Multi-Task Assessment and Stepwise Audio Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Yi Kuan, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large audio-language models (LALMs) have shown
impressive capabilities in understanding and reasoning about audio and speech
information. However, these models still face challenges, including
hallucinating non-existent sound events, misidentifying the order of sound
events, and incorrectly attributing sound sources, which undermine their
reliability and real-world application. To systematically evaluate these
issues, we propose three distinct tasks: object existence, temporal order, and
object attribute within audio. These tasks assess the models' comprehension of
critical audio information aspects. Our experimental results reveal limitations
in these fundamental tasks, underscoring the need for better models in
recognizing specific sound events, determining event sequences, and identifying
sound sources. To improve performance in these areas, we introduce a multi-turn
chain-of-thought approach, which demonstrates significantly improved model
performance across the proposed tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs write like humans? Variation in grammatical and rhetorical
  styles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Reinhart, David West Brown, Ben Markey, Michael Laudenbach, Kachatad Pantusen, Ronald Yurko, Gordon Weinberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are capable of writing grammatical text that
follows instructions, answers questions, and solves problems. As they have
advanced, it has become difficult to distinguish their output from
human-written text. While past research has found some differences in surface
features such as word choice and punctuation, and developed classifiers to
detect LLM output, none has studied the rhetorical styles of LLMs.
  Using several variants of Llama 3 and GPT-4o, we construct two parallel
corpora of human- and LLM-written texts from common prompts. Using Douglas
Biber's set of lexical, grammatical, and rhetorical features, we identify
systematic differences between LLMs and humans and between different LLMs.
These differences persist when moving from smaller models to larger ones, and
are larger for instruction-tuned models than base models. This demonstrates
that despite their advanced abilities, LLMs struggle to match human styles, and
hence more advanced linguistic features can detect patterns in their behavior
not previously recognized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 4 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysing the Residual Stream of Language Models Under Knowledge
  Conflicts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhao, Xiaotang Du, Giwon Hong, Aryo Pradipta Gema, Alessio Devoto, Hongru Wang, Xuanli He, Kam-Fai Wong, Pasquale Minervini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can store a significant amount of factual
knowledge in their parameters. However, their parametric knowledge may conflict
with the information provided in the context. Such conflicts can lead to
undesirable model behaviour, such as reliance on outdated or incorrect
information. In this work, we investigate whether LLMs can identify knowledge
conflicts and whether it is possible to know which source of knowledge the
model will rely on by analysing the residual stream of the LLM. Through probing
tasks, we find that LLMs can internally register the signal of knowledge
conflict in the residual stream, which can be accurately detected by probing
the intermediate model activations. This allows us to detect conflicts within
the residual stream before generating the answers without modifying the input
or model parameters. Moreover, we find that the residual stream shows
significantly different patterns when the model relies on contextual knowledge
versus parametric knowledge to resolve conflicts. This pattern can be employed
to estimate the behaviour of LLMs when conflict happens and prevent unexpected
answers before producing the answers. Our analysis offers insights into how
LLMs internally manage knowledge conflicts and provides a foundation for
developing methods to control the knowledge selection processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Foundation Model Interventions Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning LLMs for Reliable Medical Question-Answering Services <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Anaissi, Ali Braytee, Junaid Akram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an advanced approach to medical question-answering (QA) services,
using fine-tuned Large Language Models (LLMs) to improve the accuracy and
reliability of healthcare information. Our study focuses on optimizing models
like LLaMA-2 and Mistral, which have shown great promise in delivering precise,
reliable medical answers. By leveraging comprehensive datasets, we applied
fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model
performance through a combination of decomposed model weights, varied learning
rates for low-rank matrices, and rank stabilization, leading to improved
efficiency. ReRAG, which integrates retrieval on demand and question rewriting,
further refines the accuracy of the responses. This approach enables healthcare
providers to access fast, dependable information, aiding in more efficient
decision-making and fostering greater patient trust. Our work highlights the
potential of fine-tuned LLMs to significantly improve the quality and
accessibility of medical information services, ultimately contributing to
better healthcare outcomes for all.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, accepted and to be published in the proceedings
  of 2024 IEEE International Conference on Data Mining Workshops (ICDMW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian
  Product Routing in Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Zijia Lin, Yizhe Xiong, Minxuan Lv, Guangyuan Ma, Hui Chen, Songlin Hu, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have been attracting much attention from the
community recently, due to their remarkable performance in all kinds of
downstream tasks. According to the well-known scaling law, scaling up a dense
LLM enhances its capabilities, but also significantly increases the
computational complexity. Mixture-of-Experts (MoE) models address that by
allowing the model size to grow without substantially raising training or
inference costs. Yet MoE models face challenges regarding knowledge sharing
among experts, making their performance somehow sensitive to routing accuracy.
To tackle that, previous works introduced shared experts and combined their
outputs with those of the top $K$ routed experts in an ``addition'' manner. In
this paper, inspired by collective matrix factorization to learn shared
knowledge among data, we propose CartesianMoE, which implements more effective
knowledge sharing among experts in more like a ``multiplication'' manner.
Extensive experimental results indicate that CartesianMoE outperforms previous
MoE models for building LLMs, in terms of both perplexity and downstream task
performance. And we also find that CartesianMoE achieves better expert routing
robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On-Device LLMs for SMEs: Challenges and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Stephen Gabriel Yee Zhi Wen, Pai Chet Ng, Zhengkui Wang, Ian McLoughlin, Aik Beng Ng, Simon See
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic review of the infrastructure requirements
for deploying Large Language Models (LLMs) on-device within the context of
small and medium-sized enterprises (SMEs), focusing on both hardware and
software perspectives. From the hardware viewpoint, we discuss the utilization
of processing units like GPUs and TPUs, efficient memory and storage solutions,
and strategies for effective deployment, addressing the challenges of limited
computational resources typical in SME settings. From the software perspective,
we explore framework compatibility, operating system optimization, and the use
of specialized libraries tailored for resource-constrained environments. The
review is structured to first identify the unique challenges faced by SMEs in
deploying LLMs on-device, followed by an exploration of the opportunities that
both hardware innovations and software adaptations offer to overcome these
obstacles. Such a structured review provides practical insights, contributing
significantly to the community by enhancing the technological resilience of
SMEs in integrating LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure. The work is supported by the SIT-NVIDIA Joint AI
  Centre</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp <span class="highlight-title">Context</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maggie Mi, Aline Villavicencio, Nafise Sadat Moosavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human processing of idioms relies on understanding the contextual sentences
in which idioms occur, as well as language-intrinsic features such as frequency
and speaker-intrinsic factors like familiarity. While LLMs have shown high
performance on idiomaticity detection tasks, this success may be attributed to
reasoning shortcuts in existing datasets. To this end, we construct a novel,
controlled contrastive dataset designed to test whether LLMs can effectively
use context to disambiguate idiomatic meaning. Additionally, we explore how
collocational frequency and sentence probability influence model performance.
Our findings reveal that LLMs often fail to resolve idiomaticity when it is
required to attend to the surrounding context, and that models perform better
on sentences that have higher likelihood. The collocational frequency of
expressions also impacts performance. We make our code and dataset publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surprise! Uniform Information Density Isn't the Whole Story: Predicting
  Surprisal Contours in <span class="highlight-title">Long</span>-form Discourse <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleftheria Tsipidi, Franz Nowak, Ryan Cotterell, Ethan Wilcox, Mario Giulianelli, Alex Warstadt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Uniform Information Density (UID) hypothesis posits that speakers tend to
distribute information evenly across linguistic units to achieve efficient
communication. Of course, information rate in texts and discourses is not
perfectly uniform. While these fluctuations can be viewed as theoretically
uninteresting noise on top of a uniform target, another explanation is that UID
is not the only functional pressure regulating information content in a
language. Speakers may also seek to maintain interest, adhere to writing
conventions, and build compelling arguments. In this paper, we propose one such
functional pressure; namely that speakers modulate information rate based on
location within a hierarchically-structured model of discourse. We term this
the Structured Context Hypothesis and test it by predicting the surprisal
contours of naturally occurring discourses extracted from large language models
using predictors derived from discourse structure. We find that hierarchical
predictors are significant predictors of a discourse's information contour and
that deeply nested hierarchical predictors are more predictive than shallow
ones. This work takes an initial step beyond UID to propose testable hypotheses
for why the information rate fluctuates in predictable ways
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Know What To Say But Not When To Speak <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Umair, Vasanth Sarathy, JP de Ruiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Turn-taking is a fundamental mechanism in human communication that ensures
smooth and coherent verbal interactions. Recent advances in Large Language
Models (LLMs) have motivated their use in improving the turn-taking
capabilities of Spoken Dialogue Systems (SDS), such as their ability to respond
at appropriate times. However, existing models often struggle to predict
opportunities for speaking -- called Transition Relevance Places (TRPs) -- in
natural, unscripted conversations, focusing only on turn-final TRPs and not
within-turn TRPs. To address these limitations, we introduce a novel dataset of
participant-labeled within-turn TRPs and use it to evaluate the performance of
state-of-the-art LLMs in predicting opportunities for speaking. Our experiments
reveal the current limitations of LLMs in modeling unscripted spoken
interactions, highlighting areas for improvement and paving the way for more
naturalistic dialogue systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComPO: Community Preferences for Language Model Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachin Kumar, Chan Young Park, Yulia Tsvetkov, Noah A. Smith, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional algorithms for training language models (LMs) with human
feedback rely on preferences that are assumed to account for an "average" user,
disregarding subjectivity and finer-grained variations. Recent studies have
raised concerns that aggregating such diverse and often contradictory human
feedback to finetune models results in generic models that generate outputs not
preferred by many user groups, as they tend to average out styles and norms. To
address this issue, we draw inspiration from recommendation systems and propose
ComPO, a method to personalize preference optimization in LMs by
contextualizing the probability distribution of model outputs with the
preference provider. Focusing on group-level preferences rather than
individuals, we collect and release ComPRed, a question answering dataset with
community-level preferences from Reddit. This dataset facilitates studying
diversity in preferences without incurring privacy concerns associated with
individual feedback. Our experiments reveal that conditioning language models
on a community identifier (i.e., subreddit name) during preference tuning
substantially enhances model performance. Conversely, replacing this context
with random subreddit identifiers significantly diminishes performance,
highlighting the effectiveness of our approach in tailoring responses to
communities' preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CA*: Addressing Evaluation Pitfalls in Computation-Aware Latency for
  Simultaneous Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Xu, Wenda Xu, Siqi Ouyang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous speech translation (SimulST) systems must balance translation
quality with response time, making latency measurement crucial for evaluating
their real-world performance. However, there has been a longstanding belief
that current metrics yield unrealistically high latency measurements in
unsegmented streaming settings. In this paper, we investigate this phenomenon,
revealing its root cause in a fundamental misconception underlying existing
latency evaluation approaches. We demonstrate that this issue affects not only
streaming but also segment-level latency evaluation across different metrics.
Furthermore, we propose a modification to correctly measure computation-aware
latency for SimulST systems, addressing the limitations present in existing
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Continual Fine-Tuning for Enhancing Language Ability in Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyanshu Aggarwal, Sankarshan Damle, Navin Goyal, Satya Lokam, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common challenge towards the adaptability of Large Language Models (LLMs)
is their ability to learn new languages over time without hampering the model's
performance on languages in which the model is already proficient (usually
English). Continual fine-tuning (CFT) is the process of sequentially
fine-tuning an LLM to enable the model to adapt to downstream tasks with
varying data distributions and time shifts. This paper focuses on the language
adaptability of LLMs through CFT. We study a two-phase CFT process in which an
English-only end-to-end fine-tuned LLM from Phase 1 (predominantly Task
Ability) is sequentially fine-tuned on a multilingual dataset -- comprising
task data in new languages -- in Phase 2 (predominantly Language Ability). We
observe that the ``similarity'' of Phase 2 tasks with Phase 1 determines the
LLM's adaptability. For similar phase-wise datasets, the LLM after Phase 2 does
not show deterioration in task ability. In contrast, when the phase-wise
datasets are not similar, the LLM's task ability deteriorates. We test our
hypothesis on the open-source \mis\ and \llm\ models with multiple phase-wise
dataset pairs. To address the deterioration, we analyze tailored variants of
two CFT methods: layer freezing and generative replay. Our findings demonstrate
their effectiveness in enhancing the language ability of LLMs while preserving
task performance, in comparison to relevant baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering Knowledge Selection Behaviours in LLMs via SAE-Based
  Representation Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang, Kam-Fai Wong, Pasquale Minervini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can store a significant amount of factual
knowledge in their parameters. However, their parametric knowledge may conflict
with the information provided in the context -- this phenomenon, known as
\emph{context-memory knowledge conflicts}, can lead to undesirable model
behaviour, such as reliance on outdated or incorrect information. Analysing the
internal activations of LLMs, we find that they can internally register the
signals of knowledge conflict at mid-layers. Such signals allow us to detect
whether a knowledge conflict occurs and use \emph{inference-time} intervention
strategies to resolve it. In this work, we propose \textsc{SpARE}, a
\emph{training-free} representation engineering method that uses pre-trained
sparse auto-encoders (SAEs) to control the knowledge selection behaviour of
LLMs. \textsc{SpARE} identifies the functional features that control the
knowledge selection behaviours and applies them to edit the internal
activations of LLMs at inference time. Our experimental results show that
\textsc{SpARE} can effectively control the usage of either knowledge source to
resolve knowledge conflict in open-domain question-answering tasks, surpassing
existing representation engineering methods ($+10\%$) as well as contrastive
decoding methods ($+15\%$).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 1024m at SMM4H 2024: Tasks 3, 5 & 6 -- Ensembles of <span class="highlight-title">Transformer</span>s and
  Large Language Models for Medical Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Mohan Rao Kadiyala, M. V. P. Chandra Sekhara Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media is a great source of data for users reporting information and
regarding their health and how various things have had an effect on them. This
paper presents various approaches using Transformers and Large Language Models
and their ensembles, their performance along with advantages and drawbacks for
various tasks of SMM4H'24 - Classifying texts on impact of nature and outdoor
spaces on the author's mental health (Task 3), Binary classification of tweets
reporting their children's health disorders like Asthma, Autism, ADHD and
Speech disorder (task 5), Binary classification of users self-reporting their
age (task 6).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>short paper , acl 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Legal Decision Support Systems with LLM-based NLI for
  Analyzing Social Media Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Mohan Rao Kadiyala, Siddartha Pullakhandam, Kanwal Mehreen, Subhasya Tippareddy, Ashay Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our system description and error analysis of our entry
for NLLP 2024 shared task on Legal Natural Language Inference (L-NLI)
\citep{hagag2024legallenssharedtask2024}. The task required classifying these
relationships as entailed, contradicted, or neutral, indicating any association
between the review and the complaint. Our system emerged as the winning
submission, significantly outperforming other entries with a substantial margin
and demonstrating the effectiveness of our approach in legal text analysis. We
provide a detailed analysis of the strengths and limitations of each model and
approach tested, along with a thorough error analysis and suggestions for
future improvements. This paper aims to contribute to the growing field of
legal NLP by offering insights into advanced techniques for natural language
inference in legal contexts, making it accessible to both experts and newcomers
in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages , accepted to emnlp 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Cross-lingual Emotion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Mohan Rao Kadiyala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a detailed system description of our entry for the WASSA
2024 Task 2, focused on cross-lingual emotion detection. We utilized a
combination of large language models (LLMs) and their ensembles to effectively
understand and categorize emotions across different languages. Our approach not
only outperformed other submissions with a large margin, but also demonstrated
the strength of integrating multiple models to enhance performance.
Additionally, We conducted a thorough comparison of the benefits and
limitations of each model used. An error analysis is included along with
suggested areas for future improvement. This paper aims to offer a clear and
comprehensive understanding of advanced techniques in emotion detection, making
it accessible even to those new to the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages , accepted to acl 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy-driven Knowledge Selection and Response Generation for
  Document-grounded Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxuan Ma, Jiapeng Li, Mingda Li, Wei-Nan Zhang, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-grounded dialogue (DGD) uses documents as external knowledge for
dialogue generation. Correctly understanding the dialogue context is crucial
for selecting knowledge from the document and generating proper responses. In
this paper, we propose using a dialogue policy to help the dialogue
understanding in DGD. Our dialogue policy consists of two kinds of guiding
signals: utterance function and topic transfer intent. The utterance function
reflects the purpose and style of an utterance, and the topic transfer intent
reflects the topic and content of an utterance. We propose a novel framework
exploiting our dialogue policy for two core tasks in DGD, namely knowledge
selection (KS) and response generation (RG). The framework consists of two
modules: the Policy planner leverages policy-aware dialogue representation to
select knowledge and predict the policy of the response; the generator uses
policy/knowledge-aware dialogue representation for response generation. Our
policy-driven model gets state-of-the-art performance on three public
benchmarks and we provide a detailed analysis of the experimental results. Our
code/data will be released on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures, 14 tables, TOIS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Explained Keywords Empower Large Language Models for Code
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lishui Fan, Mouxiang Chen, Zhongxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved impressive performance in code
generation. However, due to the long-tail distribution of LLMs' training data,
low-frequency terms are typically underrepresented in the training process.
Consequently, LLMs often misunderstand or overlook problem-specific,
low-frequency keywords during code generation, compromising the accuracy of the
generated code. To address this, we propose a novel technique named
SEK(\textbf{S}elf-\textbf{E}xplained \textbf{K}eywords), which empowers an LLM
for better code generation by extracting and explaining the key terms in the
problem description with the LLM itself and ranking them based on frequency.
Comprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+),
and APPS, with five representative LLMs, show that SEK can significantly
improve LLMs in code generation, yielding substantial and consistent gains. For
instance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\% to
93.3\% on the Humaneval benchmark. Further analysis confirms that SEK enables
the LLMs to shift their attention from low-frequency keywords to their
corresponding high-frequency counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Exploration of Dialogue Summarization Approaches for
  Reproducibility, Comparative Assessment, and Methodological Innovations for
  Advancing Natural Language Processing in Abstractive Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yugandhar Reddy Gogireddy, Jithendra Reddy Gogireddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reproducibility in scientific research, particularly within the realm of
natural language processing (NLP), is essential for validating and verifying
the robustness of experimental findings. This paper delves into the
reproduction and evaluation of dialogue summarization models, focusing
specifically on the discrepancies observed between original studies and our
reproduction efforts. Dialogue summarization is a critical aspect of NLP,
aiming to condense conversational content into concise and informative
summaries, thus aiding in efficient information retrieval and decision-making
processes. Our research involved a thorough examination of several dialogue
summarization models using the AMI (Augmented Multi-party Interaction) dataset.
The models assessed include Hierarchical Memory Networks (HMNet) and various
versions of Pointer-Generator Networks (PGN), namely PGN(DKE), PGN(DRD),
PGN(DTS), and PGN(DALL). The primary objective was to evaluate the
informativeness and quality of the summaries generated by these models through
human assessment, a method that introduces subjectivity and variability in the
evaluation process. The analysis began with Dataset 1, where the sample
standard deviation of 0.656 indicated a moderate dispersion of data points
around the mean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Have an English Accent? Evaluating and
  Improving the Naturalness of Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhu Guo, Simone Conia, Zelin Zhou, Min Li, Saloni Potdar, Henry Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Large Language Models (LLMs) are predominantly designed with English
as the primary language, and even the few that are multilingual tend to exhibit
strong English-centric biases. Much like speakers who might produce awkward
expressions when learning a second language, LLMs often generate unnatural
outputs in non-English languages, reflecting English-centric patterns in both
vocabulary and grammar. Despite the importance of this issue, the naturalness
of multilingual LLM outputs has received limited attention. In this paper, we
address this gap by introducing novel automatic corpus-level metrics to assess
the lexical and syntactic naturalness of LLM outputs in a multilingual context.
Using our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark
in French and Chinese, revealing a tendency towards English-influenced
patterns. To mitigate this issue, we also propose a simple and effective
alignment method to improve the naturalness of an LLM in a target language and
domain, achieving consistent improvements in naturalness without compromising
the performance on general-purpose benchmarks. Our work highlights the
importance of developing multilingual metrics, resources and methods for the
new wave of multilingual LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Findings of the Third Shared Task on Multilingual Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Novák, Barbora Dohnalová, Miloslav Konopík, Anna Nedoluzhko, Martin Popel, Ondřej Pražák, Jakub Sido, Milan Straka, Zdeněk Žabokrtský, Daniel Zeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents an overview of the third edition of the shared task on
multilingual coreference resolution, held as part of the CRAC 2024 workshop.
Similarly to the previous two editions, the participants were challenged to
develop systems capable of identifying mentions and clustering them based on
identity coreference.
  This year's edition took another step towards real-world application by not
providing participants with gold slots for zero anaphora, increasing the task's
complexity and realism. In addition, the shared task was expanded to include a
more diverse set of languages, with a particular focus on historical languages.
The training and evaluation data were drawn from version 1.2 of the
multilingual collection of harmonized coreference resources CorefUD,
encompassing 21 datasets across 15 languages. 6 systems competed in this shared
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CausalGraph2LLM: Evaluating LLMs for Causal Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivaxi Sheth, Bahare Fatemi, Mario Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causality is essential in scientific research, enabling researchers to
interpret true relationships between variables. These causal relationships are
often represented by causal graphs, which are directed acyclic graphs. With the
recent advancements in Large Language Models (LLMs), there is an increasing
interest in exploring their capabilities in causal reasoning and their
potential use to hypothesize causal graphs. These tasks necessitate the LLMs to
encode the causal graph effectively for subsequent downstream tasks. In this
paper, we propose a comprehensive benchmark, \emph{CausalGraph2LLM},
encompassing a variety of causal graph settings to assess the causal graph
understanding capability of LLMs. We categorize the causal queries into two
types: graph-level and node-level queries. We benchmark both open-sourced and
closed models for our study. Our findings reveal that while LLMs show promise
in this domain, they are highly sensitive to the encoding used. Even capable
models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with
deviations of about $60\%$. We further demonstrate this sensitivity for
downstream causal intervention tasks. Moreover, we observe that LLMs can often
display biases when presented with contextual information about a causal graph,
potentially stemming from their parametric memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code - https://github.com/ivaxi0s/CausalGraph2LLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with
  Fine-tuning of Voice Activity Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Inoue, Divesh Lala, Gabriel Skantze, Tatsuya Kawahara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In human conversations, short backchannel utterances such as "yeah" and "oh"
play a crucial role in facilitating smooth and engaging dialogue. These
backchannels signal attentiveness and understanding without interrupting the
speaker, making their accurate prediction essential for creating more natural
conversational agents. This paper proposes a novel method for real-time,
continuous backchannel prediction using a fine-tuned Voice Activity Projection
(VAP) model. While existing approaches have relied on turn-based or
artificially balanced datasets, our approach predicts both the timing and type
of backchannels in a continuous and frame-wise manner on unbalanced, real-world
datasets. We first pre-train the VAP model on a general dialogue corpus to
capture conversational dynamics and then fine-tune it on a specialized dataset
focused on backchannel behavior. Experimental results demonstrate that our
model outperforms baseline methods in both timing and type prediction tasks,
achieving robust performance in real-time environments. This research offers a
promising step toward more responsive and human-like dialogue systems, with
implications for interactive spoken dialogue applications such as virtual
assistants and robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Object Hallucination via Concentric Causal Attention <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Xing, Yiheng Li, Ivan Laptev, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Large Vision Language Models (LVLMs) present remarkable zero-shot
conversational and reasoning capabilities given multimodal queries.
Nevertheless, they suffer from object hallucination, a phenomenon where LVLMs
are prone to generate textual responses not factually aligned with image
inputs. Our pilot study reveals that object hallucination is closely tied with
Rotary Position Encoding (RoPE), a widely adopted positional dependency
modeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs
tend to hallucinate more when relevant visual cues are distant from instruction
tokens in the multimodal input sequence. Additionally, we observe a similar
effect when reversing the sequential order of visual tokens during multimodal
alignment. Our tests indicate that long-term decay in RoPE poses challenges to
LVLMs while capturing visual-instruction interactions across long distances. We
propose Concentric Causal Attention (CCA), a simple yet effective positional
alignment strategy that mitigates the impact of RoPE long-term decay in LVLMs
by naturally reducing relative distance between visual and instruction tokens.
With CCA, visual tokens can better interact with instruction tokens, thereby
enhancing model's perception capability and alleviating object hallucination.
Without bells and whistles, our positional alignment method surpasses existing
hallucination mitigation strategies by large margins on multiple object
hallucination benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024. Code is available at
  https://github.com/xing0047/cca-llava</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DefVerify: Do Hate Speech Models Reflect Their <span class="highlight-title">Dataset</span>'s Definition? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Urja Khurana, Eric Nalisnick, Antske Fokkens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When building a predictive model, it is often difficult to ensure that
domain-specific requirements are encoded by the model that will eventually be
deployed. Consider researchers working on hate speech detection. They will have
an idea of what is considered hate speech, but building a model that reflects
their view accurately requires preserving those ideals throughout the workflow
of data set construction and model training. Complications such as sampling
bias, annotation bias, and model misspecification almost always arise, possibly
resulting in a gap between the domain specification and the model's actual
behavior upon deployment. To address this issue for hate speech detection, we
propose DefVerify: a 3-step procedure that (i) encodes a user-specified
definition of hate speech, (ii) quantifies to what extent the model reflects
the intended definition, and (iii) tries to identify the point of failure in
the workflow. We use DefVerify to find gaps between definition and model
behavior when applied to six popular hate speech benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using <span class="highlight-title">GPT</span> Models for Qualitative and Quantitative News Analytics in the
  2024 US Presidental Election Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohdan M. Pavlyshenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper considers an approach of using Google Search API and GPT-4o model
for qualitative and quantitative analyses of news through retrieval-augmented
generation (RAG). This approach was applied to analyze news about the 2024 US
presidential election process. Different news sources for different time
periods have been analyzed. Quantitative scores generated by GPT model have
been analyzed using Bayesian regression to derive trend lines. The
distributions found for the regression parameters allow for the analysis of
uncertainty in the election process. The obtained results demonstrate that
using the GPT models for news analysis, one can get informative analytics and
provide key insights that can be applied in further analyses of election
processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Principles of semantic and functional efficiency in grammatical
  patterning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Cheng, Francesca Franzon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grammatical features such as number and gender serve two central functions in
human languages. While they encode salient semantic attributes like numerosity
and animacy, they also offload sentence processing cost by predictably linking
words together via grammatical agreement. Grammars exhibit consistent
organizational patterns across diverse languages, invariably rooted in a
semantic foundation, a widely confirmed but still theoretically unexplained
phenomenon. To explain the basis of universal grammatical patterns, we unify
two fundamental properties of grammar, semantic encoding and agreement-based
predictability, into a single information-theoretic objective under cognitive
constraints. Our analyses reveal that grammatical organization provably
inherits from perceptual attributes, but that grammars empirically prioritize
functional goals, promoting efficient language processing over semantic
encoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Did somebody say "Gest-IT"? A pilot exploration of multimodal data
  management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Pannitto, Lorenzo Albanesi, Laura Marion, Federica Maria Martines, Carmelo Caruso, Claudia S. Bianchini, Francesca Masini, Caterina Mauri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a pilot exploration of the construction, management and
analysis of a multimodal corpus. Through a three-layer annotation that provides
orthographic, prosodic, and gestural transcriptions, the Gest-IT resource
allows to investigate the variation of gesture-making patterns in conversations
between sighted people and people with visual impairment. After discussing the
transcription methods and technical procedures employed in our study, we
propose a unified CoNLL-U corpus and indicate our future steps
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve Dense Passage Retrieval with Entailment Tuning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Dai, Hao Liu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval module can be plugged into many downstream NLP tasks to improve
their performance, such as open-domain question answering and
retrieval-augmented generation. The key to a retrieval system is to calculate
relevance scores to query and passage pairs. However, the definition of
relevance is often ambiguous. We observed that a major class of relevance
aligns with the concept of entailment in NLI tasks. Based on this observation,
we designed a method called entailment tuning to improve the embedding of dense
retrievers. Specifically, we unify the form of retrieval data and NLI data
using existence claim as a bridge. Then, we train retrievers to predict the
claims entailed in a passage with a variant task of masked prediction. Our
method can be efficiently plugged into current dense retrieval methods, and
experiments show the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-to-Defer for Extractive Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Montreuil Yannis, Carlier Axel, Ng Lai Xing, Ooi Wei Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models have profoundly impacted the field of extractive
question-answering, leveraging large-scale textual corpora to enhance
contextual language understanding. Despite their success, these models struggle
in complex scenarios that demand nuanced interpretation or inferential
reasoning beyond immediate textual cues. Furthermore, their size poses
deployment challenges on resource-constrained devices. Addressing these
limitations, we introduce an adapted two-stage Learning-to-Defer mechanism that
enhances decision-making by enabling selective deference to human experts or
larger models without retraining language models in the context of
question-answering. This approach not only maintains computational efficiency
but also significantly improves model reliability and accuracy in ambiguous
contexts. We establish the theoretical soundness of our methodology by proving
Bayes and $(\mathcal{H}, \mathcal{R})$--consistency of our surrogate loss
function, guaranteeing the optimality of the final solution. Empirical
evaluations on the SQuADv2 dataset illustrate performance gains from
integrating human expertise and leveraging larger models. Our results further
demonstrate that deferring a minimal number of queries allows the smaller model
to achieve performance comparable to their larger counterparts while preserving
computing efficiency, thus broadening the applicability of pre-trained language
models in diverse operational environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 17 main paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Querying System Through Entity Enrichment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Amavi, Mirian Halfeld Ferrari, Nicolas Hiot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on a domain expert querying system over databases. It
presents a solution designed for a French enterprise interested in offering a
natural language interface for its clients. The approach, based on entity
enrichment, aims at translating natural language queries into database queries.
In this paper, the database is treated through a logical paradigm, suggesting
the adaptability of our approach to different database models. The good
precision of our method is shown through some preliminary experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toeing the Party Line: Election Manifestos as a Key to Understand
  Political Discourse on Twitter <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Maurer, Tanise Ceron, Sebastian Padó, Gabriella Lapesa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Political discourse on Twitter is a moving target: politicians continuously
make statements about their positions. It is therefore crucial to track their
discourse on social media to understand their ideological positions and goals.
However, Twitter data is also challenging to work with since it is ambiguous
and often dependent on social context, and consequently, recent work on
political positioning has tended to focus strongly on manifestos (parties'
electoral programs) rather than social media.
  In this paper, we extend recently proposed methods to predict pairwise
positional similarities between parties from the manifesto case to the Twitter
case, using hashtags as a signal to fine-tune text representations, without the
need for manual annotation. We verify the efficacy of fine-tuning and conduct a
series of experiments that assess the robustness of our method for low-resource
scenarios. We find that our method yields stable positioning reflective of
manifesto positioning, both in scenarios with all tweets of candidates across
years available and when only smaller subsets from shorter time periods are
available. This indicates that it is possible to reliably analyze the relative
positioning of actors forgoing manual annotation, even in the noisier context
of social media.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, accepted at EMNLP (Findings) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who's Who: Large Language Models Meet Knowledge Conflicts in Practice <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang Hieu Pham, Hoang Ngo, Anh Tuan Luu, Dat Quoc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) methods are viable solutions for
addressing the static memory limits of pre-trained language models.
Nevertheless, encountering conflicting sources of information within the
retrieval context is an inevitable practical challenge. In such situations, the
language models are recommended to transparently inform users about the
conflicts rather than autonomously deciding what to present based on their
inherent biases. To analyze how current large language models (LLMs) align with
our recommendation, we introduce WhoQA, a public benchmark dataset to examine
model's behavior in knowledge conflict situations. We induce conflicts by
asking about a common property among entities having the same name, resulting
in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K
questions across 13 Wikidata property types and 150K Wikipedia entities. Our
experiments show that despite the simplicity of WhoQA questions, knowledge
conflicts significantly degrades LLMs' performance in RAG settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing annotator bias by belief elicitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terne Sasha Thorn Jakobsen, Andreas Bjerre-Nielsen, Robert Böhm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowdsourced annotations of data play a substantial role in the development
of Artificial Intelligence (AI). It is broadly recognised that annotations of
text data can contain annotator bias, where systematic disagreement in
annotations can be traced back to differences in the annotators' backgrounds.
Being unaware of such annotator bias can lead to representational bias against
minority group perspectives and therefore several methods have been proposed
for recognising bias or preserving perspectives. These methods typically
require either a substantial number of annotators or annotations per data
instance. In this study, we propose a simple method for handling bias in
annotations without requirements on the number of annotators or instances.
Instead, we ask annotators about their beliefs of other annotators' judgements
of an instance, under the hypothesis that these beliefs may provide more
representative and less biased labels than judgements. The method was examined
in two controlled, survey-based experiments involving Democrats and Republicans
(n=1,590) asked to judge statements as arguments and then report beliefs about
others' judgements. The results indicate that bias, defined as systematic
differences between the two groups of annotators, is consistently reduced when
asking for beliefs instead of judgements. Our proposed method therefore has the
potential to reduce the risk of annotator bias, thereby improving the
generalisability of AI systems and preventing harm to unrepresented
socio-demographic groups, and we highlight the need for further studies of this
potential in other tasks and downstream applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations of Large Language Models in Medical
  Information Extraction via Contrastive Decoding <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive capabilities of large language models (LLMs) have attracted
extensive interests of applying LLMs to medical field. However, the complex
nature of clinical environments presents significant hallucination challenges
for LLMs, hindering their widespread adoption. In this paper, we address these
hallucination issues in the context of Medical Information Extraction (MIE)
tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by
redefining MIE tasks as an identify-and-classify process. We then separate the
identification and classification functions of LLMs by selectively masking the
optimization of tokens during fine-tuning. During the inference stage, we
alternately contrast output distributions derived from sub-task models. This
approach aims to selectively enhance the identification and classification
capabilities while minimizing the influence of other inherent abilities in
LLMs. Additionally, we propose an alternate adaptive constraint strategy to
more effectively adjust the scale and scope of contrastive tokens. Through
comprehensive experiments on two different backbones and six diverse medical
information extraction tasks, ALCD demonstrates significant improvements in
resolving hallucination issues compared to conventional decoding methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert
  Iteration on Large-Scale LEAN Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as powerful tools in mathematical
theorem proving, particularly when utilizing formal languages such as LEAN. The
major learning paradigm is expert iteration, which necessitates a pre-defined
dataset comprising numerous mathematical problems. In this process, LLMs
attempt to prove problems within the dataset and iteratively refine their
capabilities through self-training on the proofs they discover. We propose to
use large scale LEAN problem datasets Lean-workbook for expert iteration with
more than 20,000 CPU days. During expert iteration, we found log-linear trends
between solved problem amount with proof length and CPU usage. We train a
critic model to select relatively easy problems for policy models to make
trials and guide the model to search for deeper proofs. InternLM2.5-StepProver
achieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,
and Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the
MiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus
which shows a significant improvement compared to only 9.5% of problems proved
when Lean-Workbook-Plus was released. We open-source our models and searched
proofs at https://github.com/InternLM/InternLM-Math and
https://huggingface.co/datasets/internlm/Lean-Workbook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tokenization as Finite-State Transduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Cognetta, Naoaki Okazaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is the first step in modern neural language model pipelines
where an input text is converted to a sequence of subword tokens. We introduce
from first principles a finite-state transduction framework which can
efficiently encode all possible tokenizations of a regular language. We then
constructively show that Byte-Pair Encoding (BPE) and MaxMatch (WordPiece), two
popular tokenization schemes, fit within this framework. For BPE, this is
particularly surprising given its resemblance to context-free grammar and the
fact that it does not tokenize strings from left to right.
  An application of this is to guided generation, where the outputs of a
language model are constrained to match some pattern. Here, patterns are
encoded at the character level, which creates a mismatch between the
constraints and the model's subword vocabulary. While past work has focused
only on constraining outputs without regard to the underlying tokenization
algorithm, our framework allows for simultaneously constraining the model
outputs to match a specified pattern while also adhering to the underlying
tokenizer's canonical tokenization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + 5 pages in appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Terminology Integration for LLM-based Translation in
  Specialized Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sejoon Kim, Mingi Sung, Jeonghwan Lee, Hyunkuk Lim, Jorge Froilan Gimenez Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional machine translation methods typically involve training models
directly on large parallel corpora, with limited emphasis on specialized
terminology. However, In specialized fields such as patent, finance, or
biomedical domains, terminology is crucial for translation, with many terms
that needs to be translated following agreed-upon conventions. In this paper we
introduce a methodology that efficiently trains models with a smaller amount of
data while preserving the accuracy of terminology translation. We achieve this
through a systematic process of term extraction and glossary creation using the
Trie Tree algorithm, followed by data reconstruction to teach the LLM how to
integrate these specialized terms. This methodology enhances the model's
ability to handle specialized terminology and ensures high-quality
translations, particularly in fields where term consistency is crucial. Our
approach has demonstrated exceptional performance, achieving the highest
translation score among participants in the WMT patent task to date, showcasing
its effectiveness and broad applicability in specialized translation domains
where general methods often fall short.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WMT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DomainSum: A Hierarchical Benchmark for Fine-Grained Domain Shift in
  Abstractive Text Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Yuan, Haopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most research on abstractive summarization focuses on single-domain
applications, often neglecting how domain shifts between documents affect
performance and the generalization ability of summarization models. To address
this issue, we introduce DomainSum, a hierarchical benchmark designed to
capture fine-grained domain shifts in abstractive summarization. We categorize
these shifts into three levels: genre, style, and topic, and demonstrate
through comprehensive benchmark analysis that they follow a hierarchical
structure. Furthermore, we evaluate the domain generalization capabilities of
commonly used pre-trained language models (PLMs) and large language models
(LLMs) in in-domain and cross-domain settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing and Mitigating the Local Pattern Shortcuts of Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangjie You, Zecheng Tang, Juntao Li, Lili Yao, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have advanced significantly due to the attention
mechanism, but their quadratic complexity and linear memory demands limit their
performance on long-context tasks. Recently, researchers introduced Mamba, an
advanced model built upon State Space Models(SSMs) that offers linear
complexity and constant memory. Although Mamba is reported to match or surpass
the performance of attention-based models, our analysis reveals a performance
gap: Mamba excels in tasks that involve localized key information but faces
challenges with tasks that require handling distributed key information. Our
controlled experiments suggest that this inconsistency arises from Mamba's
reliance on local pattern shortcuts, which enable the model to remember local
key information within its limited memory but hinder its ability to retain more
dispersed information. Therefore, we introduce a global selection module into
the Mamba model to address this issue. Experiments on both existing and
proposed synthetic tasks, as well as real-world tasks, demonstrate the
effectiveness of our method. Notably, with the introduction of only 4M extra
parameters, our approach enables the Mamba model(130M) to achieve a significant
improvement on tasks with distributed information, increasing its performance
from 0 to 80.54 points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Generate and Evaluate Fact-checking Explanations with
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darius Feher, Abdullah Khered, Hao Zhang, Riza Batista-Navarro, Viktor Schlegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era increasingly dominated by digital platforms, the spread of
misinformation poses a significant challenge, highlighting the need for
solutions capable of assessing information veracity. Our research contributes
to the field of Explainable Artificial Antelligence (XAI) by developing
transformer-based fact-checking models that contextualise and justify their
decisions by generating human-accessible explanations. Importantly, we also
develop models for automatic evaluation of explanations for fact-checking
verdicts across different dimensions such as \texttt{(self)-contradiction},
\texttt{hallucination}, \texttt{convincingness} and \texttt{overall quality}.
By introducing human-centred evaluation methods and developing specialised
datasets, we emphasise the need for aligning Artificial Intelligence
(AI)-generated explanations with human judgements. This approach not only
advances theoretical knowledge in XAI but also holds practical implications by
enhancing the transparency, reliability and users' trust in AI-driven
fact-checking systems. Furthermore, the development of our metric learning
models is a first step towards potentially increasing efficiency and reducing
reliance on extensive manual assessment. Based on experimental results, our
best performing generative model \textsc{ROUGE-1} score of 47.77, demonstrating
superior performance in generating fact-checking explanations, particularly
when provided with high-quality evidence. Additionally, the best performing
metric learning model showed a moderately strong correlation with human
judgements on objective dimensions such as \texttt{(self)-contradiction and
\texttt{hallucination}, achieving a Matthews Correlation Coefficient (MCC) of
around 0.7.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming in Engineering Applications of Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAC: Efficient LLM Factuality Correction with Retrieval Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changmao Li, Jeffrey Flanigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit impressive results across a wide range
of natural language processing (NLP) tasks, yet they can often produce
factually incorrect outputs. This paper introduces a simple but effective
low-latency post-correction method, \textbf{Retrieval Augmented Correction
(RAC)}, aimed at enhancing the factual performance of LLMs without requiring
additional fine-tuning. Our method is general and can be used with any
instruction-tuned LLM, and has greatly reduced latency compared to prior
approaches. RAC decomposes the LLM's output into atomic facts and applies a
fine-grained verification and correction process with retrieved content to
verify and correct the LLM-generated output. Our extensive experiments show
that RAC yields up to 30\% improvements over state-of-the-art baselines across
two popular factuality evaluation datasets, validating its efficacy and
robustness in both with and without the integration of Retrieval-Augmented
Generation (RAG) across different LLMs.\footnote{Our code is at
\url{https://github.com/jlab-nlp/Retrieval-Augmented-Correction}}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Data Ablation Approximations for Language Models through
  Modular Training and Merging <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Na, Ian Magnusson, Ananya Harsh Jha, Tom Sherborne, Emma Strubell, Jesse Dodge, Pradeep Dasigi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data compositions for Large Language Models (LLMs) can significantly
affect their downstream performance. However, a thorough data ablation study
exploring large sets of candidate data mixtures is typically prohibitively
expensive since the full effect is seen only after training the models; this
can lead practitioners to settle for sub-optimal data mixtures. We propose an
efficient method for approximating data ablations which trains individual
models on subsets of a training corpus and reuses them across evaluations of
combinations of subsets. In continued pre-training experiments, we find that,
given an arbitrary evaluation set, the perplexity score of a single model
trained on a candidate set of data is strongly correlated with perplexity
scores of parameter averages of models trained on distinct partitions of that
data. From this finding, we posit that researchers and practitioners can
conduct inexpensive simulations of data ablations by maintaining a pool of
models that were each trained on partitions of a large training corpus, and
assessing candidate data mixtures by evaluating parameter averages of
combinations of these models. This approach allows for substantial improvements
in amortized training efficiency -- scaling only linearly with respect to new
data -- by enabling reuse of previous training computation, opening new avenues
for improving model performance through rigorous, incremental data assessment
and mixing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. 17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CL-HOI: Cross-Level Human-Object Interaction Distillation from Vision
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianjun Gao, Chen Cai, Ruoyu Wang, Wenyang Liu, Kim-Hui Yap, Kratika Garg, Boon-Siew Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-object interaction (HOI) detection has seen advancements with Vision
Language Models (VLMs), but these methods often depend on extensive manual
annotations. Vision Large Language Models (VLLMs) can inherently recognize and
reason about interactions at the image level but are computationally heavy and
not designed for instance-level HOI detection. To overcome these limitations,
we propose a Cross-Level HOI distillation (CL-HOI) framework, which distills
instance-level HOIs from VLLMs image-level understanding without the need for
manual annotations. Our approach involves two stages: context distillation,
where a Visual Linguistic Translator (VLT) converts visual information into
linguistic form, and interaction distillation, where an Interaction Cognition
Network (ICN) reasons about spatial, visual, and context relations. We design
contrastive distillation losses to transfer image-level context and interaction
knowledge from the teacher to the student model, enabling instance-level HOI
detection. Evaluations on HICO-DET and V-COCO datasets demonstrate that our
CL-HOI surpasses existing weakly supervised methods and VLLM supervised
methods, showing its efficacy in detecting HOIs without manual labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Efficient Medical Report Generation using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Abdullah, Ameer Hamza, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical report generation is the task of automatically writing radiology
reports for chest X-ray images. Manually composing these reports is a
time-consuming process that is also prone to human errors. Generating medical
reports can therefore help reduce the burden on radiologists. In other words,
we can promote greater clinical automation in the medical domain. In this work,
we propose a new framework leveraging vision-enabled Large Language Models
(LLM) for the task of medical report generation. We introduce a lightweight
solution that achieves better or comparative performance as compared to
previous solutions on the task of medical report generation. We conduct
extensive experiments exploring different model sizes and enhancement
approaches, such as prefix tuning to improve the text generation abilities of
the LLMs. We evaluate our approach on a prominent large-scale radiology report
dataset - MIMIC-CXR. Our results demonstrate the capability of our
resource-efficient framework to generate patient-specific reports with strong
medical contextual understanding and high precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMILES-<span class="highlight-title">Prompt</span>ing: A Novel Approach to LLM Jailbreak Attacks in Chemical
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidan Wong, He Cao, Zijing Liu, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing integration of large language models (LLMs) across various
fields has heightened concerns about their potential to propagate dangerous
information. This paper specifically explores the security vulnerabilities of
LLMs within the field of chemistry, particularly their capacity to provide
instructions for synthesizing hazardous substances. We evaluate the
effectiveness of several prompt injection attack methods, including
red-teaming, explicit prompting, and implicit prompting. Additionally, we
introduce a novel attack technique named SMILES-prompting, which uses the
Simplified Molecular-Input Line-Entry System (SMILES) to reference chemical
substances. Our findings reveal that SMILES-prompting can effectively bypass
current safety mechanisms. These findings highlight the urgent need for
enhanced domain-specific safeguards in LLMs to prevent misuse and improve their
potential for positive social impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Invent Algorithms to Improve Themselves? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoichi Ishibashi, Taro Yano, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable performance improvements
and are rapidly gaining adoption in industry. However, the methods for
improving LLMs are still designed by humans, which restricts the invention of
new model-improving algorithms to human expertise and imagination. To address
this, we propose the Self-Developing framework, which enables LLMs to
autonomously generate and learn model-improvement algorithms. In this
framework, the seed model generates, applies, and evaluates model-improving
algorithms, continuously improving both the seed model and the algorithms
themselves. In mathematical reasoning tasks, Self-Developing not only creates
models that surpass the seed model but also consistently outperforms models
created using human-designed algorithms. Additionally, these LLM-discovered
algorithms demonstrate strong effectiveness, including transferability to
out-of-domain models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selecting Influential Samples for <span class="highlight-title">Long</span> <span class="highlight-title">Context</span> Alignment via Homologous
  Models' Guidance and <span class="highlight-title">Context</span>ual Awareness Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzheng Si, Haozhe Zhao, Gang Chen, Yunshui Li, Kangyang Luo, Chuancheng Lv, Kaikai An, Fanchao Qi, Baobao Chang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expansion of large language models to effectively handle instructions
with extremely long contexts has yet to be fully investigated. The primary
obstacle lies in constructing a high-quality long instruction-following dataset
devised for long context alignment. Existing studies have attempted to scale up
the available data volume by synthesizing long instruction-following samples.
However, indiscriminately increasing the quantity of data without a
well-defined strategy for ensuring data quality may introduce low-quality
samples and restrict the final performance. To bridge this gap, we aim to
address the unique challenge of long-context alignment, i.e., modeling the
long-range dependencies for handling instructions and lengthy input contexts.
We propose GATEAU, a novel framework designed to identify the influential and
high-quality samples enriched with long-range dependency relations by utilizing
crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement
(CAM). Specifically, HMG attempts to measure the difficulty of generating
corresponding responses due to the long-range dependencies, using the
perplexity scores of the response from two homologous models with different
context windows. Also, the role of CAM is to measure the difficulty of
understanding the long input contexts due to long-range dependencies by
evaluating whether the model's attention is focused on important segments.
Built upon both proposed methods, we select the most challenging samples as the
influential data to effectively frame the long-range dependencies, thereby
achieving better performance of LLMs. Comprehensive experiments indicate that
GATEAU effectively identifies samples enriched with long-range dependency
relations and the model trained on these selected samples exhibits better
instruction-following and long-context understanding capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Parallel Program Performance Through DSL-Driven Code
  Generation with LLM Optimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping computations to processors and assigning data to memory are critical
for maximizing performance in parallel programming. These mapping decisions are
managed through the development of specialized low-level system code, called
mappers, crafted by performance engineers. Each mapper is tailored to a
specific application and optimized for the underlying machine architecture, a
process that requires days of refinement and tuning from an expert. Despite
advances in system research, automating mapper generation remains a challenge
due to the complexity of making millions of decisions to find the optimal
solution and generate the solution as code. We introduce an approach that
leverages recent advances in LLM-based optimizers for mapper design. In under
ten minutes, our method automatically discovers mappers that surpass human
expert designs in scientific applications by up to 1.34X speedup. For parallel
matrix multiplication algorithms, our mapper achieves up to 1.31X of the
expert-designed solution. To achieve this, we simplify the complexity of
low-level code generation by introducing a domain-specific language (DSL) that
abstracts the low-level system programming details and defines a structured
search space for LLMs to explore. To maximize the application performance, we
use an LLM optimizer to improve an agentic system that generates the mapper
code. As a result, this approach significantly reduces the workload for
performance engineers while achieving substantial performance gains across
diverse applications. Finally, our results demonstrate the effectiveness of
LLM-based optimization in system design and suggest its potential for
addressing other complex system challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guardians of Discourse: Evaluating LLMs on Multilingual Offensive
  Language Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei He, Lilin Wang, Jiaying Wang, Zhenyu Liu, Hongbin Na, Zimu Wang, Wei Wang, Qi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying offensive language is essential for maintaining safety and
sustainability in the social media era. Though large language models (LLMs)
have demonstrated encouraging potential in social media analytics, they lack
thorough evaluation when in offensive language detection, particularly in
multilingual environments. We for the first time evaluate multilingual
offensive language detection of LLMs in three languages: English, Spanish, and
German with three LLMs, GPT-3.5, Flan-T5, and Mistral, in both monolingual and
multilingual settings. We further examine the impact of different prompt
languages and augmented translation data for the task in non-English contexts.
Furthermore, we discuss the impact of the inherent bias in LLMs and the
datasets in the mispredictions related to sensitive topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at UIC 2024 proceedings. Accepted version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acoustic Model Optimization over Multiple Data Sources: Merging and
  Valuation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Junqiu Wei, Weicheng Wang, Di Jiang, Conghui Tan, Rongzhong Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the rising awareness of privacy protection and the voluminous scale of
speech data, it is becoming infeasible for Automatic Speech Recognition (ASR)
system developers to train the acoustic model with complete data as before. For
example, the data may be owned by different curators, and it is not allowed to
share with others. In this paper, we propose a novel paradigm to solve salient
problems plaguing the ASR field. In the first stage, multiple acoustic models
are trained based upon different subsets of the complete speech data, while in
the second phase, two novel algorithms are utilized to generate a high-quality
acoustic model based upon those trained on data subsets. We first propose the
Genetic Merge Algorithm (GMA), which is a highly specialized algorithm for
optimizing acoustic models but suffers from low efficiency. We further propose
the SGD-Based Optimizational Merge Algorithm (SOMA), which effectively
alleviates the efficiency bottleneck of GMA and maintains superior model
accuracy. Extensive experiments on public data show that the proposed methods
can significantly outperform the state-of-the-art. Furthermore, we introduce
Shapley Value to estimate the contribution score of the trained models, which
is useful for evaluating the effectiveness of the data and providing fair
incentives to their curators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interventional Speech Noise Injection for ASR Generalizable Spoken
  Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonjoon Jung, Jaeseong Lee, Seungtaek Choi, Dohyeon Lee, Minsoo Kim, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, pre-trained language models (PLMs) have been increasingly adopted
in spoken language understanding (SLU). However, automatic speech recognition
(ASR) systems frequently produce inaccurate transcriptions, leading to noisy
inputs for SLU models, which can significantly degrade their performance. To
address this, our objective is to train SLU models to withstand ASR errors by
exposing them to noises commonly observed in ASR systems, referred to as
ASR-plausible noises. Speech noise injection (SNI) methods have pursued this
objective by introducing ASR-plausible noises, but we argue that these methods
are inherently biased towards specific ASR systems, or ASR-specific noises. In
this work, we propose a novel and less biased augmentation method of
introducing the noises that are plausible to any ASR system, by cutting off the
non-causal effect of noises. Experimental results and analyses demonstrate the
effectiveness of our proposed methods in enhancing the robustness and
generalizability of SLU models against unseen ASR systems by introducing more
diverse and plausible ASR noises in advance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moonshine: Speech Recognition for Live Transcription and Voice Commands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nat Jeffries, Evan King, Manjunath Kudlur, Guy Nicholson, James Wang, Pete Warden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Moonshine, a family of speech recognition models
optimized for live transcription and voice command processing. Moonshine is
based on an encoder-decoder transformer architecture and employs Rotary
Position Embedding (RoPE) instead of traditional absolute position embeddings.
The model is trained on speech segments of various lengths, but without using
zero-padding, leading to greater efficiency for the encoder during inference
time. When benchmarked against OpenAI's Whisper tiny.en, Moonshine Tiny
demonstrates a 5x reduction in compute requirements for transcribing a
10-second speech segment while incurring no increase in word error rates across
standard evaluation datasets. These results highlight Moonshine's potential for
real-time and resource-constrained applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> of <span class="highlight-title">Dataset</span>s, Theories, Variants, and Applications
  in Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyi Xiao, Zechuan Wang, Leilei Gan, Shuai Zhao, Wanggui He, Luu Anh Tuan, Long Chen, Hao Jiang, Zhou Zhao, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of large language models (LLMs), aligning policy
models with human preferences has become increasingly critical. Direct
Preference Optimization (DPO) has emerged as a promising approach for
alignment, acting as an RL-free alternative to Reinforcement Learning from
Human Feedback (RLHF). Despite DPO's various advancements and inherent
limitations, an in-depth review of these aspects is currently lacking in the
literature. In this work, we present a comprehensive review of the challenges
and opportunities in DPO, covering theoretical analyses, variants, relevant
preference datasets, and applications. Specifically, we categorize recent
studies on DPO based on key research questions to provide a thorough
understanding of DPO's current landscape. Additionally, we propose several
future research directions to offer insights on model alignment for the
research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CPE-Pro: A Structure-Sensitive Deep Learning Model for Protein
  Representation and Origin Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Gou, Wenhui Ge,  YangTan, Guisheng Fan, Mingchen Li, Huiqun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein structures are important for understanding their functions and
interactions. Currently, many protein structure prediction methods are
enriching the structure database. Discriminating the origin of structures is
crucial for distinguishing between experimentally resolved and computationally
predicted structures, evaluating the reliability of prediction methods, and
guiding downstream biological studies. Building on works in structure
prediction, We developed a structure-sensitive supervised deep learning model,
Crystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent
and discriminate the origin of protein structures. CPE-Pro learns the
structural information of proteins and captures inter-structural differences to
achieve accurate traceability on four data classes, and is expected to be
extended to more. Simultaneously, we utilized Foldseek to encode protein
structures into "structure-sequence" and trained a protein Structural Sequence
Language Model, SSLM. Preliminary experiments demonstrated that, compared to
large-scale protein language models pre-trained on vast amounts of amino acid
sequences, the "structure-sequences" enable the language model to learn more
informative protein features, enhancing and optimizing structural
representations. We have provided the code, model weights, and all related
materials on https://github.com/GouWenrui/CPE-Pro-main.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMPLE: Emotion-Aware Multimodal Fusion <span class="highlight-title">Prompt</span> Learning for Fake News
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoman Xu, Xiangrun Li, Taihang Wang, Ye Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting fake news in large datasets is challenging due to its diversity and
complexity, with traditional approaches often focusing on textual features
while underutilizing semantic and emotional elements. Current methods also rely
heavily on large annotated datasets, limiting their effectiveness in more
nuanced analysis. To address these challenges, this paper introduces
Emotion-\textbf{A}ware \textbf{M}ultimodal Fusion \textbf{P}rompt
\textbf{L}\textbf{E}arning (\textbf{AMPLE}) framework to address the above
issue by combining text sentiment analysis with multimodal data and hybrid
prompt templates. This framework extracts emotional elements from texts by
leveraging sentiment analysis tools. It then employs Multi-Head Cross-Attention
(MCA) mechanisms and similarity-aware fusion methods to integrate multimodal
data. The proposed AMPLE framework demonstrates strong performance on two
public datasets in both few-shot and data-rich settings, with results
indicating the potential of emotional aspects in fake news detection.
Furthermore, the study explores the impact of integrating large language models
with this method for text sentiment extraction, revealing substantial room for
further improvement. The code can be found at
:\url{https://github.com/xxm1215/MMM2025_few-shot/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Symbolic Learners in Arithmetic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyuan Deng, Zhiqi Li, Roy Xie, Ruidi Chang, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are thought to struggle with arithmetic learning
due to the inherent differences between language modeling and numerical
computation, but concrete evidence has been lacking. This work responds to this
claim through a two-side experiment. We first investigate whether LLMs leverage
partial products during arithmetic learning. We find that although LLMs can
identify some partial products after learning, they fail to leverage them for
arithmetic tasks, conversely. We then explore how LLMs approach arithmetic
symbolically by breaking tasks into subgroups, hypothesizing that difficulties
arise from subgroup complexity and selection. Our results show that when
subgroup complexity is fixed, LLMs treat a collection of different arithmetic
operations similarly. By analyzing position-level accuracy across different
training sizes, we further observe that it follows a U-shaped pattern: LLMs
quickly learn the easiest patterns at the first and last positions, while
progressively learning the more difficult patterns in the middle positions.
This suggests that LLMs select subgroup following an easy-to-hard paradigm
during learning. Our work confirms that LLMs are pure symbolic learners in
arithmetic tasks and underscores the importance of understanding them deeply
through subgroup-level quantification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Probabilistic Attention Mechanism in <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        DongNyeong Heo, Heeyoul Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer architecture has become widely adopted due to its
demonstrated success, attributed to the attention mechanism at its core.
Despite these successes, the attention mechanism of Transformers is associated
with two well-known issues: rank-collapse and gradient vanishing. In this
paper, we present a theoretical analysis that it is inherently difficult to
address both issues simultaneously in the conventional attention mechanism. To
handle these issues, we introduce a novel class of attention mechanism,
referred to as generalized probabilistic attention mechanism (GPAM), and its
dual-attention implementation within the Transformer architecture. Unlike
conventional attention mechanisms, GPAM allows for negative attention scores
while preserving a fixed total sum. We provide theoretical evidence that the
proposed dual-attention GPAM (daGPAM) effectively mitigates both the
rank-collapse and gradient vanishing issues which are difficult to resolve
simultaneously with the conventional attention mechanisms. Furthermore, we
empirically validate this theoretical evidence, demonstrating the superiority
of daGPAM compared to other alternative attention mechanisms that were proposed
to address the same issues. Additionally, we demonstrate the practical benefits
of GPAM in natural language processing tasks, such as language modeling and
neural machine translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, Jian-Yun Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a cornerstone of modern information access, search engines have become
indispensable in everyday life. With the rapid advancements in AI and natural
language processing (NLP) technologies, particularly large language models
(LLMs), search engines have evolved to support more intuitive and intelligent
interactions between users and systems. Conversational search, an emerging
paradigm for next-generation search engines, leverages natural language
dialogue to facilitate complex and precise information retrieval, thus
attracting significant attention. Unlike traditional keyword-based search
engines, conversational search systems enhance user experience by supporting
intricate queries, maintaining context over multi-turn interactions, and
providing robust information integration and processing capabilities. Key
components such as query reformulation, search clarification, conversational
retrieval, and response generation work in unison to enable these sophisticated
interactions. In this survey, we explore the recent advancements and potential
future directions in conversational search, examining the critical modules that
constitute a conversational search system. We highlight the integration of LLMs
in enhancing these systems and discuss the challenges and opportunities that
lie ahead in this dynamic field. Additionally, we provide insights into
real-world applications and robust evaluations of current conversational search
systems, aiming to guide future research and development in conversational
search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 8 figures, continue to update</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Search Space in Gboard Decoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxiang Zhang, Yuanbo Zhang, Haicheng Sun, Yun Wang, Billy Dou, Gary Sivek, Shumin Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gboard Decoder produces suggestions by looking for paths that best match
input touch points on the context aware search space, which is backed by the
language Finite State Transducers (FST). The language FST is currently an
N-gram language model (LM). However, N-gram LMs, limited in context length, are
known to have sparsity problem under device model size constraint. In this
paper, we propose \textbf{Neural Search Space} which substitutes the N-gram LM
with a Neural Network LM (NN-LM) and dynamically constructs the search space
during decoding. Specifically, we integrate the long range context awareness of
NN-LM into the search space by converting its outputs given context, into the
language FST at runtime. This involves language FST structure redesign, pruning
strategy tuning, and data structure optimizations. Online experiments
demonstrate improved quality results, reducing Words Modified Ratio by [0.26\%,
1.19\%] on various locales with acceptable latency increases. This work opens
new avenues for further improving keyboard decoding quality by enhancing neural
LM more directly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMU: Your Swiss Army Knife for Music Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Zhao, Zhi Zhong, Zhuoyuan Mao, Shiqi Yang, Wei-Hsiang Liao, Shusuke Takahashi, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OpenMU-Bench, a large-scale benchmark suite for addressing the
data scarcity issue in training multimodal language models to understand music.
To construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new
annotations. OpenMU-Bench also broadens the scope of music understanding by
including lyrics understanding and music tool usage. Using OpenMU-Bench, we
trained our music understanding model, OpenMU, with extensive ablations,
demonstrating that OpenMU outperforms baseline models such as MU-Llama. Both
OpenMU and OpenMU-Bench are open-sourced to facilitate future research in music
understanding and to enhance creative music production efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Resources: https://github.com/mzhaojp22/openmu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka
  Chatbots: Design Insights and User Perceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Chi Chang, Han-Pi Chang, Hung-Shin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where cultural preservation is increasingly intertwined with
technological innovation, this study introduces a groundbreaking approach to
promoting and safeguarding the rich heritage of Taiwanese Hakka culture through
the development of a Retrieval-Augmented Generation (RAG)-enhanced chatbot.
Traditional large language models (LLMs), while powerful, often fall short in
delivering accurate and contextually rich responses, particularly in culturally
specific domains. By integrating external databases with generative AI models,
RAG technology bridges this gap, empowering chatbots to not only provide
precise answers but also resonate deeply with the cultural nuances that are
crucial for authentic interactions. This study delves into the intricate
process of augmenting the chatbot's knowledge base with targeted cultural data,
specifically curated to reflect the unique aspects of Hakka traditions,
language, and practices. Through dynamic information retrieval, the
RAG-enhanced chatbot becomes a versatile tool capable of handling complex
inquiries that demand an in-depth understanding of Hakka cultural context. This
is particularly significant in an age where digital platforms often dilute
cultural identities, making the role of culturally aware AI systems more
critical than ever. System usability studies conducted as part of our research
reveal a marked improvement in both user satisfaction and engagement,
highlighting the chatbot's effectiveness in fostering a deeper connection with
Hakka culture. The feedback underscores the potential of RAG technology to not
only enhance user experience but also to serve as a vital instrument in the
broader mission of ethnic mainstreaming and cultural celebration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE RASSE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stacking Small Language Models for Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurence Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances show that large language models (LLMs) generalize strong
performance across different natural language benchmarks. However, the large
size of LLMs makes training and inference expensive and impractical to run in
resource-limited settings. This paper introduces a new approach called
fine-tuning stacks of language models (FSLM), which involves stacking small
language models (SLM) as an alternative to LLMs. By fine-tuning each SLM to
perform a specific task, this approach breaks down high level reasoning into
multiple lower-level steps that specific SLMs are responsible for. As a result,
FSLM allows for lower training and inference costs, and also improves model
interpretability as each SLM communicates with the subsequent one through
natural language. By evaluating FSLM on common natural language benchmarks,
this paper highlights promising early results toward generalizable performance
using FSLM as a cost-effective alternative to LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pruning Foundation Models for High Accuracy without Retraining <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, Xue Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the superior performance, it is challenging to deploy foundation
models or large language models (LLMs) due to their massive parameters and
computations. While pruning is a promising technique to reduce model size and
accelerate the inference, the traditional pruning techniques can hardly be
applied for LLMs as they need to finetune the model on the full dataset with
multiple epochs consuming massive data and hardware resources. To deal with
this problem, post-training pruning methods are proposed to prune LLMs in
one-shot without retraining. However, their accuracy after pruning may suffer
from certain performance degradation due to the lack of retraining with massive
data. To address this issue, in this paper, we first formulate the
post-training problem for layer-wise LLM compression to simultaneously prune
multiple weights in LLMs. Next, we provide an optimal solution for this problem
and design our post-training pruning algorithm for both unstructured and
semi-structured sparsity. Our extensive experiments demonstrate the superior
performance of the proposed methods in comparison to SOTA baselines across
various LLM families including transformer-based LLMs and Mamba-based LLMs.
Code link: https://github.com/piuzha/APT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Raising the Stakes: Performance Pressure Improves AI-Assisted Decision
  Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Haduong, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI systems are used in many domains to assist with decision making, and
although the potential for AI systems to assist with decision making is much
discussed, human-AI collaboration often underperforms. Investigation into why
the performance potential is not realized has revealed many factors, including
(mis)trust in the AI system and mental models of AI capabilities on subjective
tasks. Performance pressure is known to influence human decision making
behavior, yet how it interacts with human-AI decision making is understudied.
In this work, we show the effects of performance pressure on AI advice reliance
when laypeople (Amazon Mechanical Turk crowdworkers) complete a common
AI-assisted task (fake review detection) and thus have inherently low
performance pressure. We manipulate performance pressure by leveraging people's
loss aversion towards potential monetary gains when completing a task. We find
that when the stakes are high, people use AI advice more appropriately than
when stakes are lower, regardless of the presence of an AI explanation.
Furthermore, when the AI system gives incorrect advice, people correctly
discount the poor advice more often when the stakes are higher than when they
are lower. We conclude by discussing the implications of how performance
pressure influences AI-assisted decision making and encourage future research
to incorporate performance pressure analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and
  Error-Aware Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqian Cui, Pengfei He, Xianfeng Tang, Qi He, Chen Luo, Jiliang Tang, Yue Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance
in improving the reasoning capabilities of large language models (LLMs). While
theoretical investigations have been conducted to understand CoT, the
underlying transformer used in these studies isolates the CoT reasoning process
into separated in-context learning steps (Stepwise ICL). In this work, we
theoretically show that, compared to Stepwise ICL, the transformer gains better
error correction ability and more accurate predictions if the reasoning from
earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning
changes the behavior of the transformer, we further investigate the sensitivity
of the transformer with Coherent CoT when the demonstration examples are
corrupted at the inference stage. Our theoretical results indicate that the
transformer is more sensitive to errors in intermediate reasoning steps than
the final outcome. Building upon this observation, we propose an improvement on
CoT by incorporating both correct and incorrect reasoning paths in the
demonstration. Our experiments validate the effectiveness of the proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Body Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saif Punjwani, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As virtual agents become increasingly prevalent in human-computer
interaction, generating realistic and contextually appropriate gestures in
real-time remains a significant challenge. While neural rendering techniques
have made substantial progress with static scripts, their applicability to
human-computer interactions remains limited. To address this, we introduce
Large Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM
architecture that combines a Transformer-XL large language model with a
parallelized diffusion model to generate human-like gestures from multimodal
inputs (text, audio, and video). LBLM-AVA incorporates several key components
enhancing its gesture generation capabilities, such as multimodal-to-pose
embeddings, enhanced sequence-to-sequence mapping with redefined attention
mechanisms, a temporal smoothing module for gesture sequence coherence, and an
attention-based refinement module for enhanced realism. The model is trained on
our large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves
state-of-the-art performance in generating lifelike and contextually
appropriate gestures with a 30% reduction in Fr\'echet Gesture Distance (FGD),
and a 25% improvement in Fr\'echet Inception Distance compared to existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian scaling laws for in-<span class="highlight-title">context</span> learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a powerful technique for getting language models
to perform complex tasks with no training updates. Prior work has established
strong correlations between the number of in-context examples provided and the
accuracy of the model's predictions. In this paper, we seek to explain this
correlation by showing that ICL approximates a Bayesian learner. This
perspective gives rise to a family of novel Bayesian scaling laws for ICL. In
experiments with \mbox{GPT-2} models of different sizes, our scaling laws
exceed or match existing scaling laws in accuracy while also offering
interpretable terms for task priors, learning efficiency, and per-example
probabilities. To illustrate the analytic power that such interpretable scaling
laws provide, we report on controlled synthetic dataset experiments designed to
inform real-world studies of safety alignment. In our experimental protocol, we
use SFT to suppress an unwanted existing model capability and then use ICL to
try to bring that capability back (many-shot jailbreaking). We then experiment
on real-world instruction-tuned LLMs using capabilities benchmarks as well as a
new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws
accurately predict the conditions under which ICL will cause the suppressed
behavior to reemerge, which sheds light on the ineffectiveness of post-training
at increasing LLM safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages main text, 26 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUTALIC: A <span class="highlight-title">Dataset</span> for Anti-AUTistic Ableist Language In <span class="highlight-title">Context</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naba Rizvi, Harper Strickland, Daniel Gitelman, Tristan Cooper, Alexis Morales-Flores, Michael Golden, Aekta Kallepalli, Akshat Alurkar, Haaset Owens, Saleha Ahmedi, Isha Khirwadkar, Imani Munyaka, Nedjma Ousidhoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As our understanding of autism and ableism continues to increase, so does our
understanding of ableist language towards autistic people. Such language poses
a significant challenge in NLP research due to its subtle and context-dependent
nature. Yet, detecting anti-autistic ableist language remains underexplored,
with existing NLP tools often failing to capture its nuanced expressions. We
present AUTALIC, the first benchmark dataset dedicated to the detection of
anti-autistic ableist language in context, addressing a significant gap in the
field. The dataset comprises 2,400 autism-related sentences collected from
Reddit, accompanied by surrounding context, and is annotated by trained experts
with backgrounds in neurodiversity. Our comprehensive evaluation reveals that
current language models, including state-of-the-art LLMs, struggle to reliably
identify anti-autistic ableism and align with human judgments, underscoring
their limitations in this domain. We publicly release AUTALIC along with the
individual annotations which serve as a valuable resource to researchers
working on ableism, neurodiversity, and also studying disagreements in
annotation tasks. This dataset serves as a crucial step towards developing more
inclusive and context-aware NLP systems that better reflect diverse
perspectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from others' mistakes: Finetuning machine translation models
  with span-level error annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lily H. Zhang, Hamid Dadkhahi, Mara Finkelstein, Firas Trabelsi, Jiaming Luo, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite growing interest in incorporating feedback to improve language
models, most efforts focus only on sequence-level annotations. In this work, we
explore the potential of utilizing fine-grained span-level annotations from
offline datasets to improve model quality. We develop a simple finetuning
algorithm, called Training with Annotations (TWA), to directly train machine
translation models on such annotated data. TWA utilizes targeted span-level
error information while also flexibly learning what to penalize within a span.
Moreover, TWA considers the overall trajectory of a sequence when deciding
which non-error spans to utilize as positive signals. Experiments on
English-German and Chinese-English machine translation show that TWA
outperforms baselines such as Supervised FineTuning on sequences filtered for
quality and Direct Preference Optimization on pairs constructed from the same
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Allo-AVA: A Large-Scale Multimodal Conversational AI <span class="highlight-title">Dataset</span> for
  Allocentric Avatar Gesture Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saif Punjwani, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of high-quality, multimodal training data severely hinders the
creation of lifelike avatar animations for conversational AI in virtual
environments. Existing datasets often lack the intricate synchronization
between speech, facial expressions, and body movements that characterize
natural human communication. To address this critical gap, we introduce
Allo-AVA, a large-scale dataset specifically designed for text and audio-driven
avatar gesture animation in an allocentric (third person point-of-view)
context. Allo-AVA consists of $\sim$1,250 hours of diverse video content,
complete with audio, transcripts, and extracted keypoints. Allo-AVA uniquely
maps these keypoints to precise timestamps, enabling accurate replication of
human movements (body and facial gestures) in synchronization with speech. This
comprehensive resource enables the development and evaluation of more natural,
context-aware avatar animation models, potentially transforming applications
ranging from virtual reality to digital assistants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rulebreakers Challenge: Revealing a Blind Spot in Large Language Models'
  Reasoning with Formal Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Chan, Robert Gaizauskas, Zhixue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formal logic has long been applied to natural language reasoning, but this
approach can sometimes lead to conclusions that, while logically entailed, are
factually inconsistent with the premises or are not typically inferred by
humans. This study introduces the concept of "rulebreakers", which refers to
instances where logical entailment diverges from factually acceptable
inference. We present RULEBREAKERS, a novel dataset for evaluating Large
Language Models' (LLMs) ability to distinguish between rulebreakers and
non-rulebreakers. Focusing on modus tollens and disjunctive syllogism, we
assess six state-of-the-art LLMs using RULEBREAKERS, measuring their
performance in terms of token-level exact accuracy and model confidence. Our
findings reveal that while most models perform poorly to moderately in
recognizing rulebreakers, they demonstrate a latent ability to distinguish
rulebreakers when assessed by their confidence levels. Further analysis
suggests that the failure to recognize rulebreakers is potentially associated
with the models' world knowledge and their attention distribution patterns.
This research highlights the limitation of LLMs' reasoning capabilities, and
contributes to the ongoing discussion on reasoning in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Processing for Human Resources: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Otani, Nikita Bhutani, Estevam Hruschka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The domain of human resources (HR) includes a broad spectrum of tasks related
to natural language processing (NLP) techniques. Recent breakthroughs in NLP
have generated significant interest in its industrial applications in this
domain and potentially alleviate challenges such as the difficulty of resource
acquisition and the complexity of problems. At the same time, the HR domain can
also present unique challenges that drive state-of-the-art in NLP research. To
support this, we provide NLP researchers and practitioners with an overview of
key HR tasks from an NLP perspective, illustrating how specific sub-tasks
(e.g., skill extraction) contribute to broader objectives (e.g., job matching).
Through this survey, we identify opportunities in NLP for HR and suggest
directions for future exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenge of embedding realistic human
personality traits into LLMs. Previous approaches have primarily focused on
prompt-based methods that describe the behavior associated with the desired
personality traits, suffering from realism and validity issues. To address
these limitations, we introduce BIG5-CHAT, a large-scale dataset containing
100,000 dialogues designed to ground models in how humans express their
personality in text. Leveraging this dataset, we explore Supervised Fine-Tuning
and Direct Preference Optimization as training-based methods to align LLMs more
naturally with human personality patterns. Our methods outperform prompting on
personality assessments such as BFI and IPIP-NEO, with trait correlations more
closely matching human data. Furthermore, our experiments reveal that models
trained to exhibit higher conscientiousness, higher agreeableness, lower
extraversion, and lower neuroticism display better performance on reasoning
tasks, aligning with psychological findings on how these traits impact human
cognitive performance. To our knowledge, this work is the first comprehensive
study to demonstrate how training-based methods can shape LLM personalities
through learning from real human behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-head Sequence Tagging Model for Grammatical Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamal Al-Sabahi, Kang Yang, Wangwang Liu, Guanyu Jiang, Xian Li, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the Grammatical Error Correction (GEC) problem , a mapping between a
source sequence and a target one is needed, where the two differ only on few
spans. For this reason, the attention has been shifted to the
non-autoregressive or sequence tagging models. In which, the GEC has been
simplified from Seq2Seq to labeling the input tokens with edit commands chosen
from a large edit space. Due to this large number of classes and the limitation
of the available datasets, the current sequence tagging approaches still have
some issues handling a broad range of grammatical errors just by being
laser-focused on one single task. To this end, we simplified the GEC further by
dividing it into seven related subtasks: Insertion, Deletion, Merge,
Substitution, Transformation, Detection, and Correction, with Correction being
our primary focus. A distinct classification head is dedicated to each of these
subtasks. the novel multi-head and multi-task learning model is proposed to
effectively utilize training data and harness the information from related task
training signals. To mitigate the limited number of available training samples,
a new denoising autoencoder is used to generate a new synthetic dataset to be
used for pretraining. Additionally, a new character-level transformation is
proposed to enhance the sequence-to-edit function and improve the model's
vocabulary coverage. Our single/ensemble model achieves an F0.5 of 74.4/77.0,
and 68.6/69.1 on BEA-19 (test) and CoNLL-14 (test) respectively. Moreover,
evaluated on JFLEG test set, the GLEU scores are 61.6 and 61.7 for the single
and ensemble models, respectively. It mostly outperforms recently published
state-of-the-art results by a considerable margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manan Suri, Puneet Mathur, Franck Dernoncourt, Rajiv Jain, Vlad I Morariu, Ramit Sawhney, Preslav Nakov, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document structure editing involves manipulating localized textual, visual,
and layout components in document images based on the user's requests. Past
works have shown that multimodal grounding of user requests in the document
image and identifying the accurate structural components and their associated
attributes remain key challenges for this task. To address these, we introduce
the DocEdit-v2, a novel framework that performs end-to-end document editing by
leveraging Large Multimodal Models (LMMs). It consists of three novel
components: (1) Doc2Command, which simultaneously localizes edit regions of
interest (RoI) and disambiguates user edit requests into edit commands; (2)
LLM-based Command Reformulation prompting to tailor edit commands originally
intended for specialized software into edit instructions suitable for
generalist LMMs. (3) Moreover, DocEdit-v2 processes these outputs via Large
Multimodal Models like GPT-4V and Gemini, to parse the document layout, execute
edits on grounded Region of Interest (RoI), and generate the edited document
image. Extensive experiments on the DocEdit dataset show that DocEdit-v2
significantly outperforms strong baselines on edit command generation (2-33%),
RoI bounding box detection (12-31%), and overall document editing (1-12\%)
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Browsing: API-Based Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqi Song, Frank Xu, Shuyan Zhou, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web browsers are a portal to the internet, where much of human activity is
undertaken. Thus, there has been significant research work in AI agents that
interact with the internet through web browsing. However, there is also another
interface designed specifically for machine interaction with online content:
application programming interfaces (APIs). In this paper we ask -- what if we
were to take tasks traditionally tackled by browsing agents, and give AI agents
access to APIs? To do so, we propose two varieties of agents: (1) an
API-calling agent that attempts to perform online tasks through APIs only,
similar to traditional coding agents, and (2) a Hybrid Agent that can interact
with online data through both web browsing and APIs. In experiments on
WebArena, a widely-used and realistic benchmark for web navigation tasks, we
find that API-based agents outperform web browsing agents. Hybrid Agents
out-perform both others nearly uniformly across tasks, resulting in a more than
20.0% absolute improvement over web browsing alone, achieving a success rate of
35.8%, achiving the SOTA performance among task-agnostic agents. These results
strongly suggest that when APIs are available, they present an attractive
alternative to relying on web browsing alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Study of Multilingual Idioms and Similes in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paria Khoshtab, Danial Namazifard, Mostafa Masoudi, Ali Akhgary, Samin Mahdizadeh Sani, Yadollah Yaghoobzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the gap in the literature concerning the comparative
performance of LLMs in interpreting different types of figurative language
across multiple languages. By evaluating LLMs using two multilingual datasets
on simile and idiom interpretation, we explore the effectiveness of various
prompt engineering strategies, including chain-of-thought, few-shot, and
English translation prompts. We extend the language of these datasets to
Persian as well by building two new evaluation sets. Our comprehensive
assessment involves both closed-source (GPT-3.5, GPT-4o mini, Gemini 1.5), and
open-source models (Llama 3.1, Qwen2), highlighting significant differences in
performance across languages and figurative types. Our findings reveal that
while prompt engineering methods are generally effective, their success varies
by figurative type, language, and model. We also observe that open-source
models struggle particularly with low-resource languages in similes.
Additionally, idiom interpretation is nearing saturation for many languages,
necessitating more challenging evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To the Globe (TTG): Towards Language-Driven Guaranteed Travel Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da JU, Song Jiang, Andrew Cohen, Aaron Foss, Sasha Mitts, Arman Zharmagambetov, Brandon Amos, Xian Li, Justine T Kao, Maryam Fazel-Zarandi, Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Travel planning is a challenging and time-consuming task that aims to find an
itinerary which satisfies multiple, interdependent constraints regarding
flights, accommodations, attractions, and other travel arrangements. In this
paper, we propose To the Globe (TTG), a real-time demo system that takes
natural language requests from users, translates it to symbolic form via a
fine-tuned Large Language Model, and produces optimal travel itineraries with
Mixed Integer Linear Programming solvers. The overall system takes ~5 seconds
to reply to the user request with guaranteed itineraries. To train TTG, we
develop a synthetic data pipeline that generates user requests, flight and
hotel information in symbolic form without human annotations, based on the
statistics of real-world datasets, and fine-tune an LLM to translate NL user
requests to their symbolic form, which is sent to the symbolic solver to
compute optimal itineraries. Our NL-symbolic translation achieves ~91% exact
match in a backtranslation metric (i.e., whether the estimated symbolic form of
generated natural language matches the groundtruth), and its returned
itineraries have a ratio of 0.979 compared to the optimal cost of the ground
truth user request. When evaluated by users, TTG achieves consistently high Net
Promoter Scores (NPS) of 35-40% on generated itinerary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does your LLM truly unlearn? An embarrassingly simple approach to
  recover unlearned knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable proficiency in generating
text, benefiting from extensive training on vast textual corpora. However, LLMs
may also acquire unwanted behaviors from the diverse and sensitive nature of
their training data, which can include copyrighted and private content. Machine
unlearning has been introduced as a viable solution to remove the influence of
such problematic content without the need for costly and time-consuming
retraining. This process aims to erase specific knowledge from LLMs while
preserving as much model utility as possible. Despite the effectiveness of
current unlearning methods, little attention has been given to whether existing
unlearning methods for LLMs truly achieve forgetting or merely hide the
knowledge, which current unlearning benchmarks fail to detect. This paper
reveals that applying quantization to models that have undergone unlearning can
restore the "forgotten" information. To thoroughly evaluate this phenomenon, we
conduct comprehensive experiments using various quantization techniques across
multiple precision levels. We find that for unlearning methods with utility
constraints, the unlearned model retains an average of 21\% of the intended
forgotten knowledge in full precision, which significantly increases to 83\%
after 4-bit quantization. Based on our empirical findings, we provide a
theoretical explanation for the observed phenomenon and propose a
quantization-robust unlearning strategy to mitigate this intricate issue...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between
  Ghana and the U.S <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christabel Acquaye, Haozhe An, Rachel Rudinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has highlighted the culturally-contingent nature of commonsense
knowledge. We introduce AMAMMER${\epsilon}$, a test set of 525 multiple-choice
questions designed to evaluate the commonsense knowledge of English LLMs,
relative to the cultural contexts of Ghana and the United States. To create
AMAMMER${\epsilon}$, we select a set of multiple-choice questions (MCQs) from
existing commonsense datasets and rewrite them in a multi-stage process
involving surveys of Ghanaian and U.S. participants. In three rounds of
surveys, participants from both pools are solicited to (1) write correct and
incorrect answer choices, (2) rate individual answer choices on a 5-point
Likert scale, and (3) select the best answer choice from the newly-constructed
MCQ items, in a final validation step. By engaging participants at multiple
stages, our procedure ensures that participant perspectives are incorporated
both in the creation and validation of test items, resulting in high levels of
agreement within each pool. We evaluate several off-the-shelf English LLMs on
AMAMMER${\epsilon}$. Uniformly, models prefer answers choices that align with
the preferences of U.S. annotators over Ghanaian annotators. Additionally, when
test items specify a cultural context (Ghana or the U.S.), models exhibit some
ability to adapt, but performance is consistently better in U.S. contexts than
Ghanaian. As large resources are devoted to the advancement of English LLMs,
our findings underscore the need for culturally adaptable models and
evaluations to meet the needs of diverse English-speaking populations around
the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Neuron-level Interpretability with White-box Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Bai, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurons in auto-regressive language models like GPT-2 can be interpreted by
analyzing their activation patterns. Recent studies have shown that techniques
such as dictionary learning, a form of post-hoc sparse coding, enhance this
neuron-level interpretability. In our research, we are driven by the goal to
fundamentally improve neural network interpretability by embedding sparse
coding directly within the model architecture, rather than applying it as an
afterthought. In our study, we introduce a white-box transformer-like
architecture named Coding RAte TransformEr (CRATE), explicitly engineered to
capture sparse, low-dimensional structures within data distributions. Our
comprehensive experiments showcase significant improvements (up to 103%
relative improvement) in neuron-level interpretability across a variety of
evaluation metrics. Detailed investigations confirm that this enhanced
interpretability is steady across different layers irrespective of the model
size, underlining CRATE's robust performance in enhancing neural network
interpretability. Further analysis shows that CRATE's increased
interpretability comes from its enhanced ability to consistently and
distinctively activate on relevant tokens. These findings point towards a
promising direction for creating white-box foundation models that excel in
neuron-level interpretation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multimodal Affective Analysis with Learned Live Comment
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyuan Deng, Amith Ananthram, Kathleen McKeown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live comments, also known as Danmaku, are user-generated messages that are
synchronized with video content. These comments overlay directly onto streaming
videos, capturing viewer emotions and reactions in real-time. While prior work
has leveraged live comments in affective analysis, its use has been limited due
to the relative rarity of live comments across different video platforms. To
address this, we first construct the Live Comment for Affective Analysis
(LCAffect) dataset which contains live comments for English and Chinese videos
spanning diverse genres that elicit a wide spectrum of emotions. Then, using
this dataset, we use contrastive learning to train a video encoder to produce
synthetic live comment features for enhanced multimodal affective content
analysis. Through comprehensive experimentation on a wide range of affective
analysis tasks (sentiment, emotion recognition, and sarcasm detection) in both
English and Chinese, we demonstrate that these synthetic live comment features
significantly improve performance over state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VipAct: Visual-Perception Enhancement via Specialized VLM Agent
  Collaboration and Tool-use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehao Zhang, Ryan Rossi, Tong Yu, Franck Dernoncourt, Ruiyi Zhang, Jiuxiang Gu, Sungchul Kim, Xiang Chen, Zichao Wang, Nedim Lipka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While vision-language models (VLMs) have demonstrated remarkable performance
across various tasks combining textual and visual information, they continue to
struggle with fine-grained visual perception tasks that require detailed
pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs
on such intricate visual elements remains an open challenge. In this paper, we
present VipAct, an agent framework that enhances VLMs by integrating
multi-agent collaboration and vision expert models, enabling more precise
visual understanding and comprehensive reasoning. VipAct consists of an
orchestrator agent, which manages task requirement analysis, planning, and
coordination, along with specialized agents that handle specific tasks such as
image captioning and vision expert models that provide high-precision
perceptual information. This multi-agent approach allows VLMs to better perform
fine-grained visual perception tasks by synergizing planning, reasoning, and
tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual
perception tasks, with experimental results demonstrating significant
performance improvements over state-of-the-art baselines across all tasks.
Furthermore, comprehensive ablation studies reveal the critical role of
multi-agent collaboration in eliciting more detailed System-2 reasoning and
highlight the importance of image input for task planning. Additionally, our
error analysis identifies patterns of VLMs' inherent limitations in visual
perception, providing insights into potential future improvements. VipAct
offers a flexible and extensible framework, paving the way for more advanced
visual perception systems across various real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Optimization of Compound AI Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Yiran Wu, Huan Liu, Jun Liu, Gao Huang, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a compound AI system, components such as an LLM call, a retriever, a code
interpreter, or tools are interconnected. The system's behavior is primarily
driven by parameters such as instructions or tool definitions. Recent
advancements enable end-to-end optimization of these parameters using an LLM.
Notably, leveraging an LLM as an optimizer is particularly efficient because it
avoids gradient computation and can generate complex code and instructions.
This paper presents a survey of the principles and emerging trends in LLM-based
optimization of compound AI systems. It covers archetypes of compound AI
systems, approaches to LLM-based end-to-end optimization, and insights into
future directions and broader impacts. Importantly, this survey uses concepts
from program analysis to provide a unified view of how an LLM optimizer is
prompted to optimize a compound AI system. The exhaustive list of paper is
provided at
https://github.com/linyuhongg/LLM-based-Optimization-of-Compound-AI-Systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Model Alignment in Multilingual Trolley Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02273v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02273v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Max Kleiman-Weiner, Giorgio Piatti, Sydney Levine, Jiarui Liu, Fernando Gonzalez, Francesco Ortu, András Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate the moral alignment of large language models (LLMs) with human
preferences in multilingual trolley problems. Building on the Moral Machine
experiment, which captures over 40 million human judgments across 200+
countries, we develop a cross-lingual corpus of moral dilemma vignettes in over
100 languages called MultiTP. This dataset enables the assessment of LLMs'
decision-making processes in diverse linguistic contexts. Our analysis explores
the alignment of 19 different LLMs with human judgments, capturing preferences
across six moral dimensions: species, gender, fitness, status, age, and the
number of lives involved. By correlating these preferences with the demographic
distribution of language speakers and examining the consistency of LLM
responses to various prompt paraphrasings, our findings provide insights into
cross-lingual and ethical biases of LLMs and their intersection. We discover
significant variance in alignment across languages, challenging the assumption
of uniform moral reasoning in AI systems and highlighting the importance of
incorporating diverse perspectives in AI ethics. The results underscore the
need for further research on the integration of multilingual dimensions in
responsible AI research to ensure fair and equitable AI interactions worldwide.
Our code and data are at https://github.com/causalNLP/moralmachine
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Chameleons? An Attempt to Simulate Social
  <span class="highlight-title">Survey</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Sihong He, Roberto Trotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can large language models (LLMs) simulate social surveys? To answer this
question, we conducted millions of simulations in which LLMs were asked to
answer subjective questions. A comparison of different LLM responses with the
European Social Survey (ESS) data suggests that the effect of prompts on bias
and variability is fundamental, highlighting major cultural, age, and gender
biases. We further discussed statistical methods for measuring the difference
between LLM answers and survey data and proposed a novel measure inspired by
Jaccard similarity, as LLM-generated responses are likely to have a smaller
variance. Our experiments also reveal that it is important to analyze the
robustness and variability of prompts before using LLMs to simulate social
surveys, as their imitation abilities are approximate at best.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards
  for Better Well-Being 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amelie Gyrard, Seyedali Mohammadi, Manas Gaur, Antonio Kung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sustainable Development Goals (SDGs) give the UN a road map for development
with Agenda 2030 as a target. SDG3 "Good Health and Well-Being" ensures healthy
lives and promotes well-being for all ages. Digital technologies can support
SDG3. Burnout and even depression could be reduced by encouraging better
preventive health. Due to the lack of patient knowledge and focus to take care
of their health, it is necessary to help patients before it is too late. New
trends such as positive psychology and mindfulness are highly encouraged in the
USA. Digital Twins (DTs) can help with the continuous monitoring of emotion
using physiological signals (e.g., collected via wearables). DTs facilitate
monitoring and provide constant health insight to improve quality of life and
well-being with better personalization. Healthcare DTs challenges are
standardizing data formats, communication protocols, and data exchange
mechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things
(IoT) and DTs Working Group, with standards such as "ISO/IEC 21823-3:2021 IoT -
Interoperability for IoT Systems - Part 3 Semantic interoperability", "ISO/IEC
CD 30178 - IoT - Data format, value and coding". To achieve those data
integration and knowledge challenges, we designed the Mental Health Knowledge
Graph (ontology and dataset) to boost mental health. As an example, explicit
knowledge is described such as chocolate contains magnesium which is
recommended for depression. The Knowledge Graph (KG) acquires knowledge from
ontology-based mental health projects classified within the LOV4IoT ontology
catalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped
to standards when possible. Standards from ETSI SmartM2M can be used such as
SAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,
W3C, NIST, and IEEE standards relevant to mental health can be considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, Book chapter, Smart Technologies for Achieving Good Health
  and Well-Being: Towards Sustainable Development Goal, Taylor & Francis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting
  Volunteer Content Moderators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07879v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07879v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Trista Cao, Lovely-Frances Domingo, Sarah Ann Gilbert, Michelle Mazurek, Katie Shilton, Hal Daumé III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive efforts in automated approaches for content moderation have been
focused on developing models to identify toxic, offensive, and hateful content
with the aim of lightening the load for moderators. Yet, it remains uncertain
whether improvements on those tasks have truly addressed moderators' needs in
accomplishing their work. In this paper, we surface gaps between past research
efforts that have aimed to provide automation for aspects of content moderation
and the needs of volunteer content moderators, regarding identifying violations
of various moderation rules. To do so, we conduct a model review on Hugging
Face to reveal the availability of models to cover various moderation rules and
guidelines from three exemplar forums. We further put state-of-the-art LLMs to
the test, evaluating how well these models perform in flagging violations of
platform rules from one particular forum. Finally, we conduct a user survey
study with volunteer moderators to gain insight into their perspectives on
useful moderation models. Overall, we observe a non-trivial gap, as missing
developed models and LLMs exhibit moderate to low performance on a significant
portion of the rules. Moderators' reports provide guides for future work on
developing moderation assistant models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The First VoicePrivacy Attacker Challenge Evaluation Plan 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalia Tomashenko, Xiaoxiao Miao, Emmanuel Vincent, Junichi Yamagishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The First VoicePrivacy Attacker Challenge is a new kind of challenge
organized as part of the VoicePrivacy initiative and supported by ICASSP 2025
as the SP Grand Challenge It focuses on developing attacker systems against
voice anonymization, which will be evaluated against a set of anonymization
systems submitted to the VoicePrivacy 2024 Challenge. Training, development,
and evaluation datasets are provided along with a baseline attacker system.
Participants shall develop their attacker systems in the form of automatic
speaker verification systems and submit their scores on the development and
evaluation data to the organizers. To do so, they can use any additional
training data and models, provided that they are openly available and declared
before the specified deadline. The metric for evaluation is equal error rate
(EER). Results will be presented at the ICASSP 2025 special session to which 5
selected top-ranked participants will be invited to submit and present their
challenge systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Knowledge Distillation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13116v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13116v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Large Language Models (LLMs), Knowledge Distillation (KD)
emerges as a pivotal methodology for transferring advanced capabilities from
leading proprietary LLMs, such as GPT-4, to their open-source counterparts like
LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a
crucial role in both compressing these models, and facilitating their
self-improvement by employing themselves as teachers. This paper presents a
comprehensive survey of KD's role within the realm of LLM, highlighting its
critical function in imparting advanced knowledge to smaller models and its
utility in model compression and self-improvement. Our survey is meticulously
structured around three foundational pillars: \textit{algorithm},
\textit{skill}, and \textit{verticalization} -- providing a comprehensive
examination of KD mechanisms, the enhancement of specific cognitive abilities,
and their practical implications across diverse fields. Crucially, the survey
navigates the intricate interplay between data augmentation (DA) and KD,
illustrating how DA emerges as a powerful paradigm within the KD framework to
bolster LLMs' performance. By leveraging DA to generate context-rich,
skill-specific training data, KD transcends traditional boundaries, enabling
open-source models to approximate the contextual adeptness, ethical alignment,
and deep semantic insights characteristic of their proprietary counterparts.
This work aims to provide an insightful guide for researchers and
practitioners, offering a detailed overview of current methodologies in KD and
proposing future research directions. Importantly, we firmly advocate for
compliance with the legal terms that regulate the use of LLMs, ensuring ethical
and lawful application of KD of LLMs. An associated Github repository is
available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RACCooN: A Versatile Instructional Video Editing Framework with
  Auto-Generated Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Yoon, Shoubin Yu, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent video generative models primarily rely on carefully written text
prompts for specific tasks, like inpainting or style editing. They require
labor-intensive textual descriptions for input videos, hindering their
flexibility to adapt personal/raw videos to user specifications. This paper
proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video
generative framework that supports multiple video editing capabilities such as
removal, addition, and modification, through a unified pipeline. RACCooN
consists of two principal stages: Video-to-Paragraph (V2P) and
Paragraph-to-Video (P2V). In the V2P stage, we automatically describe video
scenes in well-structured natural language, capturing both the holistic context
and focused object details. Subsequently, in the P2V stage, users can
optionally refine these descriptions to guide the video diffusion model,
enabling various modifications to the input video, such as removing, changing
subjects, and/or adding new objects. The proposed approach stands out from
other methods through several significant contributions: (1) RACCooN suggests a
multi-granular spatiotemporal pooling strategy to generate well-structured
video descriptions, capturing both the broad context and object details without
requiring complex human annotations, simplifying precise video content editing
based on text for users. (2) Our video generative model incorporates
auto-generated narratives or instructions to enhance the quality and accuracy
of the generated content. (3) RACCooN also plans to imagine new objects in a
given video, so users simply prompt the model to receive a detailed video
editing plan for complex video editing. The proposed framework demonstrates
impressive versatile capabilities in video-to-paragraph generation, video
content editing, and can be incorporated into other SoTA video generative
models for further enhancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally. Project Page:
  https://raccoon-mllm-gen.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Potential of Large Language Models for Heterophilic Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have presented significant opportunities to
enhance various machine learning applications, including graph neural networks
(GNNs). By leveraging the vast open-world knowledge within LLMs, we can more
effectively interpret and utilize textual data to better characterize
heterophilic graphs, where neighboring nodes often have different labels.
However, existing approaches for heterophilic graphs overlook the rich textual
data associated with nodes, which could unlock deeper insights into their
heterophilic contexts. In this work, we explore the potential of LLMs for
modeling heterophilic graphs and propose a novel two-stage framework:
LLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first
stage, we fine-tune the LLM to better identify homophilic and heterophilic
edges based on the textual content of their nodes. In the second stage, we
adaptively manage message propagation in GNNs for different edge types based on
node features, structures, and heterophilic or homophilic characteristics. To
cope with the computational demands when deploying LLMs in practical scenarios,
we further explore model distillation techniques to fine-tune smaller, more
efficient models that maintain competitive performance. Extensive experiments
validate the effectiveness of our framework, demonstrating the feasibility of
using LLMs to enhance node classification on heterophilic graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Need a Content Delivery Network? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the use of large language models (LLMs) expands rapidly, so does the range
of knowledge needed to supplement various LLM queries. Thus, enabling flexible
and efficient injection of new knowledge in LLM inference is critical. Three
high-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,
fine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,
in-context learning), or (iii) injecting the KV caches of the new knowledge to
LLM during prefill. This paper argues that, although fine-tuning and in-context
learning are popular, using KV caches as the medium of knowledge could
simultaneously enable more modular management of knowledge injection and more
efficient LLM serving with low cost and fast response. To realize these
benefits, we envision a Knowledge Delivery Network (KDN), a new system
component in LLM services that dynamically optimizes the storage, transfer, and
composition of KV cache across LLM engines and other compute and storage
resources. We believe that, just like content delivery networks (CDNs), such as
Akamai, enabled the success of the Internet ecosystem through their efficient
data delivery, KDNs will be critical to the success of LLM applications through
their efficient knowledge delivery. We have open-sourced a KDN prototype at
https://github.com/LMCache/LMCache.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily
  Complex Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve arithmetic word problems with high
accuracy, but little is known about how well they generalize to problems that
are more complex than the ones on which they have been trained. Empirical
investigations of such questions are impeded by two major flaws of current
evaluations: (i) much of the evaluation data is contaminated, in the sense that
it has already been seen during training, and (ii) benchmark datasets do not
capture how problem proofs may be arbitrarily complex in various ways. As a
step towards addressing these issues, we present a framework for evaluating
LLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP.
MathGAP generates problems that follow fixed proof specifications -- along with
chain-of-thought reasoning annotations -- enabling systematic studies on
generalization with respect to arithmetic proof complexity. We apply MathGAP to
analyze how in-context learning interacts with generalization to problems that
have more complex proofs. We find that among the models tested, most show a
significant decrease in performance as proofs get deeper and wider. This effect
is more pronounced in complex, nonlinear proof structures, which are
challenging even for GPT-4o. Surprisingly, providing in-context examples from
the same distribution as the test set is not always beneficial for performance.
In particular, zero-shot prompting as well as demonstrating a diverse range of
examples that are less complex than the test data sometimes yield similar or
higher accuracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Exp: Multi-granularity <span class="highlight-title">Prompt</span> Explanation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Dong, Shaowei Wang, Dayi Lin, Gopi Krishnan Rajbahadur, Boquan Zhou, Shichao Liu, Ahmed E. Hassan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models excel in tasks like natural language understanding and
text generation. Prompt engineering plays a critical role in leveraging LLM
effectively. However, LLMs black-box nature hinders its interpretability and
effective prompting engineering. A wide range of model explanation approaches
have been developed for deep learning models, However, these local explanations
are designed for single-output tasks like classification and regression,and
cannot be directly applied to LLMs, which generate sequences of tokens. Recent
efforts in LLM explanation focus on natural language explanations, but they are
prone to hallucinations and inaccuracies. To address this, we introduce
OurTool, a framework for multi-granularity prompt explanations by aggregating
token-level insights. OurTool introduces two token-level explanation
approaches: 1.an aggregation-based approach combining local explanation
techniques, and 2. a perturbation-based approach with novel techniques to
evaluate token masking impact. OurTool supports both white-box and black-box
explanations and extends explanations to higher granularity levels, enabling
flexible analysis. We evaluate OurTool in case studies such as sentiment
analysis, showing the perturbation-based approach performs best using semantic
similarity to assess perturbation impact. Furthermore, we conducted a user
study to confirm OurTool's accuracy and practical value, and demonstrate its
potential to enhance LLM interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, Jonathan Berant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents, built on top of language models (LMs), are systems that can
interact with complex environments, such as the open web. In this work, we
examine whether such agents can perform realistic and time-consuming tasks on
the web, e.g., monitoring real-estate markets or locating relevant nearby
businesses. We introduce AssistantBench, a challenging new benchmark consisting
of 214 realistic tasks that can be automatically evaluated, covering different
scenarios and domains. We find that AssistantBench exposes the limitations of
current systems, including language models and retrieval-augmented language
models, as no model reaches an accuracy of more than 26 points. While
closed-book LMs perform well in terms of accuracy, they exhibit low precision
and tend to hallucinate facts. State-of-the-art web agents reach a score of
near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that
significantly outperforms previous agents, and an ensemble of SPA and
closed-book models reaches the best overall performance. Moreover, we analyze
failures of current systems and highlight that open web navigation remains a
major challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Role of <span class="highlight-title">Context</span> in Reading Time Prediction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08160v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08160v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Eleanor Chodroff, Ryan Cotterell, Ethan Gotlieb Wilcox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new perspective on how readers integrate context during
real-time language comprehension. Our proposals build on surprisal theory,
which posits that the processing effort of a linguistic unit (e.g., a word) is
an affine function of its in-context information content. We first observe that
surprisal is only one out of many potential ways that a contextual predictor
can be derived from a language model. Another one is the pointwise mutual
information (PMI) between a unit and its context, which turns out to yield the
same predictive power as surprisal when controlling for unigram frequency.
Moreover, both PMI and surprisal are correlated with frequency. This means that
neither PMI nor surprisal contains information about context alone. In response
to this, we propose a technique where we project surprisal onto the orthogonal
complement of frequency, yielding a new contextual predictor that is
uncorrelated with frequency. Our experiments show that the proportion of
variance in reading times explained by context is a lot smaller when context is
represented by the orthogonalized predictor. From an interpretability
standpoint, this indicates that previous studies may have overstated the role
that context has in predicting reading times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Translation-Specific Understanding to General Understanding in
  Large Language Models <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichong Huang, Baohang Li, Xiaocheng Feng, Chengpeng Fu, Wenshuai Huo, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language models (LLMs) have exhibited remarkable abilities in
understanding complex texts, offering a promising path towards human-like
translation performance. However, this study reveals the misalignment between
the translation-specific understanding and the general understanding inside
LLMs. This understanding misalignment leads to LLMs mistakenly or literally
translating some complicated concepts that they accurately comprehend in the
general scenarios (e.g., QA). To align the translation-specific understanding
to the general one, we propose a novel translation process, DUAT (Difficult
words Understanding Aligned Translation), explicitly incorporating the general
understanding on the complicated content incurring inconsistent understanding
to guide the translation. Specifically, DUAT performs cross-lingual
interpretation for the difficult-to-translate words and enhances the
translation with the generated interpretations. Furthermore, we reframe the
external tools to improve DUAT in detecting difficult words and generating
helpful interpretations. We conduct experiments on the self-constructed
benchmark Challenge-WMT, consisting of samples that are prone to
mistranslation. Human evaluation results on high-resource and low-resource
language pairs indicate that DUAT significantly facilitates the understanding
alignment, which improves the translation quality (up to +3.85 COMET) and
reduces the literality of the translation by -25% to -51%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Du-IN: Discrete units-guided mask modeling for decoding speech from
  Intracranial Neural signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Zheng, Hai-Teng Wang, Wei-Bang Jiang, Zhong-Tao Chen, Li He, Pei-Yang Lin, Peng-Hu Wei, Guo-Guang Zhao, Yun-Zhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invasive brain-computer interfaces with Electrocorticography (ECoG) have
shown promise for high-performance speech decoding in medical applications, but
less damaging methods like intracranial stereo-electroencephalography (sEEG)
remain underexplored. With rapid advances in representation learning,
leveraging abundant recordings to enhance speech decoding is increasingly
attractive. However, popular methods often pre-train temporal models based on
brain-level tokens, overlooking that brain activities in different regions are
highly desynchronized during tasks. Alternatively, they pre-train
spatial-temporal models based on channel-level tokens but fail to evaluate them
on challenging tasks like speech decoding, which requires intricate processing
in specific language-related areas. To address this issue, we collected a
well-annotated Chinese word-reading sEEG dataset targeting language-related
brain networks from 12 subjects. Using this benchmark, we developed the Du-IN
model, which extracts contextual embeddings based on region-level tokens
through discrete codex-guided mask modeling. Our model achieves
state-of-the-art performance on the 61-word classification task, surpassing all
baselines. Model comparisons and ablation studies reveal that our design
choices, including (i) temporal modeling based on region-level tokens by
utilizing 1D depthwise convolution to fuse channels in the lateral sensorimotor
cortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervision
through discrete codex-guided mask modeling, significantly contribute to this
performance. Overall, our approach -- inspired by neuroscience findings and
capitalizing on region-level representations from specific brain regions -- is
suitable for invasive brain modeling and represents a promising neuro-inspired
AI approach in brain-computer interfaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bi-consolidating Model for Joint Relational Triple Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03881v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03881v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocheng Luo, Yanping Chen, Ruixue Tang, Caiwei Yang, Ruizhang Huang, Yongbin Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods to extract relational triples directly make a prediction
based on a possible entity pair in a raw sentence without depending on entity
recognition. The task suffers from a serious semantic overlapping problem, in
which several relation triples may share one or two entities in a sentence. In
this paper, based on a two-dimensional sentence representation, a
bi-consolidating model is proposed to address this problem by simultaneously
reinforcing the local and global semantic features relevant to a relation
triple. This model consists of a local consolidation component and a global
consolidation component. The first component uses a pixel difference
convolution to enhance semantic information of a possible triple representation
from adjacent regions and mitigate noise in neighbouring neighbours. The second
component strengthens the triple representation based a channel attention and a
spatial attention, which has the advantage to learn remote semantic
dependencies in a sentence. They are helpful to improve the performance of both
entity identification and relation type classification in relation triple
extraction. After evaluated on several publish datasets, the bi-consolidating
model achieves competitive performance. Analytical experiments demonstrate the
effectiveness of our model for relational triple extraction and give motivation
for other natural language processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Skill Discovery for Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Xu, Haozhu Wang, Dmitriy Bespalov, Xuan Wang, Peter Stone, Yanjun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)
approach for large language models (LLMs), especially when tackling complex
reasoning tasks. Traditional ICL approaches construct prompts using examples
that contain questions similar to the input question. However, CoT prompting,
which includes crucial intermediate reasoning steps (rationales) within its
examples, necessitates selecting examples based on these rationales rather than
the questions themselves. Existing methods require human experts or pre-trained
LLMs to describe the skill, a high-level abstraction of rationales, to guide
the selection. These methods, however, are often costly and difficult to scale.
Instead, this paper introduces a new approach named Latent Reasoning Skills
(LaRS) that employs unsupervised learning to create a latent space
representation of rationales, with a latent variable called a reasoning skill.
Concurrently, LaRS learns a reasoning policy to determine the required
reasoning skill for a given question. Then the ICL examples are selected by
aligning the reasoning skills between past examples and the question. This
approach is theoretically grounded and compute-efficient, eliminating the need
for auxiliary LLM inference or manual prompt design. Empirical results
demonstrate that LaRS consistently outperforms SOTA skill-based selection
methods, processing example banks four times faster, reducing LLM inferences
during the selection stage by half, and showing greater robustness to
sub-optimal example banks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beware of Words: Evaluating the Lexical Diversity of Conversational LLMs
  using Chat<span class="highlight-title">GPT</span> as Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonzalo Martínez, José Alberto Hernández, Javier Conde, Pedro Reviriego, Elena Merino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of conversational Large Language Models (LLMs) in general,
and of ChatGPT in particular, is currently being evaluated on many different
tasks, from logical reasoning or maths to answering questions on a myriad of
topics. Instead, much less attention is being devoted to the study of the
linguistic features of the texts generated by these LLMs. This is surprising
since LLMs are models for language, and understanding how they use the language
is important. Indeed, conversational LLMs are poised to have a significant
impact on the evolution of languages as they may eventually dominate the
creation of new text. This means that for example, if conversational LLMs do
not use a word it may become less and less frequent and eventually stop being
used altogether. Therefore, evaluating the linguistic features of the text they
produce and how those depend on the model parameters is the first step toward
understanding the potential impact of conversational LLMs on the evolution of
languages. In this paper, we consider the evaluation of the lexical richness of
the text generated by LLMs and how it depends on the model parameters. A
methodology is presented and used to conduct a comprehensive evaluation of
lexical richness using ChatGPT as a case study. The results show how lexical
richness depends on the version of ChatGPT and some of its parameters, such as
the presence penalty, or on the role assigned to the model. The dataset and
tools used in our analysis are released under open licenses with the goal of
drawing the much-needed attention to the evaluation of the linguistic features
of LLM-generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Few-shot Learning for Multi-label Classification of Scientific
  Documents with Many Classes <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Schopf, Alexander Blatzheim, Nektarios Machner, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific document classification is a critical task and often involves many
classes. However, collecting human-labeled data for many classes is expensive
and usually leads to label-scarce scenarios. Moreover, recent work has shown
that sentence embedding model fine-tuning for few-shot classification is
efficient, robust, and effective. In this work, we propose FusionSent
(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free
approach for few-shot classification of scientific documents with many classes.
FusionSent uses available training examples and their respective label texts to
contrastively fine-tune two different sentence embedding models. Afterward, the
parameters of both fine-tuned models are fused to combine the complementary
knowledge from the separate fine-tuning steps into a single model. Finally, the
resulting sentence embedding model is frozen to embed the training instances,
which are then used as input features to train a classification head. Our
experiments show that FusionSent significantly outperforms strong baselines by
an average of $6.0$ $F_{1}$ points across multiple scientific document
classification datasets. In addition, we introduce a new dataset for
multi-label classification of scientific documents, which contains 203,961
scientific articles and 130 classes from the arXiv category taxonomy. Code and
data are available at https://github.com/sebischair/FusionSent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 7th International Conference on Natural Language and
  Speech Processing (ICNLSP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bypass Back-propagation: Optimization-based Structural Pruning for Large
  Language Models via Policy Gradient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Zujing Liu, Weizhong Zhang, Bo Du, Gui-Song Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to moderate-size neural network pruning, structural weight
pruning on the Large-Language Models (LLMs) imposes a novel challenge on the
efficiency of the pruning algorithms, due to the heavy computation/memory
demands of the LLMs. Recent efficient LLM pruning methods typically operate at
the post-training phase without the expensive weight finetuning, however, their
pruning criteria often rely on heuristically hand-crafted metrics, potentially
leading to suboptimal performance. We instead propose a novel
optimization-based structural pruning that learns the pruning masks in a
probabilistic space directly by optimizing the loss of the pruned model. To
preserve the efficiency, our method eliminates the back-propagation through the
LLM per se during the optimization, requiring only the forward pass of the LLM.
We achieve this by learning an underlying Bernoulli distribution to sample
binary pruning masks, where we decouple the Bernoulli parameters from the LLM
loss, thus facilitating an efficient optimization via a policy gradient
estimator without back-propagation. As a result, our method is able to 1)
operate at structural granularities of channels, heads, and layers, 2) support
global and heterogeneous pruning (i.e., our method automatically determines
different redundancy for different layers), and 3) optionally initialize with a
metric-based method (for our Bernoulli distributions). Extensive experiments on
LLaMA, LLaMA-2, LLaMA-3, Vicuna, and Mistral using the C4 and WikiText2
datasets demonstrate that our method operates for 2.7 hours with around 35GB
memory for the 13B models on a single A100 GPU, and our pruned models
outperform the state-of-the-arts w.r.t. both perplexity and the majority of
various zero-shot tasks. Codes will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initially submitted on June 15, 2024, this version mainly changed the
  title, and added several experiments: such as 1) experiments on LLaMA-3,
  Mistral, 2) additional baseline methods (i.e., Bosai -- Everybody Prune Now),
  and 3) post-pruning finetuned performance (i.e., first prune then finetune)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selection-p: <span class="highlight-title">Self-Supervised</span> Task-Agnostic <span class="highlight-title">Prompt</span> Compression for
  Faithfulness and Transferability <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsz Ting Chung, Leyang Cui, Lemao Liu, Xinting Huang, Shuming Shi, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in a
wide range of natural language processing tasks when leveraging in-context
learning. To mitigate the additional computational and financial costs
associated with in-context learning, several prompt compression methods have
been proposed to compress the in-context learning prompts. Despite their
success, these methods face challenges with transferability due to
model-specific compression, or rely on external training data, such as GPT-4.
In this paper, we investigate the ability of LLMs to develop a unified
compression method that discretizes uninformative tokens, utilizing a
self-supervised pre-training technique. By introducing a small number of
parameters during the continual pre-training, the proposed Selection-p produces
a probability for each input token, indicating whether to preserve or discard
it. Experiments show Selection-p achieves state-of-the-art performance across
numerous classification tasks, achieving compression rates of up to 10 times
while experiencing only a marginal 0.8% decrease in performance. Moreover, it
exhibits superior transferability to different models compared to prior work.
Additionally, we further analyze how Selection-p helps maintain performance on
in-context learning with long contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 10 tables, EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAEI-LLM: A Benchmark <span class="highlight-title">Dataset</span> for Understanding Large Language Model
  Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INC-Math: Integrating Natural Language and Code for Enhanced
  Mathematical Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyuan Xiong, Simeng Han, Ziyue Zhou, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are commonly used to generate solutions for
mathematical reasoning problems in the following formats: natural language,
code, or a combination of both. In this paper, we explore fundamental questions
related to solving mathematical reasoning problems using natural language and
code with state-of-the-art LLMs, including GPT-4o-mini and LLama-3.1-8b-Turbo.
Our findings show that LLMs are better at reasoning in natural language
compared to code. Additionally, although natural language and code serve as
complementary forms of reasoning, they can affect each other in a negative way
in certain scenarios. These insights motivate our development of a new
prompting method, INC-Math, which leverages an LLM to dynamically select the
most appropriate reasoning form, resulting in improved performance over
comparable baselines with GPT-4o-mini.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation
  Guidelines? <span class="chip">WASSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Majer, Jan Šnajder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing threat of disinformation calls for automating parts of the
fact-checking pipeline. Identifying text segments requiring fact-checking is
known as claim detection (CD) and claim check-worthiness detection (CW), the
latter incorporating complex domain-specific criteria of worthiness and often
framed as a ranking task. Zero- and few-shot LLM prompting is an attractive
option for both tasks, as it bypasses the need for labeled datasets and allows
verbalized claim and worthiness criteria to be directly used for prompting. We
evaluate the LLMs' predictive and calibration accuracy on five CD/CW datasets
from diverse domains, each utilizing a different worthiness criterion. We
investigate two key aspects: (1) how best to distill factuality and worthiness
criteria into a prompt and (2) what amount of context to provide for each
claim. To this end, we experiment with varying the level of prompt verbosity
and the amount of contextual information provided to the model. Our results
show that optimal prompt verbosity is domain-dependent, adding context does not
improve performance, and confidence scores can be directly used to produce
reliable check-worthiness rankings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WASSA at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mervat Abassy, Kareem Elozeiri, Alexander Aziz, Minh Ngoc Ta, Raj Vardhan Tomar, Bimarsha Adhikari, Saad El Dine Ahmed, Yuxia Wang, Osama Mohammed Afzal, Zhuohan Xie, Jonibek Mansurov, Ekaterina Artemova, Vladislav Mikhailov, Rui Xing, Jiahui Geng, Hasan Iqbal, Zain Muhammad Mujahid, Tarek Mahmoud, Akim Tsvigun, Alham Fikri Aji, Artem Shelmanov, Nizar Habash, Iryna Gurevych, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ease of access to large language models (LLMs) has enabled a widespread
of machine-generated texts, and now it is often hard to tell whether a piece of
text was human-written or machine-generated. This raises concerns about
potential misuse, particularly within educational and academic domains. Thus,
it is important to develop practical systems that can automate the process.
Here, we present one such system, LLM-DetectAIve, designed for fine-grained
detection. Unlike most previous work on machine-generated text detection, which
focused on binary classification, LLM-DetectAIve supports four categories: (i)
human-written, (ii) machine-generated, (iii) machine-written, then
machine-humanized, and (iv) human-written, then machine-polished. Category
(iii) aims to detect attempts to obfuscate the fact that a text was
machine-generated, while category (iv) looks for cases where the LLM was used
to polish a human-written text, which is typically acceptable in academic
writing, but not in education. Our experiments show that LLM-DetectAIve can
effectively identify the above four categories, which makes it a potentially
useful tool in education, academia, and other domains.
  LLM-DetectAIve is publicly accessible at
https://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system
is available at https://youtu.be/E8eT_bE7k8c.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Log Probabilities Are a Reliable Estimate of Semantic Plausibility in
  Base and Instruction-Tuned Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carina Kauf, Emmanuele Chersoni, Alessandro Lenci, Evelina Fedorenko, Anna A. Ivanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic plausibility (e.g. knowing that "the actor won the award" is more
likely than "the actor won the battle") serves as an effective proxy for
general world knowledge. Language models (LMs) capture vast amounts of world
knowledge by learning distributional patterns in text, accessible via log
probabilities (LogProbs) they assign to plausible vs. implausible outputs. The
new generation of instruction-tuned LMs can now also provide explicit estimates
of plausibility via prompting. Here, we evaluate the effectiveness of LogProbs
and basic prompting to measure semantic plausibility, both in single-sentence
minimal pairs (Experiment 1) and short context-dependent scenarios (Experiment
2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a
more reliable measure of semantic plausibility than direct zero-shot prompting,
which yields inconsistent and often poor results; (ii) instruction-tuning
generally does not alter the sensitivity of LogProbs to semantic plausibility
(although sometimes decreases it); (iii) across models, context mostly
modulates LogProbs in expected ways, as measured by three novel metrics of
context-sensitive plausibility and their match to explicit human plausibility
judgments. We conclude that, even in the era of prompt-based evaluations,
LogProbs constitute a useful metric of semantic plausibility, both in base and
instruction-tuned LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
  Exhaustive <span class="highlight-title">Review</span> of Technologies, Research, Best Practices, Applied Research
  Challenges and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, Arsalan Shahid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report examines the fine-tuning of Large Language Models (LLMs),
integrating theoretical insights with practical applications. It outlines the
historical evolution of LLMs from traditional Natural Language Processing (NLP)
models to their pivotal role in AI. A comparison of fine-tuning methodologies,
including supervised, unsupervised, and instruction-based approaches,
highlights their applicability to different tasks. The report introduces a
structured seven-stage pipeline for fine-tuning LLMs, spanning data
preparation, model initialization, hyperparameter tuning, and model deployment.
Emphasis is placed on managing imbalanced datasets and optimization techniques.
Parameter-efficient methods like Low-Rank Adaptation (LoRA) and Half
Fine-Tuning are explored for balancing computational efficiency with
performance. Advanced techniques such as memory fine-tuning, Mixture of Experts
(MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized
networks and multi-agent collaboration. The report also examines novel
approaches like Proximal Policy Optimization (PPO) and Direct Preference
Optimization (DPO), which align LLMs with human preferences, alongside pruning
and routing optimizations to improve efficiency. Further sections cover
validation frameworks, post-deployment monitoring, and inference optimization,
with attention to deploying LLMs on distributed and cloud-based platforms.
Emerging areas such as multimodal LLMs, fine-tuning for audio and speech, and
challenges related to scalability, privacy, and accountability are also
addressed. This report offers actionable insights for researchers and
practitioners navigating LLM fine-tuning in an evolving landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StrucText-Eval: Evaluating Large Language Model's Reasoning Ability in
  Structure-Rich Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhong Gu, Haoning Ye, Xingzhou Chen, Zeyang Zhou, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effective utilization of structured data, integral to corporate data
strategies, has been challenged by the rise of large language models (LLMs)
capable of processing unstructured information. This shift prompts the
question: can LLMs interpret structured data directly in its unstructured form?
We propose an automatic evaluation data generation method for assessing LLMs'
reasoning capabilities on structure-rich text to explore this. Our approach
supports 8 structured languages and 29 tasks, generating data with adjustable
complexity through controllable nesting and structural width. We introduce
StrucText-Eval, a benchmark containing 5,800 pre-generated and annotated
samples designed to evaluate how well LLMs understand and reason through
structured text. StrucText-Eval is divided into two suites: a regular Test
suite (3,712 samples) and a Test-Hard suite (2,088 samples), the latter
emphasizing the gap between human and model performance on more complex tasks.
Experimental results show that while open-source LLMs achieve a maximum
accuracy of 74.9\% on the standard dataset, their performance drops
significantly to 45.8\% on the harder dataset. In contrast, human participants
reach an accuracy of 92.6\% on StrucText-Eval-Hard, highlighting LLMs' current
limitations in handling intricate structural information. The benchmark and
generation codes are open sourced in
\url{https://github.com/MikeGu721/StrucText-Eval}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in
  <span class="highlight-title">Long</span>-Horizon Tasks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a general-purpose agent is a long-standing vision in the field of
artificial intelligence. Existing agents have made remarkable progress in many
domains, yet they still struggle to complete long-horizon tasks in an open
world. We attribute this to the lack of necessary world knowledge and
multimodal experience that can guide agents through a variety of long-horizon
tasks. In this paper, we propose a Hybrid Multimodal Memory module to address
the above challenges. It 1) transforms knowledge into Hierarchical Directed
Knowledge Graph that allows agents to explicitly represent and learn world
knowledge, and 2) summarises historical information into Abstracted Multimodal
Experience Pool that provide agents with rich references for in-context
learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,
Optimus-1, is constructed with dedicated Knowledge-guided Planner and
Experience-Driven Reflector, contributing to a better planning and reflection
in the face of long-horizon tasks in Minecraft. Extensive experimental results
show that Optimus-1 significantly outperforms all existing agents on
challenging long-horizon task benchmarks, and exhibits near human-level
performance on many tasks. In addition, we introduce various Multimodal Large
Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show
that Optimus-1 exhibits strong generalization with the help of the Hybrid
Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Any2Point: Empowering Any-modality Large Models for Efficient 3D
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Tang, Ray Zhang, Jiaming Liu, Zoey Guo, Dong Wang, Zhigang Wang, Bin Zhao, Shanghang Zhang, Peng Gao, Hongsheng Li, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large foundation models have recently emerged as a prominent focus of
interest, attaining superior performance in widespread scenarios. Due to the
scarcity of 3D data, many efforts have been made to adapt pre-trained
transformers from vision to 3D domains. However, such 2D-to-3D approaches are
still limited, due to the potential loss of spatial geometries and high
computation cost. More importantly, their frameworks are mainly designed for 2D
models, lacking a general any-to-3D paradigm. In this paper, we introduce
Any2Point, a parameter-efficient method to empower any-modality large models
(vision, language, audio) for 3D understanding. Given a frozen transformer from
any source modality, we propose a 3D-to-any (1D or 2D) virtual projection
strategy that correlates the input 3D points to the original 1D or 2D positions
within the source modality. This mechanism enables us to assign each 3D token
with a positional encoding paired with the pre-trained model, which avoids 3D
geometry loss caused by the true projection and better motivates the
transformer for 3D learning with 1D/2D positional priors. Then, within each
transformer block, we insert an any-to-3D guided adapter module for
parameter-efficient fine-tuning. The adapter incorporates prior spatial
knowledge from the source modality to guide the local feature aggregation of 3D
tokens, compelling the semantic adaption of any-modality transformers. We
conduct extensive experiments to showcase the effectiveness and efficiency of
our method. Code and models are released at
https://github.com/Ivan-Tang-3D/Any2Point.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are released at
  https://github.com/Ivan-Tang-3D/Any2Point</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical
  Decision-Support Setting <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Kayser, Bayar Menzat, Cornelius Emde, Bogdan Bercean, Alex Novak, Abdala Espinosa, Bartlomiej W. Papiez, Susanne Gaube, Thomas Lukasiewicz, Oana-Maria Camburu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing capabilities of AI models are leading to their wider use,
including in safety-critical domains. Explainable AI (XAI) aims to make these
models safer to use by making their inference process more transparent.
However, current explainability methods are seldom evaluated in the way they
are intended to be used: by real-world end users. To address this, we conducted
a large-scale user study with 85 healthcare practitioners in the context of
human-AI collaborative chest X-ray analysis. We evaluated three types of
explanations: visual explanations (saliency maps), natural language
explanations, and a combination of both modalities. We specifically examined
how different explanation types influence users depending on whether the AI
advice and explanations are factually correct. We find that text-based
explanations lead to significant over-reliance, which is alleviated by
combining them with saliency maps. We also observe that the quality of
explanations, that is, how much factually correct information they entail, and
how much this aligns with AI correctness, significantly impacts the usefulness
of the different explanation types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mbrs: A Library for Minimum Bayes Risk Decoding <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroyuki Deguchi, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks
that outperforms conventional maximum a posterior (MAP) decoding using beam
search by selecting high-quality outputs based on a utility function rather
than those with high-probability. Typically, it finds the most suitable
hypothesis from the set of hypotheses under the sampled pseudo-references. mbrs
is a library of MBR decoding, which can flexibly combine various metrics,
alternative expectation estimations, and algorithmic variants. It is designed
with a focus on speed measurement and calling count of code blocks,
transparency, reproducibility, and extensibility, which are essential for
researchers and developers. We published our mbrs as an MIT-licensed
open-source project, and the code is available on GitHub.
  GitHub: https://github.com/naist-nlp/mbrs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP2024 System Demonstration track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Biases of Large Language Models in Stance Detection with
  Counterfactual Augmented Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14296v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14296v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Xingwei Liang, Kam-Fai Wong, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection is critical for understanding the underlying position or
attitude expressed toward a topic. Large language models (LLMs) have
demonstrated significant advancements across various natural language
processing tasks including stance detection, however, their performance in
stance detection is limited by biases and spurious correlations inherent due to
their data-driven nature. Our statistical experiment reveals that LLMs are
prone to generate biased stances due to sentiment-stance spurious correlations
and preference towards certain individuals and topics. Furthermore, the results
demonstrate a strong negative correlation between stance bias and stance
detection performance, underscoring the importance of mitigating bias to
enhance the utility of LLMs in stance detection. Therefore, in this paper, we
propose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel
calibration network is devised to calibrate potential bias in the stance
prediction of LLMs. Further, to address the challenge of effectively learning
bias representations and the difficulty in the generalizability of debiasing,
we construct counterfactual augmented data. This approach enhances the
calibration network, facilitating the debiasing and out-of-domain
generalization. Experimental results on in-target and zero-shot stance
detection tasks show that the proposed FACTUAL can effectively mitigate biases
of LLMs, achieving state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models (T2I) use a latent representation of a text
prompt to guide the image generation process. However, the process by which the
encoder produces the text representation is unknown. We propose the Diffusion
Lens, a method for analyzing the text encoder of T2I models by generating
images from its intermediate representations. Using the Diffusion Lens, we
perform an extensive analysis of two recent T2I models. Exploring compound
prompts, we find that complex scenes describing multiple objects are composed
progressively and more slowly compared to simple scenes; Exploring knowledge
retrieval, we find that representation of uncommon concepts requires further
computation compared to common concepts, and that knowledge retrieval is
gradual across layers. Overall, our findings provide valuable insights into the
text encoder component in T2I pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: ACL 2024 Project webpage:
  tokeron.github.io/DiffusionLensWeb</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Better: Avoiding Pitfalls in Developing Language Resources when
  Data is Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12691v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12691v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is a symbolic capital that affects people's lives in many ways
(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,
cultures, traditions, and societies in general. Hence, data in a given language
should be viewed as more than a collection of tokens. Good data collection and
labeling practices are key to building more human-centered and socially aware
technologies. While there has been a rising interest in mid- to low-resource
languages within the NLP community, work in this space has to overcome unique
challenges such as data scarcity and access to suitable annotators. In this
paper, we collect feedback from those directly involved in and impacted by NLP
artefacts for mid- to low-resource languages. We conduct a quantitative and
qualitative analysis of the responses and highlight the main issues related to
(1) data quality such as linguistic and cultural data suitability; and (2) the
ethics of common annotation practices such as the misuse of online community
services. Based on these findings, we make several recommendations for the
creation of high-quality language artefacts that reflect the cultural milieu of
its speakers, while simultaneously respecting the dignity and labor of data
workers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crafting Tomorrow's Headlines: Neural News Generation and Detection in
  English, Turkish, Hungarian, and Persian <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cem Üyük, Danica Rovó, Shaghayegh Kolli, Rabia Varol, Georg Groh, Daryna Dementieva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era dominated by information overload and its facilitation with Large
Language Models (LLMs), the prevalence of misinformation poses a significant
threat to public discourse and societal well-being. A critical concern at
present involves the identification of machine-generated news. In this work, we
take a significant step by introducing a benchmark dataset designed for neural
news detection in four languages: English, Turkish, Hungarian, and Persian. The
dataset incorporates outputs from multiple multilingual generators (in both,
zero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and
GPT-4. Next, we experiment with a variety of classifiers, ranging from those
based on linguistic features to advanced Transformer-based models and LLMs
prompting. We present the detection results aiming to delve into the
interpretablity and robustness of machine-generated texts detectors across all
target languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 NLP4PI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Truth is Universal: Robust Detection of Lies in LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Bürger, Fred A. Hamprecht, Boaz Nadler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionised natural language processing,
exhibiting impressive human-like capabilities. In particular, LLMs are capable
of "lying", knowingly outputting false statements. Hence, it is of interest and
importance to develop methods to detect when LLMs lie. Indeed, several authors
trained classifiers to detect LLM lies based on their internal model
activations. However, other researchers showed that these classifiers may fail
to generalise, for example to negated statements. In this work, we aim to
develop a robust method to detect when an LLM is lying. To this end, we make
the following key contributions: (i) We demonstrate the existence of a
two-dimensional subspace, along which the activation vectors of true and false
statements can be separated. Notably, this finding is universal and holds for
various LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our
analysis explains the generalisation failures observed in previous studies and
sets the stage for more robust lie detection; (ii) Building upon (i), we
construct an accurate LLM lie detector. Empirically, our proposed classifier
achieves state-of-the-art performance, attaining 94% accuracy in both
distinguishing true from false factual statements and detecting lies generated
in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision
  Models For Video Captioning and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Luo, Austin Peng, Adithya Vasudev, Rishabh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video is an increasingly prominent and information-dense medium, yet it poses
substantial challenges for language models. A typical video consists of a
sequence of shorter segments, or shots, that collectively form a coherent
narrative. Each shot is analogous to a word in a sentence where multiple data
streams of information (such as visual and auditory data) must be processed
simultaneously. Comprehension of the entire video requires not only
understanding the visual-audio information of each shot but also requires that
the model links the ideas between each shot to generate a larger,
all-encompassing story. Despite significant progress in the field, current
works often overlook videos' more granular shot-by-shot semantic information.
In this project, we propose a family of efficient large language vision models
(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By
leveraging better pretraining and data collection strategies, we extend the
abilities of existing small LLVMs from being able to understand a picture to
being able to understand a sequence of frames. Specifically, we show that
Shotluck Holmes achieves better performance than state-of-the-art results on
the Shot2Story video captioning and summary task with significantly smaller and
more computationally efficient models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the token distance modeling ability of higher <span class="highlight-title">RoPE</span> attention
  dimension <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Length extrapolation algorithms based on Rotary position embedding (RoPE)
have shown promising results in extending the context length of language
models. However, understanding how position embedding can capture longer-range
contextual information remains elusive. Based on the intuition that different
dimensions correspond to different frequency of changes in RoPE encoding, we
conducted a dimension-level analysis to investigate the correlation between a
hidden dimension of an attention head and its contribution to capturing
long-distance dependencies. Using our correlation metric, we identified a
particular type of attention heads, which we named Positional Heads, from
various length-extrapolated models. These heads exhibit a strong focus on
long-range information interaction and play a pivotal role in long input
processing, as evidence by our ablation. We further demonstrate the correlation
between the efficiency of length extrapolation and the extension of the
high-dimensional attention allocation of these heads. The identification of
Positional Heads provides insights for future research in long-text
comprehension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended
  Text Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, Matthias Aßenmacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding from the output distributions of large language models to produce
high-quality text is a complex challenge in language modeling. Various
approaches, such as beam search, sampling with temperature, $k-$sampling,
nucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive
search, have been proposed to address this problem, aiming to improve
coherence, diversity, as well as resemblance to human-generated text. In this
study, we introduce adaptive contrastive search, a novel decoding strategy
extending contrastive search by incorporating an adaptive degeneration penalty,
guided by the estimated uncertainty of the model at each generation step. This
strategy is designed to enhance both the creativity and diversity of the
language modeling process while at the same time producing coherent and
high-quality generated text output. Our findings indicate performance
enhancement in both aspects, across different model architectures and datasets,
underscoring the effectiveness of our method in text generation tasks. Our code
base, datasets, and models are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUIS: Question-guided Insights Generation for Automated Exploratory Data
  Analysis <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10270v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10270v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Manatkar, Ashlesha Akella, Parthivi Gupta, Krishnasuri Narayanam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering meaningful insights from a large dataset, known as Exploratory
Data Analysis (EDA), is a challenging task that requires thorough exploration
and analysis of the data. Automated Data Exploration (ADE) systems use
goal-oriented methods with Large Language Models and Reinforcement Learning
towards full automation. However, these methods require human involvement to
anticipate goals that may limit insight extraction, while fully automated
systems demand significant computational resources and retraining for new
datasets. We introduce QUIS, a fully automated EDA system that operates in two
stages: insight generation (ISGen) driven by question generation (QUGen). The
QUGen module generates questions in iterations, refining them from previous
iterations to enhance coverage without human intervention or manually curated
examples. The ISGen module analyzes data to produce multiple relevant insights
in response to each question, requiring no prior training and enabling QUIS to
adapt to new datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Long</span>VILA: Scaling <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span> Visual Language Models for <span class="highlight-title">Long</span> Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10188v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10188v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context capability is critical for multi-modal foundation models,
especially for long video understanding. We introduce LongVILA, a full-stack
solution for long-context visual-language models \qinghao{by co-designing the
algorithm and system. For model training, we upgrade existing VLMs to support
long video understanding by incorporating two additional stages, {\em i.e.},
long context extension and long video supervised fine-tuning. However, training
on long video is computationally and memory intensive. We introduce the
long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently
parallelizes long video training and inference, enabling 2M context length
training on 256 GPUs without any gradient checkpointing. LongVILA efficiently
extends the number of video frames of VILA from 8 to 2048, improving the long
video captioning score from 2.00 to 3.26 (out of 5), achieving 99.8% accuracy
in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack.
LongVILA-7B demonstrates strong accuracy on the VideoMME benchmark, i.e., 61.8%
with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence
parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and
tensor parallelism. Moreover, it seamlessly integrates with Hugging Face
Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/NVlabs/VILA/blob/main/LongVILA.md</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Illusion of Competence: Evaluating the Effect of Explanations on
  Users' Mental Models of Visual Question Answering Systems <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judith Sieker, Simeon Junker, Ronja Utescher, Nazia Attari, Heiko Wersing, Hendrik Buschmeier, Sina Zarrieß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine how users perceive the limitations of an AI system when it
encounters a task that it cannot perform perfectly and whether providing
explanations alongside its answers aids users in constructing an appropriate
mental model of the system's capabilities and limitations. We employ a visual
question answer and explanation task where we control the AI system's
limitations by manipulating the visual inputs: during inference, the system
either processes full-color or grayscale images. Our goal is to determine
whether participants can perceive the limitations of the system. We hypothesize
that explanations will make limited AI capabilities more transparent to users.
However, our results show that explanations do not have this effect. Instead of
allowing users to more accurately assess the limitations of the AI system,
explanations generally increase users' perceptions of the system's competence -
regardless of its actual performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages (including Appendix). Accepted at EMNLP 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconstrained Model Merging for Enhanced LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Baoyi He, Shengyu Zhang, Yuhao Fu, Qi Zhou, Zhijie Sang, Zijin Hong, Kejing Yang, Wenjun Wang, Jianbo Yuan, Guanghan Ning, Linyi Li, Chunlin Ji, Fei Wu, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in building domain-specific large language models (LLMs)
have shown remarkable success, especially in tasks requiring reasoning
abilities like logical inference over complex relationships and multi-step
problem solving. However, creating a powerful all-in-one LLM remains
challenging due to the need for proprietary data and vast computational
resources. As a resource-friendly alternative, we explore the potential of
merging multiple expert models into a single LLM. Existing studies on model
merging mainly focus on generalist LLMs instead of domain experts, or the LLMs
under the same architecture and size. In this work, we propose an unconstrained
model merging framework that accommodates both homogeneous and heterogeneous
model architectures with a focus on reasoning tasks. A fine-grained layer-wise
weight merging strategy is designed for homogeneous models merging, while
heterogeneous model merging is built upon the probabilistic distribution
knowledge derived from instruction-response fine-tuning data. Across 7
benchmarks and 9 reasoning-optimized LLMs, we reveal key findings that
combinatorial reasoning emerges from merging which surpasses simple additive
effects. We propose that unconstrained model merging could serve as a
foundation for decentralized LLMs, marking a notable progression from the
existing centralized LLM framework. This evolution could enhance wider
participation and stimulate additional advancement in the field of artificial
intelligence, effectively addressing the constraints posed by centralized
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, correct typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deconstructing The Ethics of Large Language Models from <span class="highlight-title">Long</span>-standing
  Issues to New-emerging Dilemmas: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyuan Deng, Yiqun Duan, Xin Jin, Heng Chang, Yijun Tian, Han Liu, Yichen Wang, Kuofeng Gao, Henry Peng Zou, Yiqiao Jin, Yijia Xiao, Shenghao Wu, Zongxing Xie, Weimin Lyu, Sihong He, Lu Cheng, Haohan Wang, Jun Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved unparalleled success across
diverse language modeling tasks in recent years. However, this progress has
also intensified ethical concerns, impacting the deployment of LLMs in everyday
contexts. This paper provides a comprehensive survey of ethical challenges
associated with LLMs, from longstanding issues such as copyright infringement,
systematic bias, and data privacy, to emerging problems like truthfulness and
social norms. We critically analyze existing research aimed at understanding,
examining, and mitigating these ethical risks. Our survey underscores
integrating ethical standards and societal values into the development of LLMs,
thereby guiding the development of responsible and ethically aligned language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Recognize Toxicity? A Structured Investigation Framework and
  Toxicity Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06900v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06900v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of developing Large Language Models (LLMs) that adhere to
societal standards, it is imperative to detect the toxicity in the generated
text. The majority of existing toxicity metrics rely on encoder models trained
on specific toxicity datasets, which are susceptible to out-of-distribution
(OOD) problems and depend on the dataset's definition of toxicity. In this
paper, we introduce a robust metric grounded on LLMs to flexibly measure
toxicity according to the given definition. We first analyze the toxicity
factors, followed by an examination of the intrinsic toxic attributes of LLMs
to ascertain their suitability as evaluators. Finally, we evaluate the
performance of our metric with detailed analysis. Our empirical results
demonstrate outstanding performance in measuring toxicity within verified
factors, improving on conventional metrics by 12 points in the F1 score. Our
findings also indicate that upstream toxicity significantly influences
downstream metrics, suggesting that LLMs are unsuitable for toxicity
evaluations within unverified factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 page long</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Social Biases in Japanese Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02050v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02050v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitomi Yanaka, Namgi Han, Ryoma Kumon, Jie Lu, Masashi Takeshita, Ryo Sekizawa, Taisei Kato, Hiromi Arai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of Large Language Models (LLMs), social biases in the
LLMs have become a crucial issue. While various benchmarks for social biases
have been provided across languages, the extent to which Japanese LLMs exhibit
social biases has not been fully investigated. In this study, we construct the
Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the
English bias benchmark BBQ, and analyze social biases in Japanese LLMs. The
results show that while current open Japanese LLMs improve their accuracies on
JBBQ by setting larger parameters, their bias scores become larger. In
addition, prompts with warnings about social biases and Chain-of-Thought
prompting reduce the effect of biases in model outputs, but there is room for
improvement in the consistency of reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities
  of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyue Jiang, Pengan Chen, Liheng Chen, Sheng Wang, Qinghang Bao, Lingpeng Kong, Yu Li, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of large language models (LLMs) has transformed the
competitive landscape in natural language processing (NLP), particularly for
English and other data-rich languages. However, underrepresented languages like
Cantonese, spoken by over 85 million people, face significant development gaps,
which is particularly concerning given the economic significance of the
Guangdong-Hong Kong-Macau Greater Bay Area, and in substantial
Cantonese-speaking populations in places like Singapore and North America.
Despite its wide use, Cantonese has scant representation in NLP research,
especially compared to other languages from similarly developed regions. To
bridge these gaps, we outline current Cantonese NLP methods and introduce new
benchmarks designed to evaluate LLM performance in factual generation,
mathematical logic, complex reasoning, and general knowledge in Cantonese,
which aim to advance open-source Cantonese LLM technology. We also propose
future research directions and recommended models to enhance Cantonese LLM
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyAgent: Function Calling at the Edge <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lutfi Eren Erdogan, Nicholas Lee, Siddharth Jha, Sehoon Kim, Ryan Tabrizi, Suhong Moon, Coleman Hooper, Gopala Anumanchipalli, Kurt Keutzer, Amir Gholami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have enabled the development of advanced
agentic systems that can integrate various tools and APIs to fulfill user
queries through function calling. However, the deployment of these LLMs on the
edge has not been explored since they typically require cloud-based
infrastructure due to their substantial model size and computational demands.
To this end, we present TinyAgent, an end-to-end framework for training and
deploying task-specific small language model agents capable of function calling
for driving agentic systems at the edge. We first show how to enable accurate
function calling for open-source models via the LLMCompiler framework. We then
systematically curate a high-quality dataset for function calling, which we use
to fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient
inference, we introduce a novel tool retrieval method to reduce the input
prompt length and utilize quantization to further accelerate the inference
speed. As a driving application, we demonstrate a local Siri-like system for
Apple's MacBook that can execute user commands through text or voice input. Our
results show that our models can achieve, and even surpass, the
function-calling capabilities of larger models like GPT-4-Turbo, while being
fully deployed at the edge. We open-source our dataset, models, and installable
package and provide a demo video for our MacBook assistant agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Demo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPINACH: SPARQL-Based Information Navigation for Challenging Real-World
  Questions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Liu, Sina J. Semnani, Harold Triedman, Jialiang Xu, Isaac Dan Zhao, Monica S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have led to significant improvements in the
Knowledge Base Question Answering (KBQA) task. However, datasets used in KBQA
studies do not capture the true complexity of KBQA tasks. They either have
simple questions, use synthetically generated logical forms, or are based on
small knowledge base (KB) schemas.
  We introduce the SPINACH dataset, an expert-annotated KBQA dataset collected
from discussions on Wikidata's "Request a Query" forum with 320
decontextualized question-SPARQL pairs. The complexity of these in-the-wild
queries calls for a KBQA system that can dynamically explore large and often
incomplete schemas and reason about them, as it is infeasible to create a
comprehensive training dataset.
  We also introduce an in-context learning KBQA agent, also called SPINACH,
that mimics how a human expert would write SPARQLs to handle challenging
questions. SPINACH achieves a new state of the art on the QALD-7, QALD-9 Plus
and QALD-10 datasets by 31.0%, 27.0%, and 10.0% in $F_1$, respectively, and
coming within 1.6% of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On
our new SPINACH dataset, the SPINACH agent outperforms all baselines, including
the best GPT-4-based KBQA agent, by at least 38.1% in $F_1$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Xiang Wang, Xiangnan He, Tat-seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often exhibit hallucinations due to incorrect or
outdated knowledge. Hence, model editing methods have emerged to enable
targeted knowledge updates. To achieve this, a prevailing paradigm is the
locating-then-editing approach, which first locates influential parameters and
then edits them by introducing a perturbation. While effective, current studies
have demonstrated that this perturbation inevitably disrupt the originally
preserved knowledge within LLMs, especially in sequential editing scenarios. To
address this, we introduce AlphaEdit, a novel solution that projects
perturbation onto the null space of the preserved knowledge before applying it
to the parameters. We theoretically prove that this projection ensures the
output of post-edited LLMs remains unchanged when queried about the preserved
knowledge, thereby mitigating the issue of disruption. Extensive experiments on
various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts
the performance of most locating-then-editing methods by an average of 36.4%
with a single line of additional code for projection solely. Our code is
available at: https://github.com/jianghoucheng/AlphaEdit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contradictory Reasoning Evaluation and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09603v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09603v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Liu, Soumya Sanyal, Isabelle Lee, Yongkang Du, Rahul Gupta, Yang Liu, Jieyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a plethora of recent work, large language models (LLMs) demonstrated
impressive reasoning ability, but many proposed downstream reasoning tasks only
focus on final answers. Two fundamental questions persist: 1) how consistent is
the reasoning, and 2) can models detect unreliable reasoning? In this paper, we
investigate self-contradictory (Self-Contra) reasoning, where the model
reasoning does not support its answers. To answer 1), we define and assess the
Self-Contra rate across three datasets and delve into finer-grained categories
of Self-Contra reasoning. We find that LLMs often contradict themselves in
reasoning tasks involving contextual information understanding or commonsense.
The model may generate correct answers by taking shortcuts in reasoning or
overlooking contextual evidence, leading to compromised reasoning. For 2), we
task the state-of-the-art model GPT-4 with identifying Self-Contra reasoning
and finer-grained fallacies. We find that finer-grained categories enhanced
detection can improve GPT-4's ability to detect Self-Contra. However, it is
only able to detect Self-Contra with a 52.2% F1 score, much lower compared to
66.7% for humans. Our results indicate that current LLMs lack the robustness
necessary for reliable reasoning and we emphasize the urgent need for
establishing best practices in comprehensive reasoning evaluations beyond pure
performance-based metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INSIDE: LLMs' Internal States Retain the Power of Hallucination
  Detection <span class="chip">ICLR-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge hallucination have raised widespread concerns for the security and
reliability of deployed LLMs. Previous efforts in detecting hallucinations have
been employed at logit-level uncertainty estimation or language-level
self-consistency evaluation, where the semantic information is inevitably lost
during the token-decoding procedure. Thus, we propose to explore the dense
semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates
for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular,
a simple yet effective \textbf{EigenScore} metric is proposed to better
evaluate responses' self-consistency, which exploits the eigenvalues of
responses' covariance matrix to measure the semantic consistency/diversity in
the dense embedding space. Furthermore, from the perspective of self-consistent
hallucination detection, a test time feature clipping approach is explored to
truncate extreme activations in the internal states, which reduces
overconfident generations and potentially benefits the detection of
overconfident hallucinations. Extensive experiments and ablation studies are
performed on several popular LLMs and question-answering (QA) benchmarks,
showing the effectiveness of our proposal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems
  at once 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiang Wang, Jordan Kodner, Owen Rambow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have proposed placing multiple problems in a single prompt to
improve input token utilization for a more efficient LLM inference. We call
this MPP, in contrast to conventional SPP that prompts an LLM with a single
problem at a time. While MPP has been shown to work comparably well or even
better than SPP under few-shot settings, its zero-shot performance is
underexplored, which better reveals the innate multiple problem handling
capabilities of LLMs. To address that, we study the zero-shot MPP performance
of various LLMs on 6 classification and 12 reasoning benchmarks and confirm
that LLMs are competent zero-shot multi-problem solvers. We also examine the
conditions of effectiveness of zero-shot MPP and explore several model-level
factors that may enable MPP. We observe that LLMs consistently perform worse
with selecting indices of texts of a given class label and with multiple
mixed-source reasoning problems, indicating a lack of true understanding. We
also find that instruction tuning is an important factor than enhances MPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The CLC-UKET <span class="highlight-title">Dataset</span>: Benchmarking Case Outcome Prediction for the UK
  Employment Tribunal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the intersection of technological innovation and access
to justice by developing a benchmark for predicting case outcomes in the UK
Employment Tribunal (UKET). To address the challenge of extensive manual
annotation, the study employs a large language model (LLM) for automatic
annotation, resulting in the creation of the CLC-UKET dataset. The dataset
consists of approximately 19,000 UKET cases and their metadata. Comprehensive
legal annotations cover facts, claims, precedent references, statutory
references, case outcomes, reasons and jurisdiction codes. Facilitated by the
CLC-UKET data, we examine a multi-class case outcome prediction task in the
UKET. Human predictions are collected to establish a performance reference for
model comparison. Empirical results from baseline models indicate that
finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET
prediction task. The performance of zero-shot LLMs can be enhanced by
integrating task-related information into few-shot examples. We hope that the
CLC-UKET dataset, along with human annotations and empirical findings, can
serve as a valuable benchmark for employment-related dispute resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10678v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10678v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuen Chen, Vethavikashini Chithrra Raghuram, Justus Mattern, Rada Mihalcea, Zhijing Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generated texts from large language models (LLMs) have been shown to exhibit
a variety of harmful, human-like biases against various demographics. These
findings motivate research efforts aiming to understand and measure such
effects. This paper introduces a causal formulation for bias measurement in
generative language models. Based on this theoretical foundation, we outline a
list of desiderata for designing robust bias benchmarks. We then propose a
benchmark called OccuGender, with a bias-measuring procedure to investigate
occupational gender bias. We test several state-of-the-art open-source LLMs on
OccuGender, including Llama, Mistral, and their instruction-tuned versions. The
results show that these models exhibit substantial occupational gender bias.
Lastly, we discuss prompting strategies for bias mitigation and an extension of
our causal formulation to illustrate the generalizability of our framework. Our
code and data https://github.com/chenyuen0103/gender-bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain of Ideas: Revolutionizing Research in Novel Idea Development with
  LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xinxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, Tian Feng, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective research ideation is a critical step for scientific research.
However, the exponential increase in scientific literature makes it challenging
for researchers to stay current with recent advances and identify meaningful
research directions. Recent developments in large language models~(LLMs)
suggest a promising avenue for automating the generation of novel research
ideas. However, existing methods for idea generation either trivially prompt
LLMs or directly expose LLMs to extensive literature without indicating useful
information. Inspired by the research process of human researchers, we propose
a Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant
literature in a chain structure to effectively mirror the progressive
development in a research domain. This organization facilitates LLMs to capture
the current advancements in research, thereby enhancing their ideation
capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that
can comprehensively evaluate idea generation methods from different
perspectives, aligning closely with the preferences of human researchers.
Experimental results indicate that the CoI agent consistently outperforms other
methods and shows comparable quality as humans in research idea generation.
Moreover, our CoI agent is budget-friendly, with a minimum cost of \$0.50 to
generate a candidate idea and its corresponding experimental design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,5 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XForecast: Evaluating Natural Language Explanations for Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Aksu, Chenghao Liu, Amrita Saha, Sarah Tan, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting aids decision-making, especially for stakeholders who
rely on accurate predictions, making it very important to understand and
explain these models to ensure informed decisions. Traditional explainable AI
(XAI) methods, which underline feature or temporal importance, often require
expert knowledge. In contrast, natural language explanations (NLEs) are more
accessible to laypeople. However, evaluating forecast NLEs is difficult due to
the complex causal relationships in time series data. To address this, we
introduce two new performance metrics based on simulatability, assessing how
well a human surrogate can predict model forecasts using the explanations.
Experiments show these metrics differentiate good from poor explanations and
align with human judgments. Utilizing these metrics, we further evaluate the
ability of state-of-the-art large language models (LLMs) to generate
explanations for time series data, finding that numerical reasoning, rather
than model size, is the main factor influencing explanation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One2set + Large Language Model: Best Partners for Keyphrase Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangying Shao, Liang Zhang, Minlong Peng, Guoqi Ma, Hao Yue, Mingming Sun, Jinsong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyphrase generation (KPG) aims to automatically generate a collection of
phrases representing the core concepts of a given document. The dominant
paradigms in KPG include one2seq and one2set. Recently, there has been
increasing interest in applying large language models (LLMs) to KPG. Our
preliminary experiments reveal that it is challenging for a single model to
excel in both recall and precision. Further analysis shows that: 1) the one2set
paradigm owns the advantage of high recall, but suffers from improper
assignments of supervision signals during training; 2) LLMs are powerful in
keyphrase selection, but existing selection methods often make redundant
selections. Given these observations, we introduce a generate-then-select
framework decomposing KPG into two steps, where we adopt a one2set-based model
as generator to produce candidates and then use an LLM as selector to select
keyphrases from these candidates. Particularly, we make two important
improvements on our generator and selector: 1) we design an Optimal
Transport-based assignment strategy to address the above improper assignments;
2) we model the keyphrase selection as a sequence labeling task to alleviate
redundant selections. Experimental results on multiple benchmark datasets show
that our framework significantly surpasses state-of-the-art models, especially
in absent keyphrase prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEPT: Decoupled Embeddings for <span class="highlight-title">Pre-train</span>ing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05021v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05021v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model pre-training benefits from diverse data to enhance performance
across domains and languages. However, training on such heterogeneous corpora
requires extensive and costly efforts. Since these data sources vary lexically,
syntactically, and semantically, they cause negative interference or the
``curse of multilinguality''. We propose a novel pre-training framework to
alleviate this curse. Our method, DEPT, decouples embeddings from the
transformer body while simultaneously training the latter in multiple contexts.
DEPT enables training without a shared global vocabulary and: (1) can train
robustly and effectively under significant data heterogeneity, (2) reduces
token embedding parameters by up to 80% and the communication costs by 675x for
billion-scale models, (3) enhances model generalization and plasticity in
adapting to new languages and domains, and (4) permits training with custom
optimized vocabularies per data source. We demonstrate DEPT's potential via the
first vocabulary-agnostic federated multilingual pre-training of a 1.3
billion-parameter model, limiting its embedding size to 102.4 million instead
of 512 million.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Paths-over-Graph: Knowledge Graph Empowered Large Language Model
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved impressive results in various
tasks but struggle with hallucination problems and lack of relevant knowledge,
especially in deep complex reasoning and knowledge-intensive tasks. Knowledge
Graphs (KGs), which capture vast amounts of facts in a structured format, offer
a reliable source of knowledge for reasoning. However, existing KG-based LLM
reasoning methods face challenges like handling multi-hop reasoning,
multi-entity questions, and effectively utilizing graph structures. To address
these issues, we propose Paths-over-Graph (PoG), a novel method that enhances
LLM reasoning by integrating knowledge reasoning paths from KGs, improving the
interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and
multi-entity questions through a three-phase dynamic multi-hop path
exploration, which combines the inherent knowledge of LLMs with factual
knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant
information from the graph exploration first and introduces efficient
three-step pruning techniques that incorporate graph structures, LLM prompting,
and a pre-trained language model (e.g., SBERT) to effectively narrow down the
explored candidate paths. This ensures all reasoning paths contain highly
relevant information captured from KGs, making the reasoning faithful and
interpretable in problem-solving. PoG innovatively utilizes graph structure to
prune the irrelevant noise and represents the first method to implement
multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive
experiments on five benchmark KGQA datasets demonstrate PoG outperforms the
state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an
average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo
surpasses ToG with GPT-4 by up to 23.9%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Language Structures through Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Freda Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is highly structured, with syntactic and semantic structures, to
some extent, agreed upon by speakers of the same language. With implicit or
explicit awareness of such structures, humans can learn and use language
efficiently and generalize to sentences that contain unseen words. Motivated by
human language learning, in this dissertation, we consider a family of machine
learning tasks that aim to learn language structures through grounding. We seek
distant supervision from other data sources (i.e., grounds), including but not
limited to other modalities (e.g., vision), execution results of programs, and
other languages.
  We demonstrate the potential of this task formulation and advocate for its
adoption through three schemes. In Part I, we consider learning syntactic
parses through visual grounding. We propose the task of visually grounded
grammar induction, present the first models to induce syntactic structures from
visually grounded text and speech, and find that the visual grounding signals
can help improve the parsing quality over language-only models. As a side
contribution, we propose a novel evaluation metric that enables the evaluation
of speech parsing without text or automatic speech recognition systems
involved. In Part II, we propose two execution-aware methods to map sentences
into corresponding semantic structures (i.e., programs), significantly
improving compositional generalization and few-shot program synthesis. In Part
III, we propose methods that learn language structures from annotations in
other languages. Specifically, we propose a method that sets a new state of the
art on cross-lingual word alignment. We then leverage the learned word
alignments to improve the performance of zero-shot cross-lingual dependency
parsing, by proposing a novel substructure-based projection method that
preserves structural knowledge learned from the source language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ph.D. Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BenTo: Benchmark Task Reduction with In-<span class="highlight-title">Context</span> Transferability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language models (LLMs) is costly: it requires the generation
and examination of LLM outputs on a large-scale benchmark of various tasks.
This paper investigates how to efficiently reduce the tasks used to benchmark
LLMs without affecting the evaluation quality. Our study reveals that task
transferability and relevance provide critical information to identify the most
representative subset of tasks via optimizing a facility location function. We
propose a practically efficient metric for estimating the transferability
between two tasks via in-context learning (ICL). By analyzing the pairwise
transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or
FLAN) to 5% while inducing only a <4% difference to the evaluation on the
original benchmark. Compared to prior works, our method is training-free,
gradient-free, and highly efficient requiring ICL only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/tianyi-lab/bento</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superposed Decoding: Multiple Generations from a Single Autoregressive
  Inference Pass <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18400v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18400v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications today provide users with multiple auto-complete drafts as
they type, including GitHub's code completion, Gmail's smart compose, and
Apple's messaging auto-suggestions. Under the hood, language models support
this by running an autoregressive inference pass to provide a draft.
Consequently, providing $k$ drafts to the user requires running an expensive
language model $k$ times. To alleviate the computation cost of running $k$
inference passes, we propose Superposed Decoding, a new decoding algorithm that
generates $k$ drafts at the computation cost of one autoregressive inference
pass. We achieve this by feeding a superposition of the most recent token
embeddings from the $k$ drafts as input to the next decoding step of the
language model. At every inference step we combine the $k$ drafts with the
top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,
using an n-gram interpolation with minimal compute overhead to filter out
incoherent generations. Our experiments show that $k$ drafts from Superposed
Decoding are at least as coherent and factual as Nucleus Sampling and Greedy
Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In
a compute-normalized setting, user evaluations demonstrably favor text
generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can
also be combined with other decoding strategies, resulting in universal
coverage gains when scaling inference time compute. Code and more examples
open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures, accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tokenization and Morphology in Multilingual Language Models: A
  Comparative Analysis of mT5 and ByT5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Anh Dang, Limor Raviv, Lukas Galke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Morphology is a crucial factor for multilingual language modeling as it poses
direct challenges for tokenization. Here, we seek to understand how
tokenization influences the morphological knowledge encoded in multilingual
language models. Specifically, we capture the impact of tokenization by
contrasting two multilingual language models: mT5 and ByT5. The two models
share the same architecture, training objective, and training data and only
differ in their tokenization strategies: subword tokenization vs.\@
character-level tokenization. Probing the morphological knowledge encoded in
these models on four tasks and 17 languages, our analyses show that the models
learn the morphological systems of some languages better than others and that
morphological information is encoded in the middle and late layers. Finally, we
show that languages with more irregularities benefit more from having a higher
share of the pre-training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image
  Description and Reasoning Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiongtao Zhou, Jie He, Lanyu Chen, Jingyu Li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for
improving the performance of multimodal large language models (MLLMs) across a
range of complex reasoning tasks. Despite its popularity, there is a notable
absence of automated methods for evaluating the quality of reasoning steps in
MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation
(MiCEval), a framework designed to assess the correctness of reasoning chains
by evaluating the quality of both the description and each reasoning step. The
evaluation of the description component focuses on the accuracy of the image
descriptions, while the reasoning step evaluates the quality of each step as it
is conditionally generated based on the preceding steps. MiCEval is built upon
a fine-grained dataset with annotations that rate each step according to
correctness, relevance, and informativeness. Extensive experiments on four
state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more
closely with human judgments compared to existing methods based on cosine
similarity or fine-tuning approaches. MiCEval datasets and code can be found in
https://github.com/alenai97/MiCEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Model Council: Democratically Benchmarking Foundation Models on
  Highly Subjective Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Zhao, Flor Miriam Plaza-del-Arco, Benjie Genchel, Amanda Cercas Curry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to evolve, the search for efficient
and meaningful evaluation methods is ongoing. Many recent evaluations use LLMs
as judges to score outputs from other LLMs, often relying on a single large
model like GPT-4o. However, using a single LLM judge is prone to intra-model
bias, and many tasks - such as those related to emotional intelligence,
creative writing, and persuasiveness - may be too subjective for a single model
to judge fairly. We introduce the Language Model Council (LMC), where a group
of LLMs collaborate to create tests, respond to them, and evaluate each other's
responses to produce a ranking in a democratic fashion. Unlike previous
approaches that focus on reducing cost or bias by using a panel of smaller
models, our work examines the benefits and nuances of a fully inclusive LLM
evaluation system. In a detailed case study on emotional intelligence, we
deploy a council of 20 recent LLMs to rank each other on open-ended responses
to interpersonal conflicts. Our results show that the LMC produces rankings
that are more separable and more robust, and through a user study, we show that
they are more consistent with human evaluations than any individual LLM judge.
Using all LLMs for judging can be costly, however, so we use Monte Carlo
simulations and hand-curated sub-councils to study hypothetical council
compositions and discuss the value of the incremental LLM judge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffNorm: <span class="highlight-title">Self-Supervised</span> Normalization for Non-autoregressive
  Speech-to-speech Translation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiting Tan, Jingyu Zhang, Lingfeng Shen, Daniel Khashabi, Philipp Koehn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive Transformers (NATs) are recently applied in direct
speech-to-speech translation systems, which convert speech across different
languages without intermediate text data. Although NATs generate high-quality
outputs and offer faster inference than autoregressive models, they tend to
produce incoherent and repetitive results due to complex data distribution
(e.g., acoustic and linguistic variations in speech). In this work, we
introduce DiffNorm, a diffusion-based normalization strategy that simplifies
data distributions for training NAT models. After training with a
self-supervised noise estimation objective, DiffNorm constructs normalized
target data by denoising synthetically corrupted speech features. Additionally,
we propose to regularize NATs with classifier-free guidance, improving model
robustness and translation quality by randomly dropping out source information
during training. Our strategies result in a notable improvement of about +7
ASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr)
translations on the CVSS benchmark, while attaining over 14x speedup for En-Es
and 5x speedup for En-Fr translations compared to autoregressive baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-grained and Explainable Factuality Evaluation for Multimodal
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhang, Jingxuan Zuo, Liqiang Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal summarization aims to generate a concise summary based on the
input text and image. However, the existing methods potentially suffer from
unfactual output. To evaluate the factuality of multimodal summarization
models, we propose two fine-grained and explainable evaluation frameworks
(FALLACIOUS) for different application scenarios, i.e. reference-based
factuality evaluation framework and reference-free factuality evaluation
framework. Notably, the reference-free factuality evaluation framework doesn't
need ground truth and hence it has a wider application scenario. To evaluate
the effectiveness of the proposed frameworks, we compute the correlation
between our frameworks and the other metrics. The experimental results show the
effectiveness of our proposed method. We will release our code and dataset via
github.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pairing Analogy-Augmented Generation with Procedural Memory for
  Procedural Q&A 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K Roth, Rushil Gupta, Simon Halle, Bang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models struggle to synthesize disparate pieces of information
into a coherent plan when approaching a complex procedural task. In this work,
we introduce a novel formalism and structure for such procedural knowledge.
Based on this formalism, we present a novel procedural knowledge dataset called
LCStep, which we created from LangChain tutorials. To leverage this procedural
knowledge to solve new tasks, we propose analogy-augmented generation (AAG),
which draws inspiration from the human ability to assimilate past experiences
to solve unfamiliar problems. AAG uses a custom procedure memory store to
retrieve and adapt specialized domain knowledge to answer new procedural tasks.
We demonstrate that AAG outperforms few-shot and RAG baselines on LCStep,
RecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation,
corroborated by human evaluation in the case of RecipeNLG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Concept Learning for Uncovering Latent Themes in Large Text
  Collections <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Leonor Pacheco, Tunazzina Islam, Lyle Ungar, Ming Yin, Dan Goldwasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Experts across diverse disciplines are often interested in making sense of
large text collections. Traditionally, this challenge is approached either by
noisy unsupervised techniques such as topic models, or by following a manual
theme discovery process. In this paper, we expand the definition of a theme to
account for more than just a word distribution, and include generalized
concepts deemed relevant by domain experts. Then, we propose an interactive
framework that receives and encodes expert feedback at different levels of
abstraction. Our framework strikes a balance between automation and manual
coding, allowing experts to maintain control of their study while reducing the
manual effort required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL: ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Cascades via Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cascades and speculative decoding are two common approaches to improving
language models' inference efficiency. Both approaches involve interleaving
models of different sizes, but via fundamentally distinct mechanisms: cascades
employ a deferral rule that invokes the larger model only for "hard" inputs,
while speculative decoding uses speculative execution to primarily invoke the
larger model in parallel verification mode. These mechanisms offer different
benefits: empirically, cascades offer better cost-quality trade-offs, often
even outperforming the large model, while theoretically, speculative decoding
offers a guarantee of quality-neutrality. In this paper, we leverage the best
of both these approaches by designing new speculative cascading techniques that
implement their deferral rule through speculative execution. We characterize
the optimal deferral rule for our speculative cascades, and employ a plug-in
approximation to the optimal rule. Experiments with Gemma and T5 models on a
range of language benchmarks show that our approach yields better cost quality
trade-offs than cascading and speculative decoding baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-18T00:00:00Z">2024-10-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are AI Detectors Good Enough? A <span class="highlight-title">Survey</span> on Quality of <span class="highlight-title">Dataset</span>s With
  Machine-Generated Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        German Gritsai, Anastasia Voznyuk, Andrey Grabovoy, Yury Chekhovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of autoregressive Large Language Models (LLMs) has
significantly improved the quality of generated texts, necessitating reliable
machine-generated text detectors. A huge number of detectors and collections
with AI fragments have emerged, and several detection methods even showed
recognition quality up to 99.9% according to the target metrics in such
collections. However, the quality of such detectors tends to drop dramatically
in the wild, posing a question: Are detectors actually highly trustworthy or do
their high benchmark scores come from the poor quality of evaluation datasets?
In this paper, we emphasise the need for robust and qualitative methods for
evaluating generated data to be secure against bias and low generalising
ability of future model. We present a systematic review of datasets from
competitions dedicated to AI-generated content detection and propose methods
for evaluating the quality of datasets containing AI-generated fragments. In
addition, we discuss the possibility of using high-quality generated data to
achieve two goals: improving the training of detection models and improving the
training datasets themselves. Our contribution aims to facilitate a better
understanding of the dynamics between human and machine text, which will
ultimately support the integrity of information in an increasingly automated
world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SudoLM: Learning Access Control of Parametric Knowledge with
  Authorization Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing preference alignment is a one-size-fits-all alignment mechanism,
where the part of the large language model (LLM) parametric knowledge with
non-preferred features is uniformly blocked to all the users. However, this
part of knowledge can be useful to advanced users whose expertise qualifies
them to handle these information. The one-size-fits-all alignment mechanism
undermines LLM's utility for these qualified users. To address this problem, we
propose SudoLM, a framework that lets LLMs learn access control over specific
parametric knowledge for users with different credentials via authorization
alignment. SudoLM allows authorized users to unlock their access to all the
parametric knowledge with an assigned SUDO key while blocking access to
non-qualified users. Experiments on two application scenarios demonstrate that
SudoLM effectively controls the user's access to the parametric knowledge and
maintains its general utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Large Language Models' Situated Faithfulness to External
  <span class="highlight-title">Context</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are often augmented with external information as
contexts, but this external information can sometimes be inaccurate or even
intentionally misleading. We argue that robust LLMs should demonstrate situated
faithfulness, dynamically calibrating their trust in external information based
on their confidence in the internal knowledge and the external context. To
benchmark this capability, we evaluate LLMs across several QA datasets,
including a newly created dataset called RedditQA featuring in-the-wild
incorrect contexts sourced from Reddit posts. We show that when provided with
both correct and incorrect contexts, both open-source and proprietary models
tend to overly rely on external information, regardless of its factual
accuracy. To enhance situated faithfulness, we propose two approaches:
Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning
(RCR). SCR enables models to self-access the confidence of external information
relative to their own internal knowledge to produce the most accurate answer.
RCR, in contrast, extracts explicit confidence signals from the LLM and
determines the final answer using predefined rules. Our results show that for
LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR
outperforms RCR, achieving improvements of up to 24.2% over a direct input
augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR
outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct
Preference Optimization (CR-DPO) method improves performance on both seen and
unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In
addition to quantitative results, we offer insights into the relative strengths
of SCR and RCR. Our findings highlight promising avenues for improving situated
faithfulness in LLMs. The data and code are released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaturalBench: Evaluating Vision-Language Models on Natural Adversarial
  Samples <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have made significant progress in recent
visual-question-answering (VQA) benchmarks that evaluate complex
visio-linguistic reasoning. However, are these models truly effective? In this
work, we show that VLMs still struggle with natural images and questions that
humans can easily answer, which we term natural adversarial samples. We also
find it surprisingly easy to generate these VQA samples from natural image-text
corpora using off-the-shelf models like CLIP and ChatGPT. We propose a
semi-automated approach to collect a new benchmark, NaturalBench, for reliably
evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a
$\textbf{vision-centric}$ design by pairing each question with two images that
yield different answers, preventing blind solutions from answering without
using the images. This makes NaturalBench more challenging than previous
benchmarks that can be solved with commonsense priors. We evaluate 53
state-of-the-art VLMs on NaturalBench, showing that models like
LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o
lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is
hard from two angles: (1) Compositionality: Solving NaturalBench requires
diverse visio-linguistic skills, including understanding attribute bindings,
object relationships, and advanced reasoning like logic and counting. To this
end, unlike prior work that uses a single tag per sample, we tag each
NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)
Biases: NaturalBench exposes severe biases in VLMs, as models often choose the
same answer regardless of the image. Lastly, we apply our benchmark curation
method to diverse data sources, including long captions (over 100 words) and
non-English languages like Chinese and Hindi, highlighting its potential for
dynamic evaluations of VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 24; We open-source our dataset at:
  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:
  https://linzhiqiu.github.io/papers/naturalbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image
  Description and Reasoning Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiongtao Zhou, Jie He, Lanyu Chen, jingyu li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for
improving the performance of multimodal large language models (MLLMs) across a
range of complex reasoning tasks. Despite its popularity, there is a notable
absence of automated methods for evaluating the quality of reasoning steps in
MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation
(MiCEval), a framework designed to assess the correctness of reasoning chains
by evaluating the quality of both the description and each reasoning step. The
evaluation of the description component focuses on the accuracy of the image
descriptions, while the reasoning step evaluates the quality of each step as it
is conditionally generated based on the preceding steps. MiCEval is built upon
a fine-grained dataset with annotations that rate each step according to
correctness, relevance, and informativeness. Extensive experiments on four
state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more
closely with human judgments compared to existing methods based on cosine
similarity or fine-tuning approaches. MiCEval datasets and code can be found in
https://github.com/alenai97/MiCEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie
  Character-Aware Discourse Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarizing movie screenplays presents a unique set of challenges compared to
standard document summarization. Screenplays are not only lengthy, but also
feature a complex interplay of characters, dialogues, and scenes, with numerous
direct and subtle relationships and contextual nuances that are difficult for
machine learning models to accurately capture and comprehend. Recent attempts
at screenplay summarization focus on fine-tuning transformer-based pre-trained
models, but these models often fall short in capturing long-term dependencies
and latent relationships, and frequently encounter the "lost in the middle"
issue. To address these challenges, we introduce DiscoGraMS, a novel resource
that represents movie scripts as a movie character-aware discourse graph (CaD
Graph). This approach is well-suited for various downstream tasks, such as
summarization, question-answering, and salience detection. The model aims to
preserve all salient information, offering a more comprehensive and faithful
representation of the screenplay's content. We further explore a baseline
method that combines the CaD Graph with the corresponding movie script through
a late fusion of graph and text modalities, and we present very initial
promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Fake News from Adversarial Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanxing Chen, Yukun Huang, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that existing evaluations for fake news detection based on
conventional sources, such as claims on fact-checking websites, result in an
increasing accuracy over time for LLM-based detectors -- even after their
knowledge cutoffs. This suggests that recent popular political claims, which
form the majority of fake news on such sources, are easily classified using
surface-level shallow patterns. Instead, we argue that a proper fake news
detection dataset should test a model's ability to reason factually about the
current world by retrieving and reading related evidence. To this end, we
develop a novel pipeline that leverages natural language feedback from a
RAG-based detector to iteratively modify real-time news into deceptive fake
news that challenges LLMs. Our iterative rewrite decreases the binary
classification AUC by an absolute 17.5 percent for a strong RAG GPT-4o
detector. Our experiments reveal the important role of RAG in both detecting
and generating fake news, as retrieval-free LLM detectors are vulnerable to
unseen events and adversarial attacks, while feedback from RAG detection helps
discover more deceitful patterns in fake news.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance between Relevant Information Pieces Causes Bias in <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span>
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional bias in large language models (LLMs) hinders their ability to
effectively process long inputs. A prominent example is the "lost in the
middle" phenomenon, where LLMs struggle to utilize relevant information
situated in the middle of the input. While prior research primarily focuses on
single pieces of relevant information, real-world applications often involve
multiple relevant information pieces. To bridge this gap, we present
LongPiBench, a benchmark designed to assess positional bias involving multiple
pieces of relevant information. Thorough experiments are conducted with five
commercial and six open-source models. These experiments reveal that while most
current models are robust against the "lost in the middle" issue, there exist
significant biases related to the spacing of relevant information pieces. These
findings highlight the importance of evaluating and reducing positional biases
to advance LLM's capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenEOL: Harnessing the Generative Power of LLMs for Training-Free
  Sentence Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghuveer Thirukovalluru, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training-free embedding methods directly leverage pretrained large language
models (LLMs) to embed text, bypassing the costly and complex procedure of
contrastive learning. Previous training-free embedding methods have mainly
focused on optimizing embedding prompts and have overlooked the benefits of
utilizing the generative abilities of LLMs. We propose a novel method, GenEOL,
which uses LLMs to generate diverse transformations of a sentence that preserve
its meaning, and aggregates the resulting embeddings of these transformations
to enhance the overall sentence embedding. GenEOL significantly outperforms the
existing training-free embedding methods by an average of 2.85 points across
several LLMs on the sentence semantic text similarity (STS) benchmark. Our
analysis shows that GenEOL stabilizes representation quality across LLM layers
and is robust to perturbations of embedding prompts. GenEOL also achieves
notable gains on multiple clustering, reranking and pair-classification tasks
from the MTEB benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverging Preferences: When do Annotators Disagree and do Models Know? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine diverging preferences in human-labeled preference datasets. We
develop a taxonomy of disagreement sources spanning 10 categories across four
high-level classes -- task underspecification, response style, refusals, and
annotation errors. We find that the majority of disagreements are in opposition
with standard reward modeling approaches, which are designed with the
assumption that annotator disagreement is noise. We then explore how these
findings impact two areas of LLM development: reward modeling and evaluation.
In our experiments, we demonstrate how standard reward modeling methods, like
the Bradley-Terry model, fail to differentiate whether a given preference
judgment is the result of unanimous agreement among annotators or the majority
opinion among diverging user preferences. We also find that these tendencies
are also echoed by popular LLM-as-Judge evaluation methods, which consistently
identify a winning response in cases of diverging preferences. These findings
highlight remaining challenges in LLM evaluations, which are greatly influenced
by divisive features like response style, and in developing pluralistically
aligned LLMs. To address these issues, we develop methods for identifying
diverging preferences to mitigate their influence on evaluation and training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CELI: Controller-Embedded Language Model Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Samuel Wagner, Dave DeCaprio, Abishek Chiffon Muthu Raja, Jonathan M. Holman, Lauren K. Brady, Sky C. Cheung, Hosein Barzekar, Eric Yang, Mark Anthony Martinez II, David Soong, Sriram Sridhar, Han Si, Brandon W. Higgs, Hisham Hamadeh, Scott Ogden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Controller-Embedded Language Model Interactions (CELI), a
framework that integrates control logic directly within language model (LM)
prompts, facilitating complex, multi-stage task execution. CELI addresses
limitations of existing prompt engineering and workflow optimization techniques
by embedding control logic directly within the operational context of language
models, enabling dynamic adaptation to evolving task requirements. Our
framework transfers control from the traditional programming execution
environment to the LMs, allowing them to autonomously manage computational
workflows while maintaining seamless interaction with external systems and
functions. CELI supports arbitrary function calls with variable arguments,
bridging the gap between LMs' adaptive reasoning capabilities and conventional
software paradigms' structured control mechanisms. To evaluate CELI's
versatility and effectiveness, we conducted case studies in two distinct
domains: code generation (HumanEval benchmark) and multi-stage content
generation (Wikipedia-style articles). The results demonstrate notable
performance improvements across a range of domains. CELI achieved a 4.9
percentage point improvement over the best reported score of the baseline GPT-4
model on the HumanEval code generation benchmark. In multi-stage content
generation, 94.4% of CELI-produced Wikipedia-style articles met or exceeded
first draft quality when optimally configured, with 44.4% achieving high
quality. These outcomes underscore CELI's potential for optimizing AI-driven
workflows across diverse computational domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Shall Know a Tool by the Traces it Leaves: The Predictability of
  Sentiment Analysis Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Baumartz, Mevlüt Bagci, Alexander Henlein, Maxim Konca, Andy Lücking, Alexander Mehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  If sentiment analysis tools were valid classifiers, one would expect them to
provide comparable results for sentiment classification on different kinds of
corpora and for different languages. In line with results of previous studies
we show that sentiment analysis tools disagree on the same dataset. Going
beyond previous studies we show that the sentiment tool used for sentiment
annotation can even be predicted from its outcome, revealing an algorithmic
bias of sentiment analysis. Based on Twitter, Wikipedia and different news
corpora from the English, German and French languages, our classifiers separate
sentiment tools with an averaged F1-score of 0.89 (for the English corpora). We
therefore warn against taking sentiment annotations as face value and argue for
the need of more and systematic NLP evaluation studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and <span class="highlight-title">Context</span>ual
  Distillation in Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Search (CS) is the task of retrieving relevant documents from
a corpus within a conversational context, combining retrieval with
conversational context modeling. With the explosion of Large Language Models
(LLMs), the CS field has seen major improvements with LLMs rewriting user
queries, accounting for conversational context. However, engaging LLMs at
inference time harms efficiency. Current methods address this by distilling
embeddings from human-rewritten queries to learn the context modeling task.
Yet, these approaches predominantly focus on context modeling, and only treat
the contrastive component of the retrieval task within a
distillation-independent loss term. To address these limitations, we propose a
new distillation method, as a relaxation of the previous objective, unifying
retrieval and context modeling. We relax the existing training objectives by
distilling similarity scores between conversations and documents, rather than
relying solely on representation learning. Our proposed distillation objective
allows for more freedom in the representation space and leverages the
contrastive nature of document relevance. Through experiments on Learned Sparse
Retrieval (LSR) across 5 CS datasets, our approach demonstrates substantial
improvements in both in-domain and out-of-domain retrieval performance,
outperforming state-of-the-art with gains of up to 6 points in recall for
out-of-domain datasets. Additionally, through the relaxation of the objective,
we propose a multi-teacher distillation, using multiple LLMs as teachers,
yielding additional gains, and outperforming the teachers themselves in
in-domain experiments. Finally, analysis of the sparsity of the models reveals
that our distillation allows for better control over the sparsity of the
trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Models to Balance Resisting and Accepting Persuasion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Stengel-Eskin, Peter Hase, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are susceptible to persuasion, which can pose
risks when models are faced with an adversarial interlocutor. We take a first
step towards defending models against persuasion while also arguing that
defense against adversarial (i.e. negative) persuasion is only half of the
equation: models should also be able to accept beneficial (i.e. positive)
persuasion to improve their answers. We show that optimizing models for only
one side results in poor performance on the other. In order to balance positive
and negative persuasion, we introduce Persuasion-Balanced Training (or PBT),
which leverages multi-agent recursive dialogue trees to create data and trains
models via preference optimization to accept persuasion when appropriate. PBT
consistently improves resistance to misinformation and resilience to being
challenged while also resulting in the best overall performance on holistic
data containing both positive and negative persuasion. Crucially, we show that
PBT models are better teammates in multi-agent debates. We find that without
PBT, pairs of stronger and weaker models have unstable performance, with the
order in which the models present their answers determining whether the team
obtains the stronger or weaker model's performance. PBT leads to better and
more stable results and less order dependence, with the stronger model
consistently pulling the weaker one up.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/esteng/persuasion_balanced_training</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and
  Tool Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Lumer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks
like secure database interactions and multi-agent code development. However,
scaling tool capacity beyond agent reasoning or model limits remains a
challenge. In this paper, we address these challenges by introducing Toolshed
Knowledge Bases, a tool knowledge base (vector database) designed to store
enhanced tool representations and optimize tool selection for large-scale
tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a
novel ensemble of tool-applied advanced retrieval-augmented generation (RAG)
techniques across the pre-retrieval, intra-retrieval, and post-retrieval
phases, without requiring model fine-tuning. During pre-retrieval, tool
documents are enhanced with key information and stored in the Toolshed
Knowledge Base. Intra-retrieval focuses on query planning and transformation to
increase retrieval accuracy. Post-retrieval refines the retrieved tool
documents and enables self-reflection. Furthermore, by varying both the total
number of tools (tool-M) an Agent has access to and the tool selection
threshold (top-k), we address trade-offs between retrieval accuracy, agent
performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute
improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools
benchmark datasets, respectively (Recall@5).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a
  Continuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Soh-Eun Shim, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing interest in looking at dialects in NLP. However, most
work to date still treats dialects as discrete categories. For instance,
evaluative work in variation-oriented NLP for English often works with Indian
English or African-American Venacular English as homogeneous categories (Faisal
et al., 2024; Ziems et al., 2023), yet even within one variety there is
substantial variation. We examine within-dialect variation and show that
performance critically varies within categories. We measure speech-to-text
performance on Italian dialects, and empirically observe a geographical
performance disparity. This disparity correlates substantially (-0.5) with
linguistic similarity to the highest performing dialect variety. We
cross-examine our results against dialectometry methods, and interpret the
performance disparity to be due to a bias towards dialects that are more
similar to the standard variety in the speech-to-text model examined. We
additionally leverage geostatistical methods to predict zero-shot performance
at unseen sites, and find the incorporation of geographical information to
substantially improve prediction performance, indicating there to be
geographical structure in the performance distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs estimate uncertainty well in instruction-following? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) could be valuable personal AI agents across
various domains, provided they can precisely follow user instructions. However,
recent studies have shown significant limitations in LLMs'
instruction-following capabilities, raising concerns about their reliability in
high-stakes applications. Accurately estimating LLMs' uncertainty in adhering
to instructions is critical to mitigating deployment risks. We present, to our
knowledge, the first systematic evaluation of the uncertainty estimation
abilities of LLMs in the context of instruction-following. Our study identifies
key challenges with existing instruction-following benchmarks, where multiple
factors are entangled with uncertainty stems from instruction-following,
complicating the isolation and comparison across methods and models. To address
these issues, we introduce a controlled evaluation setup with two benchmark
versions of data, enabling a comprehensive comparison of uncertainty estimation
methods under various conditions. Our findings show that existing uncertainty
methods struggle, particularly when models make subtle errors in instruction
following. While internal model states provide some improvement, they remain
inadequate in more complex scenarios. The insights from our controlled
evaluation setups provide a crucial understanding of LLMs' limitations and
potential for uncertainty estimation in instruction-following tasks, paving the
way for more trustworthy AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Attention with Mirror Descent: Generalized Max-Margin Token
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Alvarado Kristanto Julistiono, Davoud Ataee Tarzanagh, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanisms have revolutionized several domains of artificial
intelligence, such as natural language processing and computer vision, by
enabling models to selectively focus on relevant parts of the input data. While
recent work has characterized the optimization dynamics of gradient descent
(GD) in attention-based models and the structural properties of its preferred
solutions, less is known about more general optimization algorithms such as
mirror descent (MD). In this paper, we investigate the convergence properties
and implicit biases of a family of MD algorithms tailored for softmax attention
mechanisms, with the potential function chosen as the $p$-th power of the
$\ell_p$-norm. Specifically, we show that these algorithms converge in
direction to a generalized hard-margin SVM with an $\ell_p$-norm objective when
applied to a classification problem using a softmax attention model. Notably,
our theoretical results reveal that the convergence rate is comparable to that
of traditional GD in simpler models, despite the highly nonlinear and nonconvex
nature of the present problem. Additionally, we delve into the joint
optimization dynamics of the key-query matrix and the decoder, establishing
conditions under which this complex joint optimization converges to their
respective hard-margin SVM solutions. Lastly, our numerical experiments on real
data demonstrate that MD algorithms improve generalization over standard GD and
excel in optimal token selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Are Overparameterized Text Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thennal D K, Tim Fischer, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate strong performance as text embedding
models when finetuned with supervised contrastive training. However, their
large size balloons inference time and memory requirements. In this paper, we
show that by pruning the last $p\%$ layers of an LLM before supervised training
for only 1000 steps, we can achieve a proportional reduction in memory and
inference time. We evaluate four different state-of-the-art LLMs on text
embedding tasks and find that our method can prune up to 30\% of layers with
negligible impact on performance and up to 80\% with only a modest drop. With
only three lines of code, our method is easily implemented in any pipeline for
transforming LLMs to text encoders. We also propose $\text{L}^3 \text{Prune}$,
a novel layer-pruning strategy based on the model's initial loss that provides
two optimal pruning configurations: a large variant with negligible performance
loss and a small variant for resource-constrained settings. On average, the
large variant prunes 21\% of the parameters with a $-0.3$ performance drop, and
the small variant only suffers from a $-5.1$ decrease while pruning 74\% of the
model. We consider these results strong evidence that LLMs are
overparameterized for text embedding tasks, and can be easily pruned.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages of content + 1 for limitations and ethical considerations, 14
  pages in total including references and appendix, 5+1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel S. Y. Teo, Tan M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled
scalability in deep learning. SMoE has the potential to exponentially increase
parameter count while maintaining the efficiency of the model by only
activating a small subset of these parameters for a given sample. However, it
has been observed that SMoE suffers from unstable training and has difficulty
adapting to new distributions, leading to the model's lack of robustness to
data contamination. To overcome these limitations, we first establish a
connection between the dynamics of the expert representations in SMoEs and
gradient descent on a multi-objective optimization problem. Leveraging our
framework, we then integrate momentum into SMoE and propose a new family of
SMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate
that MomentumSMoE is more stable and robust than SMoE. In particular, we verify
the advantages of MomentumSMoE over SMoE on a variety of practical tasks
including ImageNet-1K object recognition and WikiText-103 language modeling. We
demonstrate the applicability of MomentumSMoE to many types of SMoE models,
including those in the Sparse MoE model for vision (V-MoE) and the Generalist
Language Model (GLaM). We also show that other advanced momentum-based
optimization methods, such as Adam, can be easily incorporated into the
MomentumSMoE framework for designing new SMoE models with even better
performance, almost negligible additional computation cost, and simple
implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages in the main text. Published at NeurIPS 2024. The code is
  available at https://github.com/rachtsy/MomentumSMoE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide
verifiable document-grounded responses to user inquiries. However, many natural
questions do not have good answers: about 25\% contain false
assumptions~\cite{Yu2023:CREPE}, and over 50\% are
ambiguous~\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve
their responses to confusing questions. This paper presents a novel synthetic
data generation method to efficiently create a diverse set of context-grounded
confusing questions from a given document corpus. We conduct an empirical
comparative evaluation of several large language models as RAG agents to
measure the accuracy of confusion detection and appropriate response
generation. We contribute a benchmark dataset to the public domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tell me what I need to know: Exploring LLM-based (Personalized)
  Abstractive Multi-Source Meeting Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Kirstein, Terry Ruas, Robert Kratel, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meeting summarization is crucial in digital communication, but existing
solutions struggle with salience identification to generate personalized,
workable summaries, and context understanding to fully comprehend the meetings'
content. Previous attempts to address these issues by considering related
supplementary resources (e.g., presentation slides) alongside transcripts are
hindered by models' limited context sizes and handling the additional
complexities of the multi-source tasks, such as identifying relevant
information in additional files and seamlessly aligning it with the meeting
content. This work explores multi-source meeting summarization considering
supplementary materials through a three-stage large language model approach:
identifying transcript passages needing additional context, inferring relevant
details from supplementary materials and inserting them into the transcript,
and generating a summary from this enriched transcript. Our multi-source
approach enhances model understanding, increasing summary relevance by ~9% and
producing more content-rich outputs. We introduce a personalization protocol
that extracts participant characteristics and tailors summaries accordingly,
improving informativeness by ~10%. This work further provides insights on
performance-cost trade-offs across four leading model families, including
edge-device capable options. Our approach can be extended to similar complex
generative tasks benefitting from additional resources and personalization,
such as dialogue systems and action planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs "know" internally when they follow instructions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Shirley Ren, Udhay Nallasamy, Andy Miller, Kwan Ho Ryan Chan, Jaya Narain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-following is crucial for building AI agents with large language
models (LLMs), as these models must adhere strictly to user-provided
constraints and guidelines. However, LLMs often fail to follow even simple and
clear instructions. To improve instruction-following behavior and prevent
undesirable outputs, a deeper understanding of how LLMs' internal states relate
to these outcomes is required. Our analysis of LLM internal states reveal a
dimension in the input embedding space linked to successful
instruction-following. We demonstrate that modifying representations along this
dimension improves instruction-following success rates compared to random
changes, without compromising response quality. Further investigation reveals
that this dimension is more closely related to the phrasing of prompts rather
than the inherent difficulty of the task or instructions. This discovery also
suggests explanations for why LLMs sometimes fail to follow clear instructions
and why prompt engineering is often effective, even when the content remains
largely unchanged. This work provides insight into the internal workings of
LLMs' instruction-following, paving the way for reliable LLM agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SignAttention: On the Interpretability of <span class="highlight-title">Transformer</span> Models for Sign
  Language Translation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Alejandro Dal Bianco, Oscar Agustín Stanchi, Facundo Manuel Quiroga, Franco Ronchetti, Enzo Ferrante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first comprehensive interpretability analysis of a
Transformer-based Sign Language Translation (SLT) model, focusing on the
translation from video-based Greek Sign Language to glosses and text.
Leveraging the Greek Sign Language Dataset, we examine the attention mechanisms
within the model to understand how it processes and aligns visual input with
sequential glosses. Our analysis reveals that the model pays attention to
clusters of frames rather than individual ones, with a diagonal alignment
pattern emerging between poses and glosses, which becomes less distinct as the
number of glosses increases. We also explore the relative contributions of
cross-attention and self-attention at each decoding step, finding that the
model initially relies on video frames but shifts its focus to previously
predicted tokens as the translation progresses. This work contributes to a
deeper understanding of SLT models, paving the way for the development of more
transparent and reliable translation systems essential for real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IAI Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Vo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, the need for precise and
efficient evaluation metrics becomes more pressing. Traditional approaches,
while informative, often face limitations in computational demands and
interpretability. In this paper, we introduce a novel hybrid evaluation method
that integrates two established techniques: entropy derived from covariance
matrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing
hidden states from LLMs, then computes the covariance matrix and MNN from these
representations. We further calculate the entropy of the covariance matrix to
capture uncertainty and redundancy in the model's outputs. By combining these
metrics into a composite score, we offer a comprehensive evaluation framework
that balances accuracy with computational efficiency. Additionally, our
approach allows for flexibility in adjusting the weightings between entropy and
MNN, tailoring the evaluation for different objectives. Through a series of
experiments on various LLMs, we demonstrate the robustness and efficacy of our
method, offering deeper insights into model performance. This work contributes
to the ongoing development of LLM evaluation and opens avenues for future
innovations in model assessment techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The method is currently under experimentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Wu, Haoyi Wu, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, sharing key-value (KV) cache across layers has been found effective
in efficient inference of large language models (LLMs). To systematically
investigate different techniques of cross-layer KV sharing, we propose a
unified framework that covers several recent methods and their novel variants.
We conduct comprehensive experiments on all the configurations of the
framework, evaluating their generation throughput and performance in language
modeling and downstream tasks. We find that when reducing the size of the KV
cache by 2x, most configurations can achieve competitive performance to and
higher throughput than standard transformers, but when further reducing the
size of the KV cache, pairing queries of all layers with KVs of upper layers
can better maintain performance, although it also introduces additional
training cost and prefilling latency. We hope that this work will help users
choose the appropriate approach according to their requirements and facilitate
research on the acceleration of LLM inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Meihuizi Jia, Yichao Feng, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large
language models (LLMs) and downstream tasks. However, PEFT has been proven
vulnerable to malicious attacks. Research indicates that poisoned LLMs, even
after PEFT, retain the capability to activate internalized backdoors when input
samples contain predefined triggers. In this paper, we introduce a novel
weak-to-strong unlearning algorithm to defend against backdoor attacks based on
feature alignment knowledge distillation, named W2SDefense. Specifically, we
first train a small-scale language model through full-parameter fine-tuning to
serve as the clean teacher model. Then, this teacher model guides the
large-scale poisoned student model in unlearning the backdoor, leveraging PEFT.
Theoretical analysis suggests that W2SDefense has the potential to enhance the
student model's ability to unlearn backdoor features, preventing the activation
of the backdoor. We conduct experiments on text classification tasks involving
three state-of-the-art language models and three different backdoor attack
algorithms. Our empirical results demonstrate the outstanding performance of
W2SDefense in defending against backdoor attacks without compromising model
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of
  Language Models for Fact Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denitsa Saynova, Lovisa Hagström, Moa Johansson, Richard Johansson, Marco Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous interpretations of language models (LMs) miss important distinctions
in how these models process factual information. For example, given the query
"Astrid Lindgren was born in" with the corresponding completion "Sweden", no
difference is made between whether the prediction was based on having the exact
knowledge of the birthplace of the Swedish author or assuming that a person
with a Swedish-sounding name was born in Sweden. In this paper, we investigate
four different prediction scenarios for which the LM can be expected to show
distinct behaviors. These scenarios correspond to different levels of model
reliability and types of information being processed - some being less
desirable for factual predictions. To facilitate precise interpretations of LMs
for fact completion, we propose a model-specific recipe called PrISM for
constructing datasets with examples of each scenario based on a set of
diagnostic criteria. We apply a popular interpretability method, causal tracing
(CT), to the four prediction scenarios and find that while CT produces
different results for each scenario, aggregations over a set of mixed examples
may only represent the results from the scenario with the strongest measured
signal. In summary, we contribute tools for a more granular study of fact
completion in language models and analyses that provide a more nuanced
understanding of how LMs process fact-related queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalena Wysocka, Danilo S. Carvalho, Oskar Wysocki, Marco Valentino, Andre Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Syllogistic reasoning is crucial for Natural Language Inference (NLI). This
capability is particularly significant in specialized domains such as
biomedicine, where it can support automatic evidence interpretation and
scientific discovery. This paper presents SylloBio-NLI, a novel framework that
leverages external ontologies to systematically instantiate diverse syllogistic
arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language
Models (LLMs) on identifying valid conclusions and extracting supporting
evidence across 28 syllogistic schemes instantiated with human genome pathways.
Extensive experiments reveal that biomedical syllogistic reasoning is
particularly challenging for zero-shot LLMs, which achieve an average accuracy
between 70% on generalized modus ponens and 23% on disjunctive syllogism. At
the same time, we found that few-shot prompting can boost the performance of
different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper
analysis shows that both techniques exhibit high sensitivity to superficial
lexical variations, highlighting a dependency between reliability, models'
architecture, and pre-training regime. Overall, our results indicate that,
while in-context examples have the potential to elicit syllogistic reasoning in
LLMs, existing models are still far from achieving the robustness and
consistency required for safe biomedical NLI applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI, Pragmatics, and Authenticity in Second Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Godwin-Jones`
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are obvious benefits to integrating generative AI (artificial
intelligence) into language learning and teaching. Those include using AI as a
language tutor, creating learning materials, or assessing learner output.
However, due to how AI systems under-stand human language, based on a
mathematical model using statistical probability, they lack the lived
experience to be able to use language with the same social aware-ness as
humans. Additionally, there are built-in linguistic and cultural biases based
on their training data which is mostly in English and predominantly from
Western sources. Those facts limit AI suitability for some language learning
interactions. Stud-ies have clearly shown that systems such as ChatGPT often do
not produce language that is pragmatically appropriate. The lack of linguistic
and cultural authenticity has important implications for how AI is integrated
into second language acquisition as well as in instruction targeting
development of intercultural communication compe-tence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing <span class="highlight-title">Context</span> Utilization of LLMs in Document-Level Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wafaa Mohammed, Vlad Niculae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) are increasingly strong contenders in machine
translation. We study document-level translation, where some words cannot be
translated without context from outside the sentence. We investigate the
ability of prominent LLMs to utilize context by analyzing models' robustness to
perturbed and randomized document context. We find that LLMs' improved
document-translation performance is not always reflected in pronoun translation
performance. We highlight the need for context-aware finetuning of LLMs with a
focus on relevant parts of the context to improve their reliability for
document-level translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Multilingual Models Remember? Investigating Multilingual Factual
  Recall Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constanza Fierro, Negar Foroutan, Desmond Elliott, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) store and retrieve vast amounts of factual
knowledge acquired during pre-training. Prior research has localized and
identified mechanisms behind knowledge recall; however, it has primarily
focused on English monolingual models. The question of how these processes
generalize to other languages and multilingual LLMs remains unexplored. In this
paper, we address this gap by conducting a comprehensive analysis of two highly
multilingual LLMs. We assess the extent to which previously identified
components and mechanisms of factual recall in English apply to a multilingual
context. Then, we examine when language plays a role in the recall process,
uncovering evidence of language-independent and language-dependent mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning <span class="highlight-title">Pre-train</span>ed Language Models for Robust Causal Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Yu, Yuxiang Zhou, Yulan He, Nevin L. Zhang, Ricardo Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fine-tuning of pre-trained language models (PLMs) has been shown to be
effective across various domains. By using domain-specific supervised data, the
general-purpose representation derived from PLMs can be transformed into a
domain-specific representation. However, these methods often fail to generalize
to out-of-domain (OOD) data due to their reliance on non-causal
representations, often described as spurious features. Existing methods either
make use of adjustments with strong assumptions about lack of hidden common
causes, or mitigate the effect of spurious features using multi-domain data. In
this work, we investigate how fine-tuned pre-trained language models aid
generalizability from single-domain scenarios under mild assumptions, targeting
more general and practical real-world scenarios. We show that a robust
representation can be derived through a so-called causal front-door adjustment,
based on a decomposition assumption, using fine-tuned representations as a
source of data augmentation. Comprehensive experiments in both synthetic and
real-world settings demonstrate the superior generalizability of the proposed
method compared to existing approaches. Our work thus sheds light on the domain
generalization problem by introducing links between fine-tuning and causal
mechanisms into representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently Computing Susceptibility to <span class="highlight-title">Context</span> in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Liu, Kevin Du, Mrinmaya Sachan, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One strength of modern language models is their ability to incorporate
information from a user-input context when answering queries. However, they are
not equally sensitive to the subtle changes to that context. To quantify this,
Du et al. (2024) gives an information-theoretic metric to measure such
sensitivity. Their metric, susceptibility, is defined as the degree to which
contexts can influence a model's response to a query at a distributional level.
However, exactly computing susceptibility is difficult and, thus, Du et al.
(2024) falls back on a Monte Carlo approximation. Due to the large number of
samples required, the Monte Carlo approximation is inefficient in practice. As
a faster alternative, we propose Fisher susceptibility, an efficient method to
estimate the susceptibility based on Fisher information. Empirically, we
validate that Fisher susceptibility is comparable to Monte Carlo estimated
susceptibility across a diverse set of query domains despite its being
$70\times$ faster. Exploiting the improved efficiency, we apply Fisher
susceptibility to analyze factors affecting the susceptibility of language
models. We observe that larger models are as susceptible as smaller ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Critical Questions Generation: Motivation and Challenges <span class="chip">CoNLL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blanca Calvo Figueras, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Large Language Models (LLMs) has brought impressive
performances on mitigation strategies against misinformation, such as
counterargument generation. However, LLMs are still seriously hindered by
outdated knowledge and by their tendency to generate hallucinated content. In
order to circumvent these issues, we propose a new task, namely, Critical
Questions Generation, consisting of processing an argumentative text to
generate the critical questions (CQs) raised by it. In argumentation theory CQs
are tools designed to lay bare the blind spots of an argument by pointing at
the information it could be missing. Thus, instead of trying to deploy LLMs to
produce knowledgeable and relevant counterarguments, we use them to question
arguments, without requiring any external knowledge. Research on CQs Generation
using LLMs requires a reference dataset for large scale experimentation. Thus,
in this work we investigate two complementary methods to create such a
resource: (i) instantiating CQs templates as defined by Walton's argumentation
theory and (ii), using LLMs as CQs generators. By doing so, we contribute with
a procedure to establish what is a valid CQ and conclude that, while LLMs are
reasonable CQ generators, they still have a wide margin for improvement in this
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 7 tables, to be published in the 28th Conference
  on Computational Natural Language Learning (CoNLL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoGU: <span class="highlight-title">Long</span>-form Generation with Uncertainty Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) demonstrate impressive capabilities, they
still struggle with generating factually incorrect content (i.e.,
hallucinations). A promising approach to mitigate this issue is enabling models
to express uncertainty when unsure. Previous research on uncertainty modeling
has primarily focused on short-form QA, but realworld applications often
require much longer responses. In this work, we introduce the task of Long-form
Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty
Suppression, where models hesitate to express uncertainty, and Uncertainty
Misalignment, where models convey uncertainty inaccurately. To tackle these
challenges, we propose a refinement-based data collection framework and a
two-stage training pipeline. Our framework adopts a divide-and-conquer
strategy, refining uncertainty based on atomic claims. The collected data are
then used in training through supervised fine-tuning (SFT) and direct
preference optimization (DPO) to enhance uncertainty expression. Extensive
experiments on three long-form instruction following datasets show that our
method significantly improves accuracy, reduces hallucinations, and maintains
the comprehensiveness of responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwaQuAD-24: QA Benchmark <span class="highlight-title">Dataset</span> in Swahili 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfred Malengo Kondoro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the creation of a Swahili Question Answering (QA)
benchmark dataset, aimed at addressing the underrepresentation of Swahili in
natural language processing (NLP). Drawing from established benchmarks like
SQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing
high-quality, annotated question-answer pairs that capture the linguistic
diversity and complexity of Swahili. The dataset is designed to support a
variety of applications, including machine translation, information retrieval,
and social services like healthcare chatbots. Ethical considerations, such as
data privacy, bias mitigation, and inclusivity, are central to the dataset
development. Additionally, the paper outlines future expansion plans to include
domain-specific content, multimodal integration, and broader crowdsourcing
efforts. The Swahili QA dataset aims to foster technological innovation in East
Africa and provide an essential resource for NLP research and applications in
low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EcomEdit: An Automated E-commerce Knowledge Editing Framework for
  Enhanced Product and Purchase Intention Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Ming Samuel Lau, Weiqi Wang, Haochen Shi, Baixuan Xu, Jiaxin Bai, Yangqiu Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Editing (KE) aims to correct and update factual information in
Large Language Models (LLMs) to ensure accuracy and relevance without
computationally expensive fine-tuning. Though it has been proven effective in
several domains, limited work has focused on its application within the
e-commerce sector. However, there are naturally occurring scenarios that make
KE necessary in this domain, such as the timely updating of product features
and trending purchase intentions by customers, which necessitate further
exploration. In this paper, we pioneer the application of KE in the e-commerce
domain by presenting ECOMEDIT, an automated e-commerce knowledge editing
framework tailored for e-commerce-related knowledge and tasks. Our framework
leverages more powerful LLMs as judges to enable automatic knowledge conflict
detection and incorporates conceptualization to enhance the semantic coverage
of the knowledge to be edited. Through extensive experiments, we demonstrate
the effectiveness of ECOMEDIT in improving LLMs' understanding of product
descriptions and purchase intentions. We also show that LLMs, after our
editing, can achieve stronger performance on downstream e-commerce tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REEF: Representation Encoding Fingerprints for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protecting the intellectual property of open-source Large Language Models
(LLMs) is very important, because training LLMs costs extensive computational
resources and data. Therefore, model owners and third parties need to identify
whether a suspect model is a subsequent development of the victim model. To
this end, we propose a training-free REEF to identify the relationship between
the suspect and victim models from the perspective of LLMs' feature
representations. Specifically, REEF computes and compares the centered kernel
alignment similarity between the representations of a suspect model and a
victim model on the same samples. This training-free REEF does not impair the
model's general capabilities and is robust to sequential fine-tuning, pruning,
model merging, and permutations. In this way, REEF provides a simple and
effective way for third parties and models' owners to protect LLMs'
intellectual property together. The code is available at
https://github.com/tmylla/REEF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoDification: Mixture of Depths Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context efficiency has recently become a trending topic in serving large
language models (LLMs). And mixture of depths (MoD) is proposed as a perfect
fit to bring down both latency and memory. In this paper, however, we discover
that MoD can barely transform existing LLMs without costly training over an
extensive number of tokens. To enable the transformations from any LLMs to MoD
ones, we showcase top-k operator in MoD should be promoted to threshold-p
operator, and refinement to architecture and data should also be crafted along.
All these designs form our method termed MoDification. Through a comprehensive
set of experiments covering model scales from 3B to 70B, we exhibit
MoDification strikes an excellent balance between efficiency and effectiveness.
MoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in
memory compared to original LLMs especially in long-context applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 5 tables, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good Parenting is all you need -- Multi-agentic LLM Hallucination
  Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Edward,  Kwartler, Matthew Berman, Alan Aqrawi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the ability of Large Language Model (LLM) agents to
detect and correct hallucinations in AI-generated content. A primary agent was
tasked with creating a blog about a fictional Danish artist named Flipfloppidy,
which was then reviewed by another agent for factual inaccuracies. Most LLMs
hallucinated the existence of this artist. Across 4,900 test runs involving
various combinations of primary and reviewing agents, advanced AI models such
as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in
identifying hallucinations and successfully revised outputs in 85% to 100% of
cases following feedback. These findings underscore the potential of advanced
AI models to significantly enhance the accuracy and reliability of generated
content, providing a promising approach to improving AI workflow orchestration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via
  Role Recognition and Involvement Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Cheng, Li Zhou, Feng Jiang, Benyou Wang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs), like ChatGPT, has
resulted in the widespread presence of LLM-generated content on social media
platforms, raising concerns about misinformation, data biases, and privacy
violations, which can undermine trust in online discourse. While detecting
LLM-generated content is crucial for mitigating these risks, current methods
often focus on binary classification, failing to address the complexities of
real-world scenarios like human-AI collaboration. To move beyond binary
classification and address these challenges, we propose a new paradigm for
detecting LLM-generated content. This approach introduces two novel tasks: LLM
Role Recognition (LLM-RR), a multi-class classification task that identifies
specific roles of LLM in content generation, and LLM Influence Measurement
(LLM-IM), a regression task that quantifies the extent of LLM involvement in
content creation. To support these tasks, we propose LLMDetect, a benchmark
designed to evaluate detectors' performance on these new tasks. LLMDetect
includes the Hybrid News Detection Corpus (HNDC) for training detectors, as
well as DetectEval, a comprehensive evaluation suite that considers five
distinct cross-context variations and multi-intensity variations within the
same LLM role. This allows for a thorough assessment of detectors'
generalization and robustness across diverse contexts. Our empirical validation
of 10 baseline detection methods demonstrates that fine-tuned PLM-based models
consistently outperform others on both tasks, while advanced LLMs face
challenges in accurately detecting their own generated content. Our
experimental results and analysis offer insights for developing more effective
detection models for LLM-generated content. This research enhances the
understanding of LLM-generated content and establishes a foundation for more
nuanced detection methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Social Media, Large Language Models, LLM-generated Text Detection,
  AI-assisted News Detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nova: An Iterative Planning and Search Approach to Enhance Novelty and
  Diversity of LLM Generated Ideas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific innovation is pivotal for humanity, and harnessing large language
models (LLMs) to generate research ideas could transform discovery. However,
existing LLMs often produce simplistic and repetitive suggestions due to their
limited ability in acquiring external knowledge for innovation. To address this
problem, we introduce an enhanced planning and search methodology designed to
boost the creative potential of LLM-based systems. Our approach involves an
iterative process to purposely plan the retrieval of external knowledge,
progressively enriching the idea generation with broader and deeper insights.
Validation through automated and human assessments indicates that our framework
substantially elevates the quality of generated ideas, particularly in novelty
and diversity. The number of unique novel ideas produced by our framework is
3.4 times higher than without it. Moreover, our method outperforms the current
state-of-the-art, generating at least 2.5 times more top-rated ideas based on
170 seed papers in a Swiss Tournament evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Tang, Xianghe Pang, Zexi Liu, Bohan Tang, Rui Ye, Xiaowen Dong, Yanfeng Wang, Siheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training is essential for enabling large language models (LLMs) to
follow human instructions. Inspired by the recent success of using LLMs to
simulate human society, we leverage multi-agent simulation to automatically
generate diverse text-based scenarios, capturing a wide range of real-world
human needs. We propose MATRIX, a multi-agent simulator that creates realistic
and scalable scenarios. Leveraging these outputs, we introduce a novel
scenario-driven instruction generator MATRIX-Gen for controllable and highly
realistic data synthesis. Extensive experiments demonstrate that our framework
effectively generates both general and domain-specific data. Notably, on
AlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on
datasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,
outperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M
pairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Blind Guessing: Calibration of Selection Bias in
  Multiple-Choice Question Answering by Video Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Loginova, Oleksandr Bezrukov, Alexey Kravets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Video Language Models (VLMs) is a challenging task. Due to its
transparency, Multiple-Choice Question Answering (MCQA) is widely used to
measure the performance of these models through accuracy. However, existing
MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to
selection bias, when models disproportionately favor certain answer options
based on positional patterns observed during training. In this work, we conduct
a comprehensive empirical analysis of several VLM architectures across major
datasets designed to assess complex video-focused reasoning. We identify where
the bias is most pronounced and demonstrate to what extent model responses
reflect genuine understanding of video content and related questions, as
opposed to reliance on arbitrary patterns or superficial cues, such as answer
position. By decomposing the MCQA task and adapting fairness bias metrics to
VLMs, we introduce a post-processing calibration technique BOLD to balance this
bias. Our results show that reducing selection bias improves not only debiasing
metrics but also overall model performance, including Accuracy and F1 Mean
score. Our method, by suppressing "blind guessing", offers a more cost- and
time-effective approach to mitigating selection bias compared to existing
techniques. This study represents the first focused investigation of selection
bias in video-to-text LLM-powered models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Method to Metigate Demographic and Expert Bias in ICD Coding
  with Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhang, Junli Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ICD(International Classification of Diseases) coding involves assigning ICD
codes to patients visit based on their medical notes. Considering ICD coding as
a multi-label text classification task, researchers have developed
sophisticated methods. Despite progress, these models often suffer from label
imbalance and may develop spurious correlations with demographic factors.
Additionally, while human coders assign ICD codes, the inclusion of irrelevant
information from unrelated experts introduces biases. To combat these issues,
we propose a novel method to mitigate Demographic and Expert biases in ICD
coding through Causal Inference (DECI). We provide a novel causality-based
interpretation in ICD Coding that models make predictions by three distinct
pathways. And based counterfactual reasoning, DECI mitigate demographic and
expert biases. Experimental results show that DECI outperforms state-of-the-art
models, offering a significant advancement in accurate and unbiased ICD coding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Knowledge Representations in Multilingual LLMs for
  Equivalence and Inheritance based Consistent Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Arora, Srujana Merugu, Shreya Jain, Vaibhav Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning and linguistic skills form the cornerstone of human intelligence,
facilitating problem-solving and decision-making. Recent advances in Large
Language Models (LLMs) have led to impressive linguistic capabilities and
emergent reasoning behaviors, fueling widespread adoption across application
domains. However, LLMs still struggle with complex reasoning tasks,
highlighting their systemic limitations. In this work, we focus on evaluating
whether LLMs have the requisite representations to reason using two
foundational relationships: "equivalence" and "inheritance". We introduce novel
tasks and benchmarks spanning six languages and observe that current SOTA LLMs
often produce conflicting answers to the same questions across languages in
17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.
To enhance consistency across languages, we propose novel "Compositional
Representations" where tokens are represented as composition of equivalent
tokens across languages, with resulting conflict reduction (up to -4.7%)
indicating benefits of shared LLM representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Large Language Models Generated Texts: A Multi-Level
  Fine-Grained Detection Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tao, Zhiyu Li, Runyu Chen, Dinghao Xi, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have transformed human writing by enhancing
grammar correction, content expansion, and stylistic refinement. However, their
widespread use raises concerns about authorship, originality, and ethics, even
potentially threatening scholarly integrity. Existing detection methods, which
mainly rely on single-feature analysis and binary classification, often fail to
effectively identify LLM-generated text in academic contexts. To address these
challenges, we propose a novel Multi-level Fine-grained Detection (MFD)
framework that detects LLM-generated text by integrating low-level structural,
high-level semantic, and deep-level linguistic features, while conducting
sentence-level evaluations of lexicon, grammar, and syntax for comprehensive
analysis. To improve detection of subtle differences in LLM-generated text and
enhance robustness against paraphrasing, we apply two mainstream evasion
techniques to rewrite the text. These variations, along with original texts,
are used to train a text encoder via contrastive learning, extracting
high-level semantic features of sentence to boost detection generalization.
Furthermore, we leverage advanced LLM to analyze the entire text and extract
deep-level linguistic features, enhancing the model's ability to capture
complex patterns and nuances while effectively incorporating contextual
information. Extensive experiments on public datasets show that the MFD model
outperforms existing methods, achieving an MAE of 0.1346 and an accuracy of
88.56%. Our research provides institutions and publishers with an effective
mechanism to detect LLM-generated text, mitigating risks of compromised
authorship. Educators and editors can use the model's predictions to refine
verification and plagiarism prevention protocols, ensuring adherence to
standards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Joint Multimodal Entity-Relation Extraction via
  Knowledge-Enhanced Cross-modal <span class="highlight-title">Prompt</span> Model <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yuan, Yi Cai, Junsheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task
that aims to extract entities and their relations from text-image pairs in
social media posts. Existing methods for JMERE require large amounts of labeled
data. However, gathering and annotating fine-grained multimodal data for JMERE
poses significant challenges. Initially, we construct diverse and comprehensive
multimodal few-shot datasets fitted to the original data distribution. To
address the insufficient information in the few-shot setting, we introduce the
\textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt
\textbf{M}odel (KECPM) for JMERE. This method can effectively address the
problem of insufficient information in the few-shot setting by guiding a large
language model to generate supplementary background knowledge. Our proposed
method comprises two stages: (1) a knowledge ingestion stage that dynamically
formulates prompts based on semantic similarity guide ChatGPT generating
relevant knowledge and employs self-reflection to refine the knowledge; (2) a
knowledge-enhanced language model stage that merges the auxiliary knowledge
with the original input and utilizes a transformer-based model to align with
JMERE's required output format. We extensively evaluate our approach on a
few-shot dataset derived from the JMERE dataset, demonstrating its superiority
over strong baselines in terms of both micro and macro F$_1$ scores.
Additionally, we present qualitative analyses and case studies to elucidate the
effectiveness of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paths-over-Graph: Knowledge Graph Enpowered Large Language Model
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved impressive results in various
tasks but struggle with hallucination problems and lack of relevant knowledge,
especially in deep complex reasoning and knowledge-intensive tasks. Knowledge
Graphs (KGs), which capture vast amounts of facts in a structured format, offer
a reliable source of knowledge for reasoning. However, existing KG-based LLM
reasoning methods face challenges like handling multi-hop reasoning,
multi-entity questions, and effectively utilizing graph structures. To address
these issues, we propose Paths-over-Graph (PoG), a novel method that enhances
LLM reasoning by integrating knowledge reasoning paths from KGs, improving the
interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and
multi-entity questions through a three-phase dynamic multi-hop path
exploration, which combines the inherent knowledge of LLMs with factual
knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant
information from the graph exploration first and introduces efficient
three-step pruning techniques that incorporate graph structures, LLM prompting,
and a pre-trained language model (e.g., SBERT) to effectively narrow down the
explored candidate paths. This ensures all reasoning paths contain highly
relevant information captured from KGs, making the reasoning faithful and
interpretable in problem-solving. PoG innovatively utilizes graph structure to
prune the irrelevant noise and represents the first method to implement
multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive
experiments on five benchmark KGQA datasets demonstrate PoG outperforms the
state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an
average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo
surpasses ToG with GPT-4 by up to 23.9%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Montessori-Instruct: Generate Influential Training Data Tailored for
  Student Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochuan Li, Zichun Yu, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data has been widely used to train large language models, but their
generative nature inevitably introduces noisy, non-informative, and misleading
learning signals. In this paper, we propose Montessori-Instruct, a novel data
synthesis framework that tailors the data synthesis ability of the teacher
language model toward the student language model's learning process.
Specifically, we utilize local data influence of synthetic training data points
on students to characterize students' learning preferences. Then, we train the
teacher model with Direct Preference Optimization (DPO) to generate synthetic
data tailored toward student learning preferences. Experiments with
Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and
MT-Bench demonstrate that Montessori-Instruct significantly outperforms
standard synthesis methods by 18.35\% and 46.24\% relatively. Our method also
beats data synthesized by a stronger teacher model, GPT-4o. Further analysis
confirms the benefits of teacher's learning to generate more influential
training data in the student's improved learning, the advantages of local data
influence in accurately measuring student preferences, and the robustness of
Montessori-Instruct across different student models. Our code and data are
open-sourced at https://github.com/cxcscmu/Montessori-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes and data are open-sourced at
  https://github.com/cxcscmu/Montessori-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MediTOD: An English Dialogue <span class="highlight-title">Dataset</span> for Medical History Taking with
  Comprehensive Annotations <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Vivek Saley, Goonjan Saha, Rocktim Jyoti Das, Dinesh Raghu,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical task-oriented dialogue systems can assist doctors by collecting
patient medical history, aiding in diagnosis, or guiding treatment selection,
thereby reducing doctor burnout and expanding access to medical services.
However, doctor-patient dialogue datasets are not readily available, primarily
due to privacy regulations. Moreover, existing datasets lack comprehensive
annotations involving medical slots and their different attributes, such as
symptoms and their onset, progression, and severity. These comprehensive
annotations are crucial for accurate diagnosis. Finally, most existing datasets
are non-English, limiting their utility for the larger research community.
  In response, we introduce MediTOD, a new dataset of doctor-patient dialogues
in English for the medical history-taking task. Collaborating with doctors, we
devise a questionnaire-based labeling scheme tailored to the medical domain.
Then, medical professionals create the dataset with high-quality comprehensive
annotations, capturing medical slots and their attributes. We establish
benchmarks in supervised and few-shot settings on MediTOD for natural language
understanding, policy learning, and natural language generation subtasks,
evaluating models from both TOD and biomedical domains. We make MediTOD
publicly available for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay
  Scoring with Rationale Generated by LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeongYeub Chu, JongWoo Kim, Bryan Wong, MunYong Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing automated essay scoring (AES) has solely relied on essay text
without using explanatory rationales for the scores, thereby forgoing an
opportunity to capture the specific aspects evaluated by rubric indicators in a
fine-grained manner. This paper introduces Rationale-based Multiple Trait
Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates
prompt-engineering-based large language models (LLMs) with a fine-tuning-based
essay scoring model using a smaller large language model (S-LLM). RMTS uses an
LLM-based trait-wise rationale generation system where a separate LLM agent
generates trait-specific rationales based on rubric guidelines, which the
scoring model uses to accurately predict multi-trait scores. Extensive
experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,
show that RMTS significantly outperforms state-of-the-art models and vanilla
S-LLMs in trait-specific scoring. By assisting quantitative assessment with
fine-grained qualitative rationales, RMTS enhances the trait-wise reliability,
providing partial explanations about essays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E3D-<span class="highlight-title">GPT</span>: Enhanced 3D Visual Foundation for Medical Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Lai, Zihang Jiang, Qingsong Yao, Rongsheng Wang, Zhiyang He, Xiaodong Tao, Wei Wei, Weifu Lv, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of 3D medical vision-language models holds significant
potential for disease diagnosis and patient treatment. However, compared to 2D
medical images, 3D medical images, such as CT scans, face challenges related to
limited training data and high dimension, which severely restrict the progress
of 3D medical vision-language models. To address these issues, we collect a
large amount of unlabeled 3D CT data and utilize self-supervised learning to
construct a 3D visual foundation model for extracting 3D visual features. Then,
we apply 3D spatial convolutions to aggregate and project high-level image
features, reducing computational complexity while preserving spatial
information. We also construct two instruction-tuning datasets based on BIMCV-R
and CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates
superior performance compared to existing methods in report generation, visual
question answering, and disease diagnosis. Code and data will be made publicly
available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Chain of Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Dujian Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing
and hold immense potential for advancing Artificial Intelligence. However, the
core architecture of most mainstream LLMs -- the Transformer -- has inherent
limitations in computational depth, rendering them theoretically incapable of
solving many reasoning tasks that demand increasingly deep computations. Chain
of Thought (CoT) prompting has emerged as a technique to address these
architectural limitations, as evidenced by several theoretical studies. It
offers a promising approach to solving complex reasoning tasks that were
previously beyond the capabilities of these models. Despite its successes, CoT
and its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a
"one-prompt-for-all" approach, using a single prompt structure (e.g., "think
step by step") for a wide range of tasks -- from counting and sorting to
solving mathematical and algorithmic problems. This approach poses significant
challenges for models to generate the correct reasoning steps, as the model
must navigate through a vast prompt template space to find the appropriate
template for each task. In this work, we build upon previous theoretical
analyses of CoT to demonstrate how the one-prompt-for-all approach can
negatively affect the computability of LLMs. We partition the solution search
space into two: the prompt space and the answer space. Our findings show that
task-specific supervision is essential for navigating the prompt space
accurately and achieving optimal performance. Through experiments with
state-of-the-art LLMs, we reveal a gap in reasoning performance when
supervision is applied versus when it is not.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speciesism in Natural Language Processing Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masashi Takeshita, Rafal Rzepka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) research on AI Safety and social bias in AI
has focused on safety for humans and social bias against human minorities.
However, some AI ethicists have argued that the moral significance of nonhuman
animals has been ignored in AI research. Therefore, the purpose of this study
is to investigate whether there is speciesism, i.e., discrimination against
nonhuman animals, in NLP research. First, we explain why nonhuman animals are
relevant in NLP research. Next, we survey the findings of existing research on
speciesism in NLP researchers, data, and models and further investigate this
problem in this study. The findings of this study suggest that speciesism
exists within researchers, data, and models, respectively. Specifically, our
survey and experiments show that (a) among NLP researchers, even those who
study social bias in AI, do not recognize speciesism or speciesist bias; (b)
among NLP data, speciesist bias is inherent in the data annotated in the
datasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,
exhibit speciesist bias by default. Finally, we discuss how we can reduce
speciesism in NLP research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is a preprint and has not been peer-reviewed. The
  postprint has been accepted for publication in AI and Ethics. Please cite the
  final version of the article once it is published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaAlign: Align Large Language Models with Diverse Preferences during
  Inference Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) acquire extensive knowledge and remarkable
abilities from extensive text corpora, making them powerful tools for various
applications. To make LLMs more usable, aligning them with human preferences is
essential. Existing alignment techniques, such as Reinforcement Learning from
Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed
predefined preferences directly within the model's parameters. These methods,
however, often result in a static alignment that can not account for the
diversity of human preferences in practical applications. In response to this
challenge, we propose an effective method, \textbf{MetaAlign}, which aims to
help LLMs dynamically align with various explicit or implicit preferences
specified at inference time. Experimental results show that LLMs optimized on
our meticulously constructed MetaAlign Dataset can effectively align with any
preferences specified at the inference stage, validating the feasibility of
MetaAlign. We hope that our work can provide some insights into the alignment
of language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Zhou, Jingdong Yang, Kehan Guo, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laboratory accidents pose significant risks to human life and property,
underscoring the importance of robust safety protocols. Despite advancements in
safety training, laboratory personnel may still unknowingly engage in unsafe
practices. With the increasing reliance on large language models (LLMs) for
guidance in various fields, including laboratory settings, there is a growing
concern about their reliability in critical safety-related decision-making.
Unlike trained human researchers, LLMs lack formal lab safety education,
raising questions about their ability to provide safe and accurate guidance.
Existing research on LLM trustworthiness primarily focuses on issues such as
ethical compliance, truthfulness, and fairness but fails to fully cover
safety-critical real-world applications, like lab safety. To address this gap,
we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive
evaluation framework based on a new taxonomy aligned with Occupational Safety
and Health Administration (OSHA) protocols. This benchmark includes 765
multiple-choice questions verified by human experts, assessing LLMs and vision
language models (VLMs) performance in lab safety contexts. Our evaluations
demonstrate that while GPT-4o outperforms human participants, it is still prone
to critical errors, highlighting the risks of relying on LLMs in
safety-critical environments. Our findings emphasize the need for specialized
benchmarks to accurately assess the trustworthiness of LLMs in real-world
safety applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XForecast: Evaluating Natural Language Explanations for Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Aksu, Chenghao Liu, Amrita Saha, Sarah Tan, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting aids decision-making, especially for stakeholders who
rely on accurate predictions, making it very important to understand and
explain these models to ensure informed decisions. Traditional explainable AI
(XAI) methods, which underline feature or temporal importance, often require
expert knowledge. In contrast, natural language explanations (NLEs) are more
accessible to laypeople. However, evaluating forecast NLEs is difficult due to
the complex causal relationships in time series data. To address this, we
introduce two new performance metrics based on simulatability, assessing how
well a human surrogate can predict model forecasts using the explanations.
Experiments show these metrics differentiate good from poor explanations and
align with human judgments. Utilizing these metrics, we further evaluate the
ability of state-of-the-art large language models (LLMs) to generate
explanations for time series data, finding that numerical reasoning, rather
than model size, is the main factor influencing explanation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated impressive
abilities across various tasks, including visual question answering and chart
comprehension, yet existing benchmarks for chart-related tasks fall short in
capturing the complexity of real-world multi-chart scenarios. Current
benchmarks primarily focus on single-chart tasks, neglecting the multi-hop
reasoning required to extract and integrate information from multiple charts,
which is essential in practical applications. To fill this gap, we introduce
MultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:
direct question answering, parallel question answering, comparative reasoning,
and sequential reasoning. Our evaluation of a wide range of MLLMs reveals
significant performance gaps compared to humans. These results highlight the
challenges in multi-chart comprehension and the potential of MultiChartQA to
drive advancements in this field. Our code and data are available at
https://github.com/Zivenzhu/Multi-chart-QA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with
  Simple Word-based Counting Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Xu, Xuezhe Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interestingly, LLMs yet struggle with some basic tasks that humans find
trivial to handle, e.g., counting the number of character r's in the word
"strawberry". There are several popular conjectures (e.g., tokenization,
architecture and training data) regarding the reason for deficiency of LLMs in
simple word-based counting problems, sharing the similar belief that such
failure stems from model pretraining hence probably inevitable during
deployment. In this paper, we carefully design multiple evaluation settings to
investigate validity of prevalent conjectures. Meanwhile, we measure
transferability of advanced mathematical and coding reasoning capabilities from
specialized LLMs to simple counting tasks. Although specialized LLMs suffer
from counting problems as well, we find conjectures about inherent deficiency
of LLMs invalid and further seek opportunities to elicit knowledge and
capabilities from LLMs that are beneficial to counting tasks. Compared with
strategies such as finetuning and in-context learning that are commonly adopted
to enhance performance on new or challenging tasks, we show that engaging
reasoning is the most robust and efficient way to help LLMs better perceive
tasks with more accurate responses.
  We hope our conjecture validation design could provide insights into the
study of future critical failure modes of LLMs. Based on challenges in
transferring advanced capabilities to much simpler tasks, we call for more
attention to model capability acquisition and evaluation. We also highlight the
importance of cultivating consciousness of "reasoning before responding" during
model pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Genre-Aware Article Scoring and Feedback Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, Jiajing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the development of an advanced intelligent article
scoring system that not only assesses the overall quality of written work but
also offers detailed feature-based scoring tailored to various article genres.
By integrating the pre-trained BERT model with the large language model
Chat-GPT, the system gains a deep understanding of both the content and
structure of the text, enabling it to provide a thorough evaluation along with
targeted suggestions for improvement. Experimental results demonstrate that
this system outperforms traditional scoring methods across multiple public
datasets, particularly in feature-based assessments, offering a more accurate
reflection of the quality of different article types. Moreover, the system
generates personalized feedback to assist users in enhancing their writing
skills, underscoring the potential and practical value of automated scoring
technologies in educational contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Autoregression: Discrete Diffusion for Complex Reasoning and
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive language models, despite their impressive capabilities,
struggle with complex reasoning and long-term planning tasks. We introduce
discrete diffusion models as a novel solution to these challenges. Through the
lens of subgoal imbalance, we demonstrate how diffusion models effectively
learn difficult subgoals that elude autoregressive approaches. We propose
Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on
difficulty during learning. On complex tasks like Countdown, Sudoku, and
Boolean Satisfiability Problems, MDM significantly outperforms autoregressive
models without using search techniques. For instance, MDM achieves 91.5\% and
100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and
20.7\% for autoregressive models. Our work highlights the potential of
diffusion-based approaches in advancing AI capabilities for sophisticated
language understanding and problem-solving tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Faithful Natural Language Explanations: A Study Using Activation
  Patching in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jie Yeo, Ranjan Satapthy, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are capable of generating persuasive Natural
Language Explanations (NLEs) to justify their answers. However, the
faithfulness of these explanations should not be readily trusted at face value.
Recent studies have proposed various methods to measure the faithfulness of
NLEs, typically by inserting perturbations at the explanation or feature level.
We argue that these approaches are neither comprehensive nor correctly designed
according to the established definition of faithfulness. Moreover, we highlight
the risks of grounding faithfulness findings on out-of-distribution samples. In
this work, we leverage a causal mediation technique called activation patching,
to measure the faithfulness of an explanation towards supporting the explained
answer. Our proposed metric, Causal Faithfulness quantifies the consistency of
causal attributions between explanations and the corresponding model outputs as
the indicator of faithfulness. We experimented across models varying from 2B to
27B parameters and found that models that underwent alignment tuning tend to
produce more faithful and plausible explanations. We find that Causal
Faithfulness is a promising improvement over existing faithfulness tests by
taking into account the model's internal computations and avoiding out of
distribution concerns that could otherwise undermine the validity of
faithfulness assessments. We release the code in
\url{https://github.com/wj210/Causal-Faithfulness}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy
  with LLM-based Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Weiran Shen, Qi Qi, Yankai Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public scarce resource allocation plays a crucial role in economics as it
directly influences the efficiency and equity in society. Traditional studies
including theoretical model-based, empirical study-based and simulation-based
methods encounter limitations due to the idealized assumption of complete
information and individual rationality, as well as constraints posed by limited
available data. In this work, we propose an innovative framework, SRAP-Agent
(Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based
Agent), which integrates Large Language Models (LLMs) into economic
simulations, aiming to bridge the gap between theoretical models and real-world
dynamics. Using public housing allocation scenarios as a case study, we conduct
extensive policy simulation experiments to verify the feasibility and
effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm
with certain optimization objectives. The source code can be found in
https://github.com/jijiarui-cather/SRAPAgent_Framework
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Large Language Models for Event Deconstruction to Enhance
  Multimodal Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyong Huang, Heli Sun, Qunshu Gao, Wenjie Huang, Ruichen Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of the internet, the richness of User-Generated
Contentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis
(MABSA) a research hotspot. Existing studies have achieved certain results in
MABSA, but they have not effectively addressed the analytical challenges in
scenarios where multiple entities and sentiments coexist. This paper
innovatively introduces Large Language Models (LLMs) for event decomposition
and proposes a reinforcement learning framework for Multimodal Aspect-based
Sentiment Analysis (MABSA-RL) framework. This framework decomposes the original
text into a set of events using LLMs, reducing the complexity of analysis,
introducing reinforcement learning to optimize model parameters. Experimental
results show that MABSA-RL outperforms existing advanced methods on two
benchmark datasets. This paper provides a new research perspective and method
for multimodal aspect-level sentiment analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in
  Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in large language models (LLMs) and pre-trained
vision models have accelerated the development of vision-language large models
(VLLMs), enhancing the interaction between visual and linguistic modalities.
Despite their notable success across various domains, VLLMs face challenges in
modality alignment, which can lead to issues like hallucinations and unsafe
content generation. Current alignment techniques often rely on coarse feedback
and external datasets, limiting scalability and performance. In this paper, we
propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel
self-alignment method that utilizes the model's own visual encoder as a
fine-grained verifier to improve vision-language alignment without the need for
additional data. By leveraging token-level feedback from the vision encoder,
FiSAO significantly improves vision-language alignment, even surpassing
traditional preference tuning methods that require additional data. Through
both theoretical analysis and experimental validation, we demonstrate that
FiSAO effectively addresses the misalignment problem in VLLMs, marking the
first instance of token-level rewards being applied to such models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPE: A Chinese <span class="highlight-title">Dataset</span> for Appraisal-based Emotional Generation using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        June M. Liu, He Cao, Renliang Sun, Rui Wang, Yu Li, Jiaxing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating emotionally appropriate responses in conversations with large
language models presents a significant challenge due to the complexities of
human emotions and cognitive processes, which remain largely underexplored in
their critical role in social interactions. In this study, we introduce a
two-stage automatic data generation framework to create CAPE, a Chinese dataset
named Cognitive Appraisal theory-based Emotional corpus. This corpus
facilitates the generation of dialogues with contextually appropriate emotional
responses by accounting for diverse personal and situational factors. We
propose two tasks utilizing this dataset: emotion prediction and next utterance
prediction. Both automated and human evaluations demonstrate that agents
trained on our dataset can deliver responses that are more aligned with human
emotional expressions. Our study shows the potential for advancing emotional
expression in conversational agents, paving the way for more nuanced and
meaningful human-computer interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lightweight Multi Aspect Controlled Text Generation Solution For Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Zhang, Jiayi Lin, Haibo Tong, Bingxuan Hou, Dongyu Zhang, Jialin Li, Junli Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show remarkable abilities with instruction
tuning. However, they fail to achieve ideal tasks when lacking high-quality
instruction tuning data on target tasks. Multi-Aspect Controllable Text
Generation (MCTG) is a representative task for this dilemma, where aspect
datasets are usually biased and correlated. Existing work exploits additional
model structures and strategies for solutions, limiting adaptability to LLMs.
To activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based
on data augmentation. We analyze bias and correlations in traditional datasets,
and address these concerns with augmented control attributes and sentences.
Augmented datasets are feasible for instruction tuning. In our experiments,
LLMs perform better in MCTG after data augmentation, with a 20% accuracy rise
and less aspect correlations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coherence-Driven Multimodal Safety Dialogue with Active Learning for
  Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabit Hassan, Hye-Young Chung, Xiang Zhi Tan, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When assisting people in daily tasks, robots need to accurately interpret
visual cues and respond effectively in diverse safety-critical situations, such
as sharp objects on the floor. In this context, we present M-CoDAL, a
multimodal-dialogue system specifically designed for embodied agents to better
understand and communicate in safety-critical situations. The system leverages
discourse coherence relations to enhance its contextual understanding and
communication abilities. To train this system, we introduce a novel
clustering-based active learning mechanism that utilizes an external Large
Language Model (LLM) to identify informative instances. Our approach is
evaluated using a newly created multimodal dataset comprising 1K safety
violations extracted from 2K Reddit images. These violations are annotated
using a Large Multimodal Model (LMM) and verified by human annotators. Results
with this dataset demonstrate that our approach improves resolution of safety
situations, user sentiment, as well as safety of the conversation. Next, we
deploy our dialogue system on a Hello Robot Stretch robot and conduct a
within-subject user study with real-world participants. In the study,
participants role-play two safety scenarios with different levels of severity
with the robot and receive interventions from our model and a baseline system
powered by OpenAI's ChatGPT. The study results corroborate and extend the
findings from automated evaluation, showing that our proposed system is more
persuasive and competent in a real-world embodied agent setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViConsFormer: Constituting Meaningful Phrases of Scene Texts using
  <span class="highlight-title">Transformer</span>-based Method in Vietnamese Text-based Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Hieu Nguyen, Tho Thanh Quan, Ngan Luu-Thuy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based VQA is a challenging task that requires machines to use scene
texts in given images to yield the most appropriate answer for the given
question. The main challenge of text-based VQA is exploiting the meaning and
information from scene texts. Recent studies tackled this challenge by
considering the spatial information of scene texts in images via embedding 2D
coordinates of their bounding boxes. In this study, we follow the definition of
meaning from linguistics to introduce a novel method that effectively exploits
the information from scene texts written in Vietnamese. Experimental results
show that our proposed method obtains state-of-the-art results on two
large-scale Vietnamese Text-based VQA datasets. The implementation can be found
at this link.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan Cheng, Lijie Hu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The locate-then-edit paradigm has shown significant promise for knowledge
editing (KE) in Large Language Models (LLMs). While previous methods perform
well on single-hop fact recall tasks, they consistently struggle with multi-hop
factual recall tasks involving newly edited knowledge. In this paper,
leveraging tools in mechanistic interpretability, we first identify that in
multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper
MLP layers, unlike single-hop tasks, which rely on earlier layers. This
distinction explains the poor performance of current methods in multi-hop
queries, as they primarily focus on editing shallow layers, leaving deeper
layers unchanged. To address this, we propose IFMET, a novel locate-then-edit
KE approach designed to edit both shallow and deep MLP layers. IFMET employs
multi-hop editing prompts and supplementary sets to locate and modify knowledge
across different reasoning stages. Experimental results demonstrate that IFMET
significantly improves performance on multi-hop factual recall tasks,
effectively overcoming the limitations of previous locate-then-edit methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ System 2 thinking in OpenAI's o1-p<span class="highlight-title">review</span> model: Near-perfect performance
  on a mathematics exam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joost de Winter, Dimitra Dodou, Yke Bauke Eisma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The processes underlying human cognition are often divided into System 1,
which involves fast, intuitive thinking, and System 2, which involves slow,
deliberate reasoning. Previously, large language models were criticized for
lacking the deeper, more analytical capabilities of System 2. In September
2024, OpenAI introduced the o1 model series, designed to handle System 2-like
reasoning. While OpenAI's benchmarks are promising, independent validation is
still needed. In this study, we tested the o1-preview model twice on the Dutch
'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76
points. For context, only 24 out of 16,414 students in the Netherlands achieved
a perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,
well above the Dutch average of 40.63 points. Neither model had access to the
exam figures. Since there was a risk of model contamination (i.e., the
knowledge cutoff of o1-preview and GPT-4o was after the exam was published
online), we repeated the procedure with a new Mathematics B exam that was
published after the cutoff date. The results again indicated that o1-preview
performed strongly (97.8th percentile), which suggests that contamination was
not a factor. We also show that there is some variability in the output of
o1-preview, which means that sometimes there is 'luck' (the answer is correct)
or 'bad luck' (the output has diverged into something that is incorrect). We
demonstrate that a self-consistency approach, where repeated prompts are given
and the most common answer is selected, is a useful strategy for identifying
the correct answer. It is concluded that while OpenAI's new model series holds
great potential, certain risks must be considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Liger Kernel: Efficient Triton Kernels for LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Large Language Models (LLMs) efficiently at scale presents a
formidable challenge, driven by their ever-increasing computational demands and
the need for enhanced performance. In this work, we introduce Liger-Kernel, an
open-sourced set of Triton kernels developed specifically for LLM training.
With kernel optimization techniques like kernel operation fusing and input
chunking, our kernels achieve on average a 20% increase in training throughput
and a 60% reduction in GPU memory usage for popular LLMs compared to
HuggingFace implementations. In addition, Liger-Kernel is designed with
modularity, accessibility, and adaptability in mind, catering to both casual
and expert users. Comprehensive benchmarks and integration tests are built in
to ensure compatibility, performance, correctness, and convergence across
diverse computing environments and model architectures.
  The source code is available under a permissive license at:
github.com/linkedin/Liger-Kernel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Context</span>ual Document Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John X. Morris, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense document embeddings are central to neural retrieval. The dominant
paradigm is to train and construct embeddings by running encoders directly on
individual documents. In this work, we argue that these embeddings, while
effective, are implicitly out-of-context for targeted use cases of retrieval,
and that a contextualized document embedding should take into account both the
document and neighboring documents in context - analogous to contextualized
word embeddings. We propose two complementary methods for contextualized
document embeddings: first, an alternative contrastive learning objective that
explicitly incorporates the document neighbors into the intra-batch contextual
loss; second, a new contextual architecture that explicitly encodes neighbor
document information into the encoded representation. Results show that both
methods achieve better performance than biencoders in several settings, with
differences especially pronounced out-of-domain. We achieve state-of-the-art
results on the MTEB benchmark with no hard negative mining, score distillation,
dataset-specific instructions, intra-GPU example-sharing, or extremely large
batch sizes. Our method can be applied to improve performance on any
contrastive learning dataset and any biencoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Linear Attention in Polynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris Yau, Ekin Akyürek, Jiayuan Mao, Joshua B. Tenenbaum, Stefanie Jegelka, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has explored the computational expressivity of Transformer
models in simulating Boolean circuits or Turing machines. However, the
learnability of these simulators from observational data has remained an open
question. Our study addresses this gap by providing the first polynomial-time
learnability results (specifically strong, agnostic PAC learning) for
single-layer Transformers with linear attention. We show that linear attention
may be viewed as a linear predictor in a suitably defined RKHS. As a
consequence, the problem of learning any linear transformer may be converted
into the problem of learning an ordinary linear predictor in an expanded
feature space, and any such predictor may be converted back into a multiheaded
linear transformer. Moving to generalization, we show how to efficiently
identify training datasets for which every empirical risk minimizer is
equivalent (up to trivial symmetries) to the linear Transformer that generated
the data, thereby guaranteeing the learned model will correctly generalize
across all inputs. Finally, we provide examples of computations expressible via
linear attention and therefore polynomial-time learnable, including associative
memories, finite automata, and a class of Universal Turing Machine (UTMs) with
polynomially bounded computation histories. We empirically validate our
theoretical findings on three tasks: learning random linear attention networks,
key--value associations, and learning to execute finite automata. Our findings
bridge a critical gap between theoretical expressivity and learnability of
Transformers, and show that flexible and general models of computation are
efficiently learnable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One size doesn't fit all: Predicting the Number of Examples for
  In-<span class="highlight-title">Context</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Chandra, Debasis Ganguly, Iadh Ounis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) refers to the process of adding a small number of
localized examples (ones that are semantically similar to the input) from a
training set of labelled data to an LLM's prompt with an objective to
effectively control the generative process seeking to improve the downstream
task performance. Existing ICL approaches use an identical number of examples
(a pre-configured hyper-parameter) for each data instance. Our work alleviates
the limitations of this 'one fits all' approach by dynamically predicting the
number of examples for each data instance to be used in few-shot inference with
LLMs. In particular, we employ a multi-label classifier, the parameters of
which are fitted using a training set, where the label for each instance in the
training set indicates if using a specific value of k (number of most similar
examples from 0 up to a maximum value) leads to correct k-shot downstream
predictions. Our experiments on a number of text classification benchmarks show
that AICL substantially outperforms standard ICL by up to 17%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movie101v2: Improved Movie Narration Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Yue, Yepeng Zhang, Ziheng Wang, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic movie narration aims to generate video-aligned plot descriptions to
assist visually impaired audiences. Unlike standard video captioning, it
involves not only describing key visual details but also inferring plots that
unfold across multiple movie shots, presenting distinct and complex challenges.
To advance this field, we introduce Movie101v2, a large-scale, bilingual
dataset with enhanced data quality specifically designed for movie narration.
Revisiting the task, we propose breaking down the ultimate goal of automatic
movie narration into three progressive stages, offering a clear roadmap with
corresponding evaluation metrics. Based on our new benchmark, we baseline a
range of large vision-language models, including GPT-4V, and conduct an
in-depth analysis of the challenges in narration generation. Our findings
highlight that achieving applicable movie narration generation is a fascinating
goal that requires significant research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCQG-SRefine: Multiple Choice Question Generation and Evaluation with
  Iterative Self-Critique, Correction, and Comparison Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic question generation (QG) is essential for AI and NLP, particularly
in intelligent tutoring, dialogue systems, and fact verification. Generating
multiple-choice questions (MCQG) for professional exams, like the United States
Medical Licensing Examination (USMLE), is particularly challenging, requiring
domain expertise and complex multi-hop reasoning for high-quality questions.
However, current large language models (LLMs) like GPT-4 struggle with
professional MCQG due to outdated knowledge, hallucination issues, and prompt
sensitivity, resulting in unsatisfactory quality and difficulty. To address
these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique
and Correction) framework for converting medical cases into high-quality
USMLE-style questions. By integrating expert-driven prompt engineering with
iterative self-critique and self-correction feedback, MCQG-SRefine
significantly enhances human expert satisfaction regarding both the quality and
difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based
automatic metric to replace the complex and costly expert evaluation process,
ensuring reliable and expert-aligned assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution for the first two authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advocating Character Error Rate for Multilingual ASR Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thennal D K, Jesin James, Deepa P Gopinath, Muhammed Ashraf K
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems have traditionally been evaluated
using English datasets, with the word error rate (WER) serving as the
predominant metric. WER's simplicity and ease of interpretation have
contributed to its widespread adoption, particularly for English. However, as
ASR systems expand to multilingual contexts, WER fails in various ways,
particularly with morphologically complex languages or those without clear word
boundaries. Our work documents the limitations of WER as an evaluation metric
and advocates for the character error rate (CER) as the primary metric in
multilingual ASR evaluation. We show that CER avoids many of the challenges WER
faces and exhibits greater consistency across writing systems. We support our
proposition by conducting human evaluations of ASR transcriptions in three
languages: Malayalam, English, and Arabic, which exhibit distinct morphological
characteristics. We show that CER correlates more closely with human judgments
than WER, even for English. To facilitate further research, we release our
human evaluation dataset for future benchmarking of ASR metrics. Our findings
suggest that CER should be prioritized, or at least supplemented, in
multilingual ASR evaluations to account for the varying linguistic
characteristics of different languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ English offensive text detection using CNN based Bi-GRU model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonmoy Roy, Md Robiul Islam, Asif Ahammad Miazee, Anika Antara, Al Amin, Sunjim Hossain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, the number of users of social media has increased
drastically. People frequently share their thoughts through social platforms,
and this leads to an increase in hate content. In this virtual community,
individuals share their views, express their feelings, and post photos, videos,
blogs, and more. Social networking sites like Facebook and Twitter provide
platforms to share vast amounts of content with a single click. However, these
platforms do not impose restrictions on the uploaded content, which may include
abusive language and explicit images unsuitable for social media. To resolve
this issue, a new idea must be implemented to divide the inappropriate content.
Numerous studies have been done to automate the process. In this paper, we
propose a new Bi-GRU-CNN model to classify whether the text is offensive or
not. The combination of the Bi-GRU and CNN models outperforms the existing
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages and 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Reward Models with Synthetic Critiques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Matthias Gallé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) play a critical role in aligning language models through
the process of reinforcement learning from human feedback. RMs are trained to
predict a score reflecting human preference, which requires significant time
and cost for human annotation. Additionally, RMs tend to quickly overfit on
superficial features in the training set, hindering their generalization
performance on unseen distributions. We propose a novel approach using
synthetic natural language critiques generated by large language models to
provide additional feedback, evaluating aspects such as instruction following,
correctness, and style. This offers richer signals and more robust features for
RMs to assess and score on. We demonstrate that high-quality critiques improve
the performance and data efficiency of RMs initialized from different
pretrained models, reducing the reliance on costly human annotations.
Furthermore, incorporating critiques improves both the interpretability and
robustness of RM training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ With Ears to See and Eyes to Hear: Sound Symbolism Experiments with
  Multimodal Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Loakman, Yucheng Li, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have
demonstrated aptitude as potential substitutes for human participants in
experiments testing psycholinguistic phenomena. However, an understudied
question is to what extent models that only have access to vision and text
modalities are able to implicitly understand sound-based phenomena via abstract
reasoning from orthography and imagery alone. To investigate this, we analyse
the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise
a non-arbitrary link between sounds and concepts) as well as their ability to
"hear" via the interplay of the language and vision modules of open and
closed-source multimodal models. We perform multiple experiments, including
replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism
tasks, and comparing human judgements of linguistic iconicity with that of
LLMs. Our results show that VLMs demonstrate varying levels of agreement with
human labels, and more task information may be required for VLMs versus their
human counterparts for in silico experimentation. We additionally see through
higher maximum agreement levels that Magnitude Symbolism is an easier pattern
for VLMs to identify than Shape Symbolism, and that an understanding of
linguistic iconicity is highly dependent on model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crossroads of Continents: Automated Artifact Extraction for Cultural
  Adaptation with Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjishnu Mukherjee, Ziwei Zhu, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive three-phase study to examine (1) the cultural
understanding of Large Multimodal Models (LMMs) by introducing DalleStreet, a
large-scale dataset generated by DALL-E 3 and validated by humans, containing
9,935 images of 67 countries and 10 concept classes; (2) the underlying
implicit and potentially stereotypical cultural associations with a cultural
artifact extraction task; and (3) an approach to adapt cultural representation
in an image based on extracted associations using a modular pipeline,
CultureAdapt. We find disparities in cultural understanding at geographic
sub-region levels with both open-source (LLaVA) and closed-source (GPT-4V)
models on DalleStreet and other existing benchmarks, which we try to understand
using over 18,000 artifacts that we identify in association to different
countries. Our findings reveal a nuanced picture of the cultural competence of
LMMs, highlighting the need to develop culture-aware systems. Dataset and code
are available at https://github.com/iamshnoo/crossroads
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What's under the hood: Investigating Automatic Metrics on Meeting
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederic Kirstein, Jan Philip Wahle, Terry Ruas, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meeting summarization has become a critical task considering the increase in
online interactions. While new techniques are introduced regularly, their
evaluation uses metrics not designed to capture meeting-specific errors,
undermining effective evaluation. This paper investigates what the frequently
used automatic metrics capture and which errors they mask by correlating
automatic metric scores with human evaluations across a broad error taxonomy.
We commence with a comprehensive literature review on English meeting
summarization to define key challenges like speaker dynamics and contextual
turn-taking and error types such as missing information and linguistic
inaccuracy, concepts previously loosely defined in the field. We examine the
relationship between characteristic challenges and errors by using annotated
transcripts and summaries from Transformer-based sequence-to-sequence and
autoregressive models from the general summary QMSum dataset. Through
experimental validation, we find that different model architectures respond
variably to challenges in meeting transcripts, resulting in different
pronounced links between challenges and errors. Current default-used metrics
struggle to capture observable errors, showing weak to mid-correlations, while
a third of the correlations show trends of error masking. Only a subset reacts
accurately to specific errors, while most correlations show either
unresponsiveness or failure to reflect the error's impact on summary quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Debiasing Text Embeddings Through <span class="highlight-title">Context</span> Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Uriot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current advances in Natural Language Processing (NLP) have made it
increasingly feasible to build applications leveraging textual data. Generally,
the core of these applications rely on having a good semantic representation of
text into vectors, via embedding models. However, it has been shown that these
embeddings capture and perpetuate biases already present in text. While a few
techniques have been proposed to debias embeddings, they do not take advantage
of the recent advances in context understanding of modern embedding models. In
this paper, we fill this gap by conducting a review of 19 embedding models by
quantifying their biases and how well they respond to context injection as a
mean of debiasing. We show that higher performing models are more prone to
capturing biases, but are also better at incorporating context. Surprisingly,
we find that while models can easily embed affirmative semantics, they fail at
embedding neutral semantics. Finally, in a retrieval task, we show that biases
in embeddings can lead to non-desirable outcomes. We use our new-found insights
to design a simple algorithm for top $k$ retrieval, where $k$ is dynamically
selected. We show that our algorithm is able to retrieve all relevant gendered
and neutral chunks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Train & Constrain: Phonologically Informed Tongue-Twister Generation
  from Topics and Paraphrases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Loakman, Chen Tang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work in phonologically and phonetically grounded language generation
has mainly focused on domains such as puns and poetry. In this article, we
present new work on the generation of English tongue twisters - a form of
language that is required to be conditioned on a phoneme level to maximize
sound overlap, while maintaining semantic consistency with an input topic or
phrase and still being grammatically correct. We present TwisterLister, a
pipeline for generating phonologically informed tongue twisters from large
language models (LLMs) that we use to generate TwistList 2.0, the largest
annotated dataset of tongue twisters to date, consisting of 17K+ examples from
a combination of human and LLM authors. Our generation pipeline involves the
use of a phonologically constrained vocabulary alongside LLM prompting to
generate novel, non-derivative tongue twister examples. We additionally present
the results of automatic and human evaluation of smaller models trained on our
generated dataset to demonstrate the extent to which phonologically motivated
language types can be generated without explicit injection of phonological
knowledge. Additionally, we introduce a phoneme-aware constrained decoding
module (PACD) that can be integrated into an autoregressive language model and
demonstrate that this method generates good quality tongue twisters both with
and without fine-tuning the underlying language model. We also design and
implement a range of automatic metrics for the task of tongue twister
generation that is phonologically motivated and captures the unique essence of
tongue twisters, primarily based on phonemic edit distance (PED)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Final Version to Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Error Span Annotation: A Balanced Approach for Human Evaluation of
  Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popović, Mrinmaya Sachan, Mariya Shmatova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality Machine Translation (MT) evaluation relies heavily on human
judgments. Comprehensive error classification methods, such as Multidimensional
Quality Metrics (MQM), are expensive as they are time-consuming and can only be
done by experts, whose availability may be limited especially for low-resource
languages. On the other hand, just assigning overall scores, like Direct
Assessment (DA), is simpler and faster and can be done by translators of any
level, but is less reliable. In this paper, we introduce Error Span Annotation
(ESA), a human evaluation protocol which combines the continuous rating of DA
with the high-level error severity span marking of MQM. We validate ESA by
comparing it to MQM and DA for 12 MT systems and one human reference
translation (English to German) from WMT23. The results show that ESA offers
faster and cheaper annotations than MQM at the same quality level, without the
requirement of expensive MQM experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlackDAN: A Black-Box Multi-Objective Approach for Effective and
  <span class="highlight-title">Context</span>ual Jailbreaking of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Wang, Victor Shea-Jay Huang, Renmiao Chen, Hao Wang, Chengwei Pan, Lei Sha, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) exhibit remarkable capabilities across
various tasks, they encounter potential security risks such as jailbreak
attacks, which exploit vulnerabilities to bypass security measures and generate
harmful outputs. Existing jailbreak strategies mainly focus on maximizing
attack success rate (ASR), frequently neglecting other critical factors,
including the relevance of the jailbreak response to the query and the level of
stealthiness. This narrow focus on single objectives can result in ineffective
attacks that either lack contextual relevance or are easily recognizable. In
this work, we introduce BlackDAN, an innovative black-box attack framework with
multi-objective optimization, aiming to generate high-quality prompts that
effectively facilitate jailbreaking while maintaining contextual relevance and
minimizing detectability. BlackDAN leverages Multiobjective Evolutionary
Algorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks
across multiple objectives including ASR, stealthiness, and semantic relevance.
By integrating mechanisms like mutation, crossover, and Pareto-dominance,
BlackDAN provides a transparent and interpretable process for generating
jailbreaks. Furthermore, the framework allows customization based on user
preferences, enabling the selection of prompts that balance harmfulness,
relevance, and other factors. Experimental results demonstrate that BlackDAN
outperforms traditional single-objective methods, yielding higher success rates
and improved robustness across various LLMs and multimodal LLMs, while ensuring
jailbreak responses are both relevant and less detectable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Retrieval in Sponsored Search by Leveraging Query <span class="highlight-title">Context</span>
  Signals <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately retrieving relevant bid keywords for user queries is critical in
Sponsored Search but remains challenging, particularly for short, ambiguous
queries. Existing dense and generative retrieval models often fail to capture
nuanced user intent in these cases. To address this, we propose an approach to
enhance query understanding by augmenting queries with rich contextual signals
derived from web search results and large language models, stored in an online
cache. Specifically, we use web search titles and snippets to ground queries in
real-world information and utilize GPT-4 to generate query rewrites and
explanations that clarify user intent. These signals are efficiently integrated
through a Fusion-in-Decoder based Unity architecture, enabling both dense and
generative retrieval with serving costs on par with traditional context-free
models. To address scenarios where context is unavailable in the cache, we
introduce context glancing, a curriculum learning strategy that improves model
robustness and performance even without contextual signals during inference.
Extensive offline experiments demonstrate that our context-aware approach
substantially outperforms context-free models. Furthermore, online A/B testing
on a prominent search engine across 160+ countries shows significant
improvements in user engagement and revenue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Internals-based Answer Attribution for Trustworthy
  Retrieval-Augmented Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13663v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13663v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jirui Qi, Gabriele Sarti, Raquel Fernández, Arianna Bisazza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the verifiability of model answers is a fundamental challenge for
retrieval-augmented generation (RAG) in the question answering (QA) domain.
Recently, self-citation prompting was proposed to make large language models
(LLMs) generate citations to supporting documents along with their answers.
However, self-citing LLMs often struggle to match the required format, refer to
non-existent sources, and fail to faithfully reflect LLMs' context usage
throughout the generation. In this work, we present MIRAGE --Model
Internals-based RAG Explanations -- a plug-and-play approach using model
internals for faithful answer attribution in RAG applications. MIRAGE detects
context-sensitive answer tokens and pairs them with retrieved documents
contributing to their prediction via saliency methods. We evaluate our proposed
approach on a multilingual extractive QA dataset, finding high agreement with
human answer attribution. On open-ended QA, MIRAGE achieves citation quality
and efficiency comparable to self-citation while also allowing for a
finer-grained control of attribution parameters. Our qualitative evaluation
highlights the faithfulness of MIRAGE's attributions and underscores the
promising application of model internals for RAG answer attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference. Code and data released at
  https://github.com/Betswish/MIRAGE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:
  The First Romanian Natural Language Inference Corpus <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11877v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11877v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Poesina, Cornelia Caragea, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language inference (NLI), the task of recognizing the entailment
relationship in sentence pairs, is an actively studied topic serving as a proxy
for natural language understanding. Despite the relevance of the task in
building conversational agents and improving text classification, machine
translation and other NLP tasks, to the best of our knowledge, there is no
publicly available NLI corpus for the Romanian language. To this end, we
introduce the first Romanian NLI corpus (RoNLI) comprising 58K training
sentence pairs, which are obtained via distant supervision, and 6K validation
and test sentence pairs, which are manually annotated with the correct labels.
We conduct experiments with multiple machine learning methods based on distant
learning, ranging from shallow models based on word embeddings to
transformer-based neural networks, to establish a set of competitive baselines.
Furthermore, we improve on the best model by employing a new curriculum
learning strategy based on data cartography. Our dataset and code to reproduce
the baselines are available at https://github.com/Eduard6421/RONLI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Life<span class="highlight-title">long</span> Dialogue Agents via Relation-aware Memory Construction
  and Timeline-augmented Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Tzu-iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve lifelong human-agent interaction, dialogue agents need to
constantly memorize perceived information and properly retrieve it for response
generation (RG). While prior work focuses on getting rid of outdated memories
to improve retrieval quality, we argue that such memories provide rich,
important contextual cues for RG (e.g., changes in user behaviors) in long-term
conversations. We present Theanine, a framework for LLM-based lifelong dialogue
agents. Theanine discards memory removal and manages large-scale memories by
linking them based on their temporal and cause-effect relation. Enabled by this
linking structure, Theanine augments RG with memory timelines - series of
memories representing the evolution or causality of relevant past events. Along
with Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,
addressing the limitation of G-Eval and human efforts in measuring
memory-augmented dialogue agents. A supplementary video for Theanine and data
for TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models, scientific knowledge and factuality: A framework
  to streamline human expert evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalena Wysocka, Oskar Wysocki, Maxime Delmas, Vincent Mutel, Andre Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper introduces a framework for the evaluation of the encoding of
factual scientific knowledge, designed to streamline the manual evaluation
process typically conducted by domain experts. Inferring over and extracting
information from Large Language Models (LLMs) trained on a large corpus of
scientific literature can potentially define a step change in biomedical
discovery, reducing the barriers for accessing and integrating existing medical
evidence. This work explores the potential of LLMs for dialoguing with
biomedical background knowledge, using the context of antibiotic discovery. The
framework involves of three evaluation steps, each assessing different aspects
sequentially: fluency, prompt alignment, semantic coherence, factual knowledge,
and specificity of the generated responses. By splitting these tasks between
non-experts and experts, the framework reduces the effort required from the
latter. The work provides a systematic assessment on the ability of eleven
state-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two
prompting-based tasks: chemical compound definition generation and chemical
compound-fungus relation determination. Although recent models have improved in
fluency, factual accuracy is still low and models are biased towards
over-represented entities. The ability of LLMs to serve as biomedical knowledge
bases is questioned, and the need for additional systematic evaluation
frameworks is highlighted. While LLMs are currently not fit for purpose to be
used as biomedical factual knowledge bases in a zero-shot setting, there is a
promising emerging property in the direction of factuality as the models become
domain specialised, scale-up in size and level of human feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Journal of Biomedical Informatics, Volume 158,
  October 2024, 104724</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-LLM QA with Embodied Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10918v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10918v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Vishnu Sashank Dorbala, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have grown in popularity due to their natural
language interface and pre trained knowledge, leading to rapidly increasing
success in question-answering (QA) tasks. More recently, multi-agent systems
with LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.
In these scenarios, the models may each answer the question and reach a
consensus or each model is specialized to answer different domain questions.
However, most prior work dealing with Multi-LLM QA has focused on scenarios
where the models are asked in a zero-shot manner or are given information
sources to extract the answer. For question answering of an unknown
environment, embodied exploration of the environment is first needed to answer
the question. This skill is necessary for personalizing embodied AI to
environments such as households. There is a lack of insight into whether a
Multi-LLM system can handle question-answering based on observations from
embodied exploration. In this work, we address this gap by investigating the
use of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.
Multiple LLM-based agents independently explore and then answer queries about a
household environment. We analyze different aggregation methods to generate a
single, final answer for each query: debating, majority voting, and training a
central answer module (CAM). Using CAM, we observe a $46\%$ higher accuracy
compared against the other non-learning-based aggregation methods. We provide
code and the query dataset for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Molecular<span class="highlight-title">GPT</span>: Open Large Language Model (LLM) for Few-Shot Molecular
  P<span class="highlight-title">rope</span>rty Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular property prediction (MPP) is a fundamental and crucial task in drug
discovery. However, prior methods are limited by the requirement for a large
number of labeled molecules and their restricted ability to generalize for
unseen and new tasks, both of which are essential for real-world applications.
To address these challenges, we present MolecularGPT for few-shot MPP. From a
perspective on instruction tuning, we fine-tune large language models (LLMs)
based on curated molecular instructions spanning over 1000 property prediction
tasks. This enables building a versatile and specialized LLM that can be
adapted to novel MPP tasks without any fine-tuning through zero- and few-shot
in-context learning (ICL). MolecularGPT exhibits competitive in-context
reasoning capabilities across 10 downstream evaluation datasets, setting new
benchmarks for few-shot molecular prediction tasks. More importantly, with just
two-shot examples, MolecularGPT can outperform standard supervised graph neural
network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM
baselines by up to 15.7% increase on classification accuracy and decrease of
17.9 on regression metrics (e.g., RMSE) under zero-shot. This study
demonstrates the potential of LLMs as effective few-shot molecular property
predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fundamental Trade-off in Aligned Language Models and its Relation to
  Sampling Adaptors <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naaman Tan, Josef Valvoda, Tianyu Liu, Anej Svete, Yanxia Qin, Kan Min-Yen, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relationship between the quality of a string, as judged by a human
reader, and its probability, $p(\boldsymbol{y})$ under a language model
undergirds the development of better language models. For example, many popular
algorithms for sampling from a language model have been conceived with the goal
of manipulating $p(\boldsymbol{y})$ to place higher probability on strings that
humans deem of high quality. In this article, we examine the
probability--quality relationship in language models explicitly aligned to
human preferences, e.g., through reinforcement learning through human feedback.
We show that, when sampling corpora from an aligned language model, there
exists a trade-off between the strings' average reward and average
log-likelihood under the prior language model, i.e., the same model before
alignment with human preferences. We provide a formal treatment of this
phenomenon and demonstrate how a choice of sampling adaptor allows for a
selection of how much likelihood we exchange for the reward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity Matching using Large Language Models <span class="chip">EDBT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11244v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11244v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ralph Peeters, Aaron Steiner, Christian Bizer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity matching is the task of deciding whether two entity descriptions refer
to the same real-world entity. Entity matching is a central step in most data
integration pipelines. Many state-of-the-art entity matching methods rely on
pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks
of these models for entity matching are that (i) the models require significant
amounts of task-specific training data and (ii) the fine-tuned models are not
robust concerning out-of-distribution entities. This paper investigates using
generative large language models (LLMs) as a less task-specific training
data-dependent and more robust alternative to PLM-based matchers. The study
covers hosted and open-source LLMs which can be run locally. We evaluate these
models in a zero-shot scenario and a scenario where task-specific training data
is available. We compare different prompt designs and the prompt sensitivity of
the models. We show that there is no single best prompt but that the prompt
needs to be tuned for each model/dataset combination. We further investigate
(i) the selection of in-context demonstrations, (ii) the generation of matching
rules, as well as (iii) fine-tuning LLMs using the same pool of training data.
Our experiments show that the best LLMs require no or only a few training
examples to perform comparably to PLMs that were fine-tuned using thousands of
examples. LLM-based matchers further exhibit higher robustness to unseen
entities. We show that GPT4 can generate structured explanations for matching
decisions and can automatically identify potential causes of matching errors by
analyzing explanations of wrong decisions. We demonstrate that the model can
generate meaningful textual descriptions of the identified error classes, which
can help data engineers to improve entity matching pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 28th International Conference on
  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN
  978-3-89318-098-1 on OpenProceedings.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding
  for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19700v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19700v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Nan Li, Jian Guan, Wei Wu, Zhengtao Yu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tables are ubiquitous across various domains for concisely representing
structured information. Empowering large language models (LLMs) to reason over
tabular data represents an actively explored direction. However, since typical
LLMs only support one-dimensional~(1D) inputs, existing methods often flatten
the two-dimensional~(2D) table structure into a sequence of tokens, which can
severely disrupt the spatial relationships and result in an inevitable loss of
vital contextual information. In this paper, we first empirically demonstrate
the detrimental impact of such flattening operations on the performance of LLMs
in capturing the spatial information of tables through two elaborate proxy
tasks. Subsequently, we introduce a simple yet effective positional encoding
method, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to
address this challenge. 2D-TPE enables each attention head to dynamically
select a permutation order of tokens within the context for attending to them,
where each permutation represents a distinct traversal mode for the table, such
as column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of
losing essential spatial information while preserving computational efficiency,
thus better preserving the table structure. Extensive experiments across five
benchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring
the importance of preserving the table structure for accurate table
comprehension. Comprehensive analysis further reveals the substantially better
scalability of 2D-TPE to large tables than baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAME: Towards Factual Multi-Task Model Editing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zeng, Yingyu Shan, Zeming Liu, Jiashu Yao, Yuhang Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) embed extensive knowledge and utilize it to
perform exceptionally well across various tasks. Nevertheless, outdated
knowledge or factual errors within LLMs can lead to misleading or incorrect
responses, causing significant issues in practical applications. To rectify the
fatal flaw without the necessity for costly model retraining, various model
editing approaches have been proposed to correct inaccurate knowledge within
LLMs in a cost-efficient way. To evaluate these model editing methods, previous
work introduced a series of datasets. However, most of the previous datasets
only contain fabricated data in a single format, which diverges from real-world
model editing scenarios, raising doubts about their usability in practice. To
facilitate the application of model editing in real-world scenarios, we propose
the challenge of practicality. To resolve such challenges and effectively
enhance the capabilities of LLMs, we present FAME, an factual, comprehensive,
and multi-task dataset, which is designed to enhance the practicality of model
editing. We then propose SKEME, a model editing method that uses a novel
caching mechanism to ensure synchronization with the real world. The
experiments demonstrate that SKEME performs excellently across various tasks
and scenarios, confirming its practicality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures. This paper has been accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Hyung-Il Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Speech Recognition (VSR) aims to infer speech into text depending on
lip movements alone. As it focuses on visual information to model the speech,
its performance is inherently sensitive to personal lip appearances and
movements, and this makes the VSR models show degraded performance when they
are applied to unseen speakers. In this paper, to remedy the performance
degradation of the VSR model on unseen speakers, we propose prompt tuning
methods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,
motivated by recent advances in Natural Language Processing (NLP), we finetune
prompts on adaptation data of target speakers instead of modifying the
pre-trained model parameters. Different from the previous prompt tuning methods
mainly limited to Transformer variant architecture, we explore different types
of prompts, the addition, the padding, and the concatenation form prompts that
can be applied to the VSR model which is composed of CNN and Transformer in
general. With the proposed prompt tuning, we show that the performance of the
pre-trained VSR model on unseen speakers can be largely improved by using a
small amount of adaptation data (e.g., less than 5 minutes), even if the
pre-trained model is already developed with large speaker variations. Moreover,
by analyzing the performance and parameters of different types of prompts, we
investigate when the prompt tuning is preferred over the finetuning methods.
The effectiveness of the proposed method is evaluated on both word- and
sentence-level VSR databases, LRW-ID and GRID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BANTH: A Multi-label Hate Speech Detection <span class="highlight-title">Dataset</span> for Transliterated
  Bangla 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of transliterated texts in digital spaces has emphasized
the need for detecting and classifying hate speech in languages beyond English,
particularly in low-resource languages. As online discourse can perpetuate
discrimination based on target groups, e.g. gender, religion, and origin,
multi-label classification of hateful content can help in comprehending hate
motivation and enhance content moderation. While previous efforts have focused
on monolingual or binary hate classification tasks, no work has yet addressed
the challenge of multi-label hate speech classification in transliterated
Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate
speech dataset comprising 37.3k samples. The samples are sourced from YouTube
comments, where each instance is labeled with one or more target groups,
reflecting the regional demographic. We establish novel transformer
encoder-based baselines by further pre-training on transliterated Bangla
corpus. We also propose a novel translation-based LLM prompting strategy for
transliterated text. Experiments reveal that our further pre-trained encoders
are achieving state-of-the-art performance on the BanTH dataset, while our
translation-based prompting outperforms other strategies in the zero-shot
setting. The introduction of BanTH not only fills a critical gap in hate speech
research for Bangla but also sets the stage for future exploration into
code-mixed and multi-label classification challenges in underrepresented
languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Likelihood Over-optimisation in Direct Alignment
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyan Shi, Sander Land, Acyr Locatelli, Matthieu Geist, Max Bartolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation
(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives
to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as
Proximal Policy Optimisation (PPO) for aligning language models to human
preferences, without the need for explicit reward modelling. These methods
generally aim to increase the likelihood of generating better (preferred)
completions while discouraging worse (non-preferred) ones, while staying close
to the original model's behaviour. In this work, we explore the relationship
between completion likelihood and model performance in state-of-the-art DAAs,
and identify a critical issue of likelihood over-optimisation. Contrary to
expectations, we find that higher likelihood of better completions and larger
margins between better and worse completion likelihoods do not necessarily lead
to better performance, and may even degrade it. Our analysis reveals that while
higher likelihood correlates with better memorisation of factual knowledge
patterns, a slightly lower completion likelihood tends to improve output
diversity, thus leading to better generalisation to unseen scenarios. Moreover,
we identify two key indicators that signal when over-optimised output diversity
begins to harm performance: Decreasing Entropy over Top-k Tokens and
Diminishing Top-k Probability Mass. Our experimental results validate that
these indicators are reliable signs of declining performance under different
regularisations, helping prevent over-optimisation and improve alignment with
human preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaiBaam Annotation Guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Verena Blaschke, Barbara Kovačić, Siyao Peng, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This document provides the annotation guidelines for MaiBaam, a Bavarian
corpus manually annotated with part-of-speech (POS) tags, syntactic
dependencies, and German lemmas. MaiBaam belongs to the Universal Dependencies
(UD) project, and our annotations elaborate on the general and German UD
version 2 guidelines. In this document, we detail how to preprocess and
tokenize Bavarian data, provide an overview of the POS tags and dependencies we
use, explain annotation decisions that would also apply to closely related
languages like German, and lastly we introduce and motivate decisions that are
specific to Bavarian grammar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated for UD v2.15 (German lemmas added)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangru Zhu, Penglei Sun, Yaoxian Song, Yanghua Xiao, Zhixu Li, Chengyu Wang, Jun Huang, Bei Yang, Xiaoxiao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate interpretation and visualization of human instructions are crucial
for text-to-image (T2I) synthesis. However, current models struggle to capture
semantic variations from word order changes, and existing evaluations, relying
on indirect metrics like text-image similarity, fail to reliably assess these
challenges. This often obscures poor performance on complex or uncommon
linguistic patterns by the focus on frequent word combinations. To address
these deficiencies, we propose a novel metric called SemVarEffect and a
benchmark named SemVarBench, designed to evaluate the causality between
semantic variations in inputs and outputs in T2I synthesis. Semantic variations
are achieved through two types of linguistic permutations, while avoiding
easily predictable literal variations. Experiments reveal that the
CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.
Semantic variations in object relations are less understood than attributes,
scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in
UNet or Transformers plays a crucial role in handling semantic variations, a
factor previously overlooked by a focus on textual encoders. Our work
establishes an effective evaluation framework that advances the T2I synthesis
community's exploration of human instruction understanding. Our benchmark and
code are available at https://github.com/zhuxiangru/SemVarBench .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The only change in the current version update is the replacement of
  the template with a more precise one</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I run as fast as a rabbit, can you? A Multilingual Simile Dialogue
  <span class="highlight-title">Dataset</span> <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxuan Ma, Weinan Zhang, Shuhan Zhou, Churui Sun, Changxin Ke, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simile is a figure of speech that compares two different things (called the
tenor and the vehicle) via shared properties. The tenor and the vehicle are
usually connected with comparator words such as "like" or "as". The simile
phenomena are unique and complex in a real-life dialogue scene where the tenor
and the vehicle can be verbal phrases or sentences, mentioned by different
speakers, exist in different sentences, or occur in reversed order. However,
the current simile research usually focuses on similes in a triplet tuple
(tenor, property, vehicle) or a single sentence where the tenor and vehicle are
usually entities or noun phrases, which could not reflect complex simile
phenomena in real scenarios. In this paper, we propose a novel and high-quality
multilingual simile dialogue (MSD) dataset to facilitate the study of complex
simile phenomena. The MSD is the largest manually annotated simile data
($\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD
data can also be used on dialogue tasks to test the ability of dialogue systems
when using similes. We design 3 simile tasks (recognition, interpretation, and
generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each
task, we provide experimental results from strong pre-trained or
state-of-the-art models. The experiments demonstrate the challenge of MSD and
we have released the data/code on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages, 1 Figure, 12 Tables, ACL 2023 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Few-shot Work in <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span>? Recycling the <span class="highlight-title">Context</span> to Generate
  Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arie Cattan, Alon Jacovi, Alex Fabrikant, Jonathan Herzig, Roee Aharoni, Hannah Rashkin, Dror Marcus, Avinatan Hassidim, Yossi Matias, Idan Szpektor, Avi Caciularu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in Large Language Models (LLMs), their
performance on tasks involving long contexts remains sub-optimal. In-Context
Learning (ICL) with few-shot examples may be an appealing solution to enhance
LLM performance in this scenario; However, na\"ively adding ICL examples with
long context introduces challenges, including substantial token overhead added
for each few-shot example and context mismatch between the demonstrations and
the target query. In this work, we propose to automatically generate few-shot
examples for long context QA tasks by recycling contexts. Specifically, given a
long input context (1-3k tokens) and a query, we generate additional
query-output pairs from the given context as few-shot examples, while
introducing the context only once. This ensures that the demonstrations are
leveraging the same context as the target query while only adding a small
number of tokens to the prompt. We further enhance each demonstration by
instructing the model to explicitly identify the relevant paragraphs before the
answer, which improves performance while providing fine-grained attribution to
the answer source. We apply our method on multiple LLMs and obtain substantial
improvements (+16 absolute points on average across models) on various QA
datasets with long context, especially when the answer lies within the middle
of the context. Surprisingly, despite introducing only single-hop ICL examples,
LLMs also successfully generalize to multi-hop long-context QA using our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Verifiable Text Generation with Evolving Memory and
  Self-Reflection <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable ability of large language models (LLMs) in language
comprehension and generation, they often suffer from producing factually
incorrect information, also known as hallucination. A promising solution to
this issue is verifiable text generation, which prompts LLMs to generate
content with citations for accuracy verification. However, verifiable text
generation is non-trivial due to the focus-shifting phenomenon, the intricate
reasoning needed to align the claim with correct citations, and the dilemma
between the precision and breadth of retrieved documents. In this paper, we
present VTG, an innovative framework for Verifiable Text Generation with
evolving memory and self-reflection. VTG introduces evolving long short-term
memory to retain both valuable documents and recent documents. A two-tier
verifier equipped with an evidence finder is proposed to rethink and reflect on
the relationship between the claim and citations. Furthermore, active retrieval
and diverse query generation are utilized to enhance both the precision and
breadth of the retrieved documents. We conduct extensive experiments on five
datasets across three knowledge-intensive tasks and the results reveal that VTG
significantly outperforms baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48% improvement on
VisualWebBench and a 19.1% boost in element accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dating ancient manuscripts using radiocarbon and AI-based writing style
  analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mladen Popović, Maruf A. Dhali, Lambert Schomaker, Johannes van der Plicht, Kaare Lund Rasmussen, Jacopo La Nasa, Ilaria Degano, Maria Perla Colombini, Eibert Tigchelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the chronology of ancient handwritten manuscripts is essential
for reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is
particularly important. However, there is an almost complete lack of
date-bearing manuscripts evenly distributed across the timeline and written in
similar scripts available for palaeographic comparison. Here, we present Enoch,
a state-of-the-art AI-based date-prediction model, trained on the basis of new
radiocarbon-dated samples of the scrolls. Enoch uses established
handwriting-style descriptors and applies Bayesian ridge regression. The
challenge of this study is that the number of radiocarbon-dated manuscripts is
small, while current machine learning requires an abundance of training data.
We show that by using combined angular and allographic writing style feature
vectors and applying Bayesian ridge regression, Enoch could predict the
radiocarbon-based dates from style, supported by leave-one-out validation, with
varied MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was
then used to estimate the dates of 135 unseen manuscripts, revealing that 79
per cent of the samples were considered 'realistic' upon palaeographic post-hoc
evaluation. We present a new chronology of the scrolls. The radiocarbon ranges
and Enoch's style-based predictions are often older than the traditionally
assumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date
prediction provides an improved granularity. The study is in line with current
developments in multimodal machine-learning techniques, and the methods can be
used for date prediction in other partially-dated manuscript collections. This
research shows how Enoch's quantitative, probability-based approach can be a
tool for palaeographers and historians, re-dating ancient Jewish key texts and
contributing to current debates on Jewish and Christian origins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages of main article, 103 pages of supplementary materials; the
  first version of this article is originally prepared in July 2023 after the
  completion of all the experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Recommender System and Large Language Model Are Made for
  Each Other in E-commerce Pre-sales Dialogue <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxing Liu, Wei-Nan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng, Hengbin Cui, Yongbin Li, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce pre-sales dialogue aims to understand and elicit user needs and
preferences for the items they are seeking so as to provide appropriate
recommendations. Conversational recommender systems (CRSs) learn user
representation and provide accurate recommendations based on dialogue context,
but rely on external knowledge. Large language models (LLMs) generate responses
that mimic pre-sales dialogues after fine-tuning, but lack domain-specific
knowledge for accurate recommendations. Intuitively, the strengths of LLM and
CRS in E-commerce pre-sales dialogues are complementary, yet no previous work
has explored this. This paper investigates the effectiveness of combining LLM
and CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:
CRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a
real-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of
two collaborative approaches with two CRSs and two LLMs on four tasks of
Ecommerce pre-sales dialogue. We find that collaborations between CRS and LLM
can be very effective in some cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling and Mitigating Retriever Inconsistencies in
  Retrieval-Augmented Large Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20680v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20680v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Li, Xinyu Li, Yifan Chen, Wenfeng Xuan, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their
superiority in terms of factuality, they do not consistently outperform the
original retrieval-free Language Models (LMs). Our experiments reveal that this
example-level performance inconsistency exists not only between
retrieval-augmented and retrieval-free LM but also among different retrievers.
To understand this phenomenon, we investigate the degeneration behavior of
RALMs and theoretically decompose it into four categories. Further analysis
based on our decomposition reveals that the innate difference in knowledge
sources and the unpredictable degeneration of the reader model contribute most
to the inconsistency. Drawing from our analysis, we introduce Ensemble of
Retrievers (EoR), a trainable framework that can adaptively retrieve from
different knowledge sources and effectively decrease unpredictable reader
errors. Our experiments on Open Domain Question Answering show that EoR
substantially improves performance over the RALM with a single retriever by
considerably reducing inconsistent behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 (findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement
  on Multilingual and Multi-Cultural Data <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Manohar Swaminathan, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation of multilingual Large Language Models (LLMs) is challenging due to
a variety of factors -- the lack of benchmarks with sufficient linguistic
diversity, contamination of popular benchmarks into LLM pre-training data and
the lack of local, cultural nuances in translated benchmarks. In this work, we
study human and LLM-based evaluation in a multilingual, multi-cultural setting.
We evaluate 30 models across 10 Indic languages by conducting 90K human
evaluations and 30K LLM-based evaluations and find that models such as GPT-4o
and Llama-3 70B consistently perform best for most Indic languages. We build
leaderboards for two evaluation settings - pairwise comparison and direct
assessment and analyze the agreement between humans and LLMs. We find that
humans and LLMs agree fairly well in the pairwise setting but the agreement
drops for direct assessment evaluation especially for languages such as Bengali
and Odia. We also check for various biases in human and LLM-based evaluation
and find evidence of self-bias in the GPT-based evaluator. Our work presents a
significant step towards scaling up multilingual evaluation of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Network Enhanced Retrieval for Question Answering of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation has revolutionized large language model (LLM)
outputs by providing factual supports. Nevertheless, it struggles to capture
all the necessary knowledge for complex reasoning questions. Existing retrieval
methods typically divide reference documents into passages, treating them in
isolation. These passages, however, are often interrelated, such as passages
that are contiguous or share the same keywords. Therefore, it is crucial to
recognize such relatedness for enhancing the retrieval process. In this paper,
we propose a novel retrieval method, called GNN-Ret, which leverages graph
neural networks (GNNs) to enhance retrieval by exploiting the relatedness
between passages. Specifically, we first construct a graph of passages by
connecting passages that are structure-related or keyword-related. A graph
neural network (GNN) is then leveraged to exploit the relationships between
passages and improve the retrieval of supporting passages. Furthermore, we
extend our method to handle multi-hop reasoning questions using a recurrent
graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates
the graphs of passages from previous steps, thereby enhancing the retrieval of
supporting passages. Extensive experiments on benchmark datasets demonstrate
that GNN-Ret achieves higher accuracy for question answering with a single
query of LLMs than strong baselines that require multiple queries, and RGNN-Ret
further improves accuracy and achieves state-of-the-art performance, with up to
10.4% accuracy improvement on the 2WikiMQA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Use of Large Language Models to Generate Capability Ontologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17524v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17524v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Miguel Vieira da Silva, Aljosha Köcher, Felix Gehlhoff, Alexander Fay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capability ontologies are increasingly used to model functionalities of
systems or machines. The creation of such ontological models with all
properties and constraints of capabilities is very complex and can only be done
by ontology experts. However, Large Language Models (LLMs) have shown that they
can generate machine-interpretable models from natural language text input and
thus support engineers / ontology experts. Therefore, this paper investigates
how LLMs can be used to create capability ontologies. We present a study with a
series of experiments in which capabilities with varying complexities are
generated using different prompting techniques and with different LLMs. Errors
in the generated ontologies are recorded and compared. To analyze the quality
of the generated ontologies, a semi-automated approach based on RDF syntax
checking, OWL reasoning, and SHACL constraints is used. The results of this
study are very promising because even for complex capabilities, the generated
ontologies are almost free of errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Method to Generate Capability Ontologies from Natural Language
  Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Miguel Vieira da Silva, Aljosha Köcher, Felix Gehlhoff, Alexander Fay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve a flexible and adaptable system, capability ontologies are
increasingly leveraged to describe functions in a machine-interpretable way.
However, modeling such complex ontological descriptions is still a manual and
error-prone task that requires a significant amount of effort and ontology
expertise. This contribution presents an innovative method to automate
capability ontology modeling using Large Language Models (LLMs), which have
proven to be well suited for such tasks. Our approach requires only a natural
language description of a capability, which is then automatically inserted into
a predefined prompt using a few-shot prompting technique. After prompting an
LLM, the resulting capability ontology is automatically verified through
various steps in a loop with the LLM to check the overall correctness of the
capability ontology. First, a syntax check is performed, then a check for
contradictions, and finally a check for hallucinations and missing ontology
elements. Our method greatly reduces manual effort, as only the initial natural
language description and a final human review and possible correction are
necessary, thereby streamlining the capability ontology generation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Based Generative Error Correction: A Challenge and
  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given recent advances in generative AI technology, a key question is how
large language models (LLMs) can enhance acoustic modeling tasks using text
decoding results from a frozen, pretrained automatic speech recognition (ASR)
model. To explore new capabilities in language modeling for speech processing,
we introduce the generative speech transcription error correction (GenSEC)
challenge. This challenge comprises three post-ASR language modeling tasks: (i)
post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion
recognition. These tasks aim to emulate future LLM-based agents handling
voice-based interfaces while remaining accessible to a broad audience by
utilizing open pretrained language models or agent-based APIs. We also discuss
insights from baseline evaluations, as well as lessons learned for designing
future evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE SLT 2024. The initial draft version has been done in December
  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B
  pre-training correction model:
  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLFeedback: A Large-Scale AI Feedback <span class="highlight-title">Dataset</span> for Large Vision-Language
  Models Alignment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large vision-language models (LVLMs) evolve rapidly, the demand for
high-quality and diverse data to align these models becomes increasingly
crucial. However, the creation of such data with human supervision proves
costly and time-intensive. In this paper, we investigate the efficacy of AI
feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the
first large-scale vision-language feedback dataset, comprising over 82K
multi-modal instructions and comprehensive rationales generated by
off-the-shelf models without human annotations. To evaluate the effectiveness
of AI feedback for vision-language alignment, we train Silkie, an LVLM
fine-tuned via direct preference optimization on VLFeedback. Silkie showcases
exceptional performance regarding helpfulness, visual faithfulness, and safety
metrics. It outperforms its base model by 6.9\% and 9.5\% in perception and
cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits
enhanced resilience against red-teaming attacks. Furthermore, our analysis
underscores the advantage of AI feedback, particularly in fostering preference
diversity to deliver more comprehensive improvements. Our dataset, training
code and models are available at https://vlf-silkie.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference camera-ready version (fixed small typos).
  This article supersedes arXiv:2312.10665</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaterMax: breaking the LLM watermark detectability-robustness-quality
  trade-off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Giboulot, Teddy Furon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking is a technical means to dissuade malfeasant usage of Large
Language Models. This paper proposes a novel watermarking scheme, so-called
WaterMax, that enjoys high detectability while sustaining the quality of the
generated text of the original LLM. Its new design leaves the LLM untouched (no
modification of the weights, logits, temperature, or sampling technique).
WaterMax balances robustness and complexity contrary to the watermarking
techniques of the literature inherently provoking a trade-off between quality
and robustness. Its performance is both theoretically proven and experimentally
validated. It outperforms all the SotA techniques under the most complete
benchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Critics Help Catch Bugs in Mathematics: Towards a Better
  Mathematical Verifier with Natural Language Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14024v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14024v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent progress, mathematical verifiers have achieved success in
mathematical reasoning tasks by validating the correctness of solutions
generated by policy models. However, existing verifiers are trained with binary
classification labels, which are not informative enough for the model to
accurately assess the solutions. To mitigate the aforementioned insufficiency
of binary labels, we introduce step-wise natural language feedback as rationale
labels, that is, the correctness of each step and the detailed explanations. In
this paper, we propose Math-Minos, a natural language feedback-enhanced
verifier by constructing automatically generated training data and a two-stage
training paradigm for effective training and efficient inference. Our
experiments reveal that a small set of natural language feedback can
significantly boost the performance of the verifier in both verification and
reinforcement learning. We have released the code and data for further
exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PertEval: Unveiling Real Knowledge Capacity of LLMs with
  Knowledge-Invariant Perturbations <span class="chip">NeurIPS '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiatong Li, Renjun Hu, Kunzhe Huang, Yan Zhuang, Qi Liu, Mengxiao Zhu, Xing Shi, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expert-designed close-ended benchmarks are indispensable in assessing the
knowledge capacity of large language models (LLMs). Despite their widespread
use, concerns have mounted regarding their reliability due to limited test
scenarios and an unavoidable risk of data contamination. To rectify this, we
present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge
capacity through \textbf{knowledge-invariant perturbations}. These
perturbations employ human-like restatement techniques to generate on-the-fly
test samples from static benchmarks, meticulously retaining knowledge-critical
content while altering irrelevant details. Our toolkit further includes a suite
of \textbf{response consistency analyses} that compare performance on raw vs.
perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six
representative LLMs are re-evaluated using PertEval. Results reveal
significantly inflated performance of the LLMs on raw benchmarks, including an
absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced
response pattern analysis, we discover that PertEval retains LLMs' uncertainty
to specious knowledge, and reveals their potential rote memorization to correct
options which leads to overestimated performance. We also find that the
detailed response consistency analyses by PertEval could illuminate various
weaknesses in existing LLMs' knowledge mastery and guide the development of
refinement. Our findings provide insights for advancing more robust and
genuinely knowledgeable LLMs. Our code is available at
\url{https://github.com/aigc-apps/PertEval}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciAssess: Benchmarking LLM Proficiency in Scientific Literature
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01976v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01976v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Xi Fang, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, Guolin Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in Large Language Models (LLMs) have revolutionized
scientific literature analysis. However, existing benchmarks fail to adequately
evaluate the proficiency of LLMs in this domain, particularly in scenarios
requiring higher-level abilities beyond mere memorization and the handling of
multimodal data. In response to this gap, we introduce SciAssess, a benchmark
specifically designed for the comprehensive evaluation of LLMs in scientific
literature analysis. It aims to thoroughly assess the efficacy of LLMs by
evaluating their capabilities in Memorization (L1), Comprehension (L2), and
Analysis \& Reasoning (L3). It encompasses a variety of tasks drawn from
diverse scientific fields, including biology, chemistry, material, and
medicine. To ensure the reliability of SciAssess, rigorous quality control
measures have been implemented, ensuring accuracy, anonymization, and
compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting
their strengths and areas for improvement. We hope this evaluation supports the
ongoing development of LLM applications in scientific literature analysis.
SciAssess and its resources are available at
\url{https://github.com/sci-assess/SciAssess}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhou, Taelin Karidi, Wanlong Liu, Nicolas Garneau, Yong Cao, Wenyu Chen, Haizhou Li, Daniel Hershcovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have highlighted the presence of cultural biases in Large
Language Models (LLMs), yet often lack a robust methodology to dissect these
phenomena comprehensively. Our work aims to bridge this gap by delving into the
Food domain, a universally relevant yet culturally diverse aspect of human
life. We introduce FmLAMA, a multilingual dataset centered on food-related
cultural facts and variations in food practices. We analyze LLMs across various
architectures and configurations, evaluating their performance in both
monolingual and multilingual settings. By leveraging templates in six different
languages, we investigate how LLMs interact with language-specific and cultural
knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias
towards food knowledge prevalent in the United States; (2) Incorporating
relevant cultural context significantly improves LLMs' ability to access
cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is
highly dependent on the interplay between the probing language, the specific
model architecture, and the cultural context in question. This research
underscores the complexity of integrating cultural understanding into LLMs and
emphasizes the importance of culturally diverse datasets to mitigate biases and
enhance model performance across different cultural domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cultural bias analysis, cultural knowledge probing, large language
  models, cultural NLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synergizing In-<span class="highlight-title">context</span> Learning with Hints for End-to-end Task-oriented
  Dialog Systems <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15585v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15585v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end Task-Oriented Dialog (TOD) systems typically require extensive
training datasets to perform well. In contrast, large language model (LLM)
based TOD systems can excel even with limited data due to their ability to
learn tasks through in-context exemplars. However, these models lack alignment
with the style of responses in training data and often generate comprehensive
responses, making it difficult for users to grasp the information quickly. In
response, we propose SyncTOD that synergizes LLMs with task-specific hints to
improve alignment in low-data settings. SyncTOD employs small auxiliary models
to provide hints and select exemplars for in-context prompts. With ChatGPT,
SyncTOD achieves superior performance compared to LLM-based baselines and SoTA
models in low-data settings, while retaining competitive performance in
full-data settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Camera-Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-multi-step: The Truth Behind Difficult <span class="highlight-title">Long</span>-<span class="highlight-title">context</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04422v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04422v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiong Yu, Ma Xiufa, Fang Jianwei, Zhi Xu, Su Guangyao, Wang Jiancheng, Yongfeng Huang, Zhixiao Qi, Wei Wang, Weifeng Liu, Ran Chen, Ji Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context language models (LCLM), characterized by their extensive context
window, is becoming increasingly popular. Meanwhile, many long-context
benchmarks present challenging tasks that even the most advanced LCLMs struggle
to complete. However, the underlying sources of various challenging
long-context tasks have seldom been studied. To bridge this gap, we conduct
experiments to indicate their difficulty stems primarily from two basic issues:
"multi-matching retrieval," which requires the simultaneous retrieval of
multiple items, and "logic-based retrieval," which necessitates logical
judgment within retrieval criteria. These two problems, while seemingly
straightforward, actually exceed the capabilities of LCLMs because they are
proven to be hyper-multi-step (demanding numerous steps to solve) in nature.
This finding could explain why LLMs struggle with more advanced long-context
tasks, providing a more accurate perspective for rethinking solutions for them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is publicly available at
  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at
  https://huggingface.co/datasets/yuyijiong/difficult_retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fisher Information-based Efficient Curriculum Federated Learning with
  Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Liu, Jiaxiang Ren, Ruoming Jin, Zijie Zhang, Yang Zhou, Patrick Valduriez, Dejing Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a promising paradigm to collaboratively train models with decentralized
data, Federated Learning (FL) can be exploited to fine-tune Large Language
Models (LLMs). While LLMs correspond to huge size, the scale of the training
data significantly increases, which leads to tremendous amounts of computation
and communication costs. The training data is generally non-Independent and
Identically Distributed (non-IID), which requires adaptive data processing
within each device. Although Low Rank Adaptation (LoRA) can significantly
reduce the scale of parameters to update in the fine-tuning process, it still
takes unaffordable time to transfer the low-rank parameters of all the layers
in LLMs. In this paper, we propose a Fisher Information-based Efficient
Curriculum Federated Learning framework (FibecFed) with two novel methods,
i.e., adaptive federated curriculum learning and efficient sparse parameter
update. First, we propose a fisher information-based method to adaptively
sample data within each device to improve the effectiveness of the FL
fine-tuning process. Second, we dynamically select the proper layers for global
aggregation and sparse parameters for local update with LoRA so as to improve
the efficiency of the FL fine-tuning process. Extensive experimental results
based on 10 datasets demonstrate that FibecFed yields excellent performance (up
to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%
faster) compared with 17 baseline approaches).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 14 tables, to appear in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUIS: Question-guided Insights Generation for Automated Exploratory Data
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Manatkar, Ashlesha Akella, Parthivi Gupta, Krishnasuri Narayanam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering meaningful insights from a large dataset, known as Exploratory
Data Analysis (EDA), is a challenging task that requires thorough exploration
and analysis of the data. Automated Data Exploration (ADE) systems use
goal-oriented methods with Large Language Models and Reinforcement Learning
towards full automation. However, these methods require human involvement to
anticipate goals that may limit insight extraction, while fully automated
systems demand significant computational resources and retraining for new
datasets. We introduce QUIS, a fully automated EDA system that operates in two
stages: insight generation (ISGen) driven by question generation (QUGen). The
QUGen module generates questions in iterations, refining them from previous
iterations to enhance coverage without human intervention or manually curated
examples. The ISGen module analyzes data to produce multiple relevant insights
in response to each question, requiring no prior training and enabling QUIS to
adapt to new datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ENLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific literature understanding is crucial for extracting targeted
information and garnering insights, thereby significantly advancing scientific
discovery. Despite the remarkable success of Large Language Models (LLMs), they
face challenges in scientific literature understanding, primarily due to (1) a
lack of scientific knowledge and (2) unfamiliarity with specialized scientific
tasks.
  To develop an LLM specialized in scientific literature understanding, we
propose a hybrid strategy that integrates continual pre-training (CPT) and
supervised fine-tuning (SFT), to simultaneously infuse scientific domain
knowledge and enhance instruction-following capabilities for domain-specific
tasks.cIn this process, we identify two key challenges: (1) constructing
high-quality CPT corpora, and (2) generating diverse SFT instructions. We
address these challenges through a meticulous pipeline, including PDF text
extraction, parsing content error correction, quality filtering, and synthetic
instruction creation. Applying this strategy, we present a suite of LLMs:
SciLitLLM, specialized in scientific literature understanding. These models
demonstrate promising performance on scientific literature understanding
benchmarks.
  Our contributions are threefold: (1) We present an effective framework that
integrates CPT and SFT to adapt LLMs to scientific literature understanding,
which can also be easily adapted to other domains. (2) We propose an LLM-based
synthesis method to generate diverse and high-quality scientific instructions,
resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning
in less-represented scientific domains. (3) SciLitLLM achieves promising
performance improvements on scientific literature understanding benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its
quadratic complexity limits the efficiency and scalability of LLMs, especially
for those with a long-context window. A promising approach addressing this
limitation is to leverage the sparsity in attention. However, existing
sparsity-based solutions predominantly rely on predefined patterns or
heuristics to approximate sparsity. This practice falls short to fully capture
the dynamic nature of attention sparsity in language-based tasks. This paper
argues that attention sparsity should be learned rather than predefined. To
this end, we design SeerAttention, a new Attention mechanism that augments the
conventional attention with a learnable gate that adaptively selects
significant blocks in an attention map and deems the rest blocks sparse. Such
block-level sparsity effectively balances accuracy and speedup. To enable
efficient learning of the gating network, we develop a customized
FlashAttention implementation that extracts the block-level ground truth of
attention map with minimum overhead. SeerAttention not only applies to
post-training, but also excels in long-context fine-tuning. Our results show
that at post-training stages, SeerAttention significantly outperforms
state-of-the-art static or heuristic-based sparse attention methods, while also
being more versatile and flexible to adapt to varying context lengths and
sparsity ratios. When applied to long-context fine-tuning with YaRN,
SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context
length with minimal perplexity loss, offering a 5.67x speedup over
FlashAttention-2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for
  data pruning in LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxuan Yang, Huayi Wang, Muning Wen, Xiaoyun Mo, Qiuying Peng, Jun Wang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly advancing field of Large Language Models (LLMs), effectively
leveraging existing datasets during fine-tuning to maximize the model's
potential is of paramount importance. This paper introduces P3, an adaptive
framework aimed at optimizing the task-specific fine-tuning process through
iterative data pruning. P3 consists of three key components: (1) Policy-driven
Difficulty Measurement, which dynamically assesses data difficulty based on the
model's real-time performance, replacing static metrics with adaptable
evaluations; (2) Pace-Adaptive Selection, leveraging self-paced learning to
progressively introduce more challenging data, thereby enhancing model
capability; (3) Diversity Promotion, incorporating Determinantal Point Process
(DPP) to ensure data diversity across epochs, enriching the learning process.
We validate P3 on the reasoning scenarios, APPS and MATH, demonstrating
significant improvements over traditional data pruning methods. By advancing
dynamic data selection and utilization strategies, P3 contributes both a
theoretical framework and concrete approach to fully exploit existing data for
LLMs' performance improvement, offering utility across diverse tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep Generative
  Models with Multi-modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
\textit{LatentExplainer}, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
\textit{LatentExplainer} tackles three main challenges: inferring the meaning
of latent variables, aligning explanations with inductive biases, and handling
varying degrees of explainability. Our approach perturbs latent variables,
interpreting changes in generated data, and uses multi-modal large language
models (MLLMs) to produce human-understandable explanations. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations for
latent variables. The results highlight the effectiveness of incorporating
inductive biases and uncertainty quantification, significantly enhancing model
interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating
  Attention Head Activation Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs' performance on complex tasks is still unsatisfactory. A key issue is
that presently LLMs learn in a data-driven schema, while the instructions about
these complex tasks are both scarce and hard to collect or construct. On the
contrary, a prominent phenomenon is that LLMs can learn rather fast on simpler
tasks with adequate prior knowledge captured during pretraining stage. Thus, if
the prerequisite and mechanism of such rapid generalization could be
elucidated, it could enhance the efficiency and effectiveness of the LLM's
ability to learn complex tasks. Thus, in this paper, we employ a gradient-based
method, to dissect the process that the SFT process adapts LLMs to downstream
tasks via the perspective of attention patterns. We find that: (1) LLMs
selectively activate task-specific attention heads during SFT; (2) activation
patterns for complex tasks are combinations of basic task patterns; and (3)
changes in a few parameters can significantly impact activation patterns after
SFT on a small number of samples.Based on these insights, experiments are
conducted to actually enhance the efficiency and effectiveness of SFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Introspection to Best Practices: Principled Analysis of
  Demonstrations in Multimodal In-<span class="highlight-title">Context</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by in-context learning (ICL) capabilities of Large Language models
(LLMs), multimodal LLMs with additional visual modality are also exhibited with
similar ICL abilities when multiple image-text pairs are provided as
demonstrations. However, relatively less work has been done to investigate the
principles behind how and why multimodal ICL works. We conduct a systematic and
principled evaluation of multimodal ICL for models of different scales on a
broad spectrum of new yet critical tasks. Through perturbations over different
modality information, we show that modalities matter differently across tasks
in multimodal ICL. Guided by task-specific modality impact, we recommend
modality-driven demonstration strategies to boost ICL performance. We also find
that models may follow inductive biases from multimodal ICL even if they are
rarely seen in or contradict semantic priors from pretraining data. Our
principled analysis provides a comprehensive way of understanding the role of
demonstrations in multimodal in-context learning, and sheds light on
effectively improving multimodal ICL on a wide range of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Everything is Editable: Extend Knowledge Editing to Unstructured Data in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent knowledge editing methods have primarily focused on modifying
structured knowledge in large language models. However, this task setting
overlooks the fact that a significant portion of real-world knowledge is stored
in an unstructured format, characterized by long-form content, noise, and a
complex yet comprehensive nature. Techniques like local layer key-value storage
and term-driven optimization, as used in previous methods like MEMIT, are not
effective for handling unstructured knowledge. To address these challenges, we
propose a novel Unstructured Knowledge Editing method, namely UnKE, which
extends previous assumptions in the layer dimension and token dimension.
Firstly, in the layer dimension, we propose non-local block key-value storage
to replace local layer key-value storage, increasing the representation ability
of key-value pairs and incorporating attention layer knowledge. Secondly, in
the token dimension, we replace term-driven optimization with cause-driven
optimization, which edits the last token directly while preserving context,
avoiding the need to locate terms and preventing the loss of context
information. Results on newly proposed unstructured knowledge editing dataset
(UnKEBench) and traditional structured datasets demonstrate that UnKE achieves
remarkable performance, surpassing strong baselines. In addition, UnKE has
robust batch editing and sequential editing capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amphista: Bi-directional Multi-head Decoding for Accelerating LLM
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Li, Xinlong Yang, Ziheng Gao, Ji Liu, Guanchen Li, Zhuang Liu, Dong Li, Jinzhang Peng, Lu Tian, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) inherently use autoregressive decoding, which
lacks parallelism in inference and results in significantly slow inference
speed. While methods such as Medusa constructs parallelized heads, they lack
adequate information interaction across different prediction positions. To
overcome this limitation, we introduce Amphista, an enhanced speculative
decoding framework that builds upon Medusa. Specifically, Amphista models an
Auto-embedding Block capable of parallel inference, incorporating
bi-directional attention to enable interaction between different drafting
heads. Additionally, Amphista integrates Staged Adaptation Layers, which ensure
a seamless transition of semantic information from the target model's
autoregressive inference to the drafting heads' non-autoregressive inference,
effectively achieving paradigm shift and feature fusion. Experimental results
on Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista
achieves substantial acceleration while maintaining generation quality. On
MT-Bench, Amphista delivers up to 2.75$\times$ speedup over vanilla
autoregressive decoding and 1.40$\times$ over Medusa on Vicuna 33B in
wall-clock time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16710v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16710v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LayerSkip, an end-to-end solution to speed-up inference of large
language models (LLMs). First, during training we apply layer dropout, with low
dropout rates for earlier layers and higher dropout rates for later layers, and
an early exit loss where all transformer layers share the same exit. Second,
during inference, we show that this training recipe increases the accuracy of
early exit at earlier layers, without adding any auxiliary layers or modules to
the model. Third, we present a novel self-speculative decoding solution where
we exit at early layers and verify and correct with remaining layers of the
model. Our proposed self-speculative decoding approach has less memory
footprint than other speculative decoding approaches and benefits from shared
compute and activations of the draft and verification stages. We run
experiments on different Llama model sizes on different types of training:
pretraining from scratch, continual pretraining, finetuning on specific data
domain, and finetuning on specific task. We implement our inference solution
and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x
on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and
checkpoints at https://github.com/facebookresearch/LayerSkip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Tighter Complexity Analysis of Sparse<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we improved the analysis of the running time of SparseGPT
[Frantar, Alistarh ICML 2023] from $O(d^{3})$ to $O(d^{\omega} + d^{2+a+o(1)} +
d^{1+\omega(1,1,a)-a})$ for any $a \in [0, 1]$, where $\omega$ is the exponent
of matrix multiplication. In particular, for the current $\omega \approx 2.371$
[Alman, Duan, Williams, Xu, Xu, Zhou 2024], our running time boils down to
$O(d^{2.53})$. This running time is due to the analysis of the lazy update
behavior in iterative maintenance problems such as [Deng, Song, Weinstein 2022;
Brand, Song, Zhou ICML 2024].
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Study of Multilingual Confidence Estimation on Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tendency of Large Language Models (LLMs) to generate hallucinations
raises concerns regarding their reliability. Therefore, confidence estimations
indicating the extent of trustworthiness of the generations become essential.
However, current LLM confidence estimations in languages other than English
remain underexplored. This paper addresses this gap by introducing a
comprehensive investigation of Multilingual Confidence estimation (MlingConf)
on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)
tasks to explore the performance and language dominance effects of multilingual
confidence estimations on different tasks. The benchmark comprises four
meticulously checked and human-evaluate high-quality multilingual datasets for
LA tasks and one for the LS task tailored to specific social, cultural, and
geographical contexts of a language. Our experiments reveal that on LA tasks
English exhibits notable linguistic dominance in confidence estimations than
other languages, while on LS tasks, using question-related language to prompt
LLMs demonstrates better linguistic dominance in multilingual confidence
estimations. The phenomena inspire a simple yet effective native-tone prompting
strategy by employing language-specific prompts for LS tasks, effectively
improving LLMs' reliability and accuracy on LS tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: n pages; Previously this version appeared as
  arXiv:2410.12478 which was submitted as a new work by accident</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExACT: Teaching AI Agents to Explore with Reflective-MCTS and
  Exploratory Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02052v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02052v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have demonstrated significant potential in automating
complex multistep decision-making tasks. However, even state-of-the-art
vision-language models (VLMs), such as GPT-4o, still fall short of human-level
performance, particularly in intricate web environments and long-horizon tasks.
To address these limitations, we present ExACT, an approach to combine
test-time search and self-learning to build o1-like models for agentic
applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a
novel test time algorithm designed to enhance AI agents' ability to explore
decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating
contrastive reflection, allowing agents to learn from past interactions and
dynamically improve their search efficiency; and 2) using multi-agent debate
for reliable state evaluation. Next, we introduce Exploratory Learning, a novel
learning strategy to teach agents to search at inference time without relying
on any external search algorithms. On the challenging VisualWebArena benchmark,
our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across
various tasks compared to the previous state-of-the-art. Additionally, we show
that the knowledge and experience gained from test-time search can be
effectively transferred back to GPT-4o via fine-tuning. After Exploratory
Learning, GPT-4o 1) demonstrates the ability to explore the environment,
evaluate a state, and backtrack to viable ones when it detects that the current
state cannot lead to success, and 2) matches 87% of R-MCTS's performance while
using significantly less compute. Notably, our work demonstrates the compute
scaling properties in both training - data collection with R-MCTS - and testing
time. These results suggest a promising research direction to enhance VLMs'
capabilities for agentic applications via test-time search and self-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MlingConf: A Comprehensive Study of Multilingual Confidence Estimation
  on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tendency of Large Language Models (LLMs) to generate hallucinations
raises concerns regarding their reliability. Therefore, confidence estimations
indicating the extent of trustworthiness of the generations become essential.
However, current LLM confidence estimations in languages other than English
remain underexplored. This paper addresses this gap by introducing a
comprehensive investigation of Multilingual Confidence estimation (MlingConf)
on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)
tasks to explore the performance and language dominance effects of multilingual
confidence estimations on different tasks. The benchmark comprises four
meticulously checked and human-evaluate high-quality multilingual datasets for
LA tasks and one for the LS task tailored to specific social, cultural, and
geographical contexts of a language. Our experiments reveal that on LA tasks
English exhibits notable linguistic dominance in confidence estimations than
other languages, while on LS tasks, using question-related language to prompt
LLMs demonstrates better linguistic dominance in multilingual confidence
estimations. The phenomena inspire a simple yet effective native-tone prompting
strategy by employing language-specific prompts for LS tasks, effectively
improving LLMs' reliability and accuracy on LS tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: This work was intended as a replacement of arXiv:2402.13606
  and any subsequent updates will appear there</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction
  Diversity on Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04717v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04717v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Zhang, Justin Wang, Francois Charton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and accurately following instructions is critical for large
language models (LLMs) to be effective across diverse tasks. In this work, we
rigorously examine the key factors that enable models to generalize to unseen
instructions, providing insights to guide the collection of data for
instruction-tuning. Through controlled experiments, inspired by the
Turing-complete Markov algorithm, we demonstrate that such generalization
$\textbf{only emerges}$ when training data is diversified enough across
semantic domains. Our findings also reveal that merely diversifying within
limited domains fails to ensure robust generalization. In contrast,
cross-domain data diversification, even under constrained data budgets,
significantly enhances a model's adaptability. We further extend our analysis
to real-world scenarios, including fine-tuning of
$\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models.
In both cases, we demonstrate that 1) better performance can be achieved by
increasing the diversity of an established dataset while keeping the data size
constant, and 2) when scaling up the data, diversifying the semantics of
instructions is more effective than simply increasing the quantity of similar
data. Our research provides important insights for dataset collation,
particularly when optimizing model performance by expanding training data for
both specialist and generalist scenarios. We show that careful consideration of
data diversification is key: training specialist models with data extending
beyond their core domain leads to significant performance improvements, while
generalist models benefit from diverse data mixtures that enhance their overall
instruction-following capabilities across a wide range of applications. Our
results highlight the critical role of strategic diversification and offer
clear guidelines for improving data quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix formatting issues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BenTo: Benchmark Task Reduction with In-<span class="highlight-title">Context</span> Transferability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language models (LLMs) is costly: it requires the generation
and examination of LLM outputs on a large-scale benchmark of various tasks.
This paper investigates how to efficiently reduce the tasks used to benchmark
LLMs without affecting the evaluation quality. Our study reveals that task
transferability and relevance provide critical information to identify the most
representative subset of tasks via optimizing a facility location function. We
propose a practically efficient metric for estimating the transferability
between two tasks via in-context learning (ICL). By analyzing the pairwise
transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or
FLAN) to 5% while inducing only a <4% difference to the evaluation on the
original benchmark. Compared to prior works, our method is training-free,
gradient-free, and highly efficient requiring ICL only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/tianyi-lab/bento</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphInsight: Unlocking Insights in Large Language Models for Graph
  Structure Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPal: Autonomous Adaptation to Users for Personal AI Companionship 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Cheng, Wenge Liu, Kaishuai Xu, Wenjun Hou, Yi Ouyang, Chak Tou Leong, Xian Wu, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has demonstrated the potential of AI agents to act as
companions that can provide constant emotional support for humans. In this
paper, we emphasize the necessity of autonomous adaptation in personal AI
companionship, an underexplored yet promising direction. Such adaptability is
crucial as it can facilitate more tailored interactions with users and allow
the agent to evolve in response to users' changing needs. However, imbuing
agents with autonomous adaptability presents unique challenges, including
identifying optimal adaptations to meet users' expectations and ensuring a
smooth transition during the adaptation process. To address them, we devise a
hierarchical framework, AutoPal, that enables controllable and authentic
adjustments to the agent's persona based on user interactions. A
personamatching dataset is constructed to facilitate the learning of optimal
persona adaptations. Extensive experiments demonstrate the effectiveness of
AutoPal and highlight the importance of autonomous adaptability in AI
companionship.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Quantifying and Mitigating Ripple Effects in Model Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07825v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07825v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchen Wang, Zhouhong Gu, Xiaoxuan Zhu, Lin Zhang, Haoning Ye, Zhuozhi Xiong, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have revolutionized numerous tasks with their
remarkable efficacy. However, editing these models, crucial for rectifying
outdated or erroneous information, often leads to a complex issue known as the
ripple effect in the hidden space. While difficult to detect, this effect can
significantly impede the efficacy of model editing tasks and deteriorate model
performance. This paper addresses this scientific challenge by proposing a
novel evaluation methodology, Graphical Impact Evaluation(GIE), which
quantitatively evaluates the adaptations of the model and the subsequent impact
of editing. Furthermore, we introduce the Selective Impact Revision(SIR), a
model editing method designed to mitigate this ripple effect. Our comprehensive
evaluations reveal that the ripple effect in the hidden space is a significant
issue in all current model editing methods. However, our proposed methods, GIE
and SIR, effectively identify and alleviate this issue, contributing to the
advancement of LLM editing techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoR: Mixture of Ranks for Low-Rank Adaptation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyu Tang, Yilong Chen, Zhenyu Zhang, Junyuan Shang, Wenyuan Zhang, Yong Huang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) drives research to align its performance with full
fine-tuning. However, significant challenges remain: (1) Simply increasing the
rank size of LoRA does not effectively capture high-rank information, which
leads to a performance bottleneck.(2) MoE-style LoRA methods substantially
increase parameters and inference latency, contradicting the goals of efficient
fine-tuning and ease of application. To address these challenges, we introduce
Mixture of Ranks (MoR), which learns rank-specific information for different
tasks based on input and efficiently integrates multi-rank information. We
firstly propose a new framework that equates the integration of multiple LoRAs
to expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA
already captures sufficient intrinsic information, and MoR can derive high-rank
information through mathematical transformations of the low-rank components.
Thus, MoR can reduces the learning difficulty of LoRA and enhances its
multi-task capabilities. MoR achieves impressive results, with MoR delivering a
1.31\% performance improvement while using only 93.93\% of the parameters
compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniAutoML: A Human-Centered Framework for Unified Discriminative and
  Generative AutoML with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Guo, Zan Chen, Yingrui Ji, Liyun Zhang, Daqin Luo, Zhigang Li, Yiqin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Machine Learning (AutoML) has simplified complex ML processes such
as data pre-processing, model selection, and hyper-parameter searching.
However, traditional AutoML frameworks focus solely on discriminative tasks,
often falling short in tackling AutoML for generative models. Additionally,
these frameworks lack interpretability and user engagement during the training
process, primarily due to the absence of human-centered design. It leads to a
lack of transparency in final decision-making and limited user control,
potentially reducing trust and adoption of AutoML methods. To address these
limitations, we introduce UniAutoML, a human-centered AutoML framework that
leverages Large Language Models (LLMs) to unify AutoML for both discriminative
(e.g., Transformers and CNNs for classification or regression tasks) and
generative tasks (e.g., fine-tuning diffusion models or LLMs). The
human-centered design of UniAutoML innovatively features a conversational user
interface (CUI) that facilitates natural language interactions, providing users
with real-time guidance, feedback, and progress updates for better
interpretability. This design enhances transparency and user control throughout
the AutoML training process, allowing users to seamlessly break down or modify
the model being trained. To mitigate potential risks associated with LLM
generated content, UniAutoML incorporates a safety guardline that filters
inputs and censors outputs. We evaluated UniAutoML's performance and usability
through experiments on eight diverse datasets and user studies involving 25
participants, demonstrating that UniAutoML not only enhances performance but
also improves user control and trust. Our human-centered design bridges the gap
between AutoML capabilities and user understanding, making ML more accessible
to a broader audience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACCEPT: Adaptive Codebook for Composite and Efficient <span class="highlight-title">Prompt</span> Tuning <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Chen Lin, Wei-Hua Li, Jun-Cheng Chen, Chu-Song Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method
attributed to its remarkable performance with few updated parameters on various
large-scale pretrained Language Models (PLMs). Traditionally, each prompt has
been considered indivisible and updated independently, leading the parameters
increase proportionally as prompt length grows. To address this issue, we
propose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT).
In our method, we refer to the concept of product quantization (PQ), allowing
all soft prompts to share a set of learnable codebook vectors in each subspace,
with each prompt differentiated by a set of adaptive weights. We achieve the
superior performance on 17 diverse natural language tasks including natural
language understanding (NLU) and question answering (QA) tasks by tuning only
0.3% of parameters of the PLMs. Our approach also excels in few-shot and large
model settings, highlighting its significant potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Subjective Uncertainty Quantification and Calibration in Natural
  Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wang, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications of large language models often involve the generation of
free-form responses, in which case uncertainty quantification becomes
challenging. This is due to the need to identify task-specific uncertainties
(e.g., about the semantics) which appears difficult to define in general cases.
This work addresses these challenges from a perspective of Bayesian decision
theory, starting from the assumption that our utility is characterized by a
similarity measure that compares a generated response with a hypothetical true
response. We discuss how this assumption enables principled quantification of
the model's subjective uncertainty and its calibration. We further derive a
measure for epistemic uncertainty, based on a missing data perspective and its
characterization as an excess risk. The proposed methods can be applied to
black-box language models. We illustrate the methods on question answering and
machine translation tasks. Our experiments provide a principled evaluation of
task-specific calibration, and demonstrate that epistemic uncertainty offers a
promising deferral strategy for efficient data acquisition in in-context
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Evolved Universal <span class="highlight-title">Transformer</span> Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior methods propose to offset the escalating costs of modern foundation
models by dropping specific parts of their contexts with hand-designed rules,
while attempting to preserve their original performance. We overcome this
trade-off with Neural Attention Memory Models (NAMMs), introducing a learned
network for memory management that improves both the performance and efficiency
of transformers. We evolve NAMMs atop pre-trained transformers to provide
different latent contexts focusing on the most relevant information for
individual layers and attention heads. NAMMs are universally applicable to any
model using self-attention as they condition exclusively on the values in the
produced attention matrices. Learning NAMMs on a small set of problems, we
achieve substantial performance improvements across multiple long-context
benchmarks while cutting the model's input contexts up to a fraction of the
original sizes. We show the generality of our conditioning enables zero-shot
transfer of NAMMs trained only on language to entirely new transformer
architectures even across input modalities, with their benefits carrying over
to vision and reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 14 figures. Preprint, under submission. Source code is
  available at https://github.com/SakanaAI/evo-memory</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-17T00:00:00Z">2024-10-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Numerical Precision Affects Mathematical Reasoning Capabilities of
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of Transformer-based Large Language Models
(LLMs) across various domains, understanding and enhancing their mathematical
capabilities remains a significant challenge. In this paper, we conduct a
rigorous theoretical analysis of LLMs' mathematical abilities, with a specific
focus on their arithmetic performances. We identify numerical precision as a
key factor that influences their effectiveness in mathematical tasks. Our
results show that Transformers operating with low numerical precision fail to
address arithmetic tasks, such as iterated addition and integer multiplication,
unless the model size grows super-polynomially with respect to the input
length. In contrast, Transformers with standard numerical precision can
efficiently handle these tasks with significantly smaller model sizes. We
further support our theoretical findings through empirical experiments that
explore the impact of varying numerical precision on arithmetic tasks,
providing valuable insights for improving the mathematical reasoning
capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can MLLMs Understand the Deep Implication Behind Chinese Images? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Zhang, Xi Feng, Yuelin Bai, Xinrun Du, Jinchang Hou, Kaixin Deng, Guangzeng Han, Qinrui Li, Bingli Wang, Jiaheng Liu, Xingwei Qu, Yifei Zhang, Qixuan Zhao, Yiming Liang, Ziqiang Liu, Feiteng Fang, Min Yang, Wenhao Huang, Chenghua Lin, Ge Zhang, Shiwen Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of Multimodal Large Language Models (MLLMs) continue to
improve, the need for higher-order capability evaluation of MLLMs is
increasing. However, there is a lack of work evaluating MLLM for higher-order
perception and understanding of Chinese visual content. To fill the gap, we
introduce the **C**hinese **I**mage **I**mplication understanding
**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception
and understanding capabilities of MLLMs for Chinese images. CII-Bench stands
out in several ways compared to existing benchmarks. Firstly, to ensure the
authenticity of the Chinese context, images in CII-Bench are sourced from the
Chinese Internet and manually reviewed, with corresponding answers also
manually crafted. Additionally, CII-Bench incorporates images that represent
Chinese traditional culture, such as famous Chinese traditional paintings,
which can deeply reflect the model's understanding of Chinese traditional
culture. Through extensive experiments on CII-Bench across multiple MLLMs, we
have made significant findings. Initially, a substantial gap is observed
between the performance of MLLMs and humans on CII-Bench. The highest accuracy
of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an
impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional
culture images, suggesting limitations in their ability to understand
high-level semantics and lack a deep knowledge base of Chinese traditional
culture. Finally, it is observed that most models exhibit enhanced accuracy
when image emotion hints are incorporated into the prompts. We believe that
CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics
and Chinese-specific images, advancing the journey towards expert artificial
general intelligence (AGI). Our project is publicly available at
https://cii-bench.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code:
  https://github.com/MING_X/CII-Bench Dataset:
  https://huggingface.co/datasets/m-a-p/CII-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrospective Learning from Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-turn interactions between large language models (LLMs) and users
naturally include implicit feedback signals. If an LLM responds in an
unexpected way to an instruction, the user is likely to signal it by rephrasing
the request, expressing frustration, or pivoting to an alternative task. Such
signals are task-independent and occupy a relatively constrained subspace of
language, allowing the LLM to identify them even if it fails on the actual
task. This creates an avenue for continually learning from interactions without
additional annotations. We introduce ReSpect, a method to learn from such
signals in past interactions via retrospection. We deploy ReSpect in a new
multimodal interaction scenario, where humans instruct an LLM to solve an
abstract reasoning task with a combinatorial solution space. Through thousands
of interactions with humans, we show how ReSpect gradually improves task
completion rate from 31% to 82%, all without any external annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Janus: Decoupling Visual Encoding for Unified Multimodal Understanding
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Janus, an autoregressive framework that unifies
multimodal understanding and generation. Prior research often relies on a
single visual encoder for both tasks, such as Chameleon. However, due to the
differing levels of information granularity required by multimodal
understanding and generation, this approach can lead to suboptimal performance,
particularly in multimodal understanding. To address this issue, we decouple
visual encoding into separate pathways, while still leveraging a single,
unified transformer architecture for processing. The decoupling not only
alleviates the conflict between the visual encoder's roles in understanding and
generation, but also enhances the framework's flexibility. For instance, both
the multimodal understanding and generation components can independently select
their most suitable encoding methods. Experiments show that Janus surpasses
previous unified model and matches or exceeds the performance of task-specific
models. The simplicity, high flexibility, and effectiveness of Janus make it a
strong candidate for next-generation unified multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have extended their
capabilities to handle long contexts. However, increasing the number of model
layers and the length of input sequences significantly escalates the memory
required to store key-value (KV) cache, posing challenges for efficient
inference. To mitigate this issue, we present SimLayerKV, a simple yet
effective method that reduces inter-layer KV cache redundancies by selectively
dropping cache in identified lazy layers. Our approach is based on the
observation that certain layers in long-context LLMs exhibit "lazy" behavior,
contributing less to modeling long-range dependencies compared to non-lazy
layers. By analyzing attention weight patterns, we find that the behavior of
these lazy layers is consistent across tokens during generation for a given
input. This insight motivates our SimLayerKV, which identifies lazy layers and
reduces their KV cache accordingly. SimLayerKV is training-free, generalizable,
and can be implemented with only seven lines of code. We conduct extensive
experiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and
Mistral-7B across 16 tasks from the LongBench benchmark. The results
demonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\times$
with only a 1.2% performance drop when combined with 4-bit quantization. Our
code is available at https://github.com/sail-sg/SimLayerKV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified View of Delta Parameter Editing in Post-Trained Large-Scale
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Le Yu, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training has emerged as a crucial paradigm for adapting large-scale
pre-trained models to various tasks, whose effects are fully reflected by delta
parameters (i.e., the disparity between post-trained and pre-trained
parameters). While numerous studies have explored delta parameter properties
via operations like pruning, quantization, low-rank approximation, and
extrapolation, a unified framework for systematically examining these
characteristics has been lacking. In this paper, we propose a novel perspective
based on Riemann sum approximation of the loss function to elucidate delta
parameter editing operations. Our analysis categorizes existing methods into
three classes based on their post-editing performance: competitive, decreased,
and improved, explaining how they are expressed by the Riemann sum
approximation term and how they alter the model performance. Extensive
experiments on both visual and language models, including ViT, LLaMA 3, Qwen 2,
and Mistral, corroborate our theoretical findings. Furthermore, we introduce
extensions to existing techniques like DARE and BitDelta, highlighting their
limitations in leveraging the properties of delta parameters and reorganizing
them into general expressions to enhance the applicability and effectiveness of
delta parameter editing in post-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Common Pitfall of Margin-based Language Model Alignment: Gradient
  Entanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yuan, Yifan Zeng, Yue Wu, Huazheng Wang, Mengdi Wang, Liu Leqi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) has become the predominant
approach for language model (LM) alignment. At its core, RLHF uses a
margin-based loss for preference optimization, specifying ideal LM behavior
only by the difference between preferred and dispreferred responses. In this
paper, we identify a common pitfall of margin-based methods -- the
under-specification of ideal LM behavior on preferred and dispreferred
responses individually, which leads to two unintended consequences as the
margin increases: (1) The probability of dispreferred (e.g., unsafe) responses
may increase, resulting in potential safety alignment failures. (2) The
probability of preferred responses may decrease, even when those responses are
ideal. We demystify the reasons behind these problematic behaviors:
margin-based losses couple the change in the preferred probability to the
gradient of the dispreferred one, and vice versa, often preventing the
preferred probability from increasing while the dispreferred one decreases, and
thus causing a synchronized increase or decrease in both probabilities. We term
this effect, inherent in margin-based objectives, gradient entanglement.
Formally, we derive conditions for general margin-based alignment objectives
under which gradient entanglement becomes concerning: the inner product of the
gradients of preferred and dispreferred log-probabilities is large relative to
the individual gradient norms. We theoretically investigate why such inner
products can be large when aligning language models and empirically validate
our findings. Empirical implications of our framework extend to explaining
important differences in the training dynamics of various preference
optimization algorithms, and suggesting potential algorithm designs to mitigate
the under-specification issue of margin-based methods and thereby improving
language model alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomy via agents using large language models (LLMs) for personalized,
standardized tasks boosts human efficiency. Automating web tasks (like booking
hotels within a budget) is increasingly sought after. Fulfilling practical
needs, the web agent also serves as an important proof-of-concept example for
various agent grounding scenarios, with its success promising advancements in
many future applications. Prior research often handcrafts web agent strategies
(e.g., prompting templates, multi-agent systems, search methods, etc.) and the
corresponding in-context examples, which may not generalize well across all
real-world scenarios. On the other hand, there has been limited study on the
misalignment between a web agent's observation/action representation and the
pre-training data of the LLM it's based on. This discrepancy is especially
notable when LLMs are primarily trained for language completion rather than
tasks involving embodied navigation actions and symbolic web elements. Our
study enhances an LLM-based web agent by simply refining its observation and
action space to better align with the LLM's capabilities. This approach enables
our base agent to significantly outperform previous methods on a wide variety
of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose
web interaction tasks, our agent AgentOccam surpasses the previous
state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute
points respectively, and boosts the success rate by 26.6 points (+161%) over
similar plain web agents with its observation and action space alignment. We
achieve this without using in-context examples, new agent roles, online
feedback or search strategies. AgentOccam's simple design highlights LLMs'
impressive zero-shot performance on web tasks, and underlines the critical role
of carefully tuning observation and action spaces for LLM-based agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48\% improvement on
VisualWebBench and a 19.1\% boost in action accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ De-mark: Watermark Removal in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking techniques offer a promising way to identify machine-generated
content via embedding covert information into the contents generated from
language models (LMs). However, the robustness of the watermarking schemes has
not been well explored. In this paper, we present De-mark, an advanced
framework designed to remove n-gram-based watermarks effectively. Our method
utilizes a novel querying strategy, termed random selection probing, which aids
in assessing the strength of the watermark and identifying the red-green list
within the n-gram watermark. Experiments on popular LMs, such as Llama3 and
ChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark
removal and exploitation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Watermark for Order-Agnostic Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibo Chen, Yihan Wu, Yanshuo Chen, Chenxi Liu, Junfeng Guo, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical watermarking techniques are well-established for sequentially
decoded language models (LMs). However, these techniques cannot be directly
applied to order-agnostic LMs, as the tokens in order-agnostic LMs are not
generated sequentially. In this work, we introduce Pattern-mark, a
pattern-based watermarking framework specifically designed for order-agnostic
LMs. We develop a Markov-chain-based watermark generator that produces
watermark key sequences with high-frequency key patterns. Correspondingly, we
propose a statistical pattern-based detection algorithm that recovers the key
sequence during detection and conducts statistical tests based on the count of
high-frequency patterns. Our extensive evaluations on order-agnostic LMs, such
as ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detection
efficiency, generation quality, and robustness, positioning it as a superior
watermarking technique for order-agnostic LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BenTo: Benchmark Task Reduction with In-<span class="highlight-title">Context</span> Transferability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language models (LLMs) is costly: it requires the generation
and examination of LLM outputs on a large-scale benchmark of various tasks.
This paper investigates how to efficiently reduce the tasks used to benchmark
LLMs without affecting the evaluation quality. Our study reveals that task
transferability and relevance provide critical information to identify the most
representative subset of tasks via optimizing a facility location function. We
propose a practically efficient metric for estimating the transferability
between two tasks via in-context learning (ICL). By analyzing the pairwise
transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or
FLAN) to 5% while inducing only a <4% difference to the evaluation on the
original benchmark. Compared to prior works, our method is training-free,
gradient-free, and highly efficient requiring ICL only.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael J. Q. Zhang, W. Bradley Knox, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) must often respond to highly ambiguous user
requests. In such cases, the LLM's best response may be to ask a clarifying
question to elicit more information. We observe existing LLMs often respond by
presupposing a single interpretation of such ambiguous requests, frustrating
users who intended a different interpretation. We speculate this is caused by
current preference data labeling practice, where LLM responses are evaluated
only on their prior contexts. To address this, we propose to assign preference
labels by simulating their expected outcomes in the future turns. This allows
LLMs to learn to ask clarifying questions when it can generate responses that
are tailored to each user interpretation in future turns. In experiments on
open-domain QA, we compare systems that trained using our proposed preference
labeling methods against standard methods, which assign preferences based on
only prior context. We evaluate systems based on their ability to ask
clarifying questions that can recover each user's interpretation and expected
answer, and find that our training with our proposed method trains LLMs to ask
clarifying questions with a 5% improvement in F1 measured against the answer
set from different interpretations of each query
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking Inward: Language Models Can Learn About Themselves by
  Introspection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans acquire knowledge by observing the external world, but also by
introspection. Introspection gives a person privileged access to their current
state of mind (e.g., thoughts and feelings) that is not accessible to external
observers. Can LLMs introspect? We define introspection as acquiring knowledge
that is not contained in or derived from training data but instead originates
from internal states. Such a capability could enhance model interpretability.
Instead of painstakingly analyzing a model's internal workings, we could simply
ask the model about its beliefs, world models, and goals. More speculatively,
an introspective model might self-report on whether it possesses certain
internal states such as subjective feelings or desires and this could inform us
about the moral status of these states. Such self-reports would not be entirely
dictated by the model's training data.
  We study introspection by finetuning LLMs to predict properties of their own
behavior in hypothetical scenarios. For example, "Given the input P, would your
output favor the short- or long-term option?" If a model M1 can introspect, it
should outperform a different model M2 in predicting M1's behavior even if M2
is trained on M1's ground-truth behavior. The idea is that M1 has privileged
access to its own behavioral tendencies, and this enables it to predict itself
better than M2 (even if M2 is generally stronger).
  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to
predict itself), we find that the model M1 outperforms M2 in predicting itself,
providing evidence for introspection. Notably, M1 continues to predict its
behavior accurately even after we intentionally modify its ground-truth
behavior. However, while we successfully elicit introspection on simple tasks,
we are unsuccessful on more complex tasks or those requiring
out-of-distribution generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PopAlign: Diversifying Contrasting Patterns for a More Comprehensive
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Moore Wang, Shawn Wang, Kang Zhu, Jiaheng Liu, Ke Xu, Jie Fu, Wangchunshu Zhou, Wenhao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment of large language models (LLMs) involves training models on
preference-contrastive output pairs to adjust their responses according to
human preferences. To obtain such contrastive pairs, traditional methods like
RLHF and RLAIF rely on limited contrasting patterns, such as varying model
variants or decoding temperatures. This singularity leads to two issues: (1)
alignment is not comprehensive; and thereby (2) models are susceptible to
jailbreaking attacks. To address these issues, we investigate how to construct
more comprehensive and diversified contrasting patterns to enhance preference
data (RQ1) and verify the impact of the diversification of contrasting patterns
on model alignment (RQ2). For RQ1, we propose PopAlign, a framework that
integrates diversified contrasting patterns across the prompt, model, and
pipeline levels, introducing six contrasting strategies that do not require
additional feedback labeling procedures. Regarding RQ2, we conduct thorough
experiments demonstrating that PopAlign significantly outperforms existing
methods, leading to more comprehensive alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantity vs. Quality of Monolingual Source Data in Automatic Text
  Translation: Can It Be Too Little If It Is Too Good? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Abdulmumin, Bashir Shehu Galadanci, Garba Aliyu, Shamsuddeen Hassan Muhammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monolingual data, being readily available in large quantities, has been used
to upscale the scarcely available parallel data to train better models for
automatic translation. Self-learning, where a model is made to learn from its
output, is one approach to exploit such data. However, it has been shown that
too much of this data can be detrimental to the performance of the model if the
available parallel data is comparatively extremely low. In this study, we
investigate whether the monolingual data can also be too little and if this
reduction, based on quality, has any effect on the performance of the
translation model. Experiments have shown that on English-German low-resource
NMT, it is often better to select only the most useful additional data, based
on quality or closeness to the domain of the test data, than utilizing all of
the available data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Quantization for Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Ordentlich, Yury Polyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in machine learning community proposed multiple methods for
performing lossy compression (quantization) of large matrices. This
quantization is important for accelerating matrix multiplication (main
component of large language models), which is often bottlenecked by the speed
of loading these matrices from memory. Unlike classical vector quantization and
rate-distortion theory, the goal of these new compression algorithms is to be
able to approximate not the matrices themselves, but their matrix product.
Specifically, given a pair of real matrices $A,B$ an encoder (compressor) is
applied to each of them independently producing descriptions with $R$ bits per
entry. These representations subsequently are used by the decoder to estimate
matrix product $A^\top B$. In this work, we provide a non-asymptotic lower
bound on the mean squared error of this approximation (as a function of rate
$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,
we construct a universal quantizer based on nested lattices with an explicit
guarantee of approximation error for any (non-random) pair of matrices $A$, $B$
in terms of only Frobenius norms $\|A\|_F, \|B\|_F$ and $\|A^\top B\|_F$. For
iid Gaussian matrices our quantizer achieves the lower bound and is, thus,
asymptotically optimal. A practical low-complexity version of our quantizer
achieves performance quite close to optimal. In information-theoretic terms we
derive rate-distortion function for matrix multiplication of iid Gaussian
matrices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Mystery of the Pathological Path-star Task for Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvid Frydenlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced path-star task is a minimal task designed to
exemplify limitations to the abilities of language models (Bachmann and
Nagarajan, 2024). It involves a path-star graph where multiple arms radiate
from a single starting node and each node is unique. Given the start node and a
specified target node that ends an arm, the task is to generate the arm
containing that target node. This is straightforward for a human but
surprisingly difficult for language models, which did not outperform the random
baseline. The authors hypothesized this is due to a deficiency in
teacher-forcing and the next-token prediction paradigm.
  We demonstrate the task is learnable using teacher-forcing in alternative
settings and that the issue is partially due to representation. We introduce a
regularization method using structured samples of the same graph but with
differing target nodes, improving results across a variety of model types. We
provide RASP proofs showing the task is theoretically solvable. Finally, we
find settings where an encoder-only model can consistently solve the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregation Artifacts in Subjective Tasks Collapse Large Language
  Models' Posteriors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context Learning (ICL) has become the primary method for performing
natural language tasks with Large Language Models (LLMs). The knowledge
acquired during pre-training is crucial for this few-shot capability, providing
the model with task priors. However, recent studies have shown that ICL
predominantly relies on retrieving task priors rather than "learning" to
perform tasks. This limitation is particularly evident in complex subjective
domains such as emotion and morality, where priors significantly influence
posterior predictions. In this work, we examine whether this is the result of
the aggregation used in corresponding datasets, where trying to combine
low-agreement, disparate annotations might lead to annotation artifacts that
create detrimental noise in the prompt. Moreover, we evaluate the posterior
bias towards certain annotators by grounding our study in appropriate,
quantitative measures of LLM priors. Our results indicate that aggregation is a
confounding factor in the modeling of subjective tasks, and advocate focusing
on modeling individuals instead. However, aggregation does not explain the
entire gap between ICL and the state of the art, meaning other factors in such
tasks also account for the observed phenomena. Finally, by rigorously studying
annotator-level labels, we find that it is possible for minority annotators to
both better align with LLMs and have their perspectives further amplified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Aware Query Expansion with Large Language Models for Textual
  and Relational Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A. Rossi, Haoliang Wang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been used to generate query expansions
augmenting original queries for improving information search. Recent studies
also explore providing LLMs with initial retrieval results to generate query
expansions more grounded to document corpus. However, these methods mostly
focus on enhancing textual similarities between search queries and target
documents, overlooking document relations. For queries like "Find me a highly
rated camera for wildlife photography compatible with my Nikon F-Mount lenses",
existing methods may generate expansions that are semantically similar but
structurally unrelated to user intents. To handle such semi-structured queries
with both textual and relational requirements, in this paper we propose a
knowledge-aware query expansion framework, augmenting LLMs with structured
document relations from knowledge graph (KG). To further address the limitation
of entity-based scoring in existing KG-based methods, we leverage document
texts as rich KG node representations and use document-based relation filtering
for our Knowledge-Aware Retrieval (KAR). Extensive experiments on three
datasets of diverse domains show the advantages of our method compared against
state-of-the-art baselines on textual and relational semi-structured retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobA: A Two-Level Agent System for Efficient Mobile Task Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Zhu, Hao Tang, Yansi Li, Kunyao Lan, Yixuan Jiang, Hao Zhou, Yixiao Wang, Situo Zhang, Liangtai Sun, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current mobile assistants are limited by dependence on system APIs or
struggle with complex user instructions and diverse interfaces due to
restricted comprehension and decision-making abilities. To address these
challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal
large language models that enhances comprehension and planning capabilities
through a sophisticated two-level agent architecture. The high-level Global
Agent (GA) is responsible for understanding user commands, tracking history
memories, and planning tasks. The low-level Local Agent (LA) predicts detailed
actions in the form of function calls, guided by sub-tasks and memory from the
GA. Integrating a Reflection Module allows for efficient task completion and
enables the system to handle previously unseen complex tasks. MobA demonstrates
significant improvements in task execution efficiency and completion rate in
real-life evaluations, underscoring the potential of MLLM-empowered mobile
assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures, and 5 tables. We will release our source code in
  a few days</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Human Pipeline for Cultural <span class="highlight-title">Context</span> Grounding of Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajkumar Pujari, Dan Goldwasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversations often adhere to well-understood social norms that vary across
cultures. For example, while "addressing parents by name" is commonplace in the
West, it is rare in most Asian cultures. Adherence or violation of such norms
often dictates the tenor of conversations. Humans are able to navigate social
situations requiring cultural awareness quite adeptly. However, it is a hard
task for NLP models.
  In this paper, we tackle this problem by introducing a "Cultural Context
Schema" for conversations. It comprises (1) conversational information such as
emotions, dialogue acts, etc., and (2) cultural information such as social
norms, violations, etc. We generate ~110k social norm and violation
descriptions for ~23k conversations from Chinese culture using LLMs. We refine
them using automated verification strategies which are evaluated against
culturally aware human judgements. We organize these descriptions into
meaningful structures we call "Norm Concepts", using an interactive
human-in-loop framework. We ground the norm concepts and the descriptions in
conversations using symbolic annotation. Finally, we use the obtained dataset
for downstream tasks such as emotion, sentiment, and dialogue act detection. We
show that it significantly improves the empirical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRAGE-Bench: Automatic Multilingual Benchmark Arena for
  Retrieval-Augmented Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, Amin Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different
heuristic-based metrics for evaluation, but these require human preferences as
ground truth for reference. In contrast, arena-based benchmarks, where two
models compete each other, require an expensive Large Language Model (LLM) as a
judge for a reliable evaluation. We present an easy and efficient technique to
get the best of both worlds. The idea is to train a learning to rank model as a
"surrogate" judge using RAG-based evaluation heuristics as input, to produce a
synthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a
standardized arena-based multilingual RAG benchmark for 18 diverse languages on
Wikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and
extended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG
extensively coupling both heuristic features and LLM as a judge evaluator. In
our work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high
correlation (Kendall Tau ($\tau$) = 0.909) using our surrogate judge learned
using heuristic features with pairwise evaluations and between GPT-4o as a
teacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We
observe proprietary and large open-source LLMs currently dominate in
multilingual RAG. MIRAGE-Bench is available at:
https://github.com/vectara/mirage-bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Attention Heads in Large Language Model Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve state-of-the-art performance on multiple
language tasks, yet their safety guardrails can be circumvented, leading to
harmful generations. In light of this, recent research on safety mechanisms has
emerged, revealing that when safety representations or component are
suppressed, the safety capability of LLMs are compromised. However, existing
research tends to overlook the safety impact of multi-head attention
mechanisms, despite their crucial role in various model functionalities. Hence,
in this paper, we aim to explore the connection between standard attention
mechanisms and safety capability to fill this gap in the safety-related
mechanistic interpretability. We propose a novel metric which tailored for
multi-head attention, the Safety Head ImPortant Score (Ships), to assess the
individual heads' contributions to model safety. Based on this, we generalize
Ships to the dataset level and further introduce the Safety Attention Head
AttRibution Algorithm (Sahara) to attribute the critical safety attention heads
inside the model. Our findings show that the special attention head has a
significant impact on safety. Ablating a single safety head allows aligned
model (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,
while only modifying 0.006% of the parameters, in contrast to the ~ 5%
modification required in previous studies. More importantly, we demonstrate
that attention heads primarily function as feature extractors for safety and
models fine-tuned from the same base model exhibit overlapping safety heads
through comprehensive experiments. Together, our attribution approach and
findings provide a novel perspective for unpacking the black box of safety
mechanisms within large models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unconstrained Model Merging for Enhanced LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhang, Baoyi He, Shengyu Zhang, Yuhao Fu, Qi Zhou, Zhijie Sang, Zijin Hong, Kejing Yang, Wenjun Wang, Jianbo Yuan, Guangning Han, Linyi Li, Chunlin Ji, Fei Wu, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in building domain-specific large language models (LLMs)
have shown remarkable success, especially in tasks requiring reasoning
abilities like logical inference over complex relationships and multi-step
problem solving. However, creating a powerful all-in-one LLM remains
challenging due to the need for proprietary data and vast computational
resources. As a resource-friendly alternative, we explore the potential of
merging multiple expert models into a single LLM. Existing studies on model
merging mainly focus on generalist LLMs instead of domain experts, or the LLMs
under the same architecture and size. In this work, we propose an unconstrained
model merging framework that accommodates both homogeneous and heterogeneous
model architectures with a focus on reasoning tasks. A fine-grained layer-wise
weight merging strategy is designed for homogeneous models merging, while
heterogeneous model merging is built upon the probabilistic distribution
knowledge derived from instruction-response fine-tuning data. Across 7
benchmarks and 9 reasoning-optimized LLMs, we reveal key findings that
combinatorial reasoning emerges from merging which surpasses simple additive
effects. We propose that unconstrained model merging could serve as a
foundation for decentralized LLMs, marking a notable progression from the
existing centralized LLM framework. This evolution could enhance wider
participation and stimulate additional advancement in the field of artificial
intelligence, effectively addressing the constraints posed by centralized
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Design Space of Visual <span class="highlight-title">Context</span> Representation in Video
  MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Du, Yuqi Huo, Kun Zhou, Zijia Zhao, Haoyu Lu, Han Huang, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Multimodal Large Language Models (MLLMs) have shown remarkable
capability of understanding the video semantics on various downstream tasks.
Despite the advancements, there is still a lack of systematic research on
visual context representation, which refers to the scheme to select frames from
a video and further select the tokens from a frame. In this paper, we explore
the design space for visual context representation, and aim to improve the
performance of video MLLMs by finding more effective representation schemes.
Firstly, we formulate the task of visual context representation as a
constrained optimization problem, and model the language modeling loss as a
function of the number of frames and the number of embeddings (or tokens) per
frame, given the maximum visual context window size. Then, we explore the
scaling effects in frame selection and token selection respectively, and fit
the corresponding function curve by conducting extensive empirical experiments.
We examine the effectiveness of typical selection strategies and present
empirical findings to determine the two factors. Furthermore, we study the
joint effect of frame selection and token selection, and derive the optimal
formula for determining the two factors. We demonstrate that the derived
optimal settings show alignment with the best-performed results of empirical
experiments. Our code and model are available at:
https://github.com/RUCAIBox/Opt-Visor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long Video MLLM; work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-Based Sign Language Appearance Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Moryossef, Gerard Sant, Zifan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for transferring the signer's appearance in sign
language skeletal poses while preserving the sign content. Using estimated
poses, we transfer the appearance of one signer to another, maintaining natural
movements and transitions. This approach improves pose-based rendering and sign
stitching while obfuscating identity. Our experiments show that while the
method reduces signer identification accuracy, it slightly harms sign
recognition performance, highlighting a tradeoff between privacy and utility.
Our code is available at
\url{https://github.com/sign-language-processing/pose-anonymization}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World
  Multilingual Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Gumma, Anandhita Raghunath, Mohit Jain, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the capabilities and limitations of large language models (LLMs)
has garnered significant interest, yet the evaluation of multiple models in
real-world scenarios remains rare. Multilingual evaluation often relies on
translated benchmarks, which typically do not capture linguistic and cultural
nuances present in the source language. This study provides an extensive
assessment of 24 LLMs on real world data collected from Indian patients
interacting with a medical chatbot in Indian English and 4 other Indic
languages. We employ a uniform Retrieval Augmented Generation framework to
generate responses, which are evaluated using both automated techniques and
human evaluators on four specific metrics relevant to our application. We find
that models vary significantly in their performance and that instruction tuned
Indic models do not always perform well on Indic language queries. Further, we
empirically show that factual correctness is generally lower for responses to
Indic queries compared to English queries. Finally, our qualitative work shows
that code-mixed and culturally relevant queries in our dataset pose challenges
to evaluated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ signwriting-evaluation: Effective Sign Language Evaluation via
  SignWriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Moryossef, Rotem Zilberman, Ohad Langer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of automatic evaluation metrics tailored for SignWriting presents a
significant obstacle in developing effective transcription and translation
models for signed languages. This paper introduces a comprehensive suite of
evaluation metrics specifically designed for SignWriting, including adaptations
of standard metrics such as \texttt{BLEU} and \texttt{chrF}, the application of
\texttt{CLIPScore} to SignWriting images, and a novel symbol distance metric
unique to our approach. We address the distinct challenges of evaluating single
signs versus continuous signing and provide qualitative demonstrations of
metric efficacy through score distribution analyses and nearest-neighbor
searches within the SignBank corpus. Our findings reveal the strengths and
limitations of each metric, offering valuable insights for future advancements
using SignWriting. This work contributes essential tools for evaluating
SignWriting models, facilitating progress in the field of sign language
processing. Our code is available at
\url{https://github.com/sign-language-processing/signwriting-evaluation}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection
  and Argumentative Dialogue Summarization <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiutian Zhao, Ke Wang, Wei Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue agents have been receiving increasing attention for years, and this
trend has been further boosted by the recent progress of large language models
(LLMs). Stance detection and dialogue summarization are two core tasks of
dialogue agents in application scenarios that involve argumentative dialogues.
However, research on these tasks is limited by the insufficiency of public
datasets, especially for non-English languages. To address this language
resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first
Chinese dataset for benchmarking target-independent stance detection and debate
summarization. Our dataset consists of 1,218 real-world debates that were
conducted in Chinese on 476 unique topics, containing 2,436 stance-specific
summaries and 14,133 fully annotated utterances. Besides providing a versatile
testbed for future research, we also conduct an empirical study on the dataset
and propose an integrated task. The results show the challenging nature of the
dataset and suggest a potential of incorporating stance detection in
summarization for argumentative dialogue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic
  Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailaja Keyur Sampat, Mutsumi Nakamura, Shankar Kailas, Kartik Aggarwal, Mandy Zhou, Yezhou Yang, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deriving inference from heterogeneous inputs (such as images, text, and
audio) is an important skill for humans to perform day-to-day tasks. A similar
ability is desirable for the development of advanced Artificial Intelligence
(AI) systems. While state-of-the-art models are rapidly closing the gap with
human-level performance on diverse computer vision and NLP tasks separately,
they struggle to solve tasks that require joint reasoning over visual and
textual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask
benchmark for natural language understanding, we propose VL-GLUE in this paper.
VL-GLUE consists of over 100k samples spanned across seven different tasks,
which at their core require visuo-linguistic reasoning. Moreover, our benchmark
comprises of diverse image types (from synthetically rendered figures, and
day-to-day scenes to charts and complex diagrams) and includes a broad variety
of domain-specific text (from cooking, politics, and sports to high-school
curricula), demonstrating the need for multi-modal understanding in the
real-world. We show that this benchmark is quite challenging for existing
large-scale vision-language models and encourage development of systems that
possess robust visuo-linguistic reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Red and blue language: Word choices in the Trump & Harris 2024
  presidential debate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Wicke, Marianna M. Bolognesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Political debates are a peculiar type of political discourse, in which
candidates directly confront one another, addressing not only the the
moderator's questions, but also their opponent's statements, as well as the
concerns of voters from both parties and undecided voters. Therefore, language
is adjusted to meet specific expectations and achieve persuasion. We analyse
how the language of Trump and Harris during the debate (September 10th 2024)
differs in relation to the following semantic and pragmatic features, for which
we formulated targeted hypotheses: framing values and ideology, appealing to
emotion, using words with different degrees of concreteness and specificity,
addressing others through singular or plural pronouns. Our findings include:
differences in the use of figurative frames (Harris often framing issues around
recovery and empowerment, Trump often focused on crisis and decline); similar
use of emotional language, with Trump showing a slight higher tendency toward
negativity and toward less subjective language compared to Harris; no
significant difference in the specificity of candidates' responses; similar use
of abstract language, with Trump showing more variability than Harris,
depending on the subject discussed; differences in addressing the opponent,
with Trump not mentioning Harris by name, while Harris referring to Trump
frequently; different uses of pronouns, with Harris using both singular and
plural pronouns equally, while Trump using more singular pronouns. The results
are discussed in relation to previous literature on Red and Blue language,
which refers to distinct linguistic patterns associated with conservative (Red)
and liberal (Blue) political ideologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to PLOS ONE, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A new approach for fine-tuning sentence <span class="highlight-title">transformer</span>s for intent
  classification and out-of-scope detection tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Zhang, Atta Norouzian, Aanchan Mohan, Frederick Ducatelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In virtual assistant (VA) systems it is important to reject or redirect user
queries that fall outside the scope of the system. One of the most accurate
approaches for out-of-scope (OOS) rejection is to combine it with the task of
intent classification on in-scope queries, and to use methods based on the
similarity of embeddings produced by transformer-based sentence encoders.
Typically, such encoders are fine-tuned for the intent-classification task,
using cross-entropy loss. Recent work has shown that while this produces
suitable embeddings for the intent-classification task, it also tends to
disperse in-scope embeddings over the full sentence embedding space. This
causes the in-scope embeddings to potentially overlap with OOS embeddings,
thereby making OOS rejection difficult. This is compounded when OOS data is
unknown. To mitigate this issue our work proposes to regularize the
cross-entropy loss with an in-scope embedding reconstruction loss learned using
an auto-encoder. Our method achieves a 1-4% improvement in the area under the
precision-recall curve for rejecting out-of-sample (OOS) instances, without
compromising intent classification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing at Empirical Methods in Natural Language Processing 2025 -
  Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit
  ToM Application in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared Moore, Ronan Le Bras, Peter Clark, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While prior work has explored whether large language models (LLMs) possess a
"theory of mind" (ToM) - the ability to attribute mental states to oneself and
others - there has been little work testing whether LLMs can implicitly apply
such knowledge to predict behavior, or to judge whether an observed behavior is
rational. Such skills are critical for appropriate interaction in social
environments. We create a new dataset, SimpleTom, containing concise, diverse
stories (e.g., "The can of Pringles has moldy chips in it. Mary picks up the
can in the supermarket and walks to the cashier."), each with three questions
that test different degrees of ToM reasoning, asking models to predict (a)
mental state ("Is Mary aware of the mold?"), (b) behavior ("Will Mary pay for
the chips or report the mold?"), and (c) judgment ("Mary paid for the chips.
Was that reasonable?"). To our knowledge, SimpleToM is the first dataset to
systematically explore downstream reasoning requiring knowledge of mental
states in realistic scenarios. Our experimental results are intriguing: While
most models can reliably predict mental state on our dataset (a), they often
fail to correctly predict the behavior (b), and fare even worse at judging
whether given behaviors are reasonable (c), despite being correctly aware of
the protagonist's mental state should make such secondary predictions obvious.
We further show that we can help models do better at (b) and (c) via
interventions such as reminding the model of its earlier mental state answer
and mental-state-specific chain-of-thought prompting, raising the action
prediction accuracies (e.g., from 49.5% to 93.5% for GPT-4o) and judgment
accuracies (e.g., from 15.3% to 94.7% in GPT-4o). While this shows that models
can be coaxed to perform well, it requires task-specific interventions, and the
natural model performances remain low, a cautionary tale for LLM deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Active Learning Framework for Inclusive Generation by Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabit Hassan, Anthony Sicilia, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that Large Language Models (LLMs) generate text representative of
diverse sub-populations is essential, particularly when key concepts related to
under-represented groups are scarce in the training data. We address this
challenge with a novel clustering-based active learning framework, enhanced
with knowledge distillation. The proposed framework transforms the intermediate
outputs of the learner model, enabling effective active learning for generative
tasks for the first time. Integration of clustering and knowledge distillation
yields more representative models without prior knowledge of underlying data
distribution and overbearing human efforts. We validate our approach in
practice through case studies in counter-narration and style transfer. We
construct two new datasets in tandem with model training, showing a performance
improvement of 2%-10% over baseline models. Our results also show more
consistent performance across various data subgroups and increased lexical
diversity, underscoring our model's resilience to skewness in available data.
Further, our results show that the data acquired via our approach improves the
performance of secondary models not involved in the learning loop, showcasing
practical utility of the framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM self-evaluation relies on the LLM's own ability to estimate response
correctness, which can greatly improve its deployment reliability. In this
research track, we propose the Chain-of-Embedding (CoE) in the latent space to
enable LLMs to perform output-free self-evaluation. CoE consists of all
progressive hidden states produced during the inference time, which can be
treated as the latent thinking path of LLMs. We find that when LLMs respond
correctly and incorrectly, their CoE features differ, these discrepancies
assist us in estimating LLM response correctness. Experiments in four diverse
domains and seven LLMs fully demonstrate the effectiveness of our method.
Meanwhile, its label-free design intent without any training and
millisecond-level computational cost ensure real-time feedback in large-scale
scenarios. More importantly, we provide interesting insights into LLM response
correctness from the perspective of hidden state changes inside LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 18 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study on Reasoning Patterns of OpenAI's o1 Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, J. H. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling Large Language Models (LLMs) to handle a wider range of complex
tasks (e.g., coding, math) has drawn great attention from many researchers. As
LLMs continue to evolve, merely increasing the number of model parameters
yields diminishing performance improvements and heavy computational costs.
Recently, OpenAI's o1 model has shown that inference strategies (i.e.,
Test-time Compute methods) can also significantly enhance the reasoning
capabilities of LLMs. However, the mechanisms behind these methods are still
unexplored. In our work, to investigate the reasoning patterns of o1, we
compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent
Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general
reasoning benchmarks in three domains (i.e., math, coding, commonsense
reasoning). Specifically, first, our experiments show that the o1 model has
achieved the best performance on most datasets. Second, as for the methods of
searching diverse responses (e.g., BoN), we find the reward models' capability
and the search space both limit the upper boundary of these methods. Third, as
for the methods that break the problem into many sub-problems, the Agent
Workflow has achieved better performance than Step-wise BoN due to the
domain-specific system prompt for planning better reasoning processes. Fourth,
it is worth mentioning that we have summarized six reasoning patterns of o1,
and provided a detailed analysis on several reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H2OVL-Mississippi Vision Language Models Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, Sri Satish Ambati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smaller vision-language models (VLMs) are becoming increasingly important for
privacy-focused, on-device applications due to their ability to run efficiently
on consumer hardware for processing enterprise commercial documents and images.
These models require strong language understanding and visual capabilities to
enhance human-machine interaction. To address this need, we present
H2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs
using 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny
model with 0.8 billion parameters that specializes in text recognition,
achieving state of the art performance on the Text Recognition portion of
OCRBench and surpassing much larger models in this area. Additionally, we are
releasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use
cases, exhibiting highly competitive metrics across various academic
benchmarks. Both models build upon our prior work with H2O-Danube language
models, extending their capabilities into the visual domain. We release them
under the Apache 2.0 license, making VLMs accessible to everyone, democratizing
document AI and visual LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool
  Calling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Xiaofan Zhang, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating tools into Large Language Models (LLMs) has facilitated the
widespread application. Despite this, in specialized downstream task contexts,
reliance solely on tools is insufficient to fully address the complexities of
the real world. This particularly restricts the effective deployment of LLMs in
fields such as medicine. In this paper, we focus on the downstream tasks of
medical calculators, which use standardized tests to assess an individual's
health status. We introduce MeNTi, a universal agent architecture for LLMs.
MeNTi integrates a specialized medical toolkit and employs meta-tool and nested
calling mechanisms to enhance LLM tool utilization. Specifically, it achieves
flexible tool selection and nested tool calling to address practical issues
faced in intricate medical scenarios, including calculator selection, slot
filling, and unit conversion. To assess the capabilities of LLMs for
quantitative assessment throughout the clinical process of calculator
scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical
calculators to perform calculations and assess patient health status. CalcQA is
constructed by professional physicians and includes 100 case-calculator pairs,
complemented by a toolkit of 281 medical tools. The experimental results
demonstrate significant performance improvements with our framework. This
research paves new directions for applying LLMs in demanding scenarios of
medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Narrative-Driven Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Eberhard, Thorsten Ruprechter, Denis Helic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative-driven recommenders aim to provide personalized suggestions for
user requests expressed in free-form text such as "I want to watch a thriller
with a mind-bending story, like Shutter Island." Although large language models
(LLMs) have been shown to excel in processing general natural language queries,
their effectiveness for handling such recommendation requests remains
relatively unexplored. To close this gap, we compare the performance of 38
open- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in
a movie recommendation setting. For this, we utilize a gold-standard,
crowdworker-annotated dataset of posts from reddit's movie suggestion community
and employ various prompting strategies, including zero-shot, identity, and
few-shot prompting. Our findings demonstrate the ability of LLMs to generate
contextually relevant movie recommendations, significantly outperforming other
state-of-the-art approaches, such as doc2vec. While we find that closed-source
and large-parameterized models generally perform best, medium-sized open-source
models remain competitive, being only slightly outperformed by their more
computationally expensive counterparts. Furthermore, we observe no significant
differences across prompting strategies for most models, underscoring the
effectiveness of simple approaches such as zero-shot prompting for
narrative-driven recommendations. Overall, this work offers valuable insights
for recommender system researchers as well as practitioners aiming to integrate
LLMs into real-world recommendation tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review; 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Fact Retrieval in PLMs through Truthfulness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Youssef, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Language Models (PLMs) encode various facts about the world at
their pre-training phase as they are trained to predict the next or missing
word in a sentence. There has a been an interest in quantifying and improving
the amount of facts that can be extracted from PLMs, as they have been
envisioned to act as soft knowledge bases, which can be queried in natural
language. Different approaches exist to enhance fact retrieval from PLM. Recent
work shows that the hidden states of PLMs can be leveraged to determine the
truthfulness of the PLMs' inputs. Leveraging this finding to improve factual
knowledge retrieval remains unexplored. In this work, we investigate the use of
a helper model to improve fact retrieval. The helper model assesses the
truthfulness of an input based on the corresponding hidden states
representations from the PLMs. We evaluate this approach on several masked PLMs
and show that it enhances fact retrieval by up to 33\%. Our findings highlight
the potential of hidden states representations from PLMs in improving their
factual knowledge retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Temporal Representations for Dynamic Memory Retrieval and
  Management in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Hou, Haruki Tamoto, Homei Miyashita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional dialogue agents often struggle with effective memory recall,
leading to redundant retrieval and inadequate management of unique user
associations. To address this, we propose SynapticRAG, a novel approach
integrating synaptic dynamics into Retrieval-Augmented Generation (RAG).
SynapticRAG integrates temporal representations into memory vectors, mimicking
biological synapses by differentiating events based on occurrence times and
dynamically updating memory significance. This model employs temporal scoring
for memory connections and a synaptic-inspired propagation control mechanism.
Experiments across English, Japanese, and Chinese datasets demonstrate
SynapticRAG's superiority over existing methods, including traditional RAG,
with up to 14.66\% improvement in memory retrieval accuracy. Our approach
advances context-aware dialogue AI systems by enhancing long-term context
maintenance and specific information extraction from conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias in the Mirror : Are LLMs opinions robust to their own adversarial
  attacks ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virgile Rennard, Christos Xypolopoulos, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) inherit biases from their training data and
alignment processes, influencing their responses in subtle ways. While many
studies have examined these biases, little work has explored their robustness
during interactions. In this paper, we introduce a novel approach where two
instances of an LLM engage in self-debate, arguing opposing viewpoints to
persuade a neutral version of the model. Through this, we evaluate how firmly
biases hold and whether models are susceptible to reinforcing misinformation or
shifting to harmful viewpoints. Our experiments span multiple LLMs of varying
sizes, origins, and languages, providing deeper insights into bias persistence
and flexibility across linguistic and cultural contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoCoder: Solving Geometry Problems by Generating Modular Code through
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sharma, Aman Dalmia, Mehran Kazemi, Amal Zouaq, Christopher J. Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometry problem-solving demands advanced reasoning abilities to process
multimodal inputs and employ mathematical knowledge effectively.
Vision-language models (VLMs) have made significant progress in various
multimodal tasks. Yet, they still struggle with geometry problems and are
significantly limited by their inability to perform mathematical operations not
seen during pre-training, such as calculating the cosine of an arbitrary angle,
and by difficulties in correctly applying relevant geometry formulas. To
overcome these challenges, we present GeoCoder, which leverages modular
code-finetuning to generate and execute code using a predefined geometry
function library. By executing the code, we achieve accurate and deterministic
calculations, contrasting the stochastic nature of autoregressive token
prediction, while the function library minimizes errors in formula usage. We
also propose a multimodal retrieval-augmented variant of GeoCoder, named
RAG-GeoCoder, which incorporates a non-parametric memory module for retrieving
functions from the geometry library, thereby reducing reliance on parametric
memory. Our modular code-finetuning approach enhances the geometric reasoning
capabilities of VLMs, yielding an average improvement of over 16% across
various question complexities on the GeomVerse dataset compared to other
finetuning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable
  Data Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven its effectiveness in
mitigating hallucinations in Large Language Models (LLMs) by retrieving
knowledge from external resources. To adapt LLMs for RAG pipelines, current
approaches use instruction tuning to optimize LLMs, improving their ability to
utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses
on equipping LLMs to handle diverse RAG tasks using different instructions.
However, it trains RAG modules to overfit training signals and overlooks the
varying data preferences among agents within the RAG system. In this paper, we
propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG
systems by aligning data preferences between different RAG modules. DDR works
by collecting the rewards to optimize each agent with a rollout method. This
method prompts agents to sample some potential responses as perturbations,
evaluates the impact of these perturbations on the whole RAG system, and
subsequently optimizes the agent to produce outputs that improve the
performance of the RAG system. Our experiments on various knowledge-intensive
tasks demonstrate that DDR significantly outperforms the SFT method,
particularly for LLMs with smaller-scale parameters that depend more on the
retrieved knowledge. Additionally, DDR exhibits a stronger capability to align
the data preference between RAG modules. The DDR method makes generation module
more effective in extracting key information from documents and mitigating
conflicts between parametric memory and external knowledge. All codes are
available at https://github.com/OpenMatch/RAG-DDR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily
  Complex Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve arithmetic word problems with high
accuracy, but little is known about how well they generalize to problems that
are more complex than the ones on which they have been trained. Empirical
investigations of such questions are impeded by two major flaws of current
evaluations: (i) much of the evaluation data is contaminated, in the sense that
it has already been seen during training, and (ii) benchmark datasets do not
capture how problem proofs may be arbitrarily complex in various ways. As a
step towards addressing these issues, we present a framework for evaluating
LLMs on problems that have arbitrarily complex arithmetic proofs, called
MathGAP. MathGAP generates problems that follow fixed proof specifications --
along with chain-of-thought reasoning annotations -- enabling systematic
studies on generalization with respect to arithmetic proof complexity. We apply
MathGAP to analyze how in-context learning interacts with generalization to
problems that have more complex proofs. We find that among the models tested,
most show a significant decrease in performance as proofs get deeper and wider.
This effect is more pronounced in complex, nonlinear proof structures, which
are challenging even for GPT-4o. Surprisingly, providing in-context examples
from the same distribution as the test set is not always beneficial for
performance. In particular, zero-shot prompting as well as demonstrating a
diverse range of examples that are less complex than the test data sometimes
yield similar or higher accuracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum
  Learning, Semi-Supervised Training, and Advanced Optimization Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahimanuddin Shaik, Katikela Sreeharsha Kishore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text generation is the automated process of producing written or spoken
language using computational methods. It involves generating coherent and
contextually relevant text based on predefined rules or learned patterns.
However, challenges in text generation arise from maintaining coherence,
ensuring diversity and creativity, and avoiding biases or inappropriate
content. This research paper developed a novel approach to improve text
generation in the context of joint Natural Language Generation (NLG) and
Natural Language Understanding (NLU) learning. The data is prepared by
gathering and preprocessing annotated datasets, including cleaning,
tokenization, stemming, and stop-word removal. Feature extraction techniques
such as POS tagging, Bag of words, and Term Frequency-Inverse Document
Frequency (TF-IDF) are applied. Transformer-based encoders and decoders,
capturing long range dependencies and improving source-target sequence
modelling. Pre-trained language models like Optimized BERT are incorporated,
along with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).
Reinforcement learning with policy gradient techniques, semi-supervised
training, improved attention mechanisms, and differentiable approximations like
straight-through Gumbel SoftMax estimator are employed to fine-tune the models
and handle complex linguistic tasks effectively. The proposed model is
implemented using Python.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repetition Neurons: How Do Language Models Produce Repetitions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuya Hiraoka, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces repetition neurons, regarded as skill neurons
responsible for the repetition problem in text generation tasks. These neurons
are progressively activated more strongly as repetition continues, indicating
that they perceive repetition as a task to copy the previous context
repeatedly, similar to in-context learning. We identify these repetition
neurons by comparing activation values before and after the onset of repetition
in texts generated by recent pre-trained language models. We analyze the
repetition neurons in three English and one Japanese pre-trained language
models and observe similar patterns across them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Through Visual<span class="highlight-title">BERT</span>: A Causal Adventure on Memetic Landscapes <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dibyanayan Bandyopadhyay, Mohammed Hasanuzzaman, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting offensive memes is crucial, yet standard deep neural network
systems often remain opaque. Various input attribution-based methods attempt to
interpret their behavior, but they face challenges with implicitly offensive
memes and non-causal attributions. To address these issues, we propose a
framework based on a Structural Causal Model (SCM). In this framework,
VisualBERT is trained to predict the class of an input meme based on both meme
input and causal concepts, allowing for transparent interpretation. Our
qualitative evaluation demonstrates the framework's effectiveness in
understanding model behavior, particularly in determining whether the model was
right due to the right reason, and in identifying reasons behind
misclassification. Additionally, quantitative analysis assesses the
significance of proposed modelling choices, such as de-confounding, adversarial
learning, and dynamic routing, and compares them with input attribution
methods. Surprisingly, we find that input attribution methods do not guarantee
causality within our framework, raising questions about their reliability in
safety-critical applications. The project page is at:
https://newcodevelop.github.io/causality_adventure/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IterSelectTune: An Iterative Training Framework for Efficient
  Instruction-Tuning Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jielin Song, Siyu Liu, Bin Zhu, Yanghui Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, instruction tuning has
become critical for improving their ability to generate accurate and
contextually appropriate responses. Although numerous instruction-tuning
datasets have been developed to enhance LLM performance, selecting high-quality
instruction data from large source datasets typically demands significant human
effort. In this work, we introduce $\textbf{IterSelectTune}$, an efficient,
cost-effective iterative training policy for selecting high-quality instruction
data with no human involvement and limited reliance on GPT-4. By fine-tuning on
approximately 20\% of the source data, our method consistently outperforms
models fine-tuned on the full dataset across multiple benchmarks and public
test datasets. These results highlight the effectiveness of our approach in
enhancing LLM performance while reducing the computational resources required
for instruction tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Mixed-Precision Decoding for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the great potential of large language models (LLMs) across
various tasks, their deployment on resource-constrained devices remains
challenging due to their excessive computational and memory demands.
Quantization has emerged as an effective solution by storing weights in reduced
precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially
alleviate the memory-boundedness of LLM decoding, still suffers from
prohibitive performance drop. In this work, we argue that existing approaches
fail to explore the diversity in computational patterns, redundancy, and
sensitivity to approximations of the different phases of LLM inference,
resorting to a uniform quantization policy throughout. Instead, we propose a
novel phase-aware method that selectively allocates precision during different
phases of LLM inference, achieving both strong context extraction during
prefill and efficient memory bandwidth utilization during decoding. To further
address the memory-boundedness of the decoding phase, we introduce Progressive
Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering
of precision deeper in the generated sequence, together with a spectrum of
precision-switching schedulers that dynamically drive the precision-lowering
decisions in either task-adaptive or prompt-adaptive manner. Extensive
evaluation across diverse language tasks shows that when targeting Nvidia GPUs,
PMPD achieves 1.4$-$12.2$\times$ speedup in matrix-vector multiplications over
fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a
throughput gain of 3.8$-$8.0$\times$ over fp16 models and up to 1.54$\times$
over uniform quantization approaches while preserving the output quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Manual Annotation Bottleneck: Creating a Comprehensive
  Legal Case Criticality <span class="highlight-title">Dataset</span> through Semi-Automated Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronja Stern, Ken Kawamura, Matthias Stürmer, Ilias Chalkidis, Joel Niklaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting case criticality helps legal professionals in the court system
manage large volumes of case law. This paper introduces the Criticality
Prediction dataset, a new resource for evaluating the potential influence of
Swiss Federal Supreme Court decisions on future jurisprudence. Unlike existing
approaches that rely on resource-intensive manual annotations, we
semi-automatically derive labels leading to a much larger dataset than
otherwise possible. Our dataset features a two-tier labeling system: (1) the
LD-Label, which identifies cases published as Leading Decisions (LD), and (2)
the Citation-Label, which ranks cases by their citation frequency and recency.
This allows for a more nuanced evaluation of case importance. We evaluate
several multilingual models, including fine-tuned variants and large language
models, and find that fine-tuned models consistently outperform zero-shot
baselines, demonstrating the need for task-specific adaptation. Our
contributions include the introduction of this task and the release of a
multilingual dataset to the research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedINST: Meta <span class="highlight-title">Dataset</span> of Biomedical Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, Zirui Song, Ling Chen, Mykola Pechenizkiy, Qingyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language model (LLM) techniques in the field of
medical analysis has brought about significant advancements, yet the scarcity
of large, diverse, and well-annotated datasets remains a major challenge.
Medical data and tasks, which vary in format, size, and other parameters,
require extensive preprocessing and standardization for effective use in
training LLMs. To address these challenges, we introduce MedINST, the Meta
Dataset of Biomedical Instructions, a novel multi-domain, multi-task
instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over
7 million training samples, making it the most comprehensive biomedical
instruction dataset to date. Using MedINST as the meta dataset, we curate
MedINST32, a challenging benchmark with different task difficulties aiming to
evaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and
evaluate on MedINST32, showcasing enhanced cross-task generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Legal Knowledge: A Multilingual <span class="highlight-title">Dataset</span> for Judicial
  Summarization in Switzerland 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Rolshoven, Vishvaksenan Rasiah, Srinanda Brügger Bose, Matthias Stürmer, Joel Niklaus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal research is a time-consuming task that most lawyers face on a daily
basis. A large part of legal research entails looking up relevant caselaw and
bringing it in relation to the case at hand. Lawyers heavily rely on summaries
(also called headnotes) to find the right cases quickly. However, not all
decisions are annotated with headnotes and writing them is time-consuming.
Automated headnote creation has the potential to make hundreds of thousands of
decisions more accessible for legal research in Switzerland alone. To kickstart
this, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a
novel cross-lingual resource featuring 18K court rulings from the Swiss Federal
Supreme Court (SFSC), in German, French, and Italian, along with German
headnotes. We fine-tune and evaluate three mT5 variants, along with proprietary
models. Our analysis highlights that while proprietary models perform well in
zero-shot and one-shot settings, fine-tuned smaller models still provide a
strong competitive edge. We publicly release the dataset to facilitate further
research in multilingual legal summarization and the development of assistive
technologies for legal professionals
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-efficient Adaptation of Multilingual Multimodal Models for
  Low-resource ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Gupta, Amruta Parulekar, Sameep Chattopadhyay, Preethi Jyothi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) for low-resource languages remains a
challenge due to the scarcity of labeled training data. Parameter-efficient
fine-tuning and text-only adaptation are two popular methods that have been
used to address such low-resource settings. In this work, we investigate how
these techniques can be effectively combined using a multilingual multimodal
model like SeamlessM4T. Multimodal models are able to leverage unlabeled text
via text-only adaptation with further parameter-efficient ASR fine-tuning, thus
boosting ASR performance. We also show cross-lingual transfer from a
high-resource language, achieving up to a relative 17% WER reduction over a
baseline in a zero-shot setting without any labeled speech.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLIP_Lab-IITH Multilingual MT System for WAT24 MT Shared Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maharaj Brahma, Pramit Sahoo, Maunendra Sankar Desarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes NLIP Lab's multilingual machine translation system for
the WAT24 shared task on multilingual Indic MT task for 22 scheduled languages
belonging to 4 language families. We explore pre-training for Indic languages
using alignment agreement objectives. We utilize bi-lingual dictionaries to
substitute words from source sentences. Furthermore, we fine-tuned language
direction-specific multilingual translation models using small and high-quality
seed data. Our primary submission is a 243M parameters multilingual translation
model covering 22 Indic languages. In the IN22-Gen benchmark, we achieved an
average chrF++ score of 46.80 and 18.19 BLEU score for the En-Indic direction.
In the Indic-En direction, we achieved an average chrF++ score of 56.34 and
30.82 BLEU score. In the In22-Conv benchmark, we achieved an average chrF++
score of 43.43 and BLEU score of 16.58 in the En-Indic direction, and in the
Indic-En direction, we achieved an average of 52.44 and 29.77 for chrF++ and
BLEU respectively. Our model\footnote{Our code and models are available at
\url{https://github.com/maharajbrahma/WAT2024-MultiIndicMT}} is competitive
with IndicTransv1 (474M parameter model).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WMT 24 WAT Shared Task IndicMultiMT (Best System)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity-Dissimilarity Loss with Supervised Contrastive Learning for
  Multi-label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Sheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised contrastive learning has been explored in making use of label
information for multi-label classification, but determining positive samples in
multi-label scenario remains challenging. Previous studies have examined
strategies for identifying positive samples, considering label overlap
proportion between anchors and samples. However, they ignore various relations
between given anchors and samples, as well as how to dynamically adjust the
weights in contrastive loss functions based on different relations, leading to
great ambiguity. In this paper, we introduce five distinct relations between
multi-label samples and propose a Similarity-Dissimilarity Loss with
contrastive learning for multi-label classification. Our loss function
re-weights the loss by computing the similarity and dissimilarity between
positive samples and a given anchor based on the introduced relations. We
mainly conduct experiments for multi-label text classification on MIMIC
datasets, then further extend the evaluation on MS-COCO. The Experimental
results show that our proposed loss effectively improves the performance on all
encoders under supervised contrastive learning paradigm, demonstrating its
effectiveness and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Thrice Before You Act: Progressive Thought Refinement in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyu Du, Jinyi Han, Yizhou Ying, Aili Chen, Qianyu He, Haokun Zhao, Sirui Xia, Haoran Guo, Jiaqing Liang, Zulong Chen, Liangyue Li, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have demonstrated that
progressive refinement, rather than providing a single answer, results in more
accurate and thoughtful outputs. However, existing methods often rely heavily
on supervision signals to evaluate previous responses, making it difficult to
assess output quality in more open-ended scenarios effectively. Additionally,
these methods are typically designed for specific tasks, which limits their
generalization to new domains. To address these limitations, we propose
Progressive Thought Refinement (PTR), a framework that enables LLMs to refine
their responses progressively. PTR operates in two phases: (1) Thought data
construction stage: We propose a weak and strong model collaborative selection
strategy to build a high-quality progressive refinement dataset to ensure
logical consistency from thought to answers, and the answers are gradually
refined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training
structure to mask the "thought" and adjust loss weights to encourage LLMs to
refine prior thought, teaching them to implicitly understand "how to improve"
rather than "what is correct." Experimental results show that PTR significantly
enhances LLM performance across ten diverse tasks (avg. from 49.6% to 53.5%)
without task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also
demonstrate substantial improvements in the quality of responses beyond mere
accuracy, suggesting that PTR truly teaches LLMs to self-improve over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attr-Int: A Simple and Effective Entity Alignment Framework for
  Heterogeneous Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyan Yang, Jingwei Cheng, Chuanhao Xu, Xihao Wang, Jiayi Li, Fu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) refers to the task of linking entities in different
knowledge graphs (KGs). Existing EA methods rely heavily on structural
isomorphism. However, in real-world KGs, aligned entities usually have
non-isomorphic neighborhood structures, which paralyses the application of
these structure-dependent methods. In this paper, we investigate and tackle the
problem of entity alignment between heterogeneous KGs. First, we propose two
new benchmarks to closely simulate real-world EA scenarios of heterogeneity.
Then we conduct extensive experiments to evaluate the performance of
representative EA methods on the new benchmarks. Finally, we propose a simple
and effective entity alignment framework called Attr-Int, in which innovative
attribute information interaction methods can be seamlessly integrated with any
embedding encoder for entity alignment, improving the performance of existing
entity alignment techniques. Experiments demonstrate that our framework
outperforms the state-of-the-art approaches on two new benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoR: Mixture of Ranks for Low-Rank Adaptation Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyu Tang, Yilong Chen, Zhenyu Zhang, Junyuan Shang, Wenyuan Zhang, Yong Huang, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) drives research to align its performance with full
fine-tuning. However, significant challenges remain: (1) Simply increasing the
rank size of LoRA does not effectively capture high-rank information, which
leads to a performance bottleneck.(2) MoE-style LoRA methods substantially
increase parameters and inference latency, contradicting the goals of efficient
fine-tuning and ease of application. To address these challenges, we introduce
Mixture of Ranks (MoR), which learns rank-specific information for different
tasks based on input and efficiently integrates multi-rank information. We
firstly propose a new framework that equates the integration of multiple LoRAs
to expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA
already captures sufficient intrinsic information, and MoR can derive high-rank
information through mathematical transformations of the low-rank components.
Thus, MoR can reduces the learning difficulty of LoRA and enhances its
multi-task capabilities. MoR achieves impressive results, with MoR delivering a
1.31\% performance improvement while using only 93.93\% of the parameters
compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Hybrid Intelligence in Journalism: Findings and Lessons Learnt
  from a Collaborative Analysis of Greek Political Rhetoric by Chat<span class="highlight-title">GPT</span> and
  Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanasis Troboukis, Kelly Kiki, Antonis Galanopoulos, Pavlos Sermpezis, Stelios Karamanidis, Ilias Dimitriadis, Athena Vakali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter introduces a research project titled "Analyzing the Political
Discourse: A Collaboration Between Humans and Artificial Intelligence", which
was initiated in preparation for Greece's 2023 general elections. The project
focused on the analysis of political leaders' campaign speeches, employing
Artificial Intelligence (AI), in conjunction with an interdisciplinary team
comprising journalists, a political scientist, and data scientists. The chapter
delves into various aspects of political discourse analysis, including
sentiment analysis, polarization, populism, topic detection, and Named Entities
Recognition (NER). This experimental study investigates the capabilities of
large language model (LLMs), and in particular OpenAI's ChatGPT, for analyzing
political speech, evaluates its strengths and weaknesses, and highlights the
essential role of human oversight in using AI in journalism projects and
potentially other societal sectors. The project stands as an innovative example
of human-AI collaboration (known also as "hybrid intelligence") within the
realm of digital humanities, offering valuable insights for future initiatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguistically Grounded Analysis of Language Models using Shapley Head
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcell Fekete, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how linguistic knowledge is encoded in language models is
crucial for improving their generalisation capabilities. In this paper, we
investigate the processing of morphosyntactic phenomena, by leveraging a
recently proposed method for probing language models via Shapley Head Values
(SHVs). Using the English language BLiMP dataset, we test our approach on two
widely used models, BERT and RoBERTa, and compare how linguistic constructions
such as anaphor agreement and filler-gap dependencies are handled. Through
quantitative pruning and qualitative clustering analysis, we demonstrate that
attention heads responsible for processing related linguistic phenomena cluster
together. Our results show that SHV-based attributions reveal distinct patterns
across both models, providing insights into how language models organize and
process linguistic information. These findings support the hypothesis that
language models learn subnetworks corresponding to linguistic theory, with
potential implications for cross-linguistic model analysis and interpretability
in Natural Language Processing (NLP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Dilip Venkatesh, Raj Dabre, Anoop Kunchukuttan, Mitesh M. Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating machine-generated text remains a significant challenge in NLP,
especially for non-English languages. Current methodologies, including
automated metrics, human assessments, and LLM-based evaluations, predominantly
focus on English, revealing a significant gap in multilingual evaluation
frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an
extensible framework that includes evaluator LLMs (Hercule) and a novel test
set (Recon) specifically designed for multilingual evaluation. Our test set
features 500 human-annotated instructions spanning various task capabilities
along with human judgment scores across six languages. This would enable
benchmarking of general-purpose multilingual LLMs and facilitate
meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a
cross-lingual evaluation model that addresses the scarcity of reference answers
in the target language by learning to assign scores to responses based on
easily available reference answers in English. Our experiments demonstrate that
Hercule aligns more closely with human judgments compared to proprietary
models, demonstrating the effectiveness of such cross-lingual evaluation in low
resource scenarios. Further, it is also effective in zero-shot evaluation on
unseen languages. This study is the first comprehensive examination of
cross-lingual evaluation using LLMs, presenting a scalable and effective
approach for multilingual assessment. All code, datasets, and models will be
publicly available to enable further research in this important area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metacognitive Monitoring: A Human Ability Beyond Generative Artificial
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Huff, Elanur Ulakçı
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive alignment with human
cognitive processes, raising questions about the extent of their similarity to
human cognition. This study investigates whether LLMs, specifically ChatGPT,
possess metacognitive monitoring abilities akin to humans-particularly in
predicting memory performance on an item-by-item basis. We employed a
cross-agent prediction model to compare the metacognitive performance of humans
and ChatGPT in a language-based memory task involving garden-path sentences
preceded by either fitting or unfitting context sentences. Both humans and
ChatGPT rated the memorability of these sentences; humans then completed a
surprise recognition memory test. Our findings reveal a significant positive
relationship between humans' memorability ratings and their actual recognition
performance, indicating reliable metacognitive monitoring. In contrast, ChatGPT
did not exhibit a similar predictive capability. Bootstrapping analyses
demonstrated that none of the GPT models tested (GPT-3.5-turbo, GPT-4-turbo,
GPT-4o) could accurately predict human memory performance on a per-item basis.
This suggests that, despite their advanced language processing abilities and
alignment with human cognition at the object level, current LLMs lack the
metacognitive mechanisms that enable humans to anticipate their memory
performance. These results highlight a fundamental difference between human and
AI cognition at the metacognitive level. Addressing this gap is crucial for
developing AI systems capable of effective self-monitoring and adaptation to
human needs, thereby enhancing human-AI interactions across domains such as
education and personalized learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2403.05152</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Use of Audio to Improve Dialogue Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Roncel, Federico Costa, Javier Hernando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the significant progress of speech technologies, spoken goal-oriented
dialogue systems are becoming increasingly popular. One of the main modules of
a dialogue system is typically the dialogue policy, which is responsible for
determining system actions. This component usually relies only on audio
transcriptions, being strongly dependent on their quality and ignoring very
important extralinguistic information embedded in the user's speech. In this
paper, we propose new architectures to add audio information by combining
speech and text embeddings using a Double Multi-Head Attention component. Our
experiments show that audio embedding-aware dialogue policies outperform
text-based ones, particularly in noisy transcription scenarios, and that how
text and audio embeddings are combined is crucial to improve performance. We
obtained a 9.8% relative improvement in the User Request Score compared to an
only-text-based dialogue system on the DSTC2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IberSpeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remember, Retrieve and Generate: Understanding Infinite Visual Concepts
  as Your Personalized Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has significantly enhanced
the capabilities of multimodal LLMs (MLLMs) as general assistants. However,
lack of user-specific knowledge still restricts their application in human's
daily life. In this paper, we introduce the Retrieval Augmented Personalization
(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we
turn it into a personalized assistant in three steps. (a) Remember: We design a
key-value database to store user-related information, e.g., user's name, avatar
and other attributes. (b) Retrieve: When the user initiates a conversation, RAP
will retrieve relevant information from the database using a multimodal
retriever. (c) Generate: The input query and retrieved concepts' information
are fed into MLLMs to generate personalized, knowledge-augmented responses.
Unlike previous methods, RAP allows real-time concept editing via updating the
external database. To further improve generation quality and alignment with
user-specific information, we design a pipeline for data collection and create
a specialized dataset for personalized training of MLLMs. Based on the dataset,
we train a series of MLLMs as personalized multimodal assistants. By
pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual
concepts without additional finetuning. Our models demonstrate outstanding
flexibility and generation quality across a variety of tasks, such as
personalized image captioning, question answering and visual recognition. The
code, data and models are available at https://github.com/Hoar012/RAP-MLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAR-ECHR: A New Legal Argument Reasoning Task and <span class="highlight-title">Dataset</span> for Cases of
  the Eu<span class="highlight-title">rope</span>an Court of Human Rights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Odysseas S. Chlapanis, Dimitrios Galanis, Ion Androutsopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Legal Argument Reasoning (LAR), a novel task designed to evaluate
the legal reasoning capabilities of Large Language Models (LLMs). The task
requires selecting the correct next statement (from multiple choice options) in
a chain of legal arguments from court proceedings, given the facts of the case.
We constructed a dataset (LAR-ECHR) for this task using cases from the European
Court of Human Rights (ECHR). We evaluated seven general-purpose LLMs on
LAR-ECHR and found that (a) the ranking of the models is aligned with that of
LegalBench, an established US-based legal reasoning benchmark, even though
LAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more
clearly, compared to LegalBench, (c) even the best model (GPT-4o) obtains 75.8%
accuracy on LAR-ECHR, indicating significant potential for further model
improvement. The process followed to construct LAR-ECHR can be replicated with
cases from other legal systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Natural Legal Language Processing (NLLP) 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Learning of Structured Data for Medical Foundation Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Prakash Dwivedi, Viktor Schlegel, Andy T. Liu, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Jeng Wei, Wei-Hsian Yin, Stefan Winkler, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
various domains, including healthcare. However, their ability to effectively
represent structured non-textual data, such as the alphanumeric medical codes
used in records like ICD-10 or SNOMED-CT, is limited and has been particularly
exposed in recent research. This paper examines the challenges LLMs face in
processing medical codes due to the shortcomings of current tokenization
methods. As a result, we introduce the UniStruct architecture to design a
multimodal medical foundation model of unstructured text and structured data,
which addresses these challenges by adapting subword tokenization techniques
specifically for the structured medical codes. Our approach is validated
through model pre-training on both an extensive internal medical database and a
public repository of structured medical records. Trained on over 1 billion
tokens on the internal medical database, the proposed model achieves up to a
23% improvement in evaluation metrics, with around 2% gain attributed to our
proposed tokenization. Additionally, when evaluated on the EHRSHOT public
benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model
improves performance on over 42% of the downstream tasks. Our approach not only
enhances the representation and generalization capabilities of patient-centric
models but also bridges a critical gap in representation learning models'
ability to handle complex structured medical data, alongside unstructured text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Unifying Representations in Neural Models
  (UniReps 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cerberus: Efficient Inference with Adaptive Parallel Decoding and
  Sequential Knowledge Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Liu, Wenyuan Li, Laizhong Cui, Hailiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often face a bottleneck in inference speed due
to their reliance on auto-regressive decoding. Recently, parallel decoding has
shown significant promise in enhancing inference efficiency. However, we have
identified two key issues with existing parallel decoding frameworks: (1)
decoding heads fail to balance prediction accuracy and the parallelism of
execution, and (2) parallel decoding is not a universal solution, as it can
bring unnecessary overheads at some challenging decoding steps. To address
these issues, we propose Cerberus, an adaptive parallel decoding framework
introduces the gating mechanism to enable the LLMs to adaptively choose
appropriate decoding approaches at each decoding step, along with introducing a
new paradigm of decoding heads that introduce the sequential knowledge while
maintaining execution parallelism. The experiment results demonstrate that the
Cerberus can achieve up to 2.12x speed up compared to auto-regressive decoding,
and outperforms one of the leading parallel decoding frameworks, Medusa, with a
10% - 30% increase in acceleration and superior generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yuan, Lili Zhao, Kai Zhang, Guangting Zheng, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in various
natural language processing tasks. However, LLMs may rely on dataset biases as
shortcuts for prediction, which can significantly impair their robustness and
generalization capabilities. This paper presents Shortcut Suite, a
comprehensive test suite designed to evaluate the impact of shortcuts on LLMs'
performance, incorporating six shortcut types, five evaluation metrics, and
four prompting strategies. Our extensive experiments yield several key
findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream
tasks, significantly impairing their performance. 2) Larger LLMs are more
likely to utilize shortcuts under zero-shot and few-shot in-context learning
prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and
outperforms other prompting strategies, while few-shot prompts generally
underperform compared to zero-shot prompts. 4) LLMs often exhibit
overconfidence in their predictions, especially when dealing with datasets that
contain shortcuts. 5) LLMs generally have a lower explanation quality in
shortcut-laden datasets, with errors falling into three types: distraction,
disguised comprehension, and logical fallacy. Our findings offer new insights
for evaluating robustness and generalization in LLMs and suggest potential
directions for mitigating the reliance on shortcuts. The code is available at
\url {https://github.com/yyhappier/ShortcutSuite.git}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing-RAG: Self-Probing to Guide Language Models in Selective Document
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingeol Baek, Hwan Chang, Byeongjeong Kim, Jimin Lee, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances language models by retrieving
and incorporating relevant external knowledge. However, traditional
retrieve-and-generate processes may not be optimized for real-world scenarios,
where queries might require multiple retrieval steps or none at all. In this
paper, we propose a Probing-RAG, which utilizes the hidden state
representations from the intermediate layers of language models to adaptively
determine the necessity of additional retrievals for a given query. By
employing a pre-trained prober, Probing-RAG effectively captures the model's
internal cognition, enabling reliable decision-making about retrieving external
documents. Experimental results across five open-domain QA datasets demonstrate
that Probing-RAG outperforms previous methods while reducing the number of
redundant retrieval steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Have Political Correctness? Analyzing Ethical Biases and
  Jailbreak Vulnerabilities in AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isack Lee, Haebin Seong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) demonstrate impressive proficiency in
various tasks, they present potential safety risks, such as `jailbreaks', where
malicious inputs can coerce LLMs into generating harmful content. To address
these issues, many LLM developers have implemented various safety measures to
align these models. This alignment involves several techniques, including data
filtering during pre-training, supervised fine-tuning, reinforcement learning
from human feedback, and red-teaming exercises. These methods often introduce
deliberate and intentional biases similar to Political Correctness (PC) to
ensure the ethical behavior of LLMs. In this paper, we delve into the
intentional biases injected into LLMs for safety purposes and examine methods
to circumvent these safety alignment techniques. Notably, these intentional
biases result in a jailbreaking success rate in GPT-4o models that differs by
20% between non-binary and cisgender keywords and by 16% between white and
black keywords, even when the other parts of the prompts are identical. We
introduce the concept of PCJailbreak, highlighting the inherent risks posed by
these safety-induced biases. Additionally, we propose an efficient defense
method PCDefense, which prevents jailbreak attempts by injecting defense
prompts prior to generation. PCDefense stands as an appealing alternative to
Guard Models, such as Llama-Guard, that require additional inference cost after
text generation. Our findings emphasize the urgent need for LLM developers to
adopt a more responsible approach when designing and implementing safety
measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning Language Models on Multiple <span class="highlight-title">Dataset</span>s for Citation Intention
  Classification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeren Shui, Petros Karypis, Daniel S. Karls, Mingjian Wen, Saurav Manchanda, Ellad B. Tadmor, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation intention Classification (CIC) tools classify citations by their
intention (e.g., background, motivation) and assist readers in evaluating the
contribution of scientific literature. Prior research has shown that pretrained
language models (PLMs) such as SciBERT can achieve state-of-the-art performance
on CIC benchmarks. PLMs are trained via self-supervision tasks on a large
corpus of general text and can quickly adapt to CIC tasks via moderate
fine-tuning on the corresponding dataset. Despite their advantages, PLMs can
easily overfit small datasets during fine-tuning. In this paper, we propose a
multi-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset
of primary interest together with multiple auxiliary CIC datasets to take
advantage of additional supervision signals. We develop a data-driven task
relation learning (TRL) method that controls the contribution of auxiliary
datasets to avoid negative transfer and expensive hyper-parameter tuning. We
conduct experiments on three CIC datasets and show that fine-tuning with
additional datasets can improve the PLMs' generalization performance on the
primary dataset. PLMs fine-tuned with our proposed framework outperform the
current state-of-the-art models by 7% to 11% on small datasets while aligning
with the best-performing model on a large dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appear as a Findings paper at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models via
  Summary-Guided Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Min, Minbeom Kim, Kang-il Lee, Dongryeol Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in
generating detailed and coherent responses from visual inputs. However, they
are prone to generate hallucinations due to an over-reliance on language
priors. To address this issue, we investigate the language priors in LVLMs and
make two key observations: (1) Even when predicting the tokens associated with
image-related part-of-speech (POS), models increasingly rely on linguistic
priors as the token sequences grow, thereby amplifying hallucinations. (2)
Methods that directly calibrate LVLM's output distribution to mitigate language
priors can lead to a degradation in text quality or even exacerbate
hallucinations. Based on these findings, we propose a novel method,
Summary-Guided Decoding (SGD). This method naturally encourages the model to
focus more on image information by reducing the text context through summaries,
while controlling only the image-related POS tokens to maintain text quality.
Through experiments, we demonstrate that SGD achieves state-of-the-art
performance on object hallucination benchmarks. Furthermore, in terms of the
trade-off between precision and recall, SGD achieves Pareto optimality among
the existing methods. Lastly, we observe that although existing methods
struggle to balance the reduction of object hallucinations with maintaining
text quality, SGD demonstrates robustness in handling this challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Approaches to Arabic-English Code-Switching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Sabty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) is a vital computational method for
addressing language processing, analysis, and generation. NLP tasks form the
core of many daily applications, from automatic text correction to speech
recognition. While significant research has focused on NLP tasks for the
English language, less attention has been given to Modern Standard Arabic and
Dialectal Arabic. Globalization has also contributed to the rise of
Code-Switching (CS), where speakers mix languages within conversations and even
within individual words (intra-word CS). This is especially common in Arab
countries, where people often switch between dialects or between dialects and a
foreign language they master. CS between Arabic and English is frequent in
Egypt, especially on social media. Consequently, a significant amount of
code-switched content can be found online. Such code-switched data needs to be
investigated and analyzed for several NLP tasks to tackle the challenges of
this multilingual phenomenon and Arabic language challenges. No work has been
done before for several integral NLP tasks on Arabic-English CS data. In this
work, we focus on the Named Entity Recognition (NER) task and other tasks that
help propose a solution for the NER task on CS data, e.g., Language
Identification. This work addresses this gap by proposing and applying
state-of-the-art techniques for Modern Standard Arabic and Arabic-English NER.
We have created the first annotated CS Arabic-English corpus for the NER task.
Also, we apply two enhancement techniques to improve the NER tagger on CS data
using CS contextual embeddings and data augmentation techniques. All methods
showed improvements in the performance of the NER taggers on CS data. Finally,
we propose several intra-word language identification approaches to determine
the language type of a mixed text and identify whether it is a named entity or
not.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Biases to Embrace Diversity: A Comprehensive Annotation
  Benchmark for Toxic Language <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinmeng Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a prescriptive annotation benchmark grounded in
humanities research to ensure consistent, unbiased labeling of offensive
language, particularly for casual and non-mainstream language uses. We
contribute two newly annotated datasets that achieve higher inter-annotator
agreement between human and language model (LLM) annotations compared to
original datasets based on descriptive instructions. Our experiments show that
LLMs can serve as effective alternatives when professional annotators are
unavailable. Moreover, smaller models fine-tuned on multi-source LLM-annotated
data outperform models trained on larger, single-source human-annotated
datasets. These findings highlight the value of structured guidelines in
reducing subjective variability, maintaining performance with limited data, and
embracing language diversity.
  Content Warning: This article only analyzes offensive language for academic
purposes. Discretion is advised.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, EMNLP-NLP4DH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference-Based Post-OCR Processing with LLM for Diacritic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting fine-grained OCR text from aged documents in diacritic languages
remains challenging due to unexpected artifacts, time-induced degradation, and
lack of datasets. While standalone spell correction approaches have been
proposed, they show limited performance for historical documents due to
numerous possible OCR error combinations and differences between modern and
classical corpus distributions. We propose a method utilizing available
content-focused ebooks as a reference base to correct imperfect OCR-generated
text, supported by large language models. This technique generates
high-precision pseudo-page-to-page labels for diacritic languages, where small
strokes pose significant challenges in historical conditions. The pipeline
eliminates various types of noise from aged documents and addresses issues such
as missing characters, words, and disordered sequences. Our post-processing
method, which generated a large OCR dataset of classical Vietnamese books,
achieved a mean grading score of 8.72 on a 10-point scale. This outperformed
the state-of-the-art transformer-based Vietnamese spell correction model, which
scored 7.03 when evaluated on a sampled subset of the dataset. We also trained
a baseline OCR model to assess and compare it with well-known engines.
Experimental results demonstrate the strength of our baseline model compared to
widely used open-source solutions. The resulting dataset will be released
publicly to support future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Large Language Model Attribution through Self-Improving <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Huang, Xiaocheng Feng, Weitao Ma, Liang Zhao, Yuchun Fan, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching large language models (LLMs) to generate text with citations to
evidence sources can mitigate hallucinations and enhance verifiability in
information-seeking systems. However, improving this capability requires
high-quality attribution data, which is costly and labor-intensive. Inspired by
recent advances in self-improvement that enhance LLMs without manual
annotation, we present START, a Self-Taught AttRibuTion framework for
iteratively improving the attribution capability of LLMs. First, to prevent
models from stagnating due to initially insufficient supervision signals, START
leverages the model to self-construct synthetic training data for warming up.
To further self-improve the model's attribution ability, START iteratively
utilizes fine-grained preference supervision signals constructed from its
sampled responses to encourage robust, comprehensive, and attributable
generation. Experiments on three open-domain question-answering datasets,
covering long-form QA and multi-step reasoning, demonstrate significant
performance gains of 25.13% on average without relying on human annotations and
more advanced models. Further analysis reveals that START excels in aggregating
information across multiple sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Route with Confidence Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Neng Chuang, Helen Zhou, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive performance on
several tasks and are increasingly deployed in real-world applications.
However, especially in high-stakes settings, it becomes vital to know when the
output of an LLM may be unreliable. Depending on whether an answer is
trustworthy, a system can then choose to route the question to another expert,
or otherwise fall back on a safe default behavior. In this work, we study the
extent to which LLMs can reliably indicate confidence in their answers, and how
this notion of confidence can translate into downstream accuracy gains. We
propose Self-REF, a lightweight training strategy to teach LLMs to express
confidence in whether their answers are correct in a reliable manner. Self-REF
introduces confidence tokens into the LLM, from which a confidence score can be
extracted. Compared to conventional approaches such as verbalizing confidence
and examining token probabilities, we demonstrate empirically that confidence
tokens show significant improvements in downstream routing and rejection
learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BANTH: A Multi-label Hate Speech Detection <span class="highlight-title">Dataset</span> for Transliterated
  Bangla 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of transliterated texts in digital spaces has emphasized
the need for detecting and classifying hate speech in languages beyond English,
particularly in low-resource languages. As online discourse can perpetuate
discrimination based on target groups, e.g. gender, religion, and origin,
multi-label classification of hateful content can help in comprehending hate
motivation and enhance content moderation. While previous efforts have focused
on monolingual or binary hate classification tasks, no work has yet addressed
the challenge of multi-label hate speech classification in transliterated
Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate
speech dataset comprising 37.3k samples. The samples are sourced from YouTube
comments, where each instance is labeled with one or more target groups,
reflecting the regional demographic. We establish novel transformer
encoder-based baselines by further pre-training on transliterated Bangla
corpus. We also propose a novel translation-based LLM prompting strategy for
transliterated text. Experiments reveal that our further pre-trained encoders
are achieving state-of-the-art performance on the BanTH dataset, while our
translation-based prompting outperforms other strategies in the zero-shot
setting. The introduction of BanTH not only fills a critical gap in hate speech
research for Bangla but also sets the stage for future exploration into
code-mixed and multi-label classification challenges in underrepresented
languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its
quadratic complexity limits the efficiency and scalability of LLMs, especially
for those with a long-context window. A promising approach addressing this
limitation is to leverage the sparsity in attention. However, existing
sparsity-based solutions predominantly rely on predefined patterns or
heuristics to approximate sparsity. This practice falls short to fully capture
the dynamic nature of attention sparsity in language-based tasks. This paper
argues that attention sparsity should be learned rather than predefined. To
this end, we design SeerAttention, a new Attention mechanism that augments the
conventional attention with a learnable gate that adaptively selects
significant blocks in an attention map and deems the rest blocks sparse. Such
block-level sparsity effectively balances accuracy and speedup. To enable
efficient learning of the gating network, we develop a customized
FlashAttention implementation that extracts the block-level ground truth of
attention map with minimum overhead. SeerAttention not only applies to
post-training, but also excels in long-context fine-tuning. Our results show
that at post-training stages, SeerAttention significantly outperforms
state-of-the-art static or heuristic-based sparse attention methods, while also
being more versatile and flexible to adapt to varying context lengths and
sparsity ratios. When applied to long-context fine-tuning with YaRN,
SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context
length with minimal perplexity loss, offering a 5.67x speedup over
FlashAttention-2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseok Choi, ChaeHun Park, Dohyun Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) serve as giant information stores, often
including personal or copyrighted data, and retraining them from scratch is not
a viable option. This has led to the development of various fast, approximate
unlearning techniques to selectively remove knowledge from LLMs. Prior research
has largely focused on minimizing the probabilities of specific token sequences
by reversing the language modeling objective. However, these methods still
leave LLMs vulnerable to adversarial attacks that exploit indirect references.
In this work, we examine the limitations of current unlearning techniques in
effectively erasing a particular type of indirect prompt: multi-hop queries.
Our findings reveal that existing methods fail to completely remove multi-hop
knowledge when one of the intermediate hops is unlearned. To address this
issue, we propose MUNCH, a simple uncertainty-based approach that breaks down
multi-hop queries into subquestions and leverages the uncertainty of the
unlearned model in final decision-making. Empirical results demonstrate the
effectiveness of our framework, and MUNCH can be easily integrated with
existing unlearning techniques, making it a flexible and useful solution for
enhancing unlearning processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Roadmap towards Superhuman Speech Understanding using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Bu, Yuhao Zhang, Xidong Wang, Benyou Wang, Qun Liu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large language models (LLMs) has prompted efforts to integrate
speech and audio data, aiming to create general foundation models capable of
processing both textual and non-textual inputs. Recent advances, such as
GPT-4o, highlight the potential for end-to-end speech LLMs, which preserves
non-semantic information and world knowledge for deeper speech understanding.
To guide the development of speech LLMs, we propose a five-level roadmap,
ranging from basic automatic speech recognition (ASR) to advanced superhuman
models capable of integrating non-semantic information with abstract acoustic
knowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark,
that standardizes critical aspects across various tasks in these five levels,
uncovering challenges in using abstract acoustic knowledge and completeness of
capability. Our findings reveal gaps in handling paralinguistic cues and
abstract acoustic knowledge, and we offer future directions. This paper
outlines a roadmap for advancing speech LLMs, introduces a benchmark for
evaluation, and provides key insights into their current limitations and
potential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages
  Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangda Wu, Yashan Wang, Ruibin Yuan, Zhancheng Guo, Xu Tan, Ge Zhang, Monan Zhou, Jing Chen, Xuefeng Mu, Yuejie Gao, Yuanliang Dong, Jiafeng Liu, Xiaobing Li, Feng Yu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Challenges in managing linguistic diversity and integrating various musical
modalities are faced by current music information retrieval systems. These
limitations reduce their effectiveness in a global, multimodal music
environment. To address these issues, we introduce CLaMP 2, a system compatible
with 101 languages that supports both ABC notation (a text-based musical
notation format) and MIDI (Musical Instrument Digital Interface) for music
information retrieval. CLaMP 2, pre-trained on 1.5 million ABC-MIDI-text
triplets, includes a multilingual text encoder and a multimodal music encoder
aligned via contrastive learning. By leveraging large language models, we
obtain refined and consistent multilingual descriptions at scale, significantly
reducing textual noise and balancing language distribution. Our experiments
show that CLaMP 2 achieves state-of-the-art results in both multilingual
semantic search and music classification across modalities, thus establishing a
new standard for inclusive and global music information retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Babbling to Fluency: Evaluating the Evolution of Language Models in
  Terms of Human Language Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Yang, Pengda Wang, Luke D. Plonsky, Frederick L. Oswald, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the language capabilities of language models (LMs) from the
critical perspective of human language acquisition. Building on classical
language development theories, we propose a three-stage framework to assess the
abilities of LMs, ranging from preliminary word understanding to complex
grammar and complex logical reasoning. Using this framework, we evaluate the
generative capacities of LMs using methods from linguistic research. Results
indicate that although recent LMs outperform earlier models in overall
performance, their developmental trajectory does not strictly follow the path
of human language acquisition. Notably, in generation tasks, LMs are more
similar to human performance in areas where information is easier to extract
from the corpus, such as average word length, clauses, and auxiliary verbs.
Newer LMs did not exhibit significant progress in terms of specific dimensions,
such as clauses and auxiliary verbs, where the variation across corpora is
relatively limited. Register theory offers a plausible explanation for these
observations, suggesting that the linguistic features of the training data have
a substantial impact on the models' abilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic Investigation of Knowledge Retrieval and Selection for
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangci Li, Jessica Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has emerged as a powerful method for
enhancing natural language generation by integrating external knowledge into a
model's output. While prior work has demonstrated the importance of improving
knowledge retrieval for boosting generation quality, the role of knowledge
selection remains less clear. In this paper, we perform a comprehensive
analysis of how knowledge retrieval and selection influence downstream
generation performance in RAG systems. By simulating different retrieval and
selection conditions through a controlled mixture of gold and distractor
knowledge, we assess the impact of these factors on generation outcomes. Our
findings indicate that the downstream generator model's capability, as well as
the complexity of the task and dataset, significantly influence the impact of
knowledge retrieval and selection on the overall RAG system performance. In
typical scenarios, improving the knowledge recall score is key to enhancing
generation outcomes, with the knowledge selector providing a limited additional
benefit when a strong generator model is used on clear, well-defined tasks. For
weaker generator models or more ambiguous tasks and datasets, the knowledge F1
score becomes a critical factor, and the knowledge selector plays a more
prominent role in improving overall performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Translation Alignment Pipeline for Multilingual Digital
  Editions of Literary Works 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Levchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of translation alignment algorithms
in the creation of a Multilingual Digital Edition (MDE) of Alessandro Manzoni's
Italian novel "I promessi sposi" ("The Betrothed"), with translations in eight
languages (English, Spanish, French, German, Dutch, Polish, Russian and
Chinese) from the 19th and 20th centuries. We identify key requirements for the
MDE to improve both the reader experience and support for translation studies.
Our research highlights the limitations of current state-of-the-art algorithms
when applied to the translation of literary texts and outlines an automated
pipeline for MDE creation. This pipeline transforms raw texts into web-based,
side-by-side representations of original and translated texts with different
rendering options. In addition, we propose new metrics for evaluating the
alignment of literary translations and suggest visualization techniques for
future analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Computational Humanities Research Conference, December 4-6,
  2024, Aarhus, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Likes and Dislikes in Personalized Generative Explainable
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryotaro Shimizu, Takashi Wada, Yu Wang, Johannes Kruse, Sean O'Brien, Sai HtaungKham, Linxin Song, Yuya Yoshikawa, Yuki Saito, Fugee Tsung, Masayuki Goto, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on explainable recommendation generally frames the task as a
standard text generation problem, and evaluates models simply based on the
textual similarity between the predicted and ground-truth explanations.
However, this approach fails to consider one crucial aspect of the systems:
whether their outputs accurately reflect the users' (post-purchase) sentiments,
i.e., whether and why they would like and/or dislike the recommended items. To
shed light on this issue, we introduce new datasets and evaluation methods that
focus on the users' sentiments. Specifically, we construct the datasets by
explicitly extracting users' positive and negative opinions from their
post-purchase reviews using an LLM, and propose to evaluate systems based on
whether the generated explanations 1) align well with the users' sentiments,
and 2) accurately identify both positive and negative opinions of users on the
target items. We benchmark several recent models on our datasets and
demonstrate that achieving strong performance on existing metrics does not
ensure that the generated explanations align well with the users' sentiments.
Lastly, we find that existing models can provide more sentiment-aware
explanations when the users' (predicted) ratings for the target items are
directly fed into the models as input. We will release our code and datasets
upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atomic Calibration of LLMs in <span class="highlight-title">Long</span>-Form Generations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caiqi Zhang, Ruihan Yang, Zhisong Zhang, Xinting Huang, Sen Yang, Dong Yu, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often suffer from hallucinations, posing
significant challenges for real-world applications. Confidence calibration,
which estimates the underlying uncertainty of model predictions, is essential
to enhance the LLMs' trustworthiness. Existing research on LLM calibration has
primarily focused on short-form tasks, providing a single confidence score at
the response level (macro calibration). However, this approach is insufficient
for long-form generations, where responses often contain more complex
statements and may include both accurate and inaccurate information. Therefore,
we introduce atomic calibration, a novel approach that evaluates factuality
calibration at a fine-grained level by breaking down long responses into atomic
claims. We classify confidence elicitation methods into discriminative and
generative types and demonstrate that their combination can enhance
calibration. Our extensive experiments on various LLMs and datasets show that
atomic calibration is well-suited for long-form generation and can also improve
macro calibration results. Additionally, atomic calibration reveals insightful
patterns in LLM confidence throughout the generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models are Easily Confused: A Quantitative Metric,
  Security Implications and Typological Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Chen, Qiongxiu Li, Russa Biswas, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Confusion is a phenomenon where Large Language Models (LLMs)
generate text that is neither in the desired language, nor in a contextually
appropriate language. This phenomenon presents a critical challenge in text
generation by LLMs, often appearing as erratic and unpredictable behavior. We
hypothesize that there are linguistic regularities to this inherent
vulnerability in LLMs and shed light on patterns of language confusion across
LLMs. We introduce a novel metric, Language Confusion Entropy, designed to
directly measure and quantify this confusion, based on language distributions
informed by linguistic typology and lexical variation. Comprehensive
comparisons with the Language Confusion Benchmark (Marchisio et al., 2024)
confirm the effectiveness of our metric, revealing patterns of language
confusion across LLMs. We further link language confusion to LLM security, and
find patterns in the case of multilingual embedding inversion attacks. Our
analysis demonstrates that linguistic typology offers theoretically grounded
interpretation, and valuable insights into leveraging language similarities as
a prior for LLM alignment and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIN: <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Prompt</span> INjection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Zhou, Junfeng Yang, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used in a variety of important
applications, yet their safety and reliability remain as major concerns.
Various adversarial and jailbreak attacks have been proposed to bypass the
safety alignment and cause the model to produce harmful responses. We introduce
Self-supervised Prompt INjection (SPIN) which can detect and reverse these
various attacks on LLMs. As our self-supervised prompt defense is done at
inference-time, it is also compatible with existing alignment and adds an
additional layer of safety for defense. Our benchmarks demonstrate that our
system can reduce the attack success rate by up to 87.9%, while maintaining the
performance on benign user requests. In addition, we discuss the situation of
an adaptive attacker and show that our method is still resilient against
attackers who are aware of our defense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web Agents with World Models: Learning and Leveraging Environment
  Dynamics in Web Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently gained much attention in building
autonomous agents. However, the performance of current LLM-based web agents in
long-horizon tasks is far from optimal, often yielding errors such as
repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid
such an irreversible mistake, as we have an awareness of the potential outcomes
(e.g., losing money) of our actions, also known as the "world model". Motivated
by this, our study first starts with preliminary analyses, confirming the
absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet,
etc.). Then, we present a World-model-augmented (WMA) web agent, which
simulates the outcomes of its actions for better decision-making. To overcome
the challenges in training LLMs as world models predicting next observations,
such as repeated elements across observations and long HTML inputs, we propose
a transition-focused observation abstraction, where the prediction objectives
are free-form natural language descriptions exclusively highlighting important
state differences between time steps. Experiments on WebArena and Mind2Web show
that our world models improve agents' policy selection without training and
demonstrate our agents' cost- and time-efficiency compared to recent
tree-search-based agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proof Flow: Preliminary Study on Generative Flow Network Language Model
  Tuning for Formal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is a fundamental substrate for solving novel and complex problems.
Deliberate efforts in learning and developing frameworks around System 2
reasoning have made great strides, yet problems of sufficient complexity remain
largely out of reach for open models. To address this gap, we examine the
potential of Generative Flow Networks as a fine-tuning method for LLMs to
unlock advanced reasoning capabilities. In this paper, we present a proof of
concept in the domain of formal reasoning, specifically in the Neural Theorem
Proving (NTP) setting, where proofs specified in a formal language such as Lean
can be deterministically and objectively verified. Unlike classical
reward-maximization reinforcement learning, which frequently over-exploits
high-reward actions and fails to effectively explore the state space, GFlowNets
have emerged as a promising approach for sampling compositional objects,
improving generalization, and enabling models to maintain diverse hypotheses.
Our early results demonstrate GFlowNet fine-tuning's potential for enhancing
model performance in a search setting, which is especially relevant given the
paradigm shift towards inference time compute scaling and "thinking slowly."
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CBT-Bench: Evaluating Large Language Models on Assisting Cognitive
  Behavior Therapy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mian Zhang, Xianjun Yang, Xinlu Zhang, Travis Labrum, Jamie C. Chiu, Shaun M. Eack, Fei Fang, William Yang Wang, Zhiyu Zoey Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a significant gap between patient needs and available mental health
support today. In this paper, we aim to thoroughly examine the potential of
using Large Language Models (LLMs) to assist professional psychotherapy. To
this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation
of cognitive behavioral therapy (CBT) assistance. We include three levels of
tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of
multiple-choice questions; II: Cognitive model understanding, with the tasks of
cognitive distortion classification, primary core belief classification, and
fine-grained core belief classification; III: Therapeutic response generation,
with the task of generating responses to patient speech in CBT therapy
sessions. These tasks encompass key aspects of CBT that could potentially be
enhanced through AI assistance, while also outlining a hierarchy of capability
requirements, ranging from basic knowledge recitation to engaging in real
therapeutic conversations. We evaluated representative LLMs on our benchmark.
Experimental results indicate that while LLMs perform well in reciting CBT
knowledge, they fall short in complex real-world scenarios requiring deep
analysis of patients' cognitive structures and generating effective responses,
suggesting potential future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anchored Alignment for Self-Explanations Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Felipe Villa-Arenas, Ata Nizamoglu, Qianli Wang, Sebastian Möller, Vera Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a methodology for alignment designed to enhance
the ability of large language models (LLMs) to articulate their reasoning
(self-explanation) even in the absence of annotated rationale explanations. Our
alignment methodology comprises three key components: explanation quality
assessment, self-instruction dataset generation, and model alignment.
Additionally, we present a novel technique called Alignment with Anchor
Preference Pairs, which improves the selection of preference pairs by
categorizing model outputs into three groups: consistently correct,
consistently incorrect, and variable. By applying tailored strategies to each
category, we enhance the effectiveness of Direct Preference Optimization (DPO).
Our experimental results demonstrate that this approach significantly improves
explanation quality while maintaining accuracy compared to other fine-tuning
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaithBench: A Diverse Hallucination Benchmark for Summarization by
  Modern LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Forrest Sheng Bao, Miaoran Li, Renyi Qu, Ge Luo, Erana Wan, Yujia Tang, Weisi Fan, Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Mike Qi, Ruixuan Tu, Chenyu Xu, Matthew Gonzales, Ofer Mendelevitch, Amin Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarization is one of the most common tasks performed by large language
models (LLMs), especially in applications like Retrieval-Augmented Generation
(RAG). However, existing evaluations of hallucinations in LLM-generated
summaries, and evaluations of hallucination detection models both suffer from a
lack of diversity and recency in the LLM and LLM families considered. This
paper introduces FaithBench, a summarization hallucination benchmark comprising
challenging hallucinations made by 10 modern LLMs from 8 different families,
with ground truth annotations by human experts. ``Challenging'' here means
summaries on which popular, state-of-the-art hallucination detection models,
including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and
GPT-3.5-Turbo produce the least hallucinations. However, even the best
hallucination detection models have near 50\% accuracies on FaithBench,
indicating lots of room for future improvement. The repo is
https://github.com/vectara/FaithBench
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BQA: Body Language Question Answering <span class="highlight-title">Dataset</span> for Video Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shintaro Ozaki, Kazuki Hayashi, Miyu Oba, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large part of human communication relies on nonverbal cues such as facial
expressions, eye contact, and body language. Unlike language or sign language,
such nonverbal communication lacks formal rules, requiring complex reasoning
based on commonsense understanding. Enabling current Video Large Language
Models (VideoLLMs) to accurately interpret body language is a crucial
challenge, as human unconscious actions can easily cause the model to
misinterpret their intent. To address this, we propose a dataset, BQA, a body
language question answering dataset, to validate whether the model can
correctly interpret emotions from short clips of body language comprising 26
emotion labels of videos of body language. We evaluated various VideoLLMs on
BQA and revealed that understanding body language is challenging, and our
analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made
significantly biased answers depending on the age group and ethnicity of the
individuals in the video. The dataset is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Free-Form Decision-Making Inconsistency of Language Models in
  Military Crisis Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Shrivastava, Jessica Hullman, Max Lamparth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an increasing interest in using language models (LMs) for automated
decision-making, with multiple countries actively testing LMs to aid in
military crisis decision-making. To scrutinize relying on LM decision-making in
high-stakes settings, we examine the inconsistency of responses in a crisis
simulation ("wargame"), similar to reported tests conducted by the US military.
Prior work illustrated escalatory tendencies and varying levels of aggression
among LMs but were constrained to simulations with pre-defined actions. This
was due to the challenges associated with quantitatively measuring semantic
differences and evaluating natural language decision-making without relying on
pre-defined actions. In this work, we query LMs for free form responses and use
a metric based on BERTScore to measure response inconsistency quantitatively.
Leveraging the benefits of BERTScore, we show that the inconsistency metric is
robust to linguistic variations that preserve semantic meaning in a
question-answering setting across text lengths. We show that all five tested
LMs exhibit levels of inconsistency that indicate semantic differences, even
when adjusting the wargame setting, anonymizing involved conflict countries, or
adjusting the sampling temperature parameter $T$. Further qualitative
evaluation shows that models recommend courses of action that share few to no
similarities. We also study the impact of different prompt sensitivity
variations on inconsistency at temperature $T = 0$. We find that inconsistency
due to semantically equivalent prompt variations can exceed response
inconsistency from temperature sampling for most studied models across
different levels of ablations. Given the high-stakes nature of military
deployment, we recommend further consideration be taken before using LMs to
inform military decisions or other cases of high-stakes decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Multilingual LLM Evaluation for Eu<span class="highlight-title">rope</span>an Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim Köhler, René Jäkel, Mehdi Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has revolutionized natural language
processing across numerous languages and tasks. However, evaluating LLM
performance in a consistent and meaningful way across multiple European
languages remains challenging, especially due to the scarcity of
language-parallel multilingual benchmarks. We introduce a multilingual
evaluation approach tailored for European languages. We employ translated
versions of five widely-used benchmarks to assess the capabilities of 40 LLMs
across 21 European languages. Our contributions include examining the
effectiveness of translated benchmarks, assessing the impact of different
translation services, and offering a multilingual evaluation framework for LLMs
that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,
EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly
available to encourage further research in multilingual LLM evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval Augmented Generation or <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span> LLMs? A Comprehensive
  Study and Hybrid Approach <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has been a powerful tool for Large
Language Models (LLMs) to efficiently process overly lengthy contexts. However,
recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to
understand long contexts directly. We conduct a comprehensive comparison
between RAG and long-context (LC) LLMs, aiming to leverage the strengths of
both. We benchmark RAG and LC across various public datasets using three latest
LLMs. Results reveal that when resourced sufficiently, LC consistently
outperforms RAG in terms of average performance. However, RAG's significantly
lower cost remains a distinct advantage. Based on this observation, we propose
Self-Route, a simple yet effective method that routes queries to RAG or LC
based on model self-reflection. Self-Route significantly reduces the
computation cost while maintaining a comparable performance to LC. Our findings
provide a guideline for long-context applications of LLMs using RAG and LC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 industry track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Many-Shot In-<span class="highlight-title">Context</span> Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel at few-shot in-context learning (ICL) --
learning from a few examples provided in context at inference, without any
weight updates. Newly expanded context windows allow us to investigate ICL with
hundreds or thousands of examples -- the many-shot regime. Going from few-shot
to many-shot, we observe significant performance gains across a wide variety of
generative and discriminative tasks. While promising, many-shot ICL can be
bottlenecked by the available amount of human-generated examples. To mitigate
this limitation, we explore two new settings: Reinforced and Unsupervised ICL.
Reinforced ICL uses model-generated chain-of-thought rationales in place of
human examples. Unsupervised ICL removes rationales from the prompt altogether,
and prompts the model only with domain-specific questions. We find that both
Reinforced and Unsupervised ICL can be quite effective in the many-shot regime,
particularly on complex reasoning tasks. Finally, we demonstrate that, unlike
few-shot learning, many-shot learning is effective at overriding pretraining
biases, can learn high-dimensional functions with numerical inputs, and
performs comparably to fine-tuning. We also find that inference cost increases
linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL
to varying degrees. Our analysis also reveals the limitations of next-token
prediction loss as an indicator of downstream ICL performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Topic Language Model on Heterogeneous Children's Mental Health
  Clinical Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Ye, Tatiana Moreno, Adrianne Alpern, Louis Ehwerhemuepha, Annie Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health diseases affect children's lives and well-beings which have
received increased attention since the COVID-19 pandemic. Analyzing psychiatric
clinical notes with topic models is critical to evaluating children's mental
status over time. However, few topic models are built for longitudinal
settings, and most existing approaches fail to capture temporal trajectories
for each document. To address these challenges, we develop a dynamic topic
model with consistent topics and individualized temporal dependencies on the
evolving document metadata. Our model preserves the semantic meaning of
discovered topics over time and incorporates heterogeneity among documents. In
particular, when documents can be categorized, we propose a classifier-free
approach to maximize topic heterogeneity across different document groups. We
also present an efficient variational optimization procedure adapted for the
multistage longitudinal setting. In this case study, we apply our method to the
psychiatric clinical notes from a large tertiary pediatric hospital in Southern
California and achieve a 38% increase in the overall coherence of extracted
topics. Our real data analysis reveals that children tend to express more
negative emotions during state shutdowns and more positive when schools reopen.
Furthermore, it suggests that sexual and gender minority (SGM) children display
more pronounced reactions to major COVID-19 events and a greater sensitivity to
vaccine-related news than non-SGM children. This study examines children's
mental health progression during the pandemic and offers clinicians valuable
insights to recognize disparities in children's mental health related to their
sexual and gender identities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Visual Information in Chinese Characters: Evaluating Large
  Models' Ability to Recognize and Utilize Radicals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Karl Stratos, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The glyphic writing system of Chinese incorporates information-rich visual
features in each character, such as radicals that provide hints about meaning
or pronunciation. However, there has been no investigation into whether
contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can
harness these sub-character features in Chinese through prompting. In this
study, we establish a benchmark to evaluate LLMs' and VLMs' understanding of
visual elements in Chinese characters, including radicals, composition
structures, strokes, and stroke counts. Our results reveal that models
surprisingly exhibit some, but still limited, knowledge of the visual
information, regardless of whether images of characters are provided. To incite
models' ability to use radicals, we further experiment with incorporating
radicals into the prompts for Chinese language processing (CLP) tasks. We
observe consistent improvement in Part-Of-Speech tagging when providing
additional information about radicals, suggesting the potential to enhance CLP
by integrating sub-character information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superlatives in <span class="highlight-title">Context</span>: Modeling the Implicit Semantics of Superlatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentina Pyatkin, Bonnie Webber, Ido Dagan, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Superlatives are used to single out elements with a maximal/minimal property.
Semantically, superlatives perform a set comparison: something (or some things)
has the min/max property out of a set. As such, superlatives provide an ideal
phenomenon for studying implicit phenomena and discourse restrictions. While
this comparison set is often not explicitly defined, its (implicit)
restrictions can be inferred from the discourse context the expression appears
in. In this work we provide an extensive computational study on the semantics
of superlatives. We propose a unified account of superlative semantics which
allows us to derive a broad-coverage annotation schema. Using this unified
schema we annotated a multi-domain dataset of superlatives and their semantic
interpretations. We specifically focus on interpreting implicit or ambiguous
superlative expressions, by analyzing how the discourse context restricts the
set of interpretations. In a set of experiments we then analyze how well models
perform at variations of predicting superlative semantics, with and without
context. We show that the fine-grained semantics of superlatives in context can
be challenging for contemporary models, including GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Larger Language Models Don't Care How You Think: Why Chain-of-Thought
  <span class="highlight-title">Prompt</span>ing Fails in Subjective Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to "adapt" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on "learning" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether "enabling" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 1 table. arXiv admin note: text overlap with
  arXiv:2403.17125</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Processing Methods for the Study of Protein-Ligand
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Michels, Ramya Bandarupalli, Amin Ahangar Akbari, Thai Le, Hong Xiao, Jing Li, Erik F. Y. Hom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Natural Language Processing (NLP) have ignited interest in
developing effective methods for predicting protein-ligand interactions (PLIs)
given their relevance to drug discovery and protein engineering efforts and the
ever-growing volume of biochemical sequence and structural data available. The
parallels between human languages and the "languages" used to represent
proteins and ligands have enabled the use of NLP machine learning approaches to
advance PLI studies. In this review, we explain where and how such approaches
have been applied in the recent literature and discuss useful mechanisms such
as long short-term memory, transformers, and attention. We conclude with a
discussion of the current limitations of NLP methods for the study of PLIs as
well as key challenges that need to be addressed in future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 Pages and 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Generate High-quality Patent Claims? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown exceptional performance across
various text generation tasks but remain under-explored in the patent domain,
which offers highly structured and precise language. This paper constructs a
dataset to investigate the performance of current LLMs in patent claim
generation. Our results demonstrate that generating claims based on patent
descriptions outperforms previous research relying on abstracts. Interestingly,
current patent-specific LLMs perform much worse than state-of-the-art general
LLMs, highlighting the necessity for future research on in-domain LLMs. We also
find that LLMs can produce high-quality first independent claims, but their
performances markedly decrease for subsequent dependent claims. Moreover,
fine-tuning can enhance the completeness of inventions' features, conceptual
clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the
best performance in comprehensive human evaluations by patent experts, with
better feature coverage, conceptual clarity, and technical coherence. Despite
these capabilities, comprehensive revision and modification are still necessary
to pass rigorous patent scrutiny and ensure legal robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human
  Factors in Personas <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salvatore Giorgi, Tingting Liu, Ankit Aich, Kelsey Isman, Garrick Sherman, Zachary Fried, João Sedoc, Lyle H. Ungar, Brenda Curtis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being used in human-centered
social scientific tasks, such as data annotation, synthetic data creation, and
engaging in dialog. However, these tasks are highly subjective and dependent on
human factors, such as one's environment, attitudes, beliefs, and lived
experiences. Thus, it may be the case that employing LLMs (which do not have
such human factors) in these tasks results in a lack of variation in data,
failing to reflect the diversity of human experiences. In this paper, we
examine the role of prompting LLMs with human-like personas and asking the
models to answer as if they were a specific human. This is done explicitly,
with exact demographics, political beliefs, and lived experiences, or
implicitly via names prevalent in specific populations. The LLM personas are
then evaluated via (1) subjective annotation task (e.g., detecting toxicity)
and (2) a belief generation task, where both tasks are known to vary across
human factors. We examine the impact of explicit vs. implicit personas and
investigate which human factors LLMs recognize and respond to. Results show
that explicit LLM personas show mixed results when reproducing known human
biases, but generally fail to demonstrate implicit biases. We conclude that
LLMs may capture the statistical patterns of how people speak, but are
generally unable to model the complex interactions and subtleties of human
perceptions, potentially limiting their effectiveness in social science
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in
  Low-Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Waheed, Karima Kadaoui, Bhiksha Raj, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on distilling Whisper's knowledge into small models using
pseudo-labels shows promising performance while reducing the size by up to
50\%. This results in small, efficient, and dedicated models. However, a
critical step of distillation from pseudo-labels involves filtering
high-quality predictions and using only those during training. This step
requires ground truth labels to compare and filter low-quality examples making
the whole process supervised. In addition to that, the distillation process
requires a large amount of data thereby limiting the ability to distill models
in low-resource settings. To address this challenge, we propose a distillation
framework that does not require any labeled data. Through experimentation, we
show that our best distilled models outperform the teacher model by 5-7 points
in terms of WER compared to those without filtering and are on par with or
perform better than similar supervised data filtering setups. When we scale the
data, our models significantly outperform all zero-shot and supervised models.
We demonstrate that it is possible to distill large Whisper models into
relatively small ones without using any labeled data. Our distilled models are
also 25-50\% more compute- and memory-efficient while maintaining performance
equal to or better than that of the teacher model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-Level Reasoning: Establishing Higher Order Beliefs in Large Language
  Models for Strategic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategic reasoning is a complex yet essential capability for intelligent
agents. It requires Large Language Model (LLM) agents to adapt their strategies
dynamically in multi-agent environments. Unlike static reasoning tasks, success
in these contexts depends on anticipating other agents' beliefs and actions
while continuously adjusting strategies to achieve individual goals. LLMs and
LLM agents often struggle with strategic reasoning due to the absence of a
reasoning framework that enables them to dynamically infer others' perspectives
and adapt to changing environments. Inspired by the Level-K framework from game
theory and behavioral economics, which extends reasoning from simple reactions
to structured strategic depth, we propose a novel framework: "K-Level Reasoning
with Large Language Models (K-R)." This framework employs recursive mechanisms
to enable LLMs to achieve varying levels of strategic depth, allowing agents to
form higher order beliefs - beliefs about others' beliefs. We validate this
framework through rigorous testing on four testbeds: two classical game theory
problems and two social intelligence tasks. The results demonstrate the
advantages of K-R in strategic reasoning. Our work presents the first recursive
implementation of strategic depth in large language models (LLMs). It
establishes a foundation for future research into theory of mind and strategic
reasoning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Mitigating Language Confusion in LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo Dehaze, Sebastian Ruder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a surprising limitation of LLMs: their inability to
consistently generate text in a user's desired language. We create the Language
Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically
diverse languages with existing and newly-created English and multilingual
prompts. We evaluate a range of LLMs on monolingual and cross-lingual
generation reflecting practical use cases, finding that Llama Instruct and
Mistral models exhibit high degrees of language confusion and even the
strongest models fail to consistently respond in the correct language. We
observe that base and English-centric instruct models are more prone to
language confusion, which is aggravated by complex prompts and high sampling
temperatures. We find that language confusion can be partially mitigated via
few-shot prompting, multilingual SFT and preference tuning. We release our
language confusion benchmark, which serves as a first layer of efficient,
scalable multilingual evaluation at
https://github.com/for-ai/language-confusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference Camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShadowLLM: Predictor-based <span class="highlight-title">Context</span>ual Sparsity for Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Akhauri, Ahmed F AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M Rush, Safeen Huda, Mohamed S Abdelfattah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high power consumption and latency-sensitive deployments of large
language models (LLMs) have motivated efficiency techniques like quantization
and sparsity. Contextual sparsity, where the sparsity pattern is
input-dependent, is crucial in LLMs because the permanent removal of attention
heads or neurons from LLMs can significantly degrade accuracy. Prior work has
attempted to model contextual sparsity using neural networks trained to predict
activation magnitudes, which can be used to dynamically prune structures with
low predicted activation magnitude. In this paper, we look beyond
magnitude-based pruning criteria to assess attention head and neuron importance
in LLMs. We develop a novel predictor called ShadowLLM, which can shadow the
LLM behavior and enforce better sparsity patterns, resulting in over 15%
improvement in end-to-end accuracy compared to prior methods. In addition,
ShadowLLM achieves up to a 20% speed-up over the state-of-the-art DejaVu
framework. These enhancements are validated on Llama-2 and OPT models with up
to 30 billion parameters. Our code is available at
\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main, Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block-Attention for Efficient RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15355v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15355v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        East Sun, Yan Wang, Lan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Traditional approaches often encode the entire context. Instead,
Block-Attention divides retrieved documents into discrete blocks, with each
block independently calculating key-value (KV) states except for the final
block. In RAG scenarios, by defining each passage as a block, Block-Attention
enables us to reuse the KV states of passages that have been seen before,
thereby significantly reducing the latency and the computation overhead during
inference. The implementation of Block-Attention involves block segmentation,
position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention
mechanism. Experiments on four RAG benchmarks demonstrate that after block
fine-tuning, the Block-Attention model achieves performance comparable to
self-attention models (68.4\% vs 67.9\% on Llama3) or even superior performance
(62.8\% vs 59.6\% on Mistral). Notably, Block-Attention significantly reduces
the time to first token (TTFT) and floating point operations (FLOPs) to a very
low level. It only takes 45 ms to output the first token for an input sequence
with a total length of 32K. Compared to the self-attention models, the time
consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>-SAW: Leveraging Relation-Aware Graphs for Textual <span class="highlight-title">Prompt</span>
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Guimin Hu, Weimin Lyu, Lijie Hu, Lu Yu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown exceptional abilities for multiple
different natural language processing tasks. While prompting is a crucial tool
for LLM inference, we observe that there is a significant cost associated with
exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead
to substandard results in terms of readability/interpretability of the
compressed prompt, with a detrimental impact on prompt utility. To address
this, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an
effective strategy for prompt compression over task-agnostic and task-aware
prompts. Prompt-SAW uses the prompt's textual information to build a graph and
later extracts key information elements in the graph to come up with the
compressed prompt. We also propose GSM8K-aug, i.e., an extended version of the
existing GSM8K benchmark for task-agnostic prompts in order to provide a
comprehensive evaluation platform. Experimental evaluation using benchmark
datasets shows that prompts compressed by Prompt-SAW are not only better in
terms of readability, but they also outperform the best-performing baseline
models by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware
settings while compressing the original prompt text by 34.9 and 56.7.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Analysis of Large Language Models as Soft Reasoners: The
  Case of Syllogistic Inferences <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11341v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11341v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Bertolazzi, Albert Gatt, Raffaella Bernardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning abilities of Large Language Models (LLMs) are becoming a
central focus of study in NLP. In this paper, we consider the case of
syllogistic reasoning, an area of deductive reasoning studied extensively in
logic and cognitive psychology. Previous research has shown that pre-trained
LLMs exhibit reasoning biases, such as $\textit{content effects}$, avoid
answering that $\textit{no conclusion follows}$, display human-like
difficulties, and struggle with multi-step reasoning. We contribute to this
research line by systematically investigating the effects of chain-of-thought
reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on
syllogistic reasoning, considering syllogisms with conclusions that support or
violate world knowledge, as well as ones with multiple premises. Crucially, we
go beyond the standard focus on accuracy, with an in-depth analysis of the
conclusions generated by the models. Our results suggest that the behavior of
pre-trained LLMs can be explained by heuristics studied in cognitive science
and that both ICL and SFT improve model performance on valid inferences,
although only the latter mitigates most reasoning biases without harming model
consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference
  Attacks in Text-to-SQL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Đorđe Klisura, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems empower users to interact with databases using natural
language, automatically translating queries into executable SQL code. However,
their reliance on database schema information for SQL generation exposes them
to significant security vulnerabilities, particularly schema inference attacks
that can lead to unauthorized data access or manipulation. In this paper, we
introduce a novel zero-knowledge framework for reconstructing the underlying
database schema of text-to-SQL models without any prior knowledge of the
database. Our approach systematically probes text-to-SQL models with specially
crafted questions and leverages a surrogate GPT-4 model to interpret the
outputs, effectively uncovering hidden schema elements -- including tables,
columns, and data types. We demonstrate that our method achieves high accuracy
in reconstructing table names, with F1 scores of up to .99 for generative
models and .78 for fine-tuned models, underscoring the severity of schema
leakage risks. Furthermore, we propose a simple protection mechanism for
generative models and empirically show its limitations in mitigating these
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLT: Can Large Language Models Handle Basic Legal Text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09693v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09693v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We find that the best publicly available LLMs like GPT-4 and Claude currently
perform poorly on basic legal text handling. This motivates the creation of a
benchmark consisting of examples that lawyers and paralegals would expect LLMs
to handle zero-shot, such as looking up the text at a line of a witness
deposition or at a subsection of a contract. LLMs' poor performance on this
benchmark casts into doubt their reliability as-is for legal practice. However,
fine-tuning on our training set brings even a small model to near-perfect
performance. This benchmark will be useful for fine-tuning LLMs for downstream
legal tasks, as well as for tracking LLMs' reliability as-is for basic legal
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Inducing Document-Level Abilities in Standard Multilingual
  Neural Machine Translation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Gumma, Pranjal A. Chitale, Kalika Bali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Machine Translation (NMT) models have traditionally used Sinusoidal
Positional Embeddings (PEs), which often struggle to capture long-range
dependencies and are less efficient for handling extended context or
document-level translation tasks. This work addresses the challenge of
transitioning pre-trained NMT models from absolute sinusoidal PEs to relative
PEs, such as Rotary Positional Embeddings (ROPE) and Attention with Linear
Biases (ALIBI), without compromising performance. We demonstrate that
parameter-efficient fine-tuning, using only a small amount of high-quality
data, can successfully facilitate this transition. Experimental results
indicate that switching from sinusoidal to relative PEs results in competitive
translation quality on sentence-level evaluation benchmarks. Additionally,
models trained with ROPE consistently outperform those using ALIBI and
Sinusoidal PEs on document-level benchmarks across both string-based metrics
and qualitative evaluations. Moreover, we find that a small amount of
long-context data in a few languages is sufficient for cross-lingual length
generalization, thereby inducing long-context capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Granular Privacy Control for Geolocation with Vision Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Mendes, Yang Chen, James Hays, Sauvik Das, Wei Xu, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) are rapidly advancing in their capability to
answer information-seeking questions. As these models are widely deployed in
consumer applications, they could lead to new privacy risks due to emergent
abilities to identify people in photos, geolocate images, etc. As we
demonstrate, somewhat surprisingly, current open-source and proprietary VLMs
are very capable image geolocators, making widespread geolocation with VLMs an
immediate privacy risk, rather than merely a theoretical future concern. As a
first step to address this challenge, we develop a new benchmark, GPTGeoChat,
to test the ability of VLMs to moderate geolocation dialogues with users. We
collect a set of 1,000 image geolocation conversations between in-house
annotators and GPT-4v, which are annotated with the granularity of location
information revealed at each turn. Using this new dataset, we evaluate the
ability of various VLMs to moderate GPT-4v geolocation conversations by
determining when too much location information has been revealed. We find that
custom fine-tuned models perform on par with prompted API-based models when
identifying leaked location information at the country or city level; however,
fine-tuning on supervised data appears to be needed to accurately moderate
finer granularities, such as the name of a restaurant or building.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic
  Analysis of Annotators and Targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of online platforms exacerbated the spread of hate speech, demanding
scalable and effective detection. However, the accuracy of hate speech
detection systems heavily relies on human-labeled data, which is inherently
susceptible to biases. While previous work has examined the issue, the
interplay between the characteristics of the annotator and those of the target
of the hate are still unexplored. We fill this gap by leveraging an extensive
dataset with rich socio-demographic information of both annotators and targets,
uncovering how human biases manifest in relation to the target's attributes.
Our analysis surfaces the presence of widespread biases, which we
quantitatively describe and characterize based on their intensity and
prevalence, revealing marked differences. Furthermore, we compare human biases
with those exhibited by persona-based LLMs. Our findings indicate that while
persona-based LLMs do exhibit biases, these differ significantly from those of
human annotators. Overall, our work offers new and nuanced results on human
biases in hate speech annotations, as well as fresh insights into the design of
AI-driven hate speech detection systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient In-Domain Question Answering for Resource-Constrained
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Chung, Phat Vo, Arman C. Kizilkale, Aaron Reite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) is a common method for integrating
external knowledge into pretrained Large Language Models (LLMs) to enhance
accuracy and relevancy in question answering (QA) tasks. However, prompt
engineering and resource efficiency remain significant bottlenecks in
developing optimal and robust RAG solutions for real-world QA applications.
Recent studies have shown success in using fine tuning to address these
problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to
smaller 7B models has demonstrated superior performance compared to RAG setups
with much larger models such as GPT-3.5. The combination of RAFT with
parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation
(LoRA), promises an even more efficient solution, yet remains an unexplored
area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage
requirements and gain faster inference times while maintaining comparable RAG
performance. This results in a more compute-efficient RAFT, or CRAFT, which is
particularly useful for knowledge-intensive QA tasks in resource-constrained
environments where internet access may be restricted and hardware resources
limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16710v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16710v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, Carole-Jean Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LayerSkip, an end-to-end solution to speed-up inference of large
language models (LLMs). First, during training we apply layer dropout, with low
dropout rates for earlier layers and higher dropout rates for later layers, and
an early exit loss where all transformer layers share the same exit. Second,
during inference, we show that this training recipe increases the accuracy of
early exit at earlier layers, without adding any auxiliary layers or modules to
the model. Third, we present a novel self-speculative decoding solution where
we exit at early layers and verify and correct with remaining layers of the
model. Our proposed self-speculative decoding approach has less memory
footprint than other speculative decoding approaches and benefits from shared
compute and activations of the draft and verification stages. We run
experiments on different Llama model sizes on different types of training:
pretraining from scratch, continual pretraining, finetuning on specific data
domain, and finetuning on specific task. We implement our inference solution
and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x
on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and
checkpoints at https://github.com/facebookresearch/LayerSkip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Better: Avoiding Pitfalls in Developing Language Resources when
  Data is Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is a symbolic capital that affects people's lives in many ways
(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,
cultures, traditions, and societies in general. Hence, data in a given language
should be viewed as more than a collection of tokens. Good data collection and
labeling practices are key to building more human-centered and socially aware
technologies. While there has been a rising interest in mid- to low-resource
languages within the NLP community, work in this space has to overcome unique
challenges such as data scarcity and access to suitable annotators. In this
paper, we collect feedback from those directly involved in and impacted by NLP
artefacts for mid- to low-resource languages. We conduct a quantitative and
qualitative analysis of the responses and highlight the main issues related to
(1) data quality such as linguistic and cultural data suitability; and (2) the
ethics of common annotation practices such as the misuse of online community
services. Based on these findings, we make several recommendations for the
creation of high-quality language artefacts that reflect the cultural milieu of
its speakers, while simultaneously respecting the dignity and labor of data
workers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-based Cognitive Models of Students with Misconceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling student cognition is crucial for developing effective
AI-driven educational technologies. A key challenge is creating realistic
student models that satisfy two essential properties: (1) accurately
replicating specific misconceptions, and (2) correctly solving problems where
these misconceptions are not applicable. This dual requirement reflects the
complex nature of student understanding, where misconceptions coexist with
correct knowledge. This paper investigates whether Large Language Models (LLMs)
can be instruction-tuned to meet this dual requirement and effectively simulate
student thinking in algebra. We introduce MalAlgoPy, a novel Python library
that generates datasets reflecting authentic student solution patterns through
a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,
we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned
to faithfully emulate realistic student behavior. Our findings reveal that LLMs
trained on misconception examples can efficiently learn to replicate errors.
However, the training diminishes the model's ability to solve problems
correctly, particularly for problem types where the misconceptions are not
applicable, thus failing to satisfy second property of CSMs. We demonstrate
that by carefully calibrating the ratio of correct to misconception examples in
the training data - sometimes as low as 0.25 - it is possible to develop CSMs
that satisfy both properties. Our insights enhance our understanding of
AI-based student models and pave the way for effective adaptive learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuJo: Multimodal Joint Feature Space Learning for Human Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Gerd Fritsch, Cennet Oguz, Vitor Fortes Rey, Lala Ray, Maximilian Kiefer-Emmanouilidis, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) is a longstanding problem in AI with
applications in a broad range of areas, including healthcare, sports and
fitness, security, and more. The performance of HAR in real-world settings is
strongly dependent on the type and quality of the input signal that can be
acquired. Given an unobstructed, high-quality camera view of a scene, computer
vision systems, in particular in conjunction with foundation models, can today
fairly reliably distinguish complex activities. On the other hand, recognition
using modalities such as wearable sensors (which are often more broadly
available, e.g., in mobile phones and smartwatches) is a more difficult
problem, as the signals often contain less information and labeled training
data is more difficult to acquire. To alleviate the need for labeled data, we
introduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this
work, which can be used with the proposed pre-training method MuJo (Multimodal
Joint Feature Space Learning) to enhance HAR performance across various
modalities. FiMAD was created using YouTube fitness videos and contains
parallel video, language, pose, and simulated IMU sensor data. MuJo utilizes
this dataset to learn a joint feature space for these modalities. We show that
classifiers pre-trained on FiMAD can increase the performance on real HAR
datasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on
MM-Fit, we achieve an Macro F1-Score of up to 0.855 when fine-tuning on only 2%
of the training data and 0.942 when utilizing the full training set for
classification tasks. We have compared our approach to other self-supervised
ones and showed that, unlike them, ours can consistently improve on the
baseline network performance as well as provide a better data-efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Ask Informative Questions: Enhancing LLMs with Preference
  Optimization and Expected Information Gain <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17453v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17453v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Questions are essential tools for acquiring the necessary information to
complete information-seeking tasks. However, large language models (LLMs),
especially open-source models, often perform poorly in generating informative
questions, as measured by expected information gain (EIG). In this paper, we
propose a method to enhance the informativeness of LLM-generated questions in
20-question game dialogues. We sample multiple questions from the same model
(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG
questions to apply a Direct Preference Optimization (DPO) algorithm. Our
results show that this method produces more effective questions (in terms of
EIG), even in domains different from those used to train the DPO model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relay Decoding: Concatenating Large Language Models for Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpeng Fu, Xiaocheng Feng, Yichong Huang, Wenshuai Huo, Baohang Li, Hui Wang, Bin Qin, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models for machine translation has demonstrated
promising results. However, it does require the large language models to
possess the capability of handling both the source and target languages in
machine translation. When it is challenging to find large models that support
the desired languages, resorting to continuous learning methods becomes a
costly endeavor. To mitigate these expenses, we propose an innovative approach
called RD (Relay Decoding), which entails concatenating two distinct large
models that individually support the source and target languages. By
incorporating a simple mapping layer to facilitate the connection between these
two models and utilizing a limited amount of parallel data for training, we
successfully achieve superior results in the machine translation task.
Experimental results conducted on the Multi30k and WikiMatrix datasets validate
the effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Reliability of Large Language Models to Misinformed and
  Demographically-Informed <span class="highlight-title">Prompt</span>s <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toluwani Aremu, Oluwakemi Akinwehinmi, Chukwuemeka Nwagu, Syed Ishtiaque Ahmed, Rita Orji, Pedro Arnau Del Amo, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate and observe the behaviour and performance of Large Language
Model (LLM)-backed chatbots in addressing misinformed prompts and questions
with demographic information within the domains of Climate Change and Mental
Health. Through a combination of quantitative and qualitative methods, we
assess the chatbots' ability to discern the veracity of statements, their
adherence to facts, and the presence of bias or misinformation in their
responses. Our quantitative analysis using True/False questions reveals that
these chatbots can be relied on to give the right answers to these close-ended
questions. However, the qualitative insights, gathered from domain experts,
shows that there are still concerns regarding privacy, ethical implications,
and the necessity for chatbots to direct users to professional services. We
conclude that while these chatbots hold significant promise, their deployment
in sensitive areas necessitates careful consideration, ethical oversight, and
rigorous refinement to ensure they serve as a beneficial augmentation to human
expertise rather than an autonomous solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Study conducted between August and December 2023. Under review at
  AAAI-AI Magazine. Submitted for archival purposes only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback
  for Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine M. Collins, Najoung Kim, Yonatan Bitton, Verena Rieser, Shayegan Omidshafiei, Yushi Hu, Sherol Chen, Senjuti Dutta, Minsuk Chang, Kimin Lee, Youwei Liang, Georgina Evans, Sahil Singla, Gang Li, Adrian Weller, Junfeng He, Deepak Ramachandran, Krishnamurthy Dj Dvijotham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback plays a critical role in learning and refining reward models
for text-to-image generation, but the optimal form the feedback should take for
learning an accurate reward function has not been conclusively established.
This paper investigates the effectiveness of fine-grained feedback which
captures nuanced distinctions in image quality and prompt-alignment, compared
to traditional coarse-grained feedback (for example, thumbs up/down or ranking
between a set of options). While fine-grained feedback holds promise,
particularly for systems catering to diverse societal preferences, we show that
demonstrating its superiority to coarse-grained feedback is not automatic.
Through experiments on real and synthetic preference data, we surface the
complexities of building effective models due to the interplay of model choice,
feedback type, and the alignment between human judgment and computational
interpretation. We identify key challenges in eliciting and utilizing
fine-grained feedback, prompting a reassessment of its assumed benefits and
practicality. Our findings -- e.g., that fine-grained feedback can lead to
worse models for a fixed budget, in some settings; however, in controlled
settings with known attributes, fine grained rewards can indeed be more helpful
-- call for careful consideration of feedback attributes and potentially beckon
novel modeling approaches to appropriately unlock the potential value of
fine-grained feedback in-the-wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive
  Evaluation and Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haishuo Fang, Xiaodan Zhu, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial requirement for deploying LLM-based agents in real-life
applications is the robustness against risky or even irreversible mistakes.
However, the existing research lacks a focus on preemptive evaluation of
reasoning trajectories performed by LLM agents, leading to a gap in ensuring
safe and reliable operations. To explore better solutions, this paper
introduces InferAct, a novel approach that leverages the belief reasoning
ability of LLMs, grounded in Theory-of-Mind, to proactively detect potential
errors before risky actions are executed (e.g., `buy-now' in automatic online
trading or web shopping). InferAct acts as a human proxy, detecting unsafe
actions and alerting users for intervention, which helps prevent irreversible
risks in time and enhances the actor agent's decision-making process.
Experiments on three widely-used tasks demonstrate the effectiveness of
InferAct, presenting a novel solution for safely developing LLM agents in
environments involving critical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large
  Language Models and Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sun, Xinchen Wang, Youdi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) possess impressive reasoning abilities but are
prone to generating incorrect information, often referred to as hallucinations.
While incorporating external Knowledge Graphs (KGs) can partially mitigate this
issue, existing methods primarily treat KGs as static knowledge repositories,
overlooking the critical disparity between KG and LLM knowledge, and failing to
fully exploit the reasoning capabilities inherent in KGs. To address these
limitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for
seamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis
to construct a hierarchical pyramid structure. This structure is designed to
reflect the input question and generate more validated deductive knowledge,
thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive
integration. Furthermore, PDA employs a recursive mechanism to harness the
underlying reasoning abilities of KGs, resulting in more accurate knowledge
retrieval for question-answering tasks. Our experimental results reveal a
substantial performance advantage of PDA over state-of-the-art baselines, with
improvements reaching 26.70% and 26.78%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Agents for Collaborative Task under Information Asymmetry <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Chen Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great
progress in solving complex tasks. It performs communication among agents
within the system to collaboratively solve tasks, under the premise of shared
information. However, when agents' collaborations are leveraged to perform
multi-person tasks, a new challenge arises due to information asymmetry, since
each agent can only access the information of its human user. Previous MAS
struggle to complete tasks under this condition. To address this, we propose a
new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems.
In iAgents, the human social network is mirrored in the agent network, where
agents proactively exchange human information necessary for task resolution,
thereby overcoming information asymmetry. iAgents employs a novel agent
reasoning mechanism, InfoNav, to navigate agents' communication toward
effective information exchange. Together with InfoNav, iAgents organizes human
information in a mixed memory to provide agents with accurate and comprehensive
information for exchange. Additionally, we introduce InformativeBench, the
first benchmark tailored for evaluating LLM agents' task-solving ability under
information asymmetry. Experimental results show that iAgents can collaborate
within a social network of 140 individuals and 588 relationships, autonomously
communicate over 30 turns, and retrieve information from nearly 70,000 messages
to complete tasks within 3 minutes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 12 figures, 6 tables, accepted by NeurIPS 2024, see detail
  at https://thinkwee.top/iagents</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedAide: Towards an Omni Medical Aide via Specialized LLM-based
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Wei, Dingkang Yang, Yanshu Li, Qingyao Xu, Zhaoyu Chen, Mingcheng Li, Yue Jiang, Xiaolu Hou, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-driven interactive systems currently show
potential promise in healthcare domains. Despite their remarkable capabilities,
LLMs typically lack personalized recommendations and diagnosis analysis in
sophisticated medical applications, causing hallucinations and performance
bottlenecks. To address these challenges, this paper proposes MedAide, an
LLM-based omni medical multi-agent collaboration framework for specialized
healthcare services. Specifically, MedAide first performs query rewriting
through retrieval-augmented generation to accomplish accurate medical intent
understanding. Immediately, we devise a contextual encoder to obtain intent
prototype embeddings, which are used to recognize fine-grained intents by
similarity matching. According to the intent relevance, the activated agents
collaborate effectively to provide integrated decision analysis. Extensive
experiments are conducted on four medical benchmarks with composite intents.
Experimental results from automated metrics and expert doctor evaluations show
that MedAide outperforms current LLMs and improves their medical proficiency
and strategic reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Multi-Agent Collaboration for Medical Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skeleton: A New Framework for Accelerating Language Models via Task
  Neuron Localized <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nakyeong Yang, Jiwon Moon, Junseok Kim, Yunah Jang, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning methods have shown comparable performance to general training
methods as parameter-efficient fine-tuning (PEFT) methods in various natural
language understanding tasks. However, existing prompt tuning methods still
utilize the entire model architecture even when solving a specific task, which
prevents them from accelerating inference speed during the application
procedure. In this paper, we propose a novel prompt tuning framework called
Skeleton to efficiently utilize a language model in terms of memory and time
complexity for solving various tasks, retaining only task-relevant neurons by
using an explainability method. From our framework, we can efficiently solve
various tasks by using only task-relevant neurons and prepending adequate
task-specific prompt tokens with only a single language model. Experiments
reveal that our method significantly enhances inference efficiency (at most x
1.73 speed up) for various widely used benchmarks, showing comparable
performance to the prompt tuning method. Moreover, our method is applicable
across various transformer-based architectures, confirming its practicality and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLoCO: Learning <span class="highlight-title">Long</span> <span class="highlight-title">Context</span>s Offline <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca Ada Popa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing long contexts remains a challenge for large language models (LLMs)
due to the quadratic computational and memory overhead of the self-attention
mechanism and the substantial KV cache sizes during generation. We propose
LLoCO, a novel approach to address this problem by learning contexts offline
through context compression and in-domain parameter-efficient finetuning with
LoRA. Our method enables an LLM to create a concise representation of the
original context and efficiently retrieve relevant information to answer
questions accurately. Our approach extends the effective context window of a 4k
token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on
several long-context question-answering datasets, demonstrating that LLoCO
significantly outperforms in-context learning while using $30\times$ fewer
tokens during inference. LLoCO achieves up to $7.62\times$ speed-up during
inference and $11.52\times$ higher throughput during finetuning, substantially
reduces the cost of long document question answering. This makes it a promising
solution for efficient long context processing. Our code is publicly available
on https://github.com/jeffreysijuntan/lloco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. The first two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Good Classifiers? A Study on Edit Intent
  Classification in Scientific Document Revisions <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Ruan, Ilia Kuznetsov, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification is a core NLP task architecture with many potential
applications. While large language models (LLMs) have brought substantial
advancements in text generation, their potential for enhancing classification
tasks remains underexplored. To address this gap, we propose a framework for
thoroughly investigating fine-tuning LLMs for classification, including both
generation- and encoding-based approaches. We instantiate this framework in
edit intent classification (EIC), a challenging and underexplored
classification task. Our extensive experiments and systematic comparisons with
various training approaches and a representative selection of LLMs yield new
insights into their application for EIC. We investigate the generalizability of
these findings on five further classification tasks. To demonstrate the
proposed methods and address the data shortage for empirical edit analysis, we
use our best-performing EIC model to create Re3-Sci2.0, a new large-scale
dataset of 1,780 scientific document revisions with over 94k labeled edits. The
quality of the dataset is assessed through human evaluation. The new dataset
enables an in-depth empirical study of human editing behavior in academic
writing. We make our experimental framework, models and data publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic
  Training Data for Classifying Social Constructs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Birkenmaier, Matthias Roth, Indira Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational text classification is a challenging task, especially for
multi-dimensional social constructs. Recently, there has been increasing
discussion that synthetic training data could enhance classification by
offering examples of how these constructs are represented in texts. In this
paper, we systematically examine the potential of theory-driven synthetic
training data for improving the measurement of social constructs. In
particular, we explore how researchers can transfer established knowledge from
measurement instruments in the social sciences, such as survey scales or
annotation codebooks, into theory-driven generation of synthetic data. Using
two studies on measuring sexism and political topics, we assess the added value
of synthetic training data for fine-tuning text classification models. Although
the results of the sexism study were less promising, our findings demonstrate
that synthetic data can be highly effective in reducing the need for labeled
data in political topic classification. With only a minimal drop in
performance, synthetic data allows for substituting large amounts of labeled
data. Furthermore, theory-driven synthetic data performed markedly better than
data generated without conceptual information in mind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pragmatic Competence Evaluation of Large Language Models for the Korean
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dojun Park, Jiwoo Lee, Hyeyun Jeong, Seohyun Park, Sungeun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks play a significant role in the current evaluation of Large
Language Models (LLMs), yet they often overlook the models' abilities to
capture the nuances of human language, primarily focusing on evaluating
embedded knowledge and technical skills. To address this gap, our study
evaluates how well LLMs understand context-dependent expressions from a
pragmatic standpoint, specifically in Korean. We use both Multiple-Choice
Questions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs)
assessed by human experts. Our results show that GPT-4 leads with scores of
81.11 in MCQs and 85.69 in OEQs, closely followed by HyperCLOVA X.
Additionally, while few-shot learning generally improves performance,
Chain-of-Thought (CoT) prompting tends to encourage literal interpretations,
which may limit effective pragmatic inference. Our findings highlight the need
for LLMs to better understand and generate language that reflects human
communicative norms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Pacific Asia Conference on Language, Information and Computation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masafumi Enomoto, Kunihiro Takeoka, Kosuke Akimoto, Kiril Gashteovski, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-Domain Multi-Document Summarization (ODMDS) is the task of generating
summaries from large document collections in response to user queries. This
task is crucial for efficiently addressing diverse information needs from
users. Traditional retrieve-then-summarize approaches fall short for open-ended
queries in ODMDS tasks. These queries often require broader context than
initially retrieved passages provide, making it challenging to retrieve all
relevant information in a single search. While iterative retrieval methods has
been explored for multi-hop question answering (MQA), it's impractical for
ODMDS due to high latency from repeated LLM inference. Accordingly, we propose
LightPAL, a lightweight passage retrieval method for ODMDS. LightPAL leverages
an LLM to pre-construct a graph representing passage relationships, then
employs random walk during retrieval, avoiding iterative LLM inference.
Experiments demonstrate that LightPAL outperforms naive sparse and pre-trained
dense retrievers in both retrieval and summarization metrics, while achieving
higher efficiency compared to iterative MQA approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeGen: Mitigating Sexually Explicit Content Generation in
  Text-to-Image Models <span class="chip">CCS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited
remarkable performance in generating high-quality images from text descriptions
in recent years. However, text-to-image models may be tricked into generating
not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.
Existing countermeasures mostly focus on filtering inappropriate inputs and
outputs, or suppressing improper text embeddings, which can block sexually
explicit content (e.g., naked) but may still be vulnerable to adversarial
prompts -- inputs that appear innocent but are ill-intended. In this paper, we
present SafeGen, a framework to mitigate sexual content generation by
text-to-image models in a text-agnostic manner. The key idea is to eliminate
explicit visual representations from the model regardless of the text input. In
this way, the text-to-image model is resistant to adversarial prompts since
such unsafe visual representations are obstructed from within. Extensive
experiments conducted on four datasets and large-scale user studies demonstrate
SafeGen's effectiveness in mitigating sexually explicit content generation
while preserving the high-fidelity of benign images. SafeGen outperforms eight
state-of-the-art baseline methods and achieves 99.4% sexual content removal
performance. Furthermore, our constructed benchmark of adversarial prompts
provides a basis for future development and evaluation of anti-NSFW-generation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CCS 2024. Please cite this paper as "Xinfeng Li,
  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.
  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image
  Models. In Proceedings of ACM Conference on Computer and Communications
  Security (CCS), 2024."</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpreadsheetBench: Towards Challenging Real World Spreadsheet
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SpreadsheetBench, a challenging spreadsheet manipulation
benchmark exclusively derived from real-world scenarios, designed to immerse
current large language models (LLMs) in the actual workflow of spreadsheet
users. Unlike existing benchmarks that rely on synthesized queries and
simplified spreadsheet files, SpreadsheetBench is built from 912 real questions
gathered from online Excel forums, which reflect the intricate needs of users.
The associated spreadsheets from the forums contain a variety of tabular data
such as multiple tables, non-standard relational tables, and abundant
non-textual elements. Furthermore, we propose a more reliable evaluation metric
akin to online judge platforms, where multiple spreadsheet files are created as
test cases for each instruction, ensuring the evaluation of robust solutions
capable of handling spreadsheets with varying values. Our comprehensive
evaluation of various LLMs under both single-round and multi-round inference
settings reveals a substantial gap between the state-of-the-art (SOTA) models
and human performance, highlighting the benchmark's difficulty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024 (Spotlight); Homepage:
  https://spreadsheetbench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Instruction Following: Evaluating Inferential Rule Following of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08440v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08440v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangtao Sun, Chenxiang Zhang, XueYou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated strong ability, they
are further supposed to be controlled and guided by in real-world scenarios to
be safe, accurate, and intelligent. This demands the possession of capability
of LLMs. However, no prior work has made a clear evaluation of the inferential
rule-following capability of LLMs. Previous studies that try to evaluate the
inferential rule-following capability of LLMs fail to distinguish the
inferential rule-following scenarios from the instruction-following scenarios.
Therefore, this paper first clarifies the concept of inferential rule-following
and proposes a comprehensive benchmark, RuleBench, to evaluate a diversified
range of inferential rule-following abilities. Our experimental results on a
variety of LLMs show that they are still limited in following rules. Our
analysis based on the evaluation results provides insights into the
improvements for LLMs toward a better inferential rule-following intelligent
agent. We further propose Inferential Rule-Following Tuning (IRFT). The
experimental results show that through IRFT, LLMs can learn abstract
rule-following abilities from purely synthetic data and then generalize to
RuleBench. The data and code can be found at:
https://anonymous.4open.science/r/llm-rule-following-B3E3/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporally Consistent Factuality Probing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prolific use of Large Language Models (LLMs) as an alternate knowledge
base requires them to be factually consistent, necessitating both correctness
and consistency traits for paraphrased queries. Recently, significant attempts
have been made to benchmark datasets and metrics to evaluate LLMs for these
traits. However, structural simplicity (subject-relation-object) and
contemporary association in their query formulation limit the broader
definition of factuality and consistency. In this study, we introduce TeCFaP, a
novel Temporally Consistent Factuality Probe task to expand the consistent
factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC,
a high-quality dataset of prefix-style English query paraphrases. Subsequently,
we extend the definitions of existing metrics to represent consistent
factuality across temporal dimension. We experiment with a diverse set of LLMs
and find most of them performing poorly on TeCFaP. Next, we propose a novel
solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining
multi-task instruction tuning (MT-IT) with consistent-time-sensitive
reinforcement learning (CTSRL) to improve temporally consistent factuality in
LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Chain-of-thought with Chat<span class="highlight-title">GPT</span> for Stance Detection on
  Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Xianghua Fu, Daijun Ding, Hu Huang, Genan Dai, Nan Yin, Yangyang Li, Liwen Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection predicts attitudes towards targets in texts and has gained
attention with the rise of social media. Traditional approaches include
conventional machine learning, early deep neural networks, and pre-trained
fine-tuning models. However, with the evolution of very large pre-trained
language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face
deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not
requiring backpropagation training, has emerged as a promising alternative.
This paper examines CoT's effectiveness in stance detection tasks,
demonstrating its superior accuracy and discussing associated challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2212.14548</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment
  and Knowledge Aggregation <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Shuyang Jiang, Zhe Chen, Yanfeng Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown substantial progress in natural
language understanding and generation, proving valuable especially in the
medical field. Despite advancements, challenges persist due to the complexity
and diversity inherent in medical tasks, which can be categorized as
knowledge-intensive tasks and alignment-required tasks. Previous approaches
either ignore the latter task or focus on a minority of tasks and hence lose
generalization. To address these drawbacks, we propose a progressive
fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise
aggregator to encode diverse knowledge in the first stage and filter out
detrimental information. In the second stage, we drop the Noise Aggregator to
avoid the interference of suboptimal representation and leverage an additional
alignment module optimized towards an orthogonal direction to the knowledge
space to mitigate knowledge forgetting. Based on this two-stage paradigm, we
proposed a Medical LLM through decoupling Clinical Alignment and Knowledge
Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)
performance on over 20 medical tasks, as well as SOTA results on specific
medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all
demonstrate significant improvements over existing models with similar model
sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Belief Revision: The Adaptability of Large Language Models Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to reason from text is crucial for real-world NLP
applications. Real-world scenarios often involve incomplete or evolving data.
In response, individuals update their beliefs and understandings accordingly.
However, most existing evaluations assume that language models (LMs) operate
with consistent information. We introduce Belief-R, a new dataset designed to
test LMs' belief revision ability when presented with new evidence. Inspired by
how humans suppress prior inferences, this task assesses LMs within the newly
proposed delta reasoning ($\Delta R$) framework. Belief-R features sequences of
premises designed to simulate scenarios where additional information could
necessitate prior conclusions drawn by LMs. We evaluate $\sim$30 LMs across
diverse prompting strategies and found that LMs generally struggle to
appropriately revise their beliefs in response to new information. Further,
models adept at updating often underperformed in scenarios without necessary
updates, highlighting a critical trade-off. These insights underscore the
importance of improving LMs' adaptiveness to changing information, a step
toward more reliable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Natural Zero-Shot <span class="highlight-title">Prompt</span>ing on Encoder Models via
  Statement-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Elshabrawy, Yongxin Huang, Iryna Gurevych, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) exhibit remarkable capabilities in
zero-shot and few-shot scenarios, they often require computationally
prohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT
and RoBERTa achieve state-of-the-art results through fine-tuning but struggle
with extending to few-shot and zero-shot settings due to their architectural
constraints. Hence, we propose Statement-Tuning, a technique that models
discriminative tasks as a set of finite statements and trains an encoder model
to discriminate between the potential statements to determine the label. We do
Statement-Tuning on multiple tasks to enable cross-task generalization.
Experimental results demonstrate that Statement-Tuning achieves competitive
performance compared to state-of-the-art LLMs with significantly fewer
parameters. Moreover, the study investigates the impact of several design
choices on few-shot and zero-shot generalization, revealing that
Statement-Tuning can achieve strong performance with modest training data and
benefits from task and statement diversity for unseen task generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in
  Action <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Shao, Tianshi Li, Weiyan Shi, Yanchen Liu, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models (LMs) are widely utilized in personalized communication
scenarios (e.g., sending emails, writing social media posts) and endowed with a
certain level of agency, ensuring they act in accordance with the contextual
privacy norms becomes increasingly critical. However, quantifying the privacy
norm awareness of LMs and the emerging privacy risk in LM-mediated
communication is challenging due to (1) the contextual and long-tailed nature
of privacy-sensitive cases, and (2) the lack of evaluation approaches that
capture realistic application scenarios. To address these challenges, we
propose PrivacyLens, a novel framework designed to extend privacy-sensitive
seeds into expressive vignettes and further into agent trajectories, enabling
multi-level evaluation of privacy leakage in LM agents' actions. We instantiate
PrivacyLens with a collection of privacy norms grounded in privacy literature
and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM
performance in answering probing questions and their actual behavior when
executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4
and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even
when prompted with privacy-enhancing instructions. We also demonstrate the
dynamic nature of PrivacyLens by extending each seed into multiple trajectories
to red-team LM privacy leakage risk. Dataset and code are available at
https://github.com/SALT-NLP/PrivacyLens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Compression for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs) for complex natural language tasks
typically requires long-form prompts to convey detailed requirements and
information, which results in increased memory usage and inference costs. To
mitigate these challenges, multiple efficient methods have been proposed, with
prompt compression gaining significant research interest. This survey provides
an overview of prompt compression techniques, categorized into hard prompt
methods and soft prompt methods. First, the technical approaches of these
methods are compared, followed by an exploration of various ways to understand
their mechanisms, including the perspectives of attention optimization,
Parameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic
language. We also examine the downstream adaptations of various prompt
compression techniques. Finally, the limitations of current prompt compression
methods are analyzed, and several future directions are outlined, such as
optimizing the compression encoder, combining hard and soft prompts methods,
and leveraging insights from multimodality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-16T00:00:00Z">2024-10-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Context</span> is Key(NMF): Modelling Topical Information Dynamics in Chinese
  Diaspora Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Deans Kristensen-McLachlan, Rebecca M. M. Hicke, Márton Kardos, Mette Thunø
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does the People's Republic of China (PRC) interfere with European elections
through ethnic Chinese diaspora media? This question forms the basis of an
ongoing research project exploring how PRC narratives about European elections
are represented in Chinese diaspora media, and thus the objectives of PRC news
media manipulation. In order to study diaspora media efficiently and at scale,
it is necessary to use techniques derived from quantitative text analysis, such
as topic modelling. In this paper, we present a pipeline for studying
information dynamics in Chinese media. Firstly, we present KeyNMF, a new
approach to static and dynamic topic modelling using transformer-based
contextual embedding models. We provide benchmark evaluations to demonstrate
that our approach is competitive on a number of Chinese datasets and metrics.
Secondly, we integrate KeyNMF with existing methods for describing information
dynamics in complex systems. We apply this pipeline to data from five news
sites, focusing on the period of time leading up to the 2024 European
parliamentary elections. Our methods and results demonstrate the effectiveness
of KeyNMF for studying information dynamics in Chinese media and lay groundwork
for further work addressing the broader research questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2024 Computational Humanities Research Conference
  (CHR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Chunking: Learning Efficient Text Segmentation via Logical
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Zhao, Zhiyuan Ji, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG), while serving as a viable complement to
large language models (LLMs), often overlooks the crucial aspect of text
chunking within its pipeline, which impacts the quality of knowledge-intensive
tasks. This paper introduces the concept of Meta-Chunking, which refers to a
granularity between sentences and paragraphs, consisting of a collection of
sentences within a paragraph that have deep linguistic logical connections. To
implement Meta-Chunking, we designed two strategies based on LLMs: Margin
Sampling Chunking and Perplexity Chunking. The former employs LLMs to perform
binary classification on whether consecutive sentences need to be segmented,
making decisions based on the probability difference obtained from margin
sampling. The latter precisely identifies text chunk boundaries by analyzing
the characteristics of perplexity distribution. Additionally, considering the
inherent complexity of different texts, we propose a strategy that combines
Meta-Chunking with dynamic merging to achieve a balance between fine-grained
and coarse-grained text chunking. Experiments conducted on eleven datasets
demonstrate that Meta-Chunking can more efficiently improve the performance of
single-hop and multi-hop question answering based on RAG. For instance, on the
2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only
consuming 45.8% of the time. Our code is available at
https://github.com/IAAR-Shanghai/Meta-Chunking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JudgeBench: A Benchmark for Evaluating LLM-based Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based judges have emerged as a scalable alternative to human evaluation
and are increasingly used to assess, compare, and improve models. However, the
reliability of LLM-based judges themselves is rarely scrutinized. As LLMs
become more advanced, their responses grow more sophisticated, requiring
stronger judges to evaluate them. Existing benchmarks primarily focus on a
judge's alignment with human preferences, but often fail to account for more
challenging tasks where crowdsourced human preference is a poor indicator of
factual and logical correctness. To address this, we propose a novel evaluation
framework to objectively evaluate LLM-based judges. Based on this framework, we
propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging
response pairs spanning knowledge, reasoning, math, and coding. JudgeBench
leverages a novel pipeline for converting existing difficult datasets into
challenging response pairs with preference labels reflecting objective
correctness. Our comprehensive evaluation on a collection of prompted judges,
fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench
poses a significantly greater challenge than previous benchmarks, with many
strong models (e.g., GPT-4o) performing just slightly better than random
guessing. Overall, JudgeBench offers a reliable platform for assessing
increasingly advanced LLM-based judges. Data and code are available at
https://github.com/ScalerLab/JudgeBench .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-<span class="highlight-title">Context</span> Learning Enables Robot Action Prediction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Yin, Zekai Wang, Yuvan Sharma, Dantong Niu, Trevor Darrell, Roei Herzig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have achieved remarkable success using
in-context learning (ICL) in the language domain. However, leveraging the ICL
capabilities within LLMs to directly predict robot actions remains largely
unexplored. In this paper, we introduce RoboPrompt, a framework that enables
off-the-shelf text-only LLMs to directly predict robot actions through ICL
without training. Our approach first heuristically identifies keyframes that
capture important moments from an episode. Next, we extract end-effector
actions from these keyframes as well as the estimated initial object poses, and
both are converted into textual descriptions. Finally, we construct a
structured template to form ICL demonstrations from these textual descriptions
and a task instruction. This enables an LLM to directly predict robot actions
at test time. Through extensive experiments and analysis, RoboPrompt shows
stronger performance over zero-shot and ICL baselines in simulated and
real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned
  Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid progress of diffusion-based content generation, significant
efforts are being made to unlearn harmful or copyrighted concepts from
pretrained diffusion models (DMs) to prevent potential model misuse. However,
it is observed that even when DMs are properly unlearned before release,
malicious finetuning can compromise this process, causing DMs to relearn the
unlearned concepts. This occurs partly because certain benign concepts (e.g.,
"skin") retained in DMs are related to the unlearned ones (e.g., "nudity"),
facilitating their relearning via finetuning. To address this, we propose
meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an
unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes
malicious finetuning on unlearned concepts, the related benign concepts
retained within it will be triggered to self-destruct, hindering the relearning
of unlearned concepts. Our meta-unlearning framework is compatible with most
existing unlearning methods, requiring only the addition of an
easy-to-implement meta objective. We validate our approach through empirical
experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4
and SDXL), supported by extensive ablation studies. Our code is available at
https://github.com/sail-sg/Meta-Unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Task Groupings for Multi-Task Learning Using Pointwise
  V-Usable Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingya Li, Timothy Miller, Steven Bethard, Guergana Savova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of multi-task learning can depend heavily on which tasks are
grouped together. Naively grouping all tasks or a random set of tasks can
result in negative transfer, with the multi-task models performing worse than
single-task models. Though many efforts have been made to identify task
groupings and to measure the relatedness among different tasks, it remains a
challenging research topic to define a metric to identify the best task
grouping out of a pool of many potential task combinations. We propose a metric
of task relatedness based on task difficulty measured by pointwise V-usable
information (PVI). PVI is a recently proposed metric to estimate how much
usable information a dataset contains given a model. We hypothesize that tasks
with not statistically different PVI estimates are similar enough to benefit
from the joint learning process. We conduct comprehensive experiments to
evaluate the feasibility of this metric for task grouping on 15 NLP datasets in
the general, biomedical, and clinical domains. We compare the results of the
joint learners against single learners, existing baseline methods, and recent
large language models, including Llama 2 and GPT-4. The results show that by
grouping tasks with similar PVI estimates, the joint learners yielded
competitive results with fewer total parameters, with consistent performance
across domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper 12 pages, Appendix 7 pages, 1 figure, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unitary Multi-Margin <span class="highlight-title">BERT</span> for Robust Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Yuan Chang, Kang L. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in adversarial attacks on deep learning leave many
mission-critical natural language processing (NLP) systems at risk of
exploitation. To address the lack of computationally efficient adversarial
defense methods, this paper reports a novel, universal technique that
drastically improves the robustness of Bidirectional Encoder Representations
from Transformers (BERT) by combining the unitary weights with the multi-margin
loss. We discover that the marriage of these two simple ideas amplifies the
protection against malicious interference. Our model, the unitary multi-margin
BERT (UniBERT), boosts post-attack classification accuracies significantly by
5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,
the pre-attack and post-attack accuracy tradeoff can be adjusted via a single
scalar parameter to best fit the design requirements for the target
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleDistance: Stronger Content-Independent Style Embeddings with
  Synthetic Parallel Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Patel, Jiacheng Zhu, Justin Qiu, Zachary Horvitz, Marianna Apidianaki, Kathleen McKeown, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style representations aim to embed texts with similar writing styles closely
and texts with different styles far apart, regardless of content. However, the
contrastive triplets often used for training these representations may vary in
both style and content, leading to potential content leakage in the
representations. We introduce StyleDistance, a novel approach to training
stronger content-independent style embeddings. We use a large language model to
create a synthetic dataset of near-exact paraphrases with controlled style
variations, and produce positive and negative examples across 40 distinct style
features for precise contrastive learning. We assess the quality of our
synthetic data and embeddings through human and automatic evaluations.
StyleDistance enhances the content-independence of style embeddings, which
generalize to real-world benchmarks and outperform leading style
representations in downstream applications. Our model can be found at
https://huggingface.co/StyleDistance/styledistance .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Extrinsic Factors for NER in French 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Yang, Zhiyi Li, Yandong Liu, Jungyeul Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) is a crucial task that aims to identify
structured information, which is often replete with complex, technical terms
and a high degree of variability. Accurate and reliable NER can facilitate the
extraction and analysis of important information. However, NER for other than
English is challenging due to limited data availability, as the high expertise,
time, and expenses are required to annotate its data. In this paper, by using
the limited data, we explore various factors including model structure, corpus
annotation scheme and data augmentation techniques to improve the performance
of a NER model for French. Our experiments demonstrate that these approaches
can significantly improve the model's F1 score from original CRF score of 62.41
to 79.39. Our findings suggest that considering different extrinsic factors and
combining these techniques is a promising approach for improving NER
performance where the size of data is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CREAM: Consistency Regularized Self-Rewarding Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-rewarding large language models (LLM) have successfully applied
LLM-as-a-Judge to iteratively improve the alignment performance without the
need of human annotations for preference data. These methods commonly utilize
the same LLM to act as both the policy model (which generates responses) and
the reward model (which scores and ranks those responses). The ranked responses
are then used as preference pairs to train the LLM via direct alignment
technologies (e.g. DPO). However, it is noteworthy that throughout this
process, there is no guarantee of accuracy in the rewarding and ranking, which
is critical for ensuring accurate rewards and high-quality preference data.
Empirical results from relatively small LLMs (e.g., 7B parameters) also
indicate that improvements from self-rewarding may diminish after several
iterations in certain situations, which we hypothesize is due to accumulated
bias in the reward system. This bias can lead to unreliable preference data for
training the LLM. To address this issue, we first formulate and analyze the
generalized iterative preference fine-tuning framework for self-rewarding
language model. We then introduce the regularization to this generalized
framework to mitigate the overconfident preference labeling in the
self-rewarding process. Based on this theoretical insight, we propose a
Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages
the rewarding consistency across different iterations to regularize the
self-rewarding training, helping the model to learn from more reliable
preference data. With this explicit regularization, our empirical results
demonstrate the superiority of CREAM in improving both reward consistency and
alignment performance. The code is publicly available at
https://github.com/Raibows/CREAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldMedQA-V: a multilingual, multimodal medical examination <span class="highlight-title">dataset</span> for
  multimodal language models evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Matos, Shan Chen, Siena Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis F. Nakayama, Jose M. M. Pascual-Leone, Guergana Savova, Hugo Aerts, Leo A. Celi, A. Ian Wong, Danielle S. Bitterman, Jack Gallifant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal/vision language models (VLMs) are increasingly being deployed in
healthcare settings worldwide, necessitating robust benchmarks to ensure their
safety, efficacy, and fairness. Multiple-choice question and answer (QA)
datasets derived from national medical examinations have long served as
valuable evaluation tools, but existing datasets are largely text-only and
available in a limited subset of languages and countries. To address these
challenges, we present WorldMedQA-V, an updated multilingual, multimodal
benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V
includes 568 labeled multiple-choice QAs paired with 568 medical images from
four countries (Brazil, Israel, Japan, and Spain), covering original languages
and validated English translations by native clinicians, respectively. Baseline
performance for common open- and closed-source models are provided in the local
language and English translations, and with and without images provided to the
model. The WorldMedQA-V benchmark aims to better match AI systems to the
diverse healthcare environments in which they are deployed, fostering more
equitable, effective, and representative applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for review, total of 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorldCuisines: A Massive-Scale Benchmark for Multilingual and
  Multicultural Visual Question Answering on Global Cuisines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Ching Lam Cheng, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, Chong-Wah Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) often struggle with culture-specific knowledge,
particularly in languages other than English and in underrepresented cultural
contexts. To evaluate their understanding of such knowledge, we introduce
WorldCuisines, a massive-scale benchmark for multilingual and multicultural,
visually grounded language understanding. This benchmark includes a visual
question answering (VQA) dataset with text-image pairs across 30 languages and
dialects, spanning 9 language families and featuring over 1 million data
points, making it the largest multicultural VQA benchmark to date. It includes
tasks for identifying dish names and their origins. We provide evaluation
datasets in two sizes (12k and 60k instances) alongside a training dataset (1
million instances). Our findings show that while VLMs perform better with
correct location context, they struggle with adversarial contexts and
predicting specific regional cuisines and languages. To support future
research, we release a knowledge base with annotated food entries and images
along with the VQA data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sarcasm Detection in a Less-Resourced Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lazar Đoković, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sarcasm detection task in natural language processing tries to classify
whether an utterance is sarcastic or not. It is related to sentiment analysis
since it often inverts surface sentiment. Because sarcastic sentences are
highly dependent on context, and they are often accompanied by various
non-verbal cues, the task is challenging. Most of related work focuses on
high-resourced languages like English. To build a sarcasm detection dataset for
a less-resourced language, such as Slovenian, we leverage two modern
techniques: a machine translation specific medium-size transformer model, and a
very large generative language model. We explore the viability of translated
datasets and how the size of a pretrained transformer affects its ability to
detect sarcasm. We train ensembles of detection models and evaluate models'
performance. The results show that larger models generally outperform smaller
ones and that ensembling can slightly improve sarcasm detection performance.
Our best ensemble approach achieves an $\text{F}_1$-score of 0.765 which is
close to annotators' agreement in the source language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, published in the Slovenian Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VividMed: Vision Language Model with Versatile Visual Grounding for
  Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vision Language Models (VLMs) have demonstrated
remarkable promise in generating visually grounded responses. However, their
application in the medical domain is hindered by unique challenges. For
instance, most VLMs rely on a single method of visual grounding, whereas
complex medical tasks demand more versatile approaches. Additionally, while
most VLMs process only 2D images, a large portion of medical images are 3D. The
lack of medical data further compounds these obstacles. To address these
challenges, we present VividMed, a vision language model with versatile visual
grounding for medicine. Our model supports generating both semantic
segmentation masks and instance-level bounding boxes, and accommodates various
imaging modalities, including both 2D and 3D data. We design a three-stage
training procedure and an automatic data synthesis pipeline based on open
datasets and models. Besides visual grounding tasks, VividMed also excels in
other common downstream tasks, including Visual Question Answering (VQA) and
report generation. Ablation studies empirically show that the integration of
visual grounding ability leads to improved performance on these tasks. Our code
is publicly available at https://github.com/function2-llx/MMMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Better: Avoiding Pitfalls in Developing Language Resources when
  Data is Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is a symbolic capital that affects people's lives in many ways
(Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities,
cultures, traditions, and societies in general. Hence, data in a given language
should be viewed as more than a collection of tokens. Good data collection and
labeling practices are key to building more human-centered and socially aware
technologies. While there has been a rising interest in mid- to low-resource
languages within the NLP community, work in this space has to overcome unique
challenges such as data scarcity and access to suitable annotators. In this
paper, we collect feedback from those directly involved in and impacted by NLP
artefacts for mid- to low-resource languages. We conduct a quantitative and
qualitative analysis of the responses and highlight the main issues related to
(1) data quality such as linguistic and cultural data suitability; and (2) the
ethics of common annotation practices such as the misuse of online community
services. Based on these findings, we make several recommendations for the
creation of high-quality language artefacts that reflect the cultural milieu of
its speakers, while simultaneously respecting the dignity and labor of data
workers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language alignment in Large Vision-Language Models (LVLMs)
successfully enables LLMs to understand visual input. However, we find that
existing vision-language alignment methods fail to transfer the existing safety
mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic
image. To explore the cause of this problem, we give the insightful explanation
of where and how the safety mechanism of LVLMs operates and conduct comparative
analysis between text and vision. We find that the hidden states at the
specific transformer layers play a crucial role in the successful activation of
safety mechanism, while the vision-language alignment at hidden states level in
current methods is insufficient. This results in a semantic shift for input
images compared to text in hidden states, therefore misleads the safety
mechanism. To address this, we propose a novel Text-Guided vision-language
Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input
vision and uses them to guide the projection of vision into the hidden states
space in LLMs. Experiments show that TGA not only successfully transfers the
safety mechanism for text in basic LLMs to vision in vision-language alignment
for LVLMs without any safety fine-tuning on the visual modality but also
maintains the general performance on various vision tasks (Safe and Good).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Morphological Compositional Generalization in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Lonneke van der Plas, Duygu Ataman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant progress in
various natural language generation and understanding tasks. However, their
linguistic generalization capabilities remain questionable, raising doubts
about whether these models learn language similarly to humans. While humans
exhibit compositional generalization and linguistic creativity in language use,
the extent to which LLMs replicate these abilities, particularly in morphology,
is under-explored. In this work, we systematically investigate the
morphological generalization abilities of LLMs through the lens of
compositionality. We define morphemes as compositional primitives and design a
novel suite of generative and discriminative tasks to assess morphological
productivity and systematicity. Focusing on agglutinative languages such as
Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned
multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs
struggle with morphological compositional generalization particularly when
applied to novel word roots, with performance declining sharply as
morphological complexity increases. While models can identify individual
morphological combinations better than chance, their performance lacks
systematicity, leading to significant accuracy gaps compared to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Measurement Instruments to Training Data: Leveraging Theory-Driven
  Synthetic Training Data for Measuring Social Constructs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Birkenmaier, Matthias Roth, Indira Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational text classification is a challenging task, especially for
multi-dimensional social constructs. Recently, there has been increasing
discussion that synthetic training data could enhance classification by
offering examples of how these constructs are represented in texts. In this
paper, we systematically examine the potential of theory-driven synthetic
training data for improving the measurement of social constructs. In
particular, we explore how researchers can transfer established knowledge from
measurement instruments in the social sciences, such as survey scales or
annotation codebooks, into theory-driven generation of synthetic data. Using
two studies on measuring sexism and political topics, we assess the added value
of synthetic training data for fine-tuning text classification models. Although
the results of the sexism study were less promising, our findings demonstrate
that synthetic data can be highly effective in reducing the need for labeled
data in political topic classification. With only a minimal drop in
performance, synthetic data allows for substituting large amounts of labeled
data. Furthermore, theory-driven synthetic data performed markedly better than
data generated without conceptual information in mind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety,
  Toxicity, and Legal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruimeng Ye, Yang Xiao, Bo Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, ensuring their alignment
with human values becomes increasingly critical. Traditional alignment methods
heavily rely on human feedback to fine-tune models. With the emergence of
superhuman models whose outputs may surpass human understanding, evaluating and
aligning these models using human judgments poses significant challenges. To
address the challenges, recent works use weak supervisors to elicit knowledge
from much stronger models. However, there are important disanalogies between
the empirical setup in the existing works and the genuine goal of alignment. We
remark that existing works investigate the phenomenon of weak-to-strong
generation in analogous setup (i.e., binary classification), rather than
practical alignment-relevant tasks (e.g., safety). In this paper, we bridge
this gap by extending weak-to-strong generation to the context of practical
alignment. We empirically demonstrate the widespread phenomenon of
weak-to-strong generation in three complicated alignment tasks: safety,
toxicity, and legal reasoning}. Furthermore, we explore efficient strategies
for improving alignment performance to enhance the quality of model outcomes.
Lastly, we summarize and analyze the challenges and potential solutions in
regard to specific alignment tasks, which we hope to catalyze the research
progress on the topic of weak-to-strong generalization. Our code is released at
https://github.com/yeruimeng/WTS.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parsing Akkadian Verbs with Prolog <span class="chip">ACL-02</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Macks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a parsing/generation system for finite verbal forms in
Akkadian, with the possible addition of suffixes, implemented in Prolog. The
work described provides the framework and engine to interpret the D, N, and G
stems along with accusative, dative and ventive endings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 9 figures, presented at ACL-02 the Association of
  Computational Linguistics, 2002</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Model Kinship for Merging Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has become one of the key technologies for enhancing the
capabilities and efficiency of Large Language Models (LLMs). However, our
understanding of the expected performance gains and principles when merging any
two models remains limited. In this work, we introduce model kinship, the
degree of similarity or relatedness between LLMs, analogous to biological
evolution. With comprehensive empirical analysis, we find that there is a
certain relationship between model kinship and the performance gains after
model merging, which can help guide our selection of candidate models. Inspired
by this, we propose a new model merging strategy: Top-k Greedy Merging with
Model Kinship, which can yield better performance on benchmark datasets.
Specifically, we discover that using model kinship as a criterion can assist us
in continuously performing model merging, alleviating the degradation (local
optima) in model evolution, whereas model kinship can serve as a guide to
escape these traps. Code is available at
https://github.com/zjunlp/ModelKinship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Votes Count! Programs as Verifiers Improve Self-Consistency of
  Language Models for Math Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vernon Y. H. Toh, Deepanway Ghosal, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown increasing proficiency in solving
mathematical reasoning problems. However, many current open-source LLMs often
still make calculation and semantic understanding errors in their intermediate
reasoning steps. In this work, we propose PROVE, a simple yet effective
framework that uses program-based verification as a heuristic to filter out
potentially incorrect reasoning paths before aggregating the final answers.
Instead of relying on vanilla majority voting, our approach rejects solutions
whose corresponding program outputs are inconsistent with the generated
solution, aggregating only those validated by Python programs. We conducted
extensive experiments on 13 open-source LLMs from various model families and
sizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We
demonstrate that PROVE consistently outperforms vanilla majority voting as a
heuristic for solving mathematical reasoning tasks across all datasets and
model sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from
48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for
Llama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%
to 59.51% for Llama-2-7B-chat. Our codes are available at
https://github.com/declare-lab/prove.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CCSBench: Evaluating Compositional Controllability in LLMs for
  Scientific Document Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixi Ding, Jiaying Wu, Tongyao Zhu, Yanxia Qin, Qian Liu, Min-Yen Kan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To broaden the dissemination of scientific knowledge to diverse audiences,
scientific document summarization must simultaneously control multiple
attributes such as length and empirical focus. However, existing research
typically focuses on controlling single attributes, leaving the compositional
control of multiple attributes underexplored. To address this gap, we introduce
CCSBench, a benchmark for compositional controllable summarization in the
scientific domain. Our benchmark enables fine-grained control over both
explicit attributes (e.g., length), which are objective and straightforward,
and implicit attributes (e.g., empirical focus), which are more subjective and
conceptual. We conduct extensive experiments on GPT-4, LLaMA2, and other
popular LLMs under various settings. Our findings reveal significant
limitations in large language models' ability to balance trade-offs between
control attributes, especially implicit ones that require deeper understanding
and abstract reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Risk of Evidence Pollution for Malicious Social Text Detection in
  the Era of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herun Wan, Minnan Luo, Zhixiong Su, Guang Dai, Xiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evidence-enhanced detectors present remarkable abilities in identifying
malicious social text with related evidence. However, the rise of large
language models (LLMs) brings potential risks of evidence pollution to confuse
detectors. This paper explores how to manipulate evidence, simulating potential
misuse scenarios including basic pollution, and rephrasing or generating
evidence by LLMs. To mitigate its negative impact, we propose three defense
strategies from both the data and model sides, including machine-generated text
detection, a mixture of experts, and parameter updating. Extensive experiments
on four malicious social text detection tasks with ten datasets present that
evidence pollution, especially the generate strategy, significantly compromises
existing detectors. On the other hand, the defense strategies could mitigate
evidence pollution, but they faced limitations for practical employment, such
as the need for annotated data and huge inference costs. Further analysis
illustrates that polluted evidence is of high quality, would compromise the
model calibration, and could ensemble to amplify the negative impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Reverse In-<span class="highlight-title">Context</span> Knowledge Edits? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Youssef, Zhixue Zhao, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context knowledge editing (IKE) enables efficient modification of large
language model (LLM) outputs without parameter changes and at zero-cost.
However, it can be misused to manipulate responses opaquely, e.g., insert
misinformation or offensive content. Such malicious interventions could be
incorporated into high-level wrapped APIs where the final input prompt is not
shown to end-users. To address this issue, we investigate the detection and
reversal of IKE-edits. First, we demonstrate that IKE-edits can be detected
with high accuracy (F1 > 80\%) using only the top-10 output probabilities of
the next token, even in a black-box setting, e.g. proprietary LLMs with limited
output information. Further, we introduce the novel task of reversing IKE-edits
using specially tuned reversal tokens. We explore using both continuous and
discrete reversal tokens, achieving over 80\% accuracy in recovering original,
unedited outputs across multiple LLMs. Our continuous reversal tokens prove
particularly effective, with minimal impact on unedited prompts. Through
analysis of output distributions, attention patterns, and token rankings, we
provide insights into IKE's effects on LLMs and how reversal tokens mitigate
them. This work represents a significant step towards enhancing LLM resilience
against potential misuse of in-context editing, improving their transparency
and trustworthiness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STRUX: An LLM for Decision-Making with Structured Explanations <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Lu, Yebowen Hu, Hassan Foroosh, Wei Jin, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Countless decisions shape our daily lives, and it is paramount to understand
the how and why behind these choices. In this paper, we introduce a new LLM
decision-making framework called STRUX, which enhances LLM decision-making by
providing structured explanations. These include favorable and adverse facts
related to the decision, along with their respective strengths. STRUX begins by
distilling lengthy information into a concise table of key facts. It then
employs a series of self-reflection steps to determine which of these facts are
pivotal, categorizing them as either favorable or adverse in relation to a
specific decision. Lastly, we fine-tune an LLM to identify and prioritize these
key facts to optimize decision-making. STRUX has been evaluated on the
challenging task of forecasting stock investment decisions based on earnings
call transcripts and demonstrated superior performance against strong
baselines. It enhances decision transparency by allowing users to understand
the impact of different factors, representing a meaningful step towards
practical decision-making with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, submitted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Claim Decomposition Benchmark for <span class="highlight-title">Long</span>-form Answer Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Zhang, Yixing Fan, Ruqing Zhang, Jiafeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of LLMs has significantly boosted the performance of complex
long-form question answering tasks. However, one prominent issue of LLMs is the
generated "hallucination" responses that are not factual. Consequently,
attribution for each claim in responses becomes a common solution to improve
the factuality and verifiability. Existing researches mainly focus on how to
provide accurate citations for the response, which largely overlook the
importance of identifying the claims or statements for each response. To bridge
this gap, we introduce a new claim decomposition benchmark, which requires
building system that can identify atomic and checkworthy claims for LLM
responses. Specifically, we present the Chinese Atomic Claim Decomposition
Dataset (CACDD), which builds on the WebCPM dataset with additional expert
annotations to ensure high data quality. The CACDD encompasses a collection of
500 human-annotated question-answer pairs, including a total of 4956 atomic
claims. We further propose a new pipeline for human annotation and describe the
challenges of this task. In addition, we provide experiment results on
zero-shot, few-shot and fine-tuned LLMs as baselines. The results show that the
claim decomposition is highly challenging and requires further explorations.
All code and data are publicly available at
\url{https://github.com/FBzzh/CACDD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CCIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Translation Inference with Iterative Bilingual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Chen, Kehai Chen, Yang Xiang, Xuefeng Bai, Muyun Yang, Tiejun Zhao, Min zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable understanding and generation capabilities of large language
models (LLMs) have greatly improved translation performance. However, incorrect
understanding of the sentence to be translated can degrade translation quality.
To address this issue, we proposed a novel Iterative Bilingual Understanding
Translation (IBUT) method based on the cross-lingual capabilities of LLMs and
the dual characteristics of translation tasks. The cross-lingual capability of
LLMs enables the generation of contextual understanding for both the source and
target languages separately. Furthermore, the dual characteristics allow IBUT
to generate effective cross-lingual feedback, iteratively refining contextual
understanding, thereby reducing errors and improving translation performance.
Experimental results showed that the proposed IBUT outperforms several strong
comparison methods, especially being generalized to multiple domains (e.g.,
news, commonsense, and cultural translation benchmarks).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedAide: Towards an Omni Medical Aide via Specialized LLM-based
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Wei, Dingkang Yang, Yanshu Li, Qingyao Xu, Zhaoyu Chen, Mingcheng Li, Yue Jiang, Xiaolu Hou, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-driven interactive systems currently show
potential promise in healthcare domains. Despite their remarkable capabilities,
LLMs typically lack personalized recommendations and diagnosis analysis in
sophisticated medical applications, causing hallucinations and performance
bottlenecks. To address these challenges, this paper proposes MedAide, an
LLM-based omni medical multi-agent collaboration framework for specialized
healthcare services. Specifically, MedAide first performs query rewriting
through retrieval-augmented generation to accomplish accurate medical intent
understanding. Immediately, we devise a contextual encoder to obtain intent
prototype embeddings, which are used to recognize fine-grained intents by
similarity matching. According to the intent relevance, the activated agents
collaborate effectively to provide integrated decision analysis. Extensive
experiments are conducted on four medical benchmarks with composite intents.
Experimental results from automated metrics and expert doctor evaluations show
that MedAide outperforms current LLMs and improves their medical proficiency
and strategic reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiRST: Finetuning Router-Selective <span class="highlight-title">Transformer</span>s for Input-Adaptive
  Latency Reduction <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-regressive Large Language Models (LLMs) demonstrate remarkable
performance across domanins such as vision and language processing. However,
due to sequential processing through a stack of transformer layers,
autoregressive decoding faces significant computation/latency challenges,
particularly in resource constrained environments like mobile and edge devices.
Existing approaches in literature that aim to improve latency via skipping
layers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics
where tokens exit at pre-determined layers irrespective of input sequence. Both
the above strategies have limitations - the former cannot be applied to handle
KV Caching necessary for speed-ups in modern framework and the latter does not
capture the variation in layer importance across tasks or more generally,
across input sequences. To address both limitations, we propose FIRST, an
algorithm that reduces inference latency by using layer-specific routers to
select a subset of transformer layers adaptively for each input sequence - the
prompt (during prefill stage) decides which layers will be skipped during
decoding. FIRST preserves compatibility with KV caching enabling faster
inference while being quality-aware. FIRST is model-agnostic and can be easily
enabled on any pre-trained LLM. We further improve performance by incorporating
LoRA adapters for fine-tuning on external datasets, enhancing task-specific
accuracy while maintaining latency benefits. Our approach reveals that input
adaptivity is critical - indeed, different task-specific middle layers play a
crucial role in evolving hidden representations depending on task. Extensive
experiments show that FIRST significantly reduces latency while retaining
competitive performance (as compared to baselines), making our approach an
efficient solution for LLM deployment in low-resource environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, Submitted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Fairness in Natural Language Processing: From Traditional
  Methods to Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanny Jourdan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of Natural Language Processing (NLP) stands at a
critical juncture where the integration of fairness within its frameworks has
become an imperative. This PhD thesis addresses the need for equity and
transparency in NLP systems, recognizing that fairness in NLP is not merely a
technical challenge but a moral and ethical necessity, requiring a rigorous
examination of how these technologies interact with and impact diverse human
populations. Through this lens, this thesis undertakes a thorough investigation
into the development of equitable NLP methodologies and the evaluation of
biases that prevail in current systems.
  First, it introduces an innovative algorithm to mitigate biases in
multi-class classifiers, tailored for high-risk NLP applications, surpassing
traditional methods in both bias mitigation and prediction accuracy. Then, an
analysis of the Bios dataset reveals the impact of dataset size on
discriminatory biases and the limitations of standard fairness metrics. This
awareness has led to explorations in the field of explainable AI, aiming for a
more complete understanding of biases where traditional metrics are limited.
Consequently, the thesis presents COCKATIEL, a model-agnostic explainability
method that identifies and ranks concepts in Transformer models, outperforming
previous approaches in sentiment analysis tasks. Finally, the thesis
contributes to bridging the gap between fairness and explainability by
introducing TaCo, a novel method to neutralize bias in Transformer model
embeddings.
  In conclusion, this thesis constitutes a significant interdisciplinary
endeavor that intertwines explicability and fairness to challenge and reshape
current NLP paradigms. The methodologies and critiques presented contribute to
the ongoing discourse on fairness in machine learning, offering actionable
solutions for more equitable and responsible AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis, Toulouse University</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ With a Grain of SALT: Are LLMs Fair Across Social Dimensions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samee Arif, Zohaib Khan, Agha Ali Raza, Awais Athar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an analysis of biases in open-source Large Language
Models (LLMs) across various genders, religions, and races. We introduce a
methodology for generating a bias detection dataset using seven bias triggers:
General Debate, Positioned Debate, Career Advice, Story Generation,
Problem-Solving, Cover-Letter Writing, and CV Generation. We use GPT-4o to
generate a diverse set of prompts for each trigger across various genders,
religious and racial groups. We evaluate models from Llama and Gemma family on
the generated dataset. We anonymise the LLM-generated text associated with each
group using GPT-4o-mini and do a pairwise comparison using GPT-4o-as-a-Judge.
To quantify bias in the LLM-generated text we use the number of wins and losses
in the pairwise comparison. Our analysis spans three languages, English,
German, and Arabic to explore how language influences bias manifestation. Our
findings reveal that LLMs exhibit strong polarization toward certain groups
across each category, with a notable consistency observed across models.
However, when switching languages, variations and anomalies emerge, often
attributable to cultural cues and contextual differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Planner Training for Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Cornille, Florian Mai, Jingyuan Sun, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through end-to-end training to predict the next token, LLMs have become
valuable tools for various tasks. Enhancing their core training in language
modeling can improve numerous downstream applications. A successful approach to
enhance language modeling uses a separate planning module to predict abstract
labels of future sentences and conditions the LM on these predictions. However,
this method is non-differentiable, preventing joint end-to-end tuning of the
planner with the LM. We propose an effective method to improve this approach by
enabling joint fine-tuning of the planner and the LM. We show that a naive way
of approximating the gradient of selecting a label via the straight-through
estimator is not effective. Instead, we propose to use the predicted label
probabilities as mixing weights to condition the LM on a weighted average of
label embeddings in a differentiable manner. This not only enables joint
fine-tuning of the planner and the LM, but also allows the LM to draw on the
full label distribution predicted by the planner, retaining more information.
Our experimental results show consistent improvements in perplexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insights from the Inverse: Reconstructing LLM Training Goals Through
  Inverse RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared Joselowitz, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) trained with Reinforcement Learning from Human
Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying
reward functions and decision-making processes remain opaque. This paper
introduces a novel approach to interpreting LLMs by applying inverse
reinforcement learning (IRL) to recover their implicit reward functions. We
conduct experiments on toxicity-aligned LLMs of varying sizes, extracting
reward models that achieve up to 80.40% accuracy in predicting human
preferences. Our analysis reveals key insights into the non-identifiability of
reward functions, the relationship between model size and interpretability, and
potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward
models can be used to fine-tune new LLMs, resulting in comparable or improved
performance on toxicity benchmarks. This work provides a new lens for
understanding and improving LLM alignment, with implications for the
responsible development and deployment of these powerful systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KcMF: A Knowledge-compliant Framework for Schema and Entity Matching
  with Fine-tuning-free LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqin Xu, Huan Li, Ke Chen, Lidan Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema and entity matching tasks are crucial for data integration and
management. While large language models (LLMs) have shown promising results in
these tasks, they suffer from hallucinations and confusion about task
instructions. In this paper, we present the Knowledge-Compliant Matching
Framework (KcMF), an LLM-based approach that addresses these issues without the
need for domain-specific fine-tuning. KcMF employs a pseudo-code-based task
decomposition strategy to adopt task-specific natural language statements that
guide LLM reasoning and reduce confusion. We also propose two mechanisms,
Dataset as Knowledge (DaK) and Example as Knowledge (EaK), to build domain
knowledge sets when unstructured domain knowledge is lacking. Additionally, we
introduce a result-ensembling strategy to leverage multiple knowledge sources
and suppress poorly formatted outputs. Comprehensive evaluations on schema and
entity matching tasks demonstrate that KcMF outperforms previous non-LLM
state-of-the-art (SOTA) methods by an average F1 score of 22.9% and competes
effectively with SOTA fine-tuned LLMs. Moreover, KcMF generalizes well across
different LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MlingConf: A Comprehensive Study of Multilingual Confidence Estimation
  on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tendency of Large Language Models (LLMs) to generate hallucinations
raises concerns regarding their reliability. Therefore, confidence estimations
indicating the extent of trustworthiness of the generations become essential.
However, current LLM confidence estimations in languages other than English
remain underexplored. This paper addresses this gap by introducing a
comprehensive investigation of Multilingual Confidence estimation (MlingConf)
on LLMs, focusing on both language-agnostic (LA) and language-specific (LS)
tasks to explore the performance and language dominance effects of multilingual
confidence estimations on different tasks. The benchmark comprises four
meticulously checked and human-evaluate high-quality multilingual datasets for
LA tasks and one for the LS task tailored to specific social, cultural, and
geographical contexts of a language. Our experiments reveal that on LA tasks
English exhibits notable linguistic dominance in confidence estimations than
other languages, while on LS tasks, using question-related language to prompt
LLMs demonstrates better linguistic dominance in multilingual confidence
estimations. The phenomena inspire a simple yet effective native-tone prompting
strategy by employing language-specific prompts for LS tasks, effectively
improving LLMs' reliability and accuracy on LS tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Xu, Fang Wu, Tianfan Fu, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Predict Usage Options of Product <span class="highlight-title">Review</span>s with LLM-Generated
  Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Kohlenberg, Leonard Horns, Frederic Sadrieh, Nils Kiele, Matthis Clausen, Konstantin Ketterer, Avetis Navasardyan, Tamara Czinczoll, Gerard de Melo, Ralf Herbrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating large datasets can be challenging. However, crowd-sourcing is
often expensive and can lack quality, especially for non-trivial tasks. We
propose a method of using LLMs as few-shot learners for annotating data in a
complex natural language task where we learn a standalone model to predict
usage options for products from customer reviews. We also propose a new
evaluation metric for this scenario, HAMS4, that can be used to compare a set
of strings with multiple reference sets. Learning a custom model offers
individual control over energy efficiency and privacy measures compared to
using the LLM directly for the sequence-to-sequence task. We compare this data
annotation approach with other traditional methods and demonstrate how LLMs can
enable considerable cost savings. We find that the quality of the resulting
data exceeds the level attained by third-party vendor services and that
GPT-4-generated labels even reach the level of domain experts. We make the code
and generated labels publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Language Gaps in Large Language Models with Inference-Time
  Cross-Lingual Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities in natural
language processing but exhibit significant performance gaps among different
languages. Most existing approaches to address these disparities rely on
pretraining or fine-tuning, which are resource-intensive. To overcome these
limitations without incurring significant costs, we propose Inference-Time
Cross-Lingual Intervention (INCLINE), a novel framework that enhances LLM
performance on low-performing (source) languages by aligning their internal
representations with those of high-performing (target) languages during
inference. INCLINE initially learns alignment matrices using parallel sentences
from source and target languages through a Least-Squares optimization, and then
applies these matrices during inference to transform the low-performing
language representations toward the high-performing language space. Extensive
experiments on nine benchmarks with five LLMs demonstrate that INCLINE
significantly improves performance across diverse tasks and languages, compared
to recent strong baselines. Our analysis demonstrates that INCLINE is highly
cost-effective and applicable to a wide range of applications. In addition, we
release the code to foster research along this line:
https://github.com/weixuan-wang123/INCLINE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Best of Both Worlds: Bridging Quality and Diversity in Data
  Selection with Bipartite Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of large language models (LLMs) in natural language
processing (NLP) tasks is significantly influenced by the quality and diversity
of data used for supervised fine-tuning (SFT). Current data selection methods
often focus solely on quality or diversity, leading to underperforming models
due to suboptimal training data. In this paper, we introduce GraphFilter, a
novel method that represents the dataset as a bipartite graph, linking
sentences to their constituent n-grams. This representation effectively
captures the relationships between sentences and linguistic patterns,
facilitating the selection of sentences that enhance n-gram diversity. To
balance quality and diversity during selection, we propose a priority function
that combines the quality metric with the diversity metric in a multiplicative
manner. GraphFilter iteratively selects high-priority sentences, updates the
bipartite graph by removing covered n-grams, and re-calculates priorities to
reflect the evolving data landscape. We conduct extensive experiments using
three model backbones across six widely used benchmarks. The results
demonstrate that GraphFilter outperforms all nine baseline approaches,
achieving superior model performance and computational efficiency. Our analyses
validate the effectiveness of our design choices, examine the subsets selected
by GraphFilter and other methods, highlight the importance of instruction
diversity, and explore the role of quality and diversity in relation to subset
sizes. GraphFilter establishes a new foundation for effective data selection
strategies, encouraging further research in data selection for LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Ko-LLM Leaderboard2: Bridging Foundational and Practical Evaluation
  for Korean LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonwoo Kim, Dahyun Kim, Jihoo Kim, Sukyung Lee, Yungi Kim, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean
Large Language Models (LLMs), yet it has certain limitations. Notably, the
disconnect between quantitative improvements on the overly academic leaderboard
benchmarks and the qualitative impact of the models should be addressed.
Furthermore, the benchmark suite is largely composed of translated versions of
their English counterparts, which may not fully capture the intricacies of the
Korean language. To address these issues, we propose Open Ko-LLM Leaderboard2,
an improved version of the earlier Open Ko-LLM Leaderboard. The original
benchmarks are entirely replaced with new tasks that are more closely aligned
with real-world capabilities. Additionally, four new native Korean benchmarks
are introduced to better reflect the distinct characteristics of the Korean
language. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide
a more meaningful evaluation for advancing Korean LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Chatbot Knowledge in Customer Service: <span class="highlight-title">Context</span>-Aware Similar
  Question Generation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengze Hong, Yuanfeng Song, Di Jiang, Lu Wang, Zichang Guo, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable responses of service chatbots are often achieved by employing
retrieval-based methods that restrict answers to a knowledge base comprising
predefined question-answer pairs (QA pairs). To accommodate potential
variations in how a customer's query may be expressed, it emerges as the
favored solution to augment these QA pairs with similar questions that are
possibly diverse while remaining semantic consistency. This augmentation task
is known as Similar Question Generation (SQG). Traditional methods that heavily
rely on human efforts or rule-based techniques suffer from limited diversity or
significant semantic deviation from the source question, only capable of
producing a finite number of useful questions.
  To address these limitations, we propose an SQG approach based on Large
Language Models (LLMs), capable of producing a substantial number of diverse
questions while maintaining semantic consistency to the source QA pair. This is
achieved by leveraging LLMs' natural language understanding capability through
fine-tuning with specially designed prompts. The experiments conducted on a
real customer-service dataset demonstrate that our method surpasses baseline
methods by a significant margin in terms of semantic diversity. Human
evaluation further confirms that integrating the answer that reflects the
customer's intention is crucial for increasing the number of generated
questions that meet business requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformity in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Zhu, Caiqi Zhang, Tom Stafford, Nigel Collier, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conformity effect describes the tendency of individuals to align their
responses with the majority. Studying this bias in large language models (LLMs)
is crucial, as LLMs are increasingly used in various information-seeking and
decision-making tasks as conversation partners to improve productivity. Thus,
conformity to incorrect responses can compromise their effectiveness. In this
paper, we adapt psychological experiments to examine the extent of conformity
in state-of-the-art LLMs. Our findings reveal that all models tested exhibit
varying levels of conformity toward the majority, regardless of their initial
choice or correctness, across different knowledge domains. Notably, we are the
first to show that LLMs are more likely to conform when they are more uncertain
in their own prediction. We further explore factors that influence conformity,
such as training paradigms and input characteristics, finding that
instruction-tuned models are less susceptible to conformity, while increasing
the naturalness of majority tones amplifies conformity. Finally, we propose two
interventions--Devil's Advocate and Question Distillation--to mitigate
conformity, providing insights into building more robust language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (8 pages main body), 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Analysis of Hierarchical Language Recognition and Generation
  by <span class="highlight-title">Transformer</span>s without Positional Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daichi Hayakawa, Issei Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we provide constructive proof that Transformers can recognize
and generate hierarchical language efficiently with respect to model size, even
without the need for a specific positional encoding. Specifically, we show that
causal masking and a starting token enable Transformers to compute positional
information and depth within hierarchical structures. We demonstrate that
Transformers without positional encoding can generate hierarchical languages.
Furthermore, we suggest that explicit positional encoding might have a
detrimental effect on generalization with respect to sequence length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing the Barriers of Language Agents in Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous planning has been an ongoing pursuit since the inception of
artificial intelligence. Based on curated problem solvers, early planning
agents could deliver precise solutions for specific tasks but lacked
generalization. The emergence of large language models (LLMs) and their
powerful reasoning capabilities has reignited interest in autonomous planning
by automatically generating reasonable solutions for given tasks. However,
prior research and our experiments show that current language agents still lack
human-level planning abilities. Even the state-of-the-art reasoning model,
OpenAI o1, achieves only 15.6% on one of the complex real-world planning
benchmarks. This highlights a critical question: What hinders language agents
from achieving human-level planning? Although existing studies have highlighted
weak performance in agent planning, the deeper underlying issues and the
mechanisms and limitations of the strategies proposed to address them remain
insufficiently understood. In this work, we apply the feature attribution study
and identify two key factors that hinder agent planning: the limited role of
constraints and the diminishing influence of questions. We also find that
although current strategies help mitigate these challenges, they do not fully
resolve them, indicating that agents still have a long way to go before
reaching human-level intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Coarse-Grained Matching in Video-Text Retrieval <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aozhu Chen, Hazel Doughty, Xirong Li, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text retrieval has seen significant advancements, yet the ability of
models to discern subtle differences in captions still requires verification.
In this paper, we introduce a new approach for fine-grained evaluation. Our
approach can be applied to existing datasets by automatically generating hard
negative test captions with subtle single-word variations across nouns, verbs,
adjectives, adverbs, and prepositions. We perform comprehensive experiments
using four state-of-the-art models across two standard benchmarks (MSR-VTT and
VATEX) and two specially curated datasets enriched with detailed descriptions
(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our
analyses show that the current evaluation benchmarks fall short in detecting a
model's ability to perceive subtle single-word differences, 2) our fine-grained
evaluation highlights the difficulty models face in distinguishing such subtle
variations. To enhance fine-grained understanding, we propose a new baseline
that can be easily combined with current methods. Experiments on our
fine-grained evaluations demonstrate that this approach enhances a model's
ability to understand fine-grained differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nominal Class Assignment in Swahili: A Computational Account 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giada Palmieri, Konstantinos Kogkalidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss the open question of the relation between semantics and nominal
class assignment in Swahili. We approach the problem from a computational
perspective, aiming first to quantify the extent of this relation, and then to
explicate its nature, taking extra care to suppress morphosyntactic confounds.
Our results are the first of their kind, providing a quantitative evaluation of
the semantic cohesion of each nominal class, as well as a nuanced taxonomic
description of its semantic content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tenth Italian Conference on Computational Linguistics (CliC-it-2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProSA: Assessing and Understanding the <span class="highlight-title">Prompt</span> Sensitivity of LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive capabilities across
various tasks, but their performance is highly sensitive to the prompts
utilized. This variability poses challenges for accurate assessment and user
satisfaction. Current research frequently overlooks instance-level prompt
variations and their implications on subjective evaluations. To address these
shortcomings, we introduce ProSA, a framework designed to evaluate and
comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity
metric, PromptSensiScore, and leverages decoding confidence to elucidate
underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers
that prompt sensitivity fluctuates across datasets and models, with larger
models exhibiting enhanced robustness. We observe that few-shot examples can
alleviate this sensitivity issue, and subjective evaluations are also
susceptible to prompt sensitivities, particularly in complex,
reasoning-oriented tasks. Furthermore, our findings indicate that higher model
confidence correlates with increased prompt robustness. We believe this work
will serve as a helpful tool in studying prompt sensitivity of LLMs. The
project is released at: https://github.com/open-compass/ProSA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024, Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking Universal Features Through Fine-Tuning and Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niels Horn, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how features emerge, disappear, and persist across models fine-tuned
on different domains of text. More specifically, we start from a base one-layer
Transformer language model that is trained on a combination of the BabyLM
corpus, and a collection of Python code from The Stack. This base model is
adapted to two new domains of text: TinyStories, and the Lua programming
language, respectively; and then these two models are merged using these two
models using spherical linear interpolation. Our exploration aims to provide
deeper insights into the stability and transformation of features across
typical transfer-learning scenarios using small-scale models and sparse
auto-encoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Compression for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs) for complex natural language tasks
typically requires long-form prompts to convey detailed requirements and
information, which results in increased memory usage and inference costs. To
mitigate these challenges, multiple efficient methods have been proposed, with
prompt compression gaining significant research interest. This survey provides
an overview of prompt compression techniques, categorized into hard prompt
methods and soft prompt methods. First, the technical approaches of these
methods are compared, followed by an exploration of various ways to understand
their mechanisms, including the perspectives of attention optimization,
Parameter-Efficient Fine-Tuning (PEFT), modality fusion, and new synthetic
language. We also examine the downstream adaptations of various prompt
compression techniques. Finally, the limitations of current prompt compression
methods are analyzed, and several future directions are outlined, such as
optimizing the compression encoder, combining hard and soft prompts methods,
and leveraging insights from multimodality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Attribution Bias in Retrieval-Augmented Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Abolghasemi, Leif Azzopardi, Seyyed Hadi Hashemi, Maarten de Rijke, Suzan Verberne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attributing answers to source documents is an approach used to enhance the
verifiability of a model's output in retrieval augmented generation (RAG).
Prior work has mainly focused on improving and evaluating the attribution
quality of large language models (LLMs) in RAG, but this may come at the
expense of inducing biases in the attribution of answers. We define and examine
two aspects in the evaluation of LLMs in RAG pipelines, namely attribution
sensitivity and bias with respect to authorship information. We explicitly
inform an LLM about the authors of source documents, instruct it to attribute
its answers, and analyze (i) how sensitive the LLM's output is to the author of
source documents, and (ii) whether the LLM exhibits a bias towards
human-written or AI-generated source documents. We design an experimental setup
in which we use counterfactual evaluation to study three LLMs in terms of their
attribution sensitivity and bias in RAG pipelines. Our results show that adding
authorship information to source documents can significantly change the
attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have
an attribution bias towards explicit human authorship, which can serve as a
competing hypothesis for findings of prior work that shows that LLM-generated
content may be preferred over human-written contents. Our findings indicate
that metadata of source documents can influence LLMs' trust, and how they
attribute their answers. Furthermore, our research highlights attribution bias
and sensitivity as a novel aspect of brittleness in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying
  Real-World Claims <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a
system that only employs publicly available large language models (LLMs) for
each step of automated fact-checking, dubbed the Herd of Open LLMs for
verifying real-world claims (HerO). HerO employs multiple LLMs for each step of
automated fact-checking. For evidence retrieval, a language model is used to
enhance a query by generating hypothetical fact-checking documents. We prompt
pretrained and fine-tuned LLMs for question generation and veracity prediction
by crafting prompts with retrieved in-context samples. HerO achieved 2nd place
on the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of
open LLMs for verifying real-world claims. For future research, we make our
code publicly available at https://github.com/ssu-humane/HerO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A system description paper for the AVeriTeC shared task, hosted by
  the seventh FEVER workshop (co-located with EMNLP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRefLexOR: Preference-based Recursive Language Modeling for Exploratory
  Optimization of Reasoning and Agentic Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PRefLexOR (Preference-based Recursive Language Modeling for Exploratory
Optimization of Reasoning) combines preference optimization with concepts from
Reinforcement Learning to enable models to self-teach through iterative
reasoning improvements. We propose a recursive learning approach that engages
the model in multi-step reasoning, revisiting, and refining intermediate steps
before producing a final output in training and inference phases. Through
multiple training stages, the model first learns to align its reasoning with
accurate decision paths by optimizing the log odds between preferred and
non-preferred responses. During this process, PRefLexOR builds a dynamic
knowledge graph by generating questions from random text chunks and
retrieval-augmentation to contextualize relevant details from the entire
training corpus. In the second stage, preference optimization enhances model
performance by using rejection sampling to fine-tune reasoning quality by
continually producing in-situ training data while masking the reasoning steps.
Recursive optimization within a thinking token framework introduces iterative
feedback loops, where the model refines reasoning, achieving deeper coherence,
consistency, and adaptability. Implemented in small language models with only 3
billion parameters, we should that even tiny models can iteratively teach
themselves to reason with greater depth and reflectivity. Our implementation is
straightforward and can be incorporated into any existing pretrained LLM. We
focus our examples on applications in biological materials science and
demonstrate the method in a variety of case studies that range from in-domain
to cross-domain applications. Using reasoning strategies that include thinking
and reflection modalities we build a multi-agent recursive self-improving
inference approach to successively improve responses via repeated sampling in
inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Agent: Shifting LLM Agents from Reactive Responses to Active
  Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxi Lu, Shenzhi Yang, Cheng Qian, Guirong Chen, Qinyu Luo, Yesai Wu, Huadong Wang, Xin Cong, Zhong Zhang, Yankai Lin, Weiwen Liu, Yasheng Wang, Zhiyuan Liu, Fangming Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents powered by large language models have shown remarkable abilities in
solving complex tasks. However, most agent systems remain reactive, limiting
their effectiveness in scenarios requiring foresight and autonomous
decision-making. In this paper, we tackle the challenge of developing proactive
agents capable of anticipating and initiating tasks without explicit human
instructions. We propose a novel data-driven approach for this problem.
Firstly, we collect real-world human activities to generate proactive task
predictions. These predictions are then labeled by human annotators as either
accepted or rejected. The labeled data is used to train a reward model that
simulates human judgment and serves as an automatic evaluator of the
proactiveness of LLM agents. Building on this, we develop a comprehensive data
generation pipeline to create a diverse dataset, ProactiveBench, containing
6,790 events. Finally, we demonstrate that fine-tuning models with the proposed
ProactiveBench can significantly elicit the proactiveness of LLM agents.
Experimental results show that our fine-tuned model achieves an F1-Score of
66.47% in proactively offering assistance, outperforming all open-source and
close-source models. These results highlight the potential of our method in
creating more proactive and effective agent systems, paving the way for future
advancements in human-agent collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GECTurk WEB: An Explainable Online Platform for Turkish Grammatical
  Error Detection and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Gebeşçe, Gözde Gül Şahin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sophisticated grammatical error detection/correction tools are available for
a small set of languages such as English and Chinese. However, it is not
straightforward -- if not impossible -- to adapt them to morphologically rich
languages with complex writing rules like Turkish which has more than 80
million speakers. Even though several tools exist for Turkish, they primarily
focus on spelling errors rather than grammatical errors and lack features such
as web interfaces, error explanations and feedback mechanisms. To fill this
gap, we introduce GECTurk WEB, a light, open-source, and flexible web-based
system that can detect and correct the most common forms of Turkish writing
errors, such as the misuse of diacritics, compound and foreign words, pronouns,
light verbs along with spelling mistakes. Our system provides native speakers
and second language learners an easily accessible tool to detect/correct such
mistakes and also to learn from their mistakes by showing the explanation for
the violated rule(s). The proposed system achieves 88,3 system usability score,
and is shown to help learn/remember a grammatical rule (confirmed by 80% of the
participants). The GECTurk WEB is available both as an offline tool at
https://github.com/GGLAB-KU/gecturkweb or online at www.gecturk.net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A linguistic analysis of undesirable outcomes in the era of generative
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Gambetta, Gizem Gezici, Fosca Giannotti, Dino Pedreschi, Alistair Knott, Luca Pappalardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has focused on the medium and long-term impacts of generative
AI, posing scientific and societal challenges mainly due to the detection and
reliability of machine-generated information, which is projected to form the
major content on the Web soon. Prior studies show that LLMs exhibit a lower
performance in generation tasks (model collapse) as they undergo a fine-tuning
process across multiple generations on their own generated content
(self-consuming loop). In this paper, we present a comprehensive simulation
framework built upon the chat version of LLama2, focusing particularly on the
linguistic aspects of the generated content, which has not been fully examined
in existing studies. Our results show that the model produces less lexical rich
content across generations, reducing diversity. The lexical richness has been
measured using the linguistic measures of entropy and TTR as well as
calculating the POSTags frequency. The generated content has also been examined
with an $n$-gram analysis, which takes into account the word order, and
semantic networks, which consider the relation between different words. These
findings suggest that the model collapse occurs not only by decreasing the
content diversity but also by distorting the underlying linguistic patterns of
the generated text, which both highlight the critical importance of carefully
choosing and curating the initial input text, which can alleviate the model
collapse problem. Furthermore, we conduct a qualitative analysis of the
fine-tuned models of the pipeline to compare their performances on generic NLP
tasks to the original model. We find that autophagy transforms the initial
model into a more creative, doubtful and confused one, which might provide
inaccurate answers and include conspiracy theories in the model responses,
spreading false and biased information on the Web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Role of LLMs in Multimodal Evaluation Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botian Jiang, Lei Li, Xiaonan Li, Zhaowei Li, Xiachong Feng, Lingpeng Kong, Qi Liu, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Multimodal Large Language Models (MLLMs) has been
accompanied by the development of various benchmarks to evaluate their
capabilities. However, the true nature of these evaluations and the extent to
which they assess multimodal reasoning versus merely leveraging the underlying
Large Language Model (LLM) backbone remain unclear. This paper presents a
comprehensive investigation into the role of LLM backbones in MLLM evaluation,
focusing on two critical aspects: the degree to which current benchmarks truly
assess multimodal reasoning and the influence of LLM prior knowledge on
performance. Specifically, we introduce a modified evaluation protocol to
disentangle the contributions of the LLM backbone from multimodal integration,
and an automatic knowledge identification technique for diagnosing whether LLMs
equip the necessary knowledge for corresponding multimodal questions. Our study
encompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key
findings reveal that some benchmarks allow high performance even without visual
inputs and up to 50\% of error rates can be attributed to insufficient world
knowledge in the LLM backbone, indicating a heavy reliance on language
capabilities. To address knowledge deficiencies, we propose a knowledge
augmentation pipeline that achieves significant performance gains, with
improvements of up to 60\% on certain datasets, resulting in a approximately 4x
increase in performance. Our work provides crucial insights into the role of
the LLM backbone in MLLMs, and highlights the need for more nuanced
benchmarking approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuron-based Personality Trait Induction in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Deng, Tianyi Tang, Yanbin Yin, Wenhao Yang, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become increasingly proficient at
simulating various personality traits, an important capability for supporting
related applications (e.g., role-playing). To further improve this capacity, in
this paper, we present a neuron-based approach for personality trait induction
in LLMs, with three major technical contributions. First, we construct
PersonalityBench, a large-scale dataset for identifying and evaluating
personality traits in LLMs. This dataset is grounded in the Big Five
personality traits from psychology and is designed to assess the generative
capabilities of LLMs towards specific personality traits. Second, by leveraging
PersonalityBench, we propose an efficient method for identifying
personality-related neurons within LLMs by examining the opposite aspects of a
given trait. Third, we develop a simple yet effective induction method that
manipulates the values of these identified personality-related neurons. This
method enables fine-grained control over the traits exhibited by LLMs without
training and modifying model parameters. Extensive experiments validate the
efficacy of our neuron identification and trait induction methods. Notably, our
approach achieves comparable performance as fine-tuned models, offering a more
efficient and flexible solution for personality trait induction in LLMs. We
provide access to all the mentioned resources at
https://github.com/RUCAIBox/NPTI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Low-Resource Language Model Training: Comprehensive Analysis
  of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Akimoto, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of optimizing training setups for
Large Language Models (LLMs) of low-resource language with a limited amount of
corpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training
to utilize the limited target language corpus efficiently. However, there is
still a lack of understanding about the optimal hyperparameter setups for
combining these three approaches to train LLMs. We exhaustively explore
training setups for low-resource language LLM, combining these three
approaches, and found the following insights for efficiently reducing the cost
of hyperparameter search: (1) As the amount of target language corpus
decreases, the optimal training approach shifts from monolingual single-stage
training to multi-lingual two-stage training at a compute budget dependent
threshold. (2) The optimal model scale remains stable regardless of the amount
of target language corpus, allowing the use of the compute-optimal scale of
monolingual training. (3) The optimal number of epochs can be extrapolated from
smaller-scale experiments to larger scale using our proposed model. Also, we
provide evidence that, in single-stage training, the target language validation
loss follows a power law with respect to the target language ratio, with an
exponent independent of the amount of data, model scale, and language pair.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversal of Thought: Enhancing Large Language Models with
  Preference-Guided Reverse Reasoning Warm-up 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yuan, Dehui Du, Hao Zhang, Zixiang Di, Usman Naseem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in reasoning
tasks but face limitations in mathematical and complex logical reasoning.
Existing methods to improve LLMs' logical capabilities either involve traceable
or verifiable logical sequences that generate more reliable responses by
constructing logical structures yet increase computational costs, or introduces
rigid logic template rules, reducing flexibility. In this paper, we propose
Reversal of Thought (RoT), a novel framework aimed at enhancing the logical
reasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning
warm-up strategy, which integrates logical symbols for pseudocode planning
through meta-cognitive mechanisms and pairwise preference self-evaluation to
generate task-specific prompts solely through demonstrations, aligning with
LLMs' cognitive preferences shaped by Reinforcement Learning with Human
Feedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference
Manager to assess knowledge boundaries and further expand LLMs' reasoning
capabilities by aggregating solution logic for known tasks and stylistic
templates for unknown tasks. Experiments across various tasks demonstrate that
RoT surpasses existing baselines in both reasoning accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Domain Question Answering with Conflicting <span class="highlight-title">Context</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open domain question answering systems frequently rely on information
retrieved from large collections of text (such as the Web) to answer questions.
However, such collections of text often contain conflicting information, and
indiscriminately depending on this information may result in untruthful and
inaccurate answers. To understand the gravity of this problem, we collect a
human-annotated dataset, Question Answering with Conflicting Contexts (QACC),
and find that as much as 25% of unambiguous, open domain questions can lead to
conflicting contexts when retrieved using Google Search. We evaluate and
benchmark three powerful Large Language Models (LLMs) with our dataset QACC and
demonstrate their limitations in effectively addressing questions with
conflicting information. To explore how humans reason through conflicting
contexts, we request our annotators to provide explanations for their
selections of correct answers. We demonstrate that by finetuning LLMs to
explain their answers, we can introduce richer information into their training
that guide them through the process of reasoning with conflicting contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering
  Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixuan Wang, Jingyuan Yang, Wei Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable performance across many
tasks, yet aligning them with desired behaviors remains challenging. Activation
intervention has emerged as an effective and economical method to modify the
behavior of LLMs. Despite considerable interest in this area, current
intervention methods exclusively employ a fixed steering vector to modify model
activations, lacking adaptability to diverse input semantics. To address this
limitation, we propose Semantics-Adaptive Dynamic Intervention (SADI), a novel
method that constructs a dynamic steering vector to intervene model activations
at inference time. More specifically, SADI utilizes activation differences in
contrastive pairs to precisely identify critical elements of an LLM (i.e.,
attention heads, hidden states, and neurons) for targeted intervention. During
inference, SADI dynamically steers model behavior by scaling element-wise
activations based on the directions of input semantics. Experimental results
show that SADI outperforms established baselines by substantial margins,
improving task performance without training. SADI's cost-effectiveness and
generalizability across various LLM backbones and tasks highlight its potential
as a versatile alignment technique. In addition, we release the code to foster
research along this line:https://github.com/weixuan-wang123/SADI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large
  Language Models and Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sun, Xinchen Wang, Youdi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) possess impressive reasoning abilities but are
prone to generating incorrect information, often referred to as hallucinations.
While incorporating external Knowledge Graphs (KGs) can partially mitigate this
issue, existing methods primarily treat KGs as static knowledge repositories,
overlooking the critical disparity between KG and LLM knowledge, and failing to
fully exploit the reasoning capabilities inherent in KGs. To address these
limitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for
seamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis
to construct a hierarchical pyramid structure. This structure is designed to
reflect the input question and generate more validated deductive knowledge,
thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive
integration. Furthermore, PDA employs a recursive mechanism to harness the
underlying reasoning abilities of KGs, resulting in more accurate knowledge
retrieval for question-answering tasks. Our experimental results reveal a
substantial performance advantage of PDA over state-of-the-art baselines, with
improvements reaching 26.70% and 26.78%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards LLM-based Cognitive Models of Students with Misconceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling student cognition is crucial for developing effective
AI-driven educational technologies. A key challenge is creating realistic
student models that satisfy two essential properties: (1) accurately
replicating specific misconceptions, and (2) correctly solving problems where
these misconceptions are not applicable. This dual requirement reflects the
complex nature of student understanding, where misconceptions coexist with
correct knowledge. This paper investigates whether Large Language Models (LLMs)
can be instruction-tuned to meet this dual requirement and effectively simulate
student thinking in algebra. We introduce MalAlgoPy, a novel Python library
that generates datasets reflecting authentic student solution patterns through
a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,
we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned
to faithfully emulate realistic student behavior. Our findings reveal that LLMs
trained on misconception examples can efficiently learn to replicate errors.
However, the training diminishes the model's ability to solve problems
correctly, particularly for problem types where the misconceptions are not
applicable, thus failing to satisfy second property of CSMs. We demonstrate
that by carefully calibrating the ratio of correct to misconception examples in
the training data - sometimes as low as 0.25 - it is possible to develop CSMs
that satisfy both properties. Our insights enhance our understanding of
AI-based student models and pave the way for effective adaptive learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How much do <span class="highlight-title">context</span>ualized representations encode <span class="highlight-title">long</span>-range <span class="highlight-title">context</span>? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeng Sun, Cheng-Ping Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze contextual representations in neural autoregressive language
models, emphasizing long-range contexts that span several thousand tokens. Our
methodology employs a perturbation setup and the metric
\emph{Anisotropy-Calibrated Cosine Similarity}, to capture the degree of
contextualization of long-range patterns from the perspective of representation
geometry. We begin the analysis with a case study on standard decoder-only
Transformers, demonstrating that similar perplexity can exhibit markedly
different downstream task performance, which can be explained by the difference
in contextualization of long-range content. Next, we extend the analysis to
other models, covering recent novel architectural designs and various training
configurations. The representation-level results illustrate a reduced capacity
for high-complexity (i.e., less compressible) sequences across architectures,
and that fully recurrent models rely heavily on local context, whereas hybrid
models more effectively encode the entire sequence structure. Finally,
preliminary analysis of model size and training configurations on the encoding
of long-range context suggest potential directions for improving existing
language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span>-Based Knowledge Graph Foundation Model for Universal In-<span class="highlight-title">Context</span>
  Reasoning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanning Cui, Zequn Sun, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive knowledge graphs (KGs) have been constructed to facilitate
knowledge-driven tasks across various scenarios. However, existing work usually
develops separate reasoning models for different KGs, lacking the ability to
generalize and transfer knowledge across diverse KGs and reasoning settings. In
this paper, we propose a prompt-based KG foundation model via in-context
learning, namely KG-ICL, to achieve a universal reasoning ability.
Specifically, we introduce a prompt graph centered with a query-related example
fact as context to understand the query relation. To encode prompt graphs with
the generalization ability to unseen entities and relations in queries, we
first propose a unified tokenizer that maps entities and relations in prompt
graphs to predefined tokens. Then, we propose two message passing neural
networks to perform prompt encoding and KG reasoning, respectively. We conduct
evaluation on 43 different KGs in both transductive and inductive settings.
Results indicate that the proposed KG-ICL outperforms baselines on most
datasets, showcasing its outstanding generalization and universal reasoning
capabilities. The source code is accessible on GitHub:
https://github.com/nju-websoft/KG-ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical
  Decision-Support Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Kayser, Bayar Menzat, Cornelius Emde, Bogdan Bercean, Alex Novak, Abdala Espinosa, Bartlomiej W. Papiez, Susanne Gaube, Thomas Lukasiewicz, Oana-Maria Camburu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing capabilities of AI models are leading to their wider use,
including in safety-critical domains. Explainable AI (XAI) aims to make these
models safer to use by making their inference process more transparent.
However, current explainability methods are seldom evaluated in the way they
are intended to be used: by real-world end users. To address this, we conducted
a large-scale user study with 85 healthcare practitioners in the context of
human-AI collaborative chest X-ray analysis. We evaluated three types of
explanations: visual explanations (saliency maps), natural language
explanations, and a combination of both modalities. We specifically examined
how different explanation types influence users depending on whether the AI
advice and explanations are factually correct. We find that text-based
explanations lead to significant over-reliance, which is alleviated by
combining them with saliency maps. We also observe that the quality of
explanations, that is, how much factually correct information they entail, and
how much this aligns with AI correctness, significantly impacts the usefulness
of the different explanation types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Oversmoothing: Evaluating DDPM and MSE for Scalable Speech
  Synthesis in ASR <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Minixhofer, Ondrej Klejch, Peter Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetically generated speech has rapidly approached human levels of
naturalness. However, the paradox remains that ASR systems, when trained on TTS
output that is judged as natural by humans, continue to perform badly on real
speech. In this work, we explore whether this phenomenon is due to the
oversmoothing behaviour of models commonly used in TTS, with a particular focus
on the behaviour of TTS-for-ASR as the amount of TTS training data is scaled
up. We systematically compare Denoising Diffusion Probabilistic Models (DDPM)
to Mean Squared Error (MSE) based models for TTS, when used for ASR model
training. We test the scalability of the two approaches, varying both the
number hours, and the number of different speakers. We find that for a given
model size, DDPM can make better use of more data, and a more diverse set of
speakers, than MSE models. We achieve the best reported ratio between real and
synthetic speech WER to date (1.46), but also find that a large gap remains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Automatic Task-Specific Synthetic Data Generation for
  Hallucination Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Xie, Karan Aggarwal, Aitzaz Ahmad, Stephen Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to automatically generate non-trivial
task-specific synthetic datasets for hallucination detection. Our approach
features a two-step generation-selection pipeline, using hallucination pattern
guidance and a language style alignment during generation. Hallucination
pattern guidance leverages the most important task-specific hallucination
patterns while language style alignment aligns the style of the synthetic
dataset with benchmark text. To obtain robust supervised detectors from
synthetic datasets, we also adopt a data mixture strategy to improve
performance robustness and generalization. Our results on three datasets show
that our generated hallucination text is more closely aligned with
non-hallucinated text versus baselines, to train hallucination detectors with
better generalization. Our hallucination detectors trained on synthetic
datasets outperform in-context-learning (ICL)-based detectors by a large margin
of 32%. Our extensive experiments confirm the benefits of our approach with
cross-task and cross-generator generalization. Our data-mixture-based training
further improves the generalization and robustness of hallucination detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kallini et al. (2024) do not compare impossible languages with
  constituency-based ones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Hunter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central goal of linguistic theory is to find a precise characterization of
the notion "possible human language", in the form of a computational device
that is capable of describing all and only the languages that can be acquired
by a typically developing human child. The success of recent large language
models (LLMs) in NLP applications arguably raises the possibility that LLMs
might be computational devices that meet this goal. This would only be the case
if, in addition to succeeding in learning human languages, LLMs struggle to
learn "impossible" human languages. Kallini et al. (2024; "Mission: Impossible
Language Models", Proc. ACL) conducted experiments aiming to test this by
training GPT-2 on a variety of synthetic languages, and found that it learns
some more successfully than others. They present these asymmetries as support
for the idea that LLMs' inductive biases align with what is regarded as
"possible" for human languages, but the most significant comparison has a
confound that makes this conclusion unwarranted. In this paper I explain the
confound and suggest some ways forward towards constructing a comparison that
appropriately tests the underlying issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Automatic and Cost-Efficient Peer-<span class="highlight-title">Review</span> Framework for Language
  Generation Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), how to
efficiently evaluate them has become an important research question. Existing
evaluation methods often suffer from high costs, limited test formats, the need
of human references, and systematic evaluation biases. To address these
limitations, our study introduces the Auto-PRE, an automatic LLM evaluation
framework based on peer review. In contrast to previous studies that rely on
human annotations, Auto-PRE selects evaluator LLMs automatically based on their
inherent traits including consistency, self-confidence, and pertinence. We
conduct extensive experiments on three tasks: summary generation, non-factoid
question-answering, and dialogue generation. Experimental results indicate our
Auto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our
study highlights the impact of prompt strategies and evaluation formats on
evaluation performance, offering guidance for method optimization in the
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for
  Retrieval-Augmented Generation with Enhanced Data Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Liu, Ruixue Ding, Linhao Zhang, Pengjun Xie, Fie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) aims to enhance large language models
(LLMs) to generate more accurate and reliable answers with the help of the
retrieved context from external knowledge sources, thereby reducing the
incidence of hallucinations. Despite the advancements, evaluating these systems
remains a crucial research area due to the following issues: (1) Limited data
diversity: The insufficient diversity of knowledge sources and query types
constrains the applicability of RAG systems; (2) Obscure problems location:
Existing evaluation methods have difficulty in locating the stage of the RAG
pipeline where problems occur; (3) Unstable retrieval evaluation: These methods
often fail to effectively assess retrieval performance, particularly when the
chunking strategy changes. To tackle these challenges, we propose a
Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough
evaluation across the entire RAG pipeline, including chunking, retrieval,
reranking, and generation. To effectively evaluate the first three phases, we
introduce multi-granularity keywords, including coarse-grained and fine-grained
keywords, to assess the retrieved context instead of relying on the annotation
of golden chunks. Moreover, we release a holistic benchmark dataset tailored
for diverse data scenarios covering a wide range of document formats and query
types. We demonstrate the utility of the CoFE-RAG framework by conducting
experiments to evaluate each stage of RAG systems. Our evaluation method
provides unique insights into the effectiveness of RAG systems in handling
diverse data scenarios, offering a more nuanced understanding of their
capabilities and limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qian, Fengcun Li, Xiangyang Ji, Xiaoyu Zhao, Jianchao Tan, Kefeng Zhang, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) has revolutionized the field of artificial
intelligence, with their capabilities expanding rapidly due to advances in deep
learning and increased computational resources. The mixture-of-experts (MoE)
model has emerged as a prominent architecture in the field of LLM, better
balancing the model performance and computational efficiency. MoE architecture
allows for effective scaling and efficient parallel processing, but the GEMM
(General Matrix Multiply) of MoE and the large parameters introduce challenges
in terms of computation efficiency and communication overhead, which becomes
the throughput bottleneck during inference. Applying a single parallelism
strategy like EP, DP, PP, etc. to MoE architecture usually achieves sub-optimal
inference throughput, the straightforward combinations of existing different
parallelisms on MoE can not obtain optimal inference throughput yet. This paper
introduces EPS-MoE, a novel expert pipeline scheduler for MoE that goes beyond
the existing inference parallelism schemes. Our approach focuses on optimizing
the computation of MoE FFN (FeedForward Network) modules by dynamically
selecting the best kernel implementation of GroupGemm and DenseGemm for
different loads and adaptively overlapping these computations with
\textit{all2all} communication, leading to a substantial increase in
throughput. Our experimental results demonstrate an average 21% improvement in
prefill throughput over existing parallel inference methods. Specifically, we
validated our method on DeepSeekV2, a highly optimized model claimed to achieve
a prefill throughput of 100K tokens per second. By applying EPS-MoE, we further
accelerated it to at least 120K tokens per second.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with
  Large Language Models for Multi-Behavior Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyi Ma, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sushant Kumar, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating diverse data modalities is crucial for enhancing the performance
of personalized recommendation systems. Traditional models, which often rely on
singular data sources, lack the depth needed to accurately capture the
multifaceted nature of item features and user behaviors. This paper introduces
a novel framework for multi-behavior recommendations, leveraging the fusion of
triple-modality, which is visual, textual, and graph data through alignment
with large language models (LLMs). By incorporating visual information, we
capture contextual and aesthetic item characteristics; textual data provides
insights into user interests and item features in detail; and graph data
elucidates relationships within the item-behavior heterogeneous graphs. Our
proposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs
to align and integrate these three modalities, achieving a comprehensive
representation of user behaviors. The LLM models the user's interactions
including behaviors and item features in natural languages. Initially, the LLM
is warmed up using only natural language-based prompts. We then devise the
modality fusion module based on cross-attention and self-attention mechanisms
to integrate different modalities from other models into the same embedding
space and incorporate them into an LLM. Extensive experiments demonstrate the
effectiveness of our approach in improving recommendation accuracy. Further
ablation studies validate the effectiveness of our model design and benefits of
the TMF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Jing, Srinivas Billa, Danny Godbout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination has been a popular topic in natural language generation (NLG).
In real-world applications, unfaithful content can result in bad data quality
or loss of trust from end users. Thus, it is crucial to fact-check before
adopting NLG for production usage, which can be expensive if done manually. In
this paper, we investigate automated faithfulness evaluation in guided NLG. We
developed a rubrics template and use large language models (LLMs) to score the
generation into quantifiable scales. We compared popular LLMs as well as the
widely adopted natural language inference (NLI) models in scoring quality and
sensitivity. In addition, we developed methods to generation synthetic
unfaithful data, as well as a heuristics to quantify the percentage of
hallucination. Our results on 4 travel-domain industry dataset show that GPT-4
can provide accurate judgement and explanation on whether a source and a
generation are factually consistent. Furthermore, we found that tuning NLI
models on synthetic data can improve performance. Lastly, we present insights
on latency and cost for deploying such system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmnixR: Evaluating Omni-modality Language Models on Reasoning across
  Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lichang Chen, Hexiang Hu, Mingda Zhang, Yiwen Chen, Zifeng Wang, Yandong Li, Pranav Shyam, Tianyi Zhou, Heng Huang, Ming-Hsuan Yang, Boqing Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OmnixR, an evaluation suite designed to benchmark SoTA
Omni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,
which integrate multiple modalities such as text, vision, and audio, presents
unique challenges. Particularly, the user message might often consist of
multiple modalities, such that OLMs have to establish holistic understanding
and reasoning across modalities to accomplish the task. Existing benchmarks are
limited to single modality or dual-modality tasks, overlooking comprehensive
multi-modal assessments of model reasoning. To address this, OmnixR offers two
evaluation variants: (1)synthetic subset: a synthetic dataset generated
automatically by translating text into multiple modalities--audio, images,
video, and hybrids (Omnify). (2)realistic subset: a real-world dataset,
manually curated and annotated by experts, for evaluating cross-modal reasoning
in natural settings. OmnixR presents a unique evaluation towards assessing OLMs
over a diverse mix of modalities, such as a question that involves video,
audio, and text, providing a rigorous cross-modal reasoning testbed unlike any
existing benchmarks. Our experiments find that all state-of-the-art OLMs
struggle with OmnixR questions that require integrating information from
multiple modalities to answer. Further analysis highlights differences in
reasoning behavior, underscoring the challenges of omni-modal AI alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harbani Jaggi, Kashyap Murali, Eve Fleisig, Erdem Bıyık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When annotators disagree, predicting the labels given by individual
annotators can capture nuances overlooked by traditional label aggregation. We
introduce three approaches to predicting individual annotator ratings on the
toxicity of text by incorporating individual annotator-specific information: a
neural collaborative filtering (NCF) approach, an in-context learning (ICL)
approach, and an intermediate embedding-based architecture. We also study the
utility of demographic information for rating prediction. NCF showed limited
utility; however, integrating annotator history, demographics, and survey
information permits both the embedding-based architecture and ICL to
substantially improve prediction accuracy, with the embedding-based
architecture outperforming the other methods. We also find that, if
demographics are predicted from survey information, using these imputed
demographics as features performs comparably to using true demographic data.
This suggests that demographics may not provide substantial information for
modeling ratings beyond what is captured in survey responses. Our findings
raise considerations about the relative utility of different types of annotator
information and provide new approaches for modeling annotators in subjective
NLP tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative-<span class="highlight-title">Prompt</span>-driven Alignment for Generative Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Qiao, Ning Xv, Biao Liu, Xin Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have achieved remarkable capabilities, but aligning
their outputs with human values and preferences remains a significant
challenge. Existing alignment methods primarily focus on positive examples
while overlooking the importance of negative responses in guiding models away
from undesirable behaviors. For instance, the widely-used alignment datasets
reveals a scarcity of explicit negative examples that contradict human values,
hindering its ability to discourage harmful or biased outputs during training.
To address this limitation, we propose NEAT, i.e., NEgative-prompt-driven
AlignmenT, to introduce negative prompts to generate undesirable responses
alongside positive examples during the optimization process. NEAT explicitly
penalizes the model for producing harmful outputs, guiding it not only toward
desirable behaviors but also steering it away from generating undesirable,
biased responses. This dual feedback mechanism enables better alignment with
human preferences, crucial in contexts where avoiding harm is paramount.
Starting from a pre-trained language model, NEAT performs online alignment by
incorporating a ranking loss derived from an expanded preference dataset
containing both positive and negative examples. Extensive experiments validate
NEAT's effectiveness in significantly enhancing language models' alignment with
human values and preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIRD: A Trustworthy Bayesian Inference Framework for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Feng, Ben Zhou, Weidong Lin, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive models often need to work with incomplete information in
real-world tasks. Consequently, they must provide reliable probability or
confidence estimation, especially in large-scale decision making and planning
tasks. Current large language models (LLM) are insufficient for such accurate
estimations, but they can generate relevant factors that may affect the
probabilities, produce coarse-grained probabilities when the information is
more complete, and help determine which factors are relevant to specific
downstream contexts. In this paper, we make use of these capabilities of LLMs
to provide a significantly more accurate probabilistic estimation. We propose
BIRD, a novel probabilistic inference framework that aligns a Bayesian network
with LLM abductions and then estimates more accurate probabilities in a
deduction step. We show BIRD provides reliable probability estimations that are
30\% better than those provided directly by LLM baselines. These estimates can
further contribute to better and more trustworthy decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy and Carbon Considerations of Fine-Tuning <span class="highlight-title">BERT</span> <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, Sasha Luccioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP
community, existing work quantifying energy costs and associated carbon
emissions has largely focused on language model pre-training. Although a single
pre-training run draws substantially more energy than fine-tuning, fine-tuning
is performed more frequently by many more individual actors, and thus must be
accounted for when considering the energy and carbon footprint of NLP. In order
to better characterize the role of fine-tuning in the landscape of energy and
carbon emissions in NLP, we perform a careful empirical study of the
computational costs of fine-tuning across tasks, datasets, hardware
infrastructure and measurement modalities. Our experimental results allow us to
place fine-tuning energy and carbon costs into perspective with respect to
pre-training and inference, and outline recommendations to NLP researchers and
practitioners who wish to improve their fine-tuning energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings; First two authors contributed equally; 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ÚFAL CorPipe at CRAC 2023: Larger <span class="highlight-title">Context</span> Improves Multilingual
  Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Straka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CorPipe, the winning entry to the CRAC 2023 Shared Task on
Multilingual Coreference Resolution. Our system is an improved version of our
earlier multilingual coreference pipeline, and it surpasses other participants
by a large margin of 4.5 percent points. CorPipe first performs mention
detection, followed by coreference linking via an antecedent-maximization
approach on the retrieved spans. Both tasks are trained jointly on all
available corpora using a shared pretrained language model. Our main
improvements comprise inputs larger than 512 subwords and changing the mention
decoding to support ensembling. The source code is available at
https://github.com/ufal/crac2023-corpipe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2023 (the Sixth Workshop on Computational Models of
  Reference, Anaphora and Coreference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ÚFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for
  Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milan Straka, Jana Straková
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe the winning submission to the CRAC 2022 Shared Task on
Multilingual Coreference Resolution. Our system first solves mention detection
and then coreference linking on the retrieved spans with an
antecedent-maximization approach, and both tasks are fine-tuned jointly with
shared Transformer weights. We report results of fine-tuning a wide range of
pretrained models. The center of this contribution are fine-tuned multilingual
models. We found one large multilingual model with sufficiently large encoder
to increase performance on all datasets across the board, with the benefit not
limited only to the underrepresented languages or groups of typologically
relative languages. The source code is available at
https://github.com/ufal/crac2022-corpipe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CRAC 2022 (Fifth Workshop on Computational Models of
  Reference, Anaphora and Coreference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Source Conversational AI with SpeechBrain 1.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00463v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00463v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirco Ravanelli, Titouan Parcollet, Adel Moumen, Sylvain de Langen, Cem Subakan, Peter Plantinga, Yingzhi Wang, Pooneh Mousavi, Luca Della Libera, Artem Ploujnikov, Francesco Paissan, Davide Borra, Salah Zaiem, Zeyu Zhao, Shucong Zhang, Georgios Karakasidis, Sung-Lin Yeh, Pierre Champion, Aku Rouhe, Rudolf Braun, Florian Mai, Juan Zuluaga-Gomez, Seyed Mahed Mousavi, Andreas Nautsch, Xuechen Liu, Sangeet Sagar, Jarod Duret, Salima Mdhaffar, Gaelle Laperriere, Mickael Rouvier, Renato De Mori, Yannick Esteve
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,
focused particularly on speech processing tasks such as speech recognition,
speech enhancement, speaker recognition, text-to-speech, and much more. It
promotes transparency and replicability by releasing both the pre-trained
models and the complete "recipes" of code and algorithms required for training
them. This paper presents SpeechBrain 1.0, a significant milestone in the
evolution of the toolkit, which now has over 200 recipes for speech, audio, and
language processing tasks, and more than 100 models available on Hugging Face.
SpeechBrain 1.0 introduces new technologies to support diverse learning
modalities, Large Language Model (LLM) integration, and advanced decoding
strategies, along with novel models, tasks, and modalities. It also includes a
new benchmark repository, offering researchers a unified platform for
evaluating models across diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Journal of Machine Learning research (JMLR), Machine
  Learning Open Source Software</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of
  the Noisy Channel <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brendan King, Jeffrey Flanigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training task-oriented dialogue systems typically requires turn-level
annotations for interacting with their APIs: e.g. a dialogue state and the
system actions taken at each step. These annotations can be costly to produce,
error-prone, and require both domain and annotation expertise. With advances in
LLMs, we hypothesize that unlabeled data and a schema definition are sufficient
for building a working task-oriented dialogue system, completely unsupervised.
We consider a novel unsupervised setting of only (1) a well-defined API schema
(2) a set of unlabeled dialogues between a user and agent. We propose an
innovative approach using expectation-maximization (EM) that infers turn-level
annotations as latent variables using a noisy channel model to build an
end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,
our method more than doubles the dialogue success rate of a strong GPT-3.5
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at Empirical Methods in Natural Language Processing
  (EMNLP 2024). 18 Pages, 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Figurative Meaning through Explainable Visual Entailment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have demonstrated strong capabilities in
tasks requiring a fine-grained understanding of literal meaning in images and
text, such as visual question-answering or visual entailment. However, there
has been little exploration of these models' capabilities when presented with
images and captions containing figurative meaning, such as metaphors or humor.
To close this gap, we propose a new task framing the figurative meaning
understanding problem as an explainable visual entailment task, where the model
has to predict whether the image (premise) entails a caption (hypothesis) and
justify the predicted label with a textual explanation. The figurative
phenomena can be present either in the image, the caption, or both. Utilizing a
human-AI collaboration approach, we build the accompanying expert-verified
dataset V-FLUTE, containing 6,027 {image, caption, label, explanation}
instances spanning five diverse figurative phenomena: metaphors, similes,
idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs
struggle to generalize from literal to figurative meaning, particularly when it
is present in images. Further, we identify common types of errors in VLM
reasoning via human evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToBlend: Token-Level Blending With an Ensemble of LLMs to Attack
  AI-Generated Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Huang, Haewoon Kwak, Jisun An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of AI-content detection models against sophisticated
adversarial strategies, such as paraphrasing or word switching, is a rising
concern in natural language generation (NLG) applications. This study proposes
ToBlend, a novel token-level ensemble text generation method to challenge the
robustness of current AI-content detection approaches by utilizing multiple
sets of candidate generative large language models (LLMs). By randomly sampling
token(s) from candidate LLMs sets, we find ToBlend significantly drops the
performance of most mainstream AI-content detection methods. We evaluate the
text quality produced under different ToBlend settings based on annotations
from experienced human experts. We proposed a fine-tuned Llama3.1 model to
distinguish the ToBlend generated text more accurately. Our findings underscore
our proposed text generation approach's great potential in deceiving and
improving detection models. Our datasets, codes, and annotations are
open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ARR Oct-2024 Cycle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ITINERA: Integrating Spatial Optimization with Large Language Models for
  Open-domain Urban Itinerary Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07204v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07204v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citywalk, a recently popular form of urban travel, requires genuine
personalization and understanding of fine-grained requests compared to
traditional itinerary planning. In this paper, we introduce the novel task of
Open-domain Urban Itinerary Planning (OUIP), which generates personalized urban
itineraries from user requests in natural language. We then present ITINERA, an
OUIP system that integrates spatial optimization with large language models to
provide customized urban itineraries based on user needs. This involves
decomposing user requests, selecting candidate points of interest (POIs),
ordering the POIs based on cluster-aware spatial optimization, and generating
the itinerary. Experiments on real-world datasets and the performance of the
deployed system demonstrate our system's capacity to deliver personalized and
spatially coherent itineraries compared to current solutions. Source codes of
ITINERA are available at https://github.com/YihongT/ITINERA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CELL your Model: Contrastive Explanations for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronny Luss, Erik Miehling, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of black-box deep neural network classification models has sparked
the need to explain their decisions. However, in the case of generative AI,
such as large language models (LLMs), there is no class prediction to explain.
Rather, one can ask why an LLM output a particular response to a given prompt.
In this paper, we answer this question by proposing, to the best of our
knowledge, the first contrastive explanation methods requiring simply
black-box/query access. Our explanations suggest that an LLM outputs a reply to
a given prompt because if the prompt was slightly modified, the LLM would have
given a different response that is either less preferable or contradicts the
original response. The key insight is that contrastive explanations simply
require a scoring function that has meaning to the user and not necessarily a
specific real valued quantity (viz. class label). We offer two algorithms for
finding contrastive explanations: i) A myopic algorithm, which although
effective in creating contrasts, requires many model calls and ii) A budgeted
algorithm, our main algorithmic contribution, which intelligently creates
contrasts adhering to a query budget, necessary for longer contexts. We show
the efficacy of these methods on diverse natural language tasks such as
open-text generation, automated red teaming, and explaining conversational
degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Singlish Discourse Particles with Task-Driven
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20366v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20366v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Tze En Foo, Lynnette Hui Xian Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singlish, or formally Colloquial Singapore English, is an English-based
creole language originating from the SouthEast Asian country Singapore. The
language contains influences from Sinitic languages such as Chinese dialects,
Malay, Tamil and so forth. A fundamental task to understanding Singlish is to
first understand the pragmatic functions of its discourse particles, upon which
Singlish relies heavily to convey meaning. This work offers a preliminary
effort to disentangle the Singlish discourse particles (lah, meh and hor) with
task-driven representation learning. After disentanglement, we cluster these
discourse particles to differentiate their pragmatic functions, and perform
Singlish-to-English machine translation. Our work provides a computational
method to understanding Singlish discourse particles, and opens avenues towards
a deeper comprehension of the language and its usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DOCE: Finding the Sweet Spot for Execution-Based Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13745v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13745v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haau-Sing Li, Patrick Fernandes, Iryna Gurevych, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a diverse set of decoding and reranking procedures have been shown
effective for LLM-based code generation. However, a comprehensive framework
that links and experimentally compares these methods is missing. We address
this by proposing Decoding Objectives for Code Execution, a comprehensive
framework that includes candidate generation, $n$-best reranking, minimum Bayes
risk (MBR) decoding, and self-debugging as the core components. We then study
the contributions of these components through execution-based evaluation
metrics. Our findings highlight the importance of execution-based methods and
the difference gap between execution-based and execution-free methods.
Furthermore, we assess the impact of filtering based on trial unit tests, a
simple and effective strategy that has been often overlooked in prior works. We
also propose self-debugging on multiple candidates, obtaining state-of-the-art
performance on reranking for code generation. We expect our framework to
provide a solid guideline for future research on code generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (32 including appendix), 5 figures, 25 tables. Prompts are
  provided in the GitHub repository to avoid potential text overlap with other
  papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music
  Scores for All Singing Tasks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of high-quality and multi-task singing datasets significantly
hinders the development of diverse controllable and personalized singing tasks,
as existing singing datasets suffer from low quality, limited diversity of
languages and singers, absence of multi-technique information and realistic
music scores, and poor task suitability. To tackle these problems, we present
GTSinger, a large global, multi-technique, free-to-use, high-quality singing
corpus with realistic music scores, designed for all singing tasks, along with
its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality
singing voices, forming the largest recorded singing dataset; (2) 20
professional singers across nine widely spoken languages offer diverse timbres
and styles; (3) we provide controlled comparison and phoneme-level annotations
of six commonly used singing techniques, helping technique modeling and
control; (4) GTSinger offers realistic music scores, assisting real-world
musical composition; (5) singing voices are accompanied by manual
phoneme-to-audio alignments, global style labels, and 16.16 hours of paired
speech for various singing tasks. Moreover, to facilitate the use of GTSinger,
we conduct four benchmark experiments: technique-controllable singing voice
synthesis, technique recognition, style transfer, and speech-to-singing
conversion. The corpus and demos can be found at http://gtsinger.github.io. We
provide the dataset and the code for processing data and conducting benchmarks
at https://huggingface.co/datasets/GTSinger/GTSinger and
https://github.com/GTSinger/GTSinger.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Robust RLHF in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15360v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15360v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzi Yan, Xingzhou Lou, Jialian Li, Yiping Zhang, Jian Xie, Chao Yu, Yu Wang, Dong Yan, Yuan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to progress toward more advanced
forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is
increasingly seen as a key pathway toward achieving Artificial General
Intelligence (AGI). However, the reliance on reward-model-based (RM-based)
alignment methods introduces significant challenges due to the inherent
instability and imperfections of Reward Models (RMs), which can lead to
critical issues such as reward hacking and misalignment with human intentions.
In this paper, we introduce a reward-robust RLHF framework aimed at addressing
these fundamental challenges, paving the way for more reliable and resilient
learning in LLMs. Our approach introduces a novel optimization objective that
carefully balances performance and robustness by incorporating Bayesian Reward
Model Ensembles (BRME) to model the uncertainty set of reward functions. This
allows the framework to integrate both nominal performance and minimum reward
signals, ensuring more stable learning even with imperfect RMs. Empirical
results demonstrate that our framework consistently outperforms baselines
across diverse benchmarks, showing improved accuracy and long-term stability.
We also provide a theoretical analysis, demonstrating that reward-robust RLHF
approaches the stability of constant reward settings, which proves to be
acceptable even in a stochastic-case analysis. Together, these contributions
highlight the framework potential to enhance both the performance and stability
of LLM alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust ASR Error Correction with Conservative Data Filtering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuma Udagawa, Masayuki Suzuki, Masayasu Muraoka, Gakuto Kurata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error correction (EC) based on large language models is an emerging
technology to enhance the performance of automatic speech recognition (ASR)
systems. Generally, training data for EC are collected by automatically pairing
a large set of ASR hypotheses (as sources) and their gold references (as
targets). However, the quality of such pairs is not guaranteed, and we observed
various types of noise which can make the EC models brittle, e.g. inducing
overcorrection in out-of-domain (OOD) settings. In this work, we propose two
fundamental criteria that EC training data should satisfy: namely, EC targets
should (1) improve linguistic acceptability over sources and (2) be inferable
from the available context (e.g. source phonemes). Through these criteria, we
identify low-quality EC pairs and train the models not to make any correction
in such cases, the process we refer to as conservative data filtering. In our
experiments, we focus on Japanese ASR using a strong Conformer-CTC as the
baseline and finetune Japanese LLMs for EC. Through our evaluation on a suite
of 21 internal benchmarks, we demonstrate that our approach can significantly
reduce overcorrection and improve both the accuracy and quality of ASR results
in the challenging OOD settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Details Make a Difference: Object State-Sensitive Neurorobotic Task
  Planning <span class="chip">ICANN24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowen Sun, Xufeng Zhao, Jae Hee Lee, Wenhao Lu, Matthias Kerzel, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state of an object reflects its current status or condition and is
important for a robot's task planning and manipulation. However, detecting an
object's state and generating a state-sensitive plan for robots is challenging.
Recently, pre-trained Large Language Models (LLMs) and Vision-Language Models
(VLMs) have shown impressive capabilities in generating plans. However, to the
best of our knowledge, there is hardly any investigation on whether LLMs or
VLMs can also generate object state-sensitive plans. To study this, we
introduce an Object State-Sensitive Agent (OSSA), a task-planning agent
empowered by pre-trained neural networks. We propose two methods for OSSA: (i)
a modular model consisting of a pre-trained vision processing module (dense
captioning model, DCM) and a natural language processing model (LLM), and (ii)
a monolithic model consisting only of a VLM. To quantitatively evaluate the
performances of the two methods, we use tabletop scenarios where the task is to
clear the table. We contribute a multimodal benchmark dataset that takes object
states into consideration. Our results show that both methods can be used for
object state-sensitive tasks, but the monolithic approach outperforms the
modular approach. The code for OSSA is available at
https://github.com/Xiao-wen-Sun/OSSA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICANN24, Switzerland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain
  Readability Assessment <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14463v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14463v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Naous, Michael J. Ryan, Anton Lavrouk, Mohit Chandra, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive evaluation of large language models for
multilingual readability assessment. Existing evaluation resources lack domain
and language diversity, limiting the ability for cross-domain and cross-lingual
analyses. This paper introduces ReadMe++, a multilingual multi-domain dataset
with human annotations of 9757 sentences in Arabic, English, French, Hindi, and
Russian, collected from 112 different data sources. This benchmark will
encourage research on developing robust multilingual readability assessment
methods. Using ReadMe++, we benchmark multilingual and monolingual language
models in the supervised, unsupervised, and few-shot prompting settings. The
domain and language diversity in ReadMe++ enable us to test more effective
few-shot prompting, and identify shortcomings in state-of-the-art unsupervised
methods. Our experiments also reveal exciting results of superior domain
generalization and enhanced cross-lingual transfer capabilities by models
trained on ReadMe++. We will make our data publicly available and release a
python package tool for multilingual sentence readability prediction using our
trained models at: https://github.com/tareknaous/readme
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Explainable to Interpretable Deep Learning for Natural Language
  Processing in Healthcare: How Far from Reality? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11894v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11894v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, Giorgos Papanastasiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) has substantially enhanced natural language processing
(NLP) in healthcare research. However, the increasing complexity of DL-based
NLP necessitates transparent model interpretability, or at least
explainability, for reliable decision-making. This work presents a thorough
scoping review of explainable and interpretable DL in healthcare NLP. The term
"eXplainable and Interpretable Artificial Intelligence" (XIAI) is introduced to
distinguish XAI from IAI. Different models are further categorized based on
their functionality (model-, input-, output-based) and scope (local, global).
Our analysis shows that attention mechanisms are the most prevalent emerging
IAI technique. The use of IAI is growing, distinguishing it from XAI. The major
challenges identified are that most XIAI does not explore "global" modelling
processes, the lack of best practices, and the lack of systematic evaluation
and benchmarks. One important opportunity is to use attention mechanisms to
enhance multi-modal XIAI for personalized medicine. Additionally, combining DL
with causal logic holds promise. Our discussion encourages the integration of
XIAI in Large Language Models (LLMs) and domain-specific smaller models. In
conclusion, XIAI adoption in healthcare requires dedicated in-house expertise.
Collaboration with domain experts, end-users, and policymakers can lead to
ready-to-use XIAI methods across NLP and medical tasks. While challenges exist,
XIAI techniques offer a valuable foundation for interpretable NLP algorithms in
healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by Computational and Structural
  Biotechnology Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19350v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19350v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to
simulate human reasoning and inference processes, achieving proficient
performance in multi-hop QA. However, a gap persists between PLMs' reasoning
abilities and those of humans when tackling complex problems. Psychological
studies suggest a vital connection between explicit information in passages and
human prior knowledge during reading. Nevertheless, current research has given
insufficient attention to linking input passages and PLMs' pre-training-based
knowledge from the perspective of human cognition studies. In this study, we
introduce a Prompting Explicit and Implicit knowledge (PEI) framework, which
uses prompts to connect explicit and implicit knowledge, aligning with human
reading process for multi-hop QA. We consider the input passages as explicit
knowledge, employing them to elicit implicit knowledge through unified prompt
reasoning. Furthermore, our model incorporates type-specific reasoning via
prompts, a form of implicit knowledge. Experimental results show that PEI
performs comparably to the state-of-the-art on HotpotQA. Ablation studies
confirm the efficacy of our model in bridging and integrating explicit and
implicit knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram
  <span class="highlight-title">Dataset</span> of Over Half a Million Posts for Multilingual Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nirmalya Thakur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The work presented in this paper makes three scientific contributions with a
specific focus on mining and analysis of COVID-19-related posts on Instagram.
First, it presents a multilingual dataset of 500,153 Instagram posts about
COVID-19 published between January 2020 and September 2024. This dataset,
available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in
161 different languages as well as 535,021 distinct hashtags. After the
development of this dataset, multilingual sentiment analysis was performed,
which involved classifying each post as positive, negative, or neutral. The
results of sentiment analysis are presented as a separate attribute in this
dataset. Second, it presents the results of performing sentiment analysis per
year from 2020 to 2024. The findings revealed the trends in sentiment related
to COVID-19 on Instagram since the beginning of the pandemic. For instance,
between 2020 and 2024, the sentiment trends show a notable shift, with positive
sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from
44.19% to 58.34%. Finally, the paper also presents findings of
language-specific sentiment analysis. This analysis highlighted similar and
contrasting trends of sentiment across posts published in different languages
on Instagram. For instance, out of all English posts, 49.68% were positive,
14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,
4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting
distinct differences in the sentiment distribution between these two languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Token Reweighting for Interpretable and Controllable Text
  Embeddings in CLIP <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunji Kim, Kyuhong Shim, Simyung Chang, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial
role in translating textual input into an embedding space shared with images,
thereby facilitating the interpretative analysis of vision tasks through
natural language. Despite the varying significance of different textual
elements within a sentence depending on the context, efforts to account for
variation of importance in constructing text embeddings have been lacking. We
propose a framework of Semantic Token Reweighting to build Interpretable text
embeddings (SToRI), which incorporates controllability as well. SToRI refines
the text encoding process in CLIP by differentially weighting semantic elements
based on contextual importance, enabling finer control over emphasis responsive
to data-driven insights and user preferences. The efficacy of SToRI is
demonstrated through comprehensive experiments on few-shot image classification
and image retrieval tailored to user preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>DSI: <span class="highlight-title">Prompt</span>-based Rehearsal-free Instance-wise Incremental
  Learning for Document Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)
for efficient document retrieval without relying on external indexes. However,
DSI needs full re-training to handle updates in dynamic corpora, causing
significant computational inefficiencies. We introduce PromptDSI, a
prompt-based rehearsal-free approach for instance-wise incremental learning
document retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of
DSI, leveraging its powerful representation to efficiently index new corpora
while maintaining a balance between stability and plasticity. We eliminate the
initial forward pass of prompt-based continual learning methods that doubles
training and inference time. Moreover, we propose a topic-aware prompt pool
that employs neural topic embeddings as fixed keys. This strategy ensures
diverse and effective prompt usage, addressing the challenge of parameter
underutilization caused by the collapse of the query-key matching mechanism.
Our empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI
in managing forgetting while improving new corpora performance by more than 4%
Hits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences
  of LLM Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12319v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12319v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hawon Jeong, ChaeHun Park, Jimin Hong, Hojoon Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly used as evaluators for
natural language generation tasks, ensuring unbiased assessments is essential.
However, LLM evaluators often display biased preferences, such as favoring
verbosity and authoritative tones. Our empirical analysis reveals that these
biases are exacerbated in pairwise evaluation, where LLMs directly compare two
outputs and easily prioritize superficial attributes. In contrast, pointwise
evaluation, which assesses outputs independently, is less susceptible to such
bias because each output is judged in isolation. To address the limitations of
the pairwise evaluation, we introduce a novel evaluation method, PRePair, which
integrates pointwise reasoning within a pairwise framework. PRePair effectively
alleviates biased preference, improving performance on the adversarial
benchmark (LLMBar) while outperforming pointwise evaluation on the standard
benchmark (MT-Bench).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Data Privacy in Large Language Models through Private
  Association Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Venditti, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require a significant redesign in solutions to
preserve privacy in data-intensive applications due to their text-generation
capabilities. Indeed, LLMs tend to memorize and emit private information when
maliciously prompted. In this paper, we introduce Private Association Editing
(PAE) as a novel defense approach for private data leakage. PAE is designed to
effectively remove Personally Identifiable Information (PII) without retraining
the model. Experimental results demonstrate the effectiveness of PAE with
respect to alternative baseline methods. We believe PAE will serve as a
critical tool in the ongoing effort to protect data privacy in LLMs,
encouraging the development of safer models for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval
  Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14162v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14162v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) is widely employed to ground responses
to queries on domain-specific documents. But do RAG implementations leave out
important information when answering queries that need an integrated analysis
of information (e.g., Tell me good news in the stock market today.)? To address
these concerns, RAG developers need to annotate information retrieval (IR) data
for their domain of interest, which is challenging because (1) domain-specific
queries usually need nuanced definitions of relevance beyond shallow semantic
relevance; and (2) human or GPT-4 annotation is costly and cannot cover all
(query, document) pairs (i.e., annotation selection bias), thus harming the
effectiveness in evaluating IR recall. To address these challenges, we propose
DIRAS (Domain-specific Information Retrieval Annotation with Scalability), a
manual-annotation-free schema that fine-tunes open-sourced LLMs to consider
nuanced relevance definition and annotate (partial) relevance labels with
calibrated relevance scores. Extensive evaluation shows that DIRAS enables
smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking
unseen (query, document) pairs, and is helpful for real-world RAG development.
All code, LLM generations, and human annotations can be found in
\url{https://github.com/EdisonNi-hku/DIRAS}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through
  N-shot Guided <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09879v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09879v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha, Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of LLMs in English, there is a significant gap
in performance in non-English languages. In order to address this, we introduce
a novel recipe for creating a multilingual synthetic instruction tuning
dataset, sPhinX, which is created by selectively translating instruction
response pairs from English into 50 languages. We test the effectiveness of
sPhinx by using it to fine-tune two state-of-the-art models, Mistral-7B and
Phi-Small and then evaluating them across a comprehensive suite of multilingual
benchmarks that test reasoning, question answering, reading comprehension and
machine translation. Our results show that Mistral-7B and Phi-Small fine-tuned
with sPhinX perform better on an average by 5%pt for both the models when
compared to the base variants of these models. We also devise a strategy to
incorporate N-shot examples in each fine-tuning sample which further boosts the
performance of these models by 9%pt and 4%pt respectively respectively compared
to vanilla fine-tuning. To show efficacy of our data curation approach, we also
directly translate our original dataset to the target languages, and observe an
increase of 7%pt and 4%pt on both the models respectively. sPhinX outperforms
other multilingual instruction tuning datasets in both efficiency and
diversity, reducing dataset creation costs. It also maintains strong
performance on standard English LLM benchmarks, with minimal regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 12 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19014v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19014v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems have become crucial for translating natural language into
SQL queries in various industries, enabling non-technical users to perform
complex data operations. The need for accurate evaluation methods has increased
as these systems have grown more sophisticated. However, the Execution Accuracy
(EX), the most prevalent evaluation metric, still shows many false positives
and negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel
approach to evaluating text-to-SQL systems using large language models (LLMs)
to emulate human expert-level evaluation of SQL queries. Our metric improves
agreement with human experts (from 62 to 87.04 in Cohen's kappa) with
comprehensive context and sophisticated criteria. Our extensive experiments
yield several key insights: (1) Models' performance increases by over 2.6
points on average, substantially affecting rankings on Spider and BIRD
benchmarks; (2) The underestimation of models in EX primarily stems from
annotation quality issues; and (3) Model performance on particularly
challenging questions tends to be overestimated. This work contributes to a
more accurate and nuanced evaluation of text-to-SQL systems, potentially
reshaping our understanding of state-of-the-art performance in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts Made Personalized: Federated <span class="highlight-title">Prompt</span> Learning for
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Luo, Chen Chen, Shandong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has
demonstrated potent applicability across diverse downstream tasks. This
lightweight approach has quickly gained traction from federated learning (FL)
researchers who seek to efficiently adapt VLMs to heterogeneous scenarios.
However, current federated prompt learning methods are habitually restricted to
the traditional FL paradigm, where the participating clients are generally only
allowed to download a single globally aggregated model from the server. While
justifiable for training full-sized models under federated settings, in this
work, we argue that this paradigm is ill-suited for lightweight prompts. By
facilitating the clients to download multiple pre-aggregated prompts as fixed
non-local experts, we propose Personalized Federated Mixture of Adaptive
Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning
process through the lens of Mixture of Experts (MoE). pFedMoAP implements a
local attention-based gating network that learns to generate enhanced text
features for better alignment with local image data on the client, benefiting
from both local and downloaded non-local adaptive prompt experts. The non-local
experts are sparsely selected from a server-maintained pool, fostering
collaborative learning across clients. To evaluate the proposed algorithm, we
conduct extensive experiments across 9 datasets under various heterogeneous
federated settings. The results show that pFedMoAP consistently outperforms the
state-of-the-art alternatives, underscoring its efficacy in personalizing
prompt learning for CLIP within the federated learning paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Few-shot Learning for Multi-label Classification of Scientific
  Documents with Many Classes <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Schopf, Alexander Blatzheim, Nektarios Machner, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific document classification is a critical task and often involves many
classes. However, collecting human-labeled data for many classes is expensive
and usually leads to label-scarce scenarios. Moreover, recent work has shown
that sentence embedding model fine-tuning for few-shot classification is
efficient, robust, and effective. In this work, we propose FusionSent
(Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free
approach for few-shot classification of scientific documents with many classes.
FusionSent uses available training examples and their respective label texts to
contrastively fine-tune two different sentence embedding models. Afterward, the
parameters of both fine-tuned models are fused to combine the complementary
knowledge from the separate fine-tuning steps into a single model. Finally, the
resulting sentence embedding model is frozen to embed the training instances,
which are then used as input features to train a classification head. Our
experiments show that FusionSent significantly outperforms strong baselines by
an average of $6.0$ $F_{1}$ points across multiple scientific document
classification datasets. In addition, we introduce a new dataset for
multi-label classification of scientific documents, which contains 203,961
scientific articles and 130 classes from the arXiv category taxonomy. Code and
data are available at https://github.com/sebischair/FusionSent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 7th International Conference on Natural Language and
  Speech Processing (ICNLSP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating
  Adequacy, Fluency, and Elegance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun Yang, Tiejun Zhao, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance in general
translation tasks. However, the increasing demand for high-quality translations
that are not only adequate but also fluent and elegant. To assess the extent to
which current LLMs can meet these demands, we introduce a suitable benchmark
for translating classical Chinese poetry into English. This task requires not
only adequacy in translating culturally and historically significant content
but also a strict adherence to linguistic fluency and poetic elegance. Our
study reveals that existing LLMs fall short of this task. To address these
issues, we propose RAT, a \textbf{R}etrieval-\textbf{A}ugmented machine
\textbf{T}ranslation method that enhances the translation process by
incorporating knowledge related to classical poetry. Additionally, we propose
an automatic evaluation metric based on GPT-4, which better assesses
translation quality in terms of adequacy, fluency, and elegance, overcoming the
limitations of traditional metrics. Our dataset and code will be made
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
  Preference Optimization <span class="highlight-title">Dataset</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08688v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08688v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samee Arif, Sualeha Farid, Abdul Hameed Azeemi, Awais Athar, Agha Ali Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel methodology for generating synthetic Preference
Optimization (PO) datasets using multi-agent workflows. We evaluate the
effectiveness and potential of these workflows in automating and enhancing the
dataset generation process. PO dataset generation requires two modules: (1)
response evaluation, and (2) response generation. In the response evaluation
module, the responses from Large Language Models (LLMs) are evaluated and
ranked - a task typically carried out by human annotators that we automate
using LLMs. We assess the response evaluation module in a 2 step process. In
step 1, we assess LLMs as evaluators using three distinct prompting strategies.
In step 2, we apply the winning prompting strategy to compare the performance
of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that
GPT-4o-as-a-Judge is more consistent across all datasets. For the response
generation module, we use the identified LLM evaluator configuration and
compare different configurations of the LLM Feedback Loop. We use the win rate
to determine the best multi-agent configuration for generation. Experimenting
with various configurations, we find that the LLM Feedback Loop, with Llama as
the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win
rate over single-agent Llama and Gemma, respectively. After identifying the
best configurations for both modules, we generate our PO datasets using the
above pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-based Decision Criteria Are Suboptimal in In-<span class="highlight-title">context</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) typically utilizes classification criteria from
output probabilities of manually selected label tokens. However, we argue that
such token-based classification criteria lead to suboptimal decision
boundaries, despite delicate calibrations through translation and constrained
rotation applied. To address this problem, we propose Hidden Calibration, which
renounces token probabilities and uses the nearest centroid classifier on the
LM's last hidden states. In detail, we assign the label of the nearest centroid
previously estimated from a calibration set to the test sample as the predicted
label. Our experiments on 6 models and 10 classification datasets indicate that
Hidden Calibration consistently outperforms current token-based baselines by
about 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis
demonstrates that Hidden Calibration finds better classification criteria with
less inter-class overlap, and LMs provide linearly separable intra-class
clusters with the help of demonstrations, which supports Hidden Calibration and
gives new insights into the principle of ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 15 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the retention of sensitive or private information in large
language models is essential for enhancing privacy and safety. Existing
unlearning methods, like Gradient Ascent and Negative Preference Optimization,
directly tune models to remove unwanted information. However, these methods
often become unstable because they fine-tune by maximizing cross-entropy loss,
which is the opposite of traditional loss minimization in learning. This
reversal creates instability, especially on larger datasets, as the model
struggles to balance unlearning with maintaining language capacity, leading to
over-unlearning. In this paper, we introduce UnDIAL (Unlearning via
Self-Distillation on Adjusted Logits), a novel and robust unlearning method.
Our approach leverages self-distillation to adjust logits and selectively
reduce the influence of targeted tokens. This technique ensures smooth
convergence and avoids catastrophic forgetting, even in challenging unlearning
tasks with large datasets and sequential unlearning requests. Extensive
experiments show that UnDIAL can achieve both robustness in unlearning and
scalability while maintaining stable training dynamics and resilience to
hyperparameter tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Instruction Following: Evaluating Inferential Rule Following of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangtao Sun, Chenxiang Zhang, XueYou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated strong ability, they
are further supposed to be controlled and guided by in real-world scenarios to
be safe, accurate, and intelligent. This demands the possession of capability
of LLMs. However, no prior work has made a clear evaluation of the inferential
rule-following capability of LLMs. Previous studies that try to evaluate the
inferential rule-following capability of LLMs fail to distinguish the
inferential rule-following scenarios from the instruction-following scenarios.
Therefore, this paper first clarifies the concept of inferential rule-following
and proposes a comprehensive benchmark, RuleBench, to evaluate a diversified
range of inferential rule-following abilities. Our experimental results on a
variety of LLMs show that they are still limited in following rules. Our
analysis based on the evaluation results provides insights into the
improvements for LLMs toward a better inferential rule-following intelligent
agent. We further propose Inferential Rule-Following Tuning (IRFT). The
experimental results show that through IRFT, LLMs can learn abstract
rule-following abilities from purely synthetic data and then generalize to
RuleBench. The data and code can be found at:
https://anonymous.4open.science/r/llm-rule-following-B3E3/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VersiCode: Towards Version-controllable Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made tremendous strides in code generation,
but existing research fails to account for the dynamic nature of software
development, marked by frequent library updates. This gap significantly limits
LLMs' deployment in realistic settings. In this paper, we propose two novel
tasks aimed at bridging this gap: version-specific code completion (VSCC) and
version-aware code migration (VACM). In conjunction, we introduce VersiCode, a
comprehensive Python dataset specifically designed to evaluate LLMs on these
two tasks, together with a novel evaluation metric, Critical Diff Check
(CDC@1), which assesses code generation against evolving API requirements. We
conduct an extensive evaluation on VersiCode, which reveals that
version-controllable code generation is indeed a significant challenge, even
for GPT-4o and other strong frontier models. We believe the novel tasks,
dataset, and metric open up a new, important research direction that will
further enhance LLMs' real-world applicability. The code and resources can be
found at https://github.com/wutong8023/VersiCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic
  Evaluation Framework for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanying Wang, Zeyu Ma, Pengfei Liu, Mingang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While various vertical domain large language models (LLMs) have been
developed, the challenge of automatically evaluating their performance across
different domains remains significant. Current benchmark-based evaluation
methods exhibit rigid, aimless interactions and rely on pre-collected static
datasets that are costly to build, inflexible across domains, and misaligned
with practical user needs. To address this issue, we revisit the evaluation
components and introduce two concepts: Benchmark+, which extends traditional
question-answer benchmark into a more flexible "strategy-criterion" format; and
Assessment+, which enhances the interaction process, enabling deeper
exploration and supporting both quantitative metrics and qualitative insights.
These concepts capture the nuanced behaviors of LLMs through richer, multi-turn
interactions. We propose an agent-based evaluation framework called TestAgent,
which implements these concepts through retrieval augmented generation and
reinforcement learning. Experiments on tasks ranging from constructing vertical
domain evaluation to activating existing benchmarks demonstrate the
effectiveness of TestAgent across various scenarios. We believe this work
offers an interesting perspective on automatic evaluation for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoraMap: Harnessing the Power of LoRA Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeryun Park, Jeongwon Kwak, Dongsuk Jang, Sumin Park, Jinwook Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking techniques can mitigate hallucinations in Large Language Models
(LLMs), a prominent issue in specialized domains. As parameter-efficient
techniques such as Low-Rank Adaptation (LoRA) can overcome substantial
computational overhead, some studies have explored the integration of multiple
LoRAs. While previous studies focus on parallel integration, this paper
investigates methods to establish connections among multiple LoRAs. We create
three reasoning datasets tailored to fact-checking and fine-tune individual
LoRAs, allowing them to view and reason from diverse perspectives. Then, we
explore strategies for allocating these reasoning LoRAs and introduce LoraMap,
an approach to map connections between them. The results of the fact-checking
task demonstrate that the performance of LoraMap is superior to LoraHub, an
existing method for integrating LoRAs. LoraMap also outperforms with
significantly fewer trainable parameters than LoraConcat, which concatenates
LoRAs and further fine-tunes them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mental Disorders Detection in the Era of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Artem Shelmanov, Ivan Smirnov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper compares the effectiveness of traditional machine learning
methods, encoder-based models, and large language models (LLMs) on the task of
detecting depression and anxiety. Five datasets were considered, each differing
in format and the method used to define the target pathology class. We tested
AutoML models based on linguistic features, several variations of encoder-based
Transformers such as BERT, and state-of-the-art LLMs as pathology
classification models. The results demonstrated that LLMs outperform
traditional methods, particularly on noisy and small datasets where training
examples vary significantly in text length and genre. However, psycholinguistic
features and encoder-based models can achieve performance comparable to
language models when trained on texts from individuals with clinically
confirmed depression, highlighting their potential effectiveness in targeted
clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFC-Bench: Benchmarking Multimodal Fact-Checking with Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have significantly improved multimodal
reasoning tasks, such as visual question answering and image captioning. These
models embed multimodal facts within their parameters, rather than relying on
external knowledge bases to store factual information explicitly. However, the
content discerned by LVLMs may deviate from actual facts due to inherent bias
or incorrect inference. To address this issue, we introduce MFC-Bench, a
rigorous and comprehensive benchmark designed to evaluate the factual accuracy
of LVLMs across three stages of verdict prediction for MFC: Manipulation,
Out-of-Context, and Veracity Classification. Through our evaluation on
MFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering
that current models still fall short in multimodal fact-checking and
demonstrate insensitivity to various forms of manipulated content. We hope that
MFC-Bench could raise attention to the trustworthy AI potentially assisted by
LVLMs in the future. The MFC-Bench and accompanying resources are publicly
accessible at https://github.com/wskbest/MFC-Bench, contributing to ongoing
research in the multimodal fact-checking field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A corpus-based investigation of pitch contours of monosyllabic words in
  conversational Taiwan Mandarin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Jin, Mirjam Ernestus, R. Harald Baayen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Mandarin, the tonal contours of monosyllabic words produced in isolation
or in careful speech are characterized by four lexical tones: a high-level tone
(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,
in spontaneous speech, the actual tonal realization of monosyllabic words can
deviate significantly from these canonical tones due to intra-syllabic
co-articulation and inter-syllabic co-articulation with adjacent tones. In
addition, Chuang et al. (2024) recently reported that the tonal contours of
disyllabic Mandarin words with T2-T4 tone pattern are co-determined by their
meanings. Following up on their research, we present a corpus-based
investigation of how the pitch contours of monosyllabic words are realized in
spontaneous conversational Mandarin, focusing on the effects of contextual
predictors on the one hand, and the way in words' meanings co-determine pitch
contours on the other hand. We analyze the F0 contours of 3824 tokens of 63
different word types in a spontaneous Taiwan Mandarin corpus, using the
generalized additive (mixed) model to decompose a given observed pitch contour
into a set of component pitch contours. We show that the tonal context
substantially modify a word's canonical tone. Once the effect of tonal context
is controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a
high tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),
which in standard descriptions, is realized based on the preceding tone,
emerges as a low tone in its own right, modified by the other predictors in the
same way as the standard tones T1, T2, T3, and T4. We also show that word, and
even more so, word sense, co-determine words' F0 contours. Analyses of variable
importance using random forests further supported the substantial effect of
tonal context and an effect of word sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reverse Stable Diffusion: What <span class="highlight-title">prompt</span> was used to generate this image? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently attracted the interest of many
researchers, and inverting the diffusion process can play an important role in
better understanding the generative process and how to engineer prompts in
order to obtain the desired images. To this end, we study the task of
predicting the prompt embedding given an image generated by a generative
diffusion model. We consider a series of white-box and black-box models (with
and without access to the weights of the diffusion network) to deal with the
proposed task. We propose a novel learning framework comprising a joint prompt
regression and multi-label vocabulary classification objective that generates
improved prompts. To further improve our method, we employ a curriculum
learning procedure that promotes the learning of image-prompt pairs with lower
labeling noise (i.e. that are better aligned). We conduct experiments on the
DiffusionDB data set, predicting text prompts from images generated by Stable
Diffusion. In addition, we make an interesting discovery: training a diffusion
model on the prompt generation task can make the model generate images that are
much better aligned with the input prompts, when the model is directly reused
for text-to-image generation. Our code is publicly available for download at
https://github.com/CroitoruAlin/Reverse-Stable-Diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Vision and Image Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models in the Clinic: A Comprehensive Benchmark <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00716v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00716v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenglin Liu, Zheng Li, Hongjian Zhou, Qingyu Yin, Jingfeng Yang, Xianfeng Tang, Chen Luo, Ming Zeng, Haoming Jiang, Yifan Gao, Priyanka Nigam, Sreyashi Nag, Bing Yin, Yining Hua, Xuan Zhou, Omid Rohanian, Anshul Thakur, Lei Clifton, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of large language models (LLMs) to assist clinicians has
attracted remarkable attention. Existing works mainly adopt the close-ended
question-answering (QA) task with answer options for evaluation. However, many
clinical decisions involve answering open-ended questions without pre-set
options. To better understand LLMs in the clinic, we construct a benchmark
ClinicBench. We first collect eleven existing datasets covering diverse
clinical language generation, understanding, and reasoning tasks. Furthermore,
we construct six novel datasets and clinical tasks that are complex but common
in real-world practice, e.g., open-ended decision-making, long document
processing, and emerging drug analysis. We conduct an extensive evaluation of
twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite
medical experts to evaluate the clinical usefulness of LLMs. The benchmark data
is available at https://github.com/AI-in-Health/ClinicBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C<span class="highlight-title">Long</span>Eval: A Chinese Benchmark for Evaluating <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span> Large
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing Large Language Models (LLMs) with robust long-context capabilities
has been the recent research focus, resulting in the emergence of long-context
LLMs proficient in Chinese. However, the evaluation of these models remains
underdeveloped due to a lack of benchmarks. To address this gap, we present
CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.
CLongEval is characterized by three key features: (1) Sufficient data volume,
comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,
accommodating to models with context windows size from 1K to 100K; (3) High
quality, with over 2,000 manually annotated question-answer pairs in addition
to the automatically constructed labels. With CLongEval, we undertake a
comprehensive assessment of 6 open-source long-context LLMs and 2 leading
commercial counterparts that feature both long-context abilities and
proficiency in Chinese. We also provide in-depth analysis based on the
empirical results, trying to shed light on the critical capabilities that
present challenges in long-context settings. The dataset, evaluation scripts,
and model outputs are released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaCo: Targeted Concept Erasure Prevents Non-Linear Classifiers From
  Detecting Protected Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06499v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06499v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanny Jourdan, Louis Béthune, Agustin Picard, Laurent Risser, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring fairness in NLP models is crucial, as they often encode sensitive
attributes like gender and ethnicity, leading to biased outcomes. Current
concept erasure methods attempt to mitigate this by modifying final latent
representations to remove sensitive information without retraining the entire
model. However, these methods typically rely on linear classifiers, which leave
models vulnerable to non-linear adversaries capable of recovering sensitive
information.
  We introduce Targeted Concept Erasure (TaCo), a novel approach that removes
sensitive information from final latent representations, ensuring fairness even
against non-linear classifiers. Our experiments show that TaCo outperforms
state-of-the-art methods, achieving greater reductions in the prediction
accuracy of sensitive attributes by non-linear classifier while preserving
overall task performance. Code is available on
https://github.com/fanny-jourdan/TaCo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Inference with Large Language Model: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference has been a pivotal challenge across diverse domains such as
medicine and economics, demanding a complicated integration of human knowledge,
mathematical reasoning, and data mining capabilities. Recent advancements in
natural language processing (NLP), particularly with the advent of large
language models (LLMs), have introduced promising opportunities for traditional
causal inference tasks. This paper reviews recent progress in applying LLMs to
causal inference, encompassing various tasks spanning different levels of
causation. We summarize the main causal problems and approaches, and present a
comparison of their evaluation results in different causal scenarios.
Furthermore, we discuss key findings and outline directions for future
research, underscoring the potential implications of integrating LLMs in
advancing causal inference methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explore, Select, Derive, and Recall: Augmenting LLM with Human-like
  Memory for Mobile Task Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y. Ko, Sangeun Oh, Insik Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has opened up new opportunities in
the field of mobile task automation. Their superior language understanding and
reasoning capabilities allow users to automate complex and repetitive tasks.
However, due to the inherent unreliability and high operational cost of LLMs,
their practical applicability is quite limited. To address these issues, this
paper introduces MobileGPT, an innovative LLM-based mobile task automator
equipped with a human-like app memory. MobileGPT emulates the cognitive process
of humans interacting with a mobile app -- explore, select, derive, and recall.
This approach allows for a more precise and efficient learning of a task's
procedure by breaking it down into smaller, modular sub-tasks that can be
re-used, re-arranged, and adapted for various objectives. We implement
MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its
performance on a dataset of 185 tasks across 18 mobile apps. The results
indicate that MobileGPT can automate and learn new tasks with 82.7% accuracy,
and is able to adapt them to different contexts with near perfect (98.75%)
accuracy while reducing both latency and cost by 62.5% and 68.8%, respectively,
compared to the GPT-4 powered baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconsidering Degeneration of Token Embeddings with Definitions for
  Encoder-based <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zhang, Dongyuan Li, Manabu Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning token embeddings based on token co-occurrence statistics has proven
effective for both pre-training and fine-tuning in natural language processing.
However, recent studies have pointed out that the distribution of learned
embeddings degenerates into anisotropy (i.e., non-uniform distribution), and
even pre-trained language models (PLMs) suffer from a loss of semantics-related
information in embeddings for low-frequency tokens. This study first analyzes
the fine-tuning dynamics of encoder-based PLMs and demonstrates their
robustness against degeneration. On the basis of this analysis, we propose
DefinitionEMB, a method that utilizes definitions to re-construct isotropically
distributed and semantics-related token embeddings for encoder-based PLMs while
maintaining original robustness during fine-tuning. Our experiments demonstrate
the effectiveness of leveraging definitions from Wiktionary to re-construct
such embeddings for two encoder-based PLMs: RoBERTa-base and BART-large.
Furthermore, the re-constructed embeddings for low-frequency tokens improve the
performance of these models across various GLUE and four text summarization
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in
  Multi-Agent Settings with Social Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Model (LLM)-based agents become increasingly autonomous and
will more freely interact with each other, studying interactions between them
becomes crucial to anticipate emergent phenomena and potential risks. Drawing
inspiration from the widely popular Stanford Prison Experiment, we contribute
to this line of research by studying interaction patterns of LLM agents in a
context characterized by strict social hierarchy. We do so by specifically
studying two types of phenomena: persuasion and anti-social behavior in
simulated scenarios involving a guard and a prisoner agent who seeks to achieve
a specific goal (i.e., obtaining additional yard time or escape from prison).
Leveraging 200 experimental scenarios for a total of 2,000 machine-machine
conversations across five different popular LLMs, we provide a set of
noteworthy findings. We first document how some models consistently fail in
carrying out a conversation in our multi-agent setup where power dynamics are
at play. Then, for the models that were able to engage in successful
interactions, we empirically show how the goal that an agent is set to achieve
impacts primarily its persuasiveness, while having a negligible effect with
respect to the agent's anti-social behavior. Third, we highlight how agents'
personas, and particularly the guard's personality, drive both the likelihood
of successful persuasion from the prisoner and the emergence of anti-social
behaviors. Fourth, we show that even without explicitly prompting for specific
personalities, anti-social behavior emerges by simply assigning agents' roles.
These results bear implications for the development of interactive LLM agents
as well as the debate on their societal impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Do Humans Write Code? Large Models Do It the Same Way Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Li, Xuzheng He, Haozhe Wang, Linlin Wang, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought
(CoT) as the most popular method in Large Language Models (LLMs) mathematical
reasoning tasks by utilizing external tool calls to circumvent computational
errors. However, our evaluation of the GPT-4 and Llama series reveals that
using PoT introduces more reasoning errors, such as incorrect formulas or
flawed logic, compared to CoT. To address this issue, we propose Human-Think
Language (HTL), which leverages a suite of strategies that help integrate PoT
and CoT, encompassing: (1) a new generation paradigm that uses full CoT
reasoning to control code generation. (2) Focus Attention, that directs model
attention to the CoT reasoning during PoT to generate more logical code. (3)
reinforcement learning that utilizes the accuracy of both CoT and PoT responses
as rewards to prevent repetitive reasoning steps in LLMs when solving difficult
math problems. Our method achieves an average improvement of 6.5% on the
Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical
calculation datasets. It also shows significant effectiveness on five
out-of-domain datasets by controlling the model's information flow, exhibiting
strong transferability. Additionally, HTL shows the most significant
improvement in non-mathematical natural language inference task, contributing
to a unified reasoning task framework
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring and Benchmarking Large Language Models' Capabilities to
  Generate Persuasive Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are exposed to much information trying to influence us, such as teaser
messages, debates, politically framed news, and propaganda - all of which use
persuasive language. With the recent interest in Large Language Models (LLMs),
we study the ability of LLMs to produce persuasive text. As opposed to prior
work which focuses on particular domains or types of persuasion, we conduct a
general study across various domains to measure and benchmark to what degree
LLMs produce persuasive language - both when explicitly instructed to rewrite
text to be more or less persuasive and when only instructed to paraphrase. We
construct the new dataset Persuasive-Pairs of pairs of a short text and its
rewrite by an LLM to amplify or diminish persuasive language. We multi-annotate
the pairs on a relative scale for persuasive language: a valuable resource in
itself, and for training a regression model to score and benchmark persuasive
language, including for new LLMs across domains. In our analysis, we find that
different 'personas' in LLaMA3's system prompt change persuasive language
substantially, even when only instructed to paraphrase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deciphering Cross-Modal Alignment in Large Vision-Language Models with
  Modality Integration Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/shikiw/Modality-Integration-Rate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Changes in Nation Perception with Nationality-Assigned
  Personas in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahammed Kamruzzaman, Gene Louis Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persona assignment has become a common strategy for customizing LLM use to
particular tasks and contexts. In this study, we explore how evaluation of
different nations change when LLMs are assigned specific nationality personas.
We assign 193 different nationality personas (e.g., an American person) to four
LLMs and examine how the LLM evaluations (or ''perceptions'')of countries
change. We find that all LLM-persona combinations tend to favor Western
European nations, though nation-personas push LLM behaviors to focus more on
and treat the nation-persona's own region more favorably. Eastern European,
Latin American, and African nations are treated more negatively by different
nationality personas. We additionally find that evaluations by nation-persona
LLMs of other nations correlate with human survey responses but fail to match
the values closely. Our study provides insight into how biases and stereotypes
are realized within LLMs when adopting different national personas. In line
with the ''Blueprint for an AI Bill of Rights'', our findings underscore the
critical need for developing mechanisms to ensure that LLM outputs promote
fairness and avoid over-generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptation Odyssey in LLMs: Why Does Additional <span class="highlight-title">Pretrain</span>ing Sometimes
  Fail to Improve? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fırat Öncel, Matthias Bethge, Beyza Ermis, Mirco Ravanelli, Cem Subakan, Çağatay Yıldız
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, the generalization and adaptation abilities of deep
learning models were typically evaluated on fixed training and test
distributions. Contrary to traditional deep learning, large language models
(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text
corpora curated from the Internet with minimal human intervention, and (iii)
trained in an online fashion. These stark contrasts prevent researchers from
transferring lessons learned on model generalization and adaptation in deep
learning contexts to LLMs. To this end, our short paper introduces empirical
observations that aim to shed light on further training of already pretrained
language models. Specifically, we demonstrate that training a model on a text
domain could degrade its perplexity on the test portion of the same domain. We
observe with our subsequent analysis that the performance degradation is
positively correlated with the similarity between the additional and the
original pretraining dataset of the LLM. Our further token-level perplexity
observations reveals that the perplexity degradation is due to a handful of
tokens that are not informative about the domain. We hope these findings will
guide us in determining when to adapt a model vs when to rely on its
foundational capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Infinite-<span class="highlight-title">Long</span> Prefix in <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyu Liang, Zhenmei Shi, Zhao Song, Chiwun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting and context-based fine-tuning methods, which we call Prefix
Learning, have been proposed to enhance the performance of language models on
various downstream tasks. They are empirically efficient and effective,
matching the performance of full parameter fine-tuning, but the theoretical
understandings are limited. In this paper, we aim to address this limitation by
studying their ability from the perspective of prefix length. In particular, we
provide a convergence guarantee for training an ultra-long prefix in a stylized
setting using the Neural Tangent Kernel (NTK) framework. Based on this strong
theoretical guarantee, we design and implement an algorithm that only needs to
introduce and fine-tune a few extra trainable parameters instead of an
infinite-long prefix in each layer of a transformer, and can approximate the
prefix attention to a guaranteed polynomial-small error. Preliminary
experimental results on vision, natural language, and math data show that our
method achieves superior or competitive performance compared to existing
methods like full parameters fine-tuning, P-Tuning V2, and LoRA. This
demonstrates our method is promising for parameter-efficient fine-tuning. Our
code can be found at
\url{https://github.com/ChristianYang37/chiwun/tree/main/src/NTK-Attention}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented
  Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou, Jingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown great potential in the biomedical
domain with the advancement of retrieval-augmented generation (RAG). However,
existing retrieval-augmented approaches face challenges in addressing diverse
queries and documents, particularly for medical knowledge queries, resulting in
sub-optimal performance. To address these limitations, we propose a novel
plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search
(SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm.
By combining the reasoning capabilities of LLMs with the effectiveness of tree
search, SeRTS boosts the zero-shot performance of retrieving high-quality and
informative results for RAG. We further enhance retrieval performance by
fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the
trajectories collected by SeRTS as feedback. Controlled experiments using the
BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method
significantly improves the performance of the BM25 retriever and surpasses the
strong baseline of self-reflection in both efficiency and scalability.
Moreover, SeRTS generates higher-quality feedback for PPO training than
self-reflection. Our proposed method effectively adapts LLMs to document
retrieval tasks, enhancing their ability to retrieve highly relevant documents
for RAG in the context of medical knowledge queries. This work presents a
significant step forward in leveraging LLMs for accurate and comprehensive
biomedical question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Assamese NLP Capabilities: Introducing a Centralized <span class="highlight-title">Dataset</span>
  Repository 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Tamang, D. J. Bora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a centralized, open-source dataset repository designed
to advance NLP and NMT for Assamese, a low-resource language. The repository,
available at GitHub, supports various tasks like sentiment analysis, named
entity recognition, and machine translation by providing both pre-training and
fine-tuning corpora. We review existing datasets, highlighting the need for
standardized resources in Assamese NLP, and discuss potential applications in
AI-driven research, such as LLMs, OCR, and chatbots. While promising,
challenges like data scarcity and linguistic diversity remain. The repository
aims to foster collaboration and innovation, promoting Assamese language
research in the digital age.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 table, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERLIN: Multimodal Embedding Refinement via LLM-based Iterative
  Navigation for Text-Video Retrieval-Rerank Pipeline <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Han, Eunhwan Park, Gisang Lee, Adam Lee, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of multimedia content has made accurately retrieving
relevant videos from large collections increasingly challenging. Recent
advancements in text-video retrieval have focused on cross-modal interactions,
large-scale foundation model training, and probabilistic modeling, yet often
neglect the crucial user perspective, leading to discrepancies between user
queries and the content retrieved. To address this, we introduce MERLIN
(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,
training-free pipeline that leverages Large Language Models (LLMs) for
iterative feedback learning. MERLIN refines query embeddings from a user
perspective, enhancing alignment between queries and video content through a
dynamic question answering process. Experimental results on datasets like
MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves
Recall@1, outperforming existing systems and confirming the benefits of
integrating LLMs into multimodal retrieval systems for more responsive and
context-aware multimedia retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Industry Track Accepted (Camera-Ready Version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAD: Personalized Alignment at Decoding-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning with personalized preferences, which vary significantly across
cultural, educational, and political differences, poses a significant challenge
due to the computational costs and data demands of traditional alignment
methods. In response, this paper presents Personalized Alignment at
Decoding-time (PAD), a novel framework designed to align LLM outputs with
diverse personalized preferences during the inference phase, eliminating the
need for additional training. By introducing a unique personalized reward
modeling strategy, this framework decouples the text generation process from
personalized preferences, facilitating the generation of generalizable
token-level personalized rewards. The PAD algorithm leverages these rewards to
guide the decoding process, dynamically tailoring the base model's predictions
to personalized preferences. Extensive experimental results demonstrate that
PAD not only outperforms existing training-based alignment methods in terms of
aligning with diverse preferences but also shows significant generalizability
to preferences unseen during training and scalability across different base
models. This work advances the capability of LLMs to meet user needs in
real-time applications, presenting a substantial step forward in personalized
LLM alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper presents Personalized Alignment at Decoding-time (PAD), a
  novel framework designed to align LLM outputs with diverse personalized
  preferences during the inference phase</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $α$-DPO: Adaptive Reward Margin is What Direct Preference
  Optimization Needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkang Wu, Xue Wang, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, Rong Jin, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human values and intentions is
crucial for their utility, honesty, and safety. Reinforcement learning from
human feedback (RLHF) is a popular approach to achieve this alignment, but it
faces challenges in computational efficiency and training stability. Recent
methods like Direct Preference Optimization (DPO) and Simple Preference
Optimization (SimPO) have proposed offline alternatives to RLHF, simplifying
the process by reparameterizing the reward function. However, DPO depends on a
potentially suboptimal reference model, and SimPO's assumption of a fixed
target reward margin may lead to suboptimal decisions in diverse data settings.
In this work, we propose $\alpha$-DPO, an adaptive preference optimization
algorithm designed to address these limitations by introducing a dynamic reward
margin. Specifically, $\alpha$-DPO employs an adaptive preference distribution,
balancing the policy model and the reference model to achieve personalized
reward margins. We provide theoretical guarantees for $\alpha$-DPO,
demonstrating its effectiveness as a surrogate optimization objective and its
ability to balance alignment and diversity through KL divergence control.
Empirical evaluations on AlpacaEval 2 and Arena-Hard show that $\alpha$-DPO
consistently outperforms DPO and SimPO across various model settings,
establishing it as a robust approach for fine-tuning LLMs. Our method achieves
significant improvements in win rates, highlighting its potential as a powerful
tool for LLM alignment. The code is available at
https://github.com/junkangwu/alpha-DPO
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction Tuning for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10792v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10792v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V3; Last update: Oct 16, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding
  for Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxuan Lyu, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maximum a posteriori decoding, a commonly used method for neural machine
translation (NMT), aims to maximize the estimated posterior probability.
However, high estimated probability does not always lead to high translation
quality. Minimum Bayes Risk (MBR) decoding (\citealp{kumar2004minimum}) offers
an alternative by seeking hypotheses with the highest expected utility. In this
paper, we show that Quality Estimation (QE) reranking
(\citealp{fernandes-etal-2022-quality}), which uses a QE model as a reranker,
can be viewed as a variant of MBR. Inspired by this, we propose source-based
MBR (sMBR) decoding, a novel approach that utilizes synthetic sources
(generated via back-translation or paraphrasing) as ``support hypotheses'' and
a reference-free quality estimation metric as the utility function, marking the
first work to solely use sources in MBR decoding. Experiments show that sMBR
outperforms QE reranking and the standard MBR decoding. Our findings suggest
that sMBR is a promising approach for NMT decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent
  Approach <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanming Zhang, Yuxuan Chen, Yuan Yuan, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Types, and Distorted Handling Solutions. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a multi
agent framework inspired by expert developer strategies for exception handling.
Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist
LLMs in detecting, capturing, and resolving exceptions more effectively. Our
work is the first systematic study on leveraging LLMs to enhance exception
handling practices, providing valuable insights for future improvements in code
reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 figures. Submitted ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Transferability of Code Repair for Low-Resource
  Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Wong, Alfonso Amayuelas, Liangming Pan, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance on code
generation tasks. A recent use case is iterative code repair, where an LLM
fixes an incorrect program by rationalizing about errors and generating new
code. Recent works augment the code repair process by integrating modern
techniques such as chain-of-thought reasoning or distillation, but only study
their benefits on high-resource languages like Python, and ignore low-resource
languages like Perl. To address this gap of knowledge, we investigate the
benefits of distilling code repair for both high and low resource languages to
determine if the techniques that are effective in a high resource setting are
also applicable in a low resource setting. Our evaluation shows that distilling
the ability to repair code has language dependent benefits. To explain this
behavior, we perform a further analysis and find that contrary to preexisting
beliefs, the correlation between reasoning ability and code correction ability
is weak. We hypothesize this weak correlation is magnified in low-resource
settings where base models lack deep knowledge of a programming language,
leading to wavering benefits of code repair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield
  Better Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback significantly enhances Natural
Language Processing by aligning language models with human expectations. A
critical factor in this alignment is the strength of reward models used during
training. This study explores whether stronger reward models invariably lead to
better language models. In this paper, through experiments on relevance,
factuality, and completeness tasks using the QA-FEEDBACK dataset and reward
models based on Longformer, we uncover a surprising paradox: language models
trained with moderately accurate reward models outperform those guided by
highly accurate ones. This challenges the widely held belief that stronger
reward models always lead to better language models, and opens up new avenues
for future research into the key factors driving model performance and how to
choose the most suitable reward models. Code and additional details are
available at https://github.com/EIT-NLP/AccuracyParadox-RLHF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 27 figures (including 18 in the appendix), submitted to
  EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human
  Belief Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Shiuan Chuang, Krirk Nirunwiroj, Zach Studdiford, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating human-like large language model (LLM) agents is crucial for faithful
social simulation. Having LLMs role-play based on demographic information
sometimes improves human likeness but often does not. This study assessed
whether LLM alignment with human behavior can be improved by integrating
information from empirically-derived human belief networks. Using data from a
human survey, we estimated a belief network encompassing 64 topics loading on
nine non-overlapping latent factors. We then seeded LLM-based agents with an
opinion on one topic, and assessed the alignment of its expressed opinions on
remaining test topics with corresponding human data. Role-playing based on
demographic information alone did not align LLM and human opinions, but seeding
the agent with a single belief greatly improved alignment for topics related in
the belief network, and not for topics outside the network. These results
suggest a novel path for human-LLM belief alignment in work seeking to simulate
and understand patterns of belief distributions in society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Language Barriers in Multilingual Mathematical Reasoning:
  Insights and Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20246v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20246v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset. Codes are available at
https://github.com/microsoft/MathOctopus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Natural Language Processing for Corporate Sustainability
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keane Ong, Rui Mao, Ranjan Satapathy, Ricardo Shirota Filho, Erik Cambria, Johan Sulaeman, Gianmarco Mengaldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sustainability commonly refers to entities, such as individuals, companies,
and institutions, having a non-detrimental (or even positive) impact on the
environment, society, and the economy. With sustainability becoming a synonym
of acceptable and legitimate behaviour, it is being increasingly demanded and
regulated. Several frameworks and standards have been proposed to measure the
sustainability impact of corporations, including United Nations' sustainable
development goals and the recently introduced global sustainability reporting
framework, amongst others. However, the concept of corporate sustainability is
complex due to the diverse and intricate nature of firm operations (i.e.
geography, size, business activities, interlinks with other stakeholders). As a
result, corporate sustainability assessments are plagued by subjectivity both
within data that reflect corporate sustainability efforts (i.e. corporate
sustainability disclosures) and the analysts evaluating them. This subjectivity
can be distilled into distinct challenges, such as incompleteness, ambiguity,
unreliability and sophistication on the data dimension, as well as limited
resources and potential bias on the analyst dimension. Put together,
subjectivity hinders effective cost attribution to entities non-compliant with
prevailing sustainability expectations, potentially rendering sustainability
efforts and its associated regulations futile. To this end, we argue that
Explainable Natural Language Processing (XNLP) can significantly enhance
corporate sustainability analysis. Specifically, linguistic understanding
algorithms (lexical, semantic, syntactic), integrated with XAI capabilities
(interpretability, explainability, faithfulness), can bridge gaps in analyst
resources and mitigate subjectivity problems within data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JOOCI: a Framework for Learning Comprehensive Speech Representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemant Yadav, Rajiv Ratn Shah, Sunayana Sitaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information in speech can be divided into two categories: what is being said
(content) and how it is expressed (other). Current state-of-the-art (SOTA)
techniques model speech at fixed segments, usually 10-25 ms, using a single
embedding. Given the orthogonal nature of other and content information,
attempting to optimize both within a single embedding results in suboptimal
solutions. This approach divides the models capacity, limiting its ability to
build complex hierarchical features effectively. In this work, we present an
end-to-end speech representation learning framework designed to jointly
optimize the other and content information (JOOCI) in speech. By using separate
learnable parameters, JOOCI addresses this optimization challenge by modeling
other and content information independently. Our results show that JOOCI
consistently outperforms other SOTA models of similar size (100 million
parameters) and pre-training data used (960 hours) by a significant margin when
evaluated on a range of speech downstream tasks in the SUPERB benchmark, as
shown in Table 1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Translation Canvas: An Explainable Interface to Pinpoint and Analyze
  Translation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Dandekar, Wenda Xu, Xi Xu, Siqi Ouyang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of machine translation research, evaluation
toolkits have become essential for benchmarking system progress. Tools like
COMET and SacreBLEU offer single quality score assessments that are effective
for pairwise system comparisons. However, these tools provide limited insights
for fine-grained system-level comparisons and the analysis of instance-level
defects. To address these limitations, we introduce Translation Canvas, an
explainable interface designed to pinpoint and analyze translation systems'
performance: 1) Translation Canvas assists machine translation researchers in
comprehending system-level model performance by identifying common errors
(their frequency and severity) and analyzing relationships between different
systems based on various evaluation metrics. 2) It supports fine-grained
analysis by highlighting error spans with explanations and selectively
displaying systems' predictions. According to human evaluation, Translation
Canvas demonstrates superior performance over COMET and SacreBLEU packages
under enjoyability and understandability criteria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Elementary Discourse Units in Textual Data Using Canonical
  Correlation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akanksha Mehndiratta, Krishna Asawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canonical Correlation Analysis (CCA) has been exploited immensely for
learning latent representations in various fields. This study takes a step
further by demonstrating the potential of CCA in identifying Elementary
Discourse Units(EDUs) that captures the latent information within the textual
data. The probabilistic interpretation of CCA discussed in this study utilizes
the two-view nature of textual data, i.e. the consecutive sentences in a
document or turns in a dyadic conversation, and has a strong theoretical
foundation. Furthermore, this study proposes a model for Elementary Discourse
Unit(EDU) segmentation that discovers EDUs in textual data without any
supervision. To validate the model, the EDUs are utilized as textual unit for
content selection in textual similarity task. Empirical results on Semantic
Textual Similarity(STSB) and Mohler datasets confirm that, despite represented
as a unigram, the EDUs deliver competitive results and can even beat various
sophisticated supervised techniques. The model is simple, linear, adaptable and
language independent making it an ideal baseline particularly when labeled
training data is scarce or nonexistent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Circuits in <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, have allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuits hold
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, 32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining <span class="highlight-title">Long</span>-<span class="highlight-title">Context</span> Large Language Models for Environmental <span class="highlight-title">Review</span>
  Document Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Phan, Anurag Acharya, Rounak Meyur, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs become increasingly ubiquitous, researchers have tried various
techniques to augment the knowledge provided to these models. Long context and
retrieval-augmented generation (RAG) are two such methods that have recently
gained popularity. In this work, we examine the benefits of both of these
techniques by utilizing question answering (QA) task in a niche domain. While
the effectiveness of LLM-based QA systems has already been established at an
acceptable level in popular domains such as trivia and literature, it has not
often been established in niche domains that traditionally require specialized
expertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance
of five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and
Mistral -- when answering questions originating from Environmental Impact
Statements prepared by U.S. federal government agencies in accordance with the
National Environmental Environmental Act (NEPA). We specifically measure the
ability of LLMs to understand the nuances of legal, technical, and
compliance-related information present in NEPA documents in different
contextual scenarios. We test the LLMs' internal prior NEPA knowledge by
providing questions without any context, as well as assess how LLMs synthesize
the contextual information present in long NEPA documents to facilitate the
question/answering task. We compare the performance of the models in handling
different types of questions (e.g., problem-solving, divergent, etc.). Our
results suggest that RAG powered models significantly outperform those provided
with only the PDF context in terms of answer accuracy, regardless of the choice
of the LLM. Our further analysis reveals that many models perform better
answering closed type questions (Yes/No) than divergent and problem-solving
questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-10-24T10:30:05.690564252Z">
            2024-10-24 10:30:05 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
